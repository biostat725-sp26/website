---
title: "Hierarchical Models"
author: "Prof. Sam Berchuck"
date: "2026-02-24"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— BIOSTAT 725 - Spring 2026](https://biostat725-sp26.netlify.app/)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    html-math-method: mathjax
filters:
  - parse-latex
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(knitr)
library(mvtnorm)
library(coda)
library(lars)
library(LaplacesDemon)
library(nimble)
library(mice)
library(bayesplot)
```

## Review of last lecture

-   Last week, we learned about classification for binary and multiclass problems.

-   **Moving forward:** Up until today, we have dealt with independent data. Today, we will look at our first example of dependent data!

## Linear regression assumptions

\begin{align*}
Y_i &= \alpha + \mathbf{x}_i \boldsymbol{\beta} + \epsilon_i,\quad \epsilon_i \sim N(0,\sigma^2)\\
&= \mu_i + \epsilon_i.
\end{align*}

Assumptions:

1.  $Y_i$ are independent observations (independence).

2.  $Y_i$ is linearly related to $\mathbf{x}_i$ (linearity).

3.  $\epsilon_i = Y_i - \mu_i$ is normally distributed (normality).

4.  $\epsilon_i$ has constant variance across $\mathbf{x}_i$ (homoskedasticity).

## Independence Assumption in Linear Regression

We assume that the residuals $\epsilon_i$ are independent:

$$\mathbb{C}(\epsilon_i, \epsilon_j) = 0, \quad \text{for} \quad i \neq j,$$ where $\mathbb{C}(X, Y)$ is the covariance between two random variables $X$ and $Y$. As a note: $\mathbb{C}(X, X) = \mathbb{V}(X)$.

-   This implies that the observations $Y_i$ and $Y_j$ are independent, and their correlation is zero.

    -   Correlation: $\rho(X,Y) = \frac{\mathbb{C}(X, Y)}{\sqrt{\mathbb{V}(X)\mathbb{V}(Y)}}$.

## Real-World: Dependent Observations

However, in real-world data, the independence assumption often does not hold:

-   **Repeated measures data** (e.g., same individual over time).

-   **Clustered data** (e.g., patients within a hospital).

-   **Longitudinal data** (e.g., disease severity measures over time).

-   **Spatial data** (e.g., disease counts observed across zip codes).

## The Challenge

-   If we assume independence in the presence of correlation:

    1.  **Biased parameter estimates**: Parameter estimation will be biased due to group-level dependencies that effect the outcome.

    2.  **Underestimated uncertainty**: The model will not account for the true variability, leading to narrower confidence intervals.

    3.  **Inaccurate Predictions**: Predictions for new groups may be biased because the model doesn't properly account for group-level variability.

-   Thus, we need a way to account for dependencies between observations, especially when data are grouped or clustered.

## Example of Hierarchical Data

-   Hierarchical data refers to data that is organized into groups or clusters, where each group contains multiple observations.

-   Consider data from patients within hospitals. Each patient is being treated in a hospital, with multiple patients belonging to each hospital.

-   In this case, the observation for a patient is indexed by two variables:

    -   $i$: hospital index.

    -   $j$: patient index, nested within hospital.

-   So, for patient $j$ within hospital $i$, we write the response as $Y_{ij}$.

## Observations with Two Indices: $Y_{ij}$

-   $Y_{ij}$ represents the response for patient $j$ in hospital $i$.

-   The first index $i$ represents group-level effects (e.g., hospital-level).

-   The second index $j$ represents individual-level observations (e.g., patient).

-   We typically say that $i = 1,\ldots,n.$ and $j = 1,\ldots,n_i$.

-   The total number of observations is $N = \sum_{i = 1}^{n}n_i$.

## Why Two Indices?

Having two indices allows us to model both:

-   **Within-group variation** (differences between patients within the same hospital).

-   **Between-group variation** (differences between hospitals).

The hierarchical structure captures both types of variation.

## Conceptualizing Hierarchical Data

Consider the example of patients within hospitals:

-   Each data point (i.e., observed data $Y_{ij}$) represents an outcome measured on a patient.

-   The data points are grouped by hospital, indicating that patients from the same hospital are likely to have similar outcomes due to shared hospital-level factors.

This is a typical example of hierarchical data.

## Hierarchical Model

Now, we can see how hierarchical data appears in the model:

$$Y_{ij} = \alpha + \mathbf{x}_{ij} \boldsymbol{\beta} + \theta_i + \epsilon_{ij},\quad \epsilon_{ij} \stackrel{iid}{\sim}N(0,\sigma^2).$$

**Subject-specific objects:**

-   $Y_{ij}$: response for patient $j$ in hospital $i$.

-   $\mathbf{x}_{ij}$ are the predictors for patient $j$ in hospital $i$.

-   $\epsilon_{ij}$: residual error for patient $j$ in hospital $i$.

## Hierarchical Model

Now, we can see how hierarchical data appears in the model:

$$Y_{ij} = \alpha + \mathbf{x}_{ij} \boldsymbol{\beta} + \theta_i + \epsilon_{ij},\quad \epsilon_{ij} \stackrel{iid}{\sim}N(0,\sigma^2).$$

**Group-specific objects:**

-   $\theta_i$: group-specific parameter for hospital $i$, accounting for hospital-level variation (group-specific, random effect).

The group-specific parameters are responsible for inducing correlation into the model.

## Hierarchical Model

Now, we can see how hierarchical data appears in the model:

$$Y_{ij} = \alpha + \mathbf{x}_{ij} \boldsymbol{\beta} + \theta_i + \epsilon_{ij},\quad \epsilon_{ij} \stackrel{iid}{\sim}N(0,\sigma^2).$$

**Population parameters:**

-   $\alpha$: intercept for the entire population.

-   $\boldsymbol{\beta}$: regression parameters for the entire population.

-   $\sigma$: residual error parameter for the entire population.

## Random Intercept Model

$$Y_{ij} = \alpha + \mathbf{x}_{ij} \boldsymbol{\beta} + \theta_i + \epsilon_{ij},\quad \epsilon_{ij} \stackrel{iid}{\sim}N(0,\sigma^2).$$

-   From a frequentist perspective, this model may be called a **random intercept** model, but in the Bayesian framework all parameters are random variables, so the terms fixed and random effects don't apply.

    -   $\theta_i$: group-specific parameters (random effect).

    -   $\alpha, \boldsymbol{\beta}, \sigma$: population parameters (common across all groups, $\boldsymbol{\beta}$ are the fixed effects).

## Prior for $\theta_i$ {.midi}

We model $\theta_i$ as a parameter drawn from a **normal distribution** centered at zero, with some variance $\tau^2$:

$$\theta_i \stackrel{iid}{\sim} N(0, \tau^2).$$

-   **Mean at zero**: This assumption reflects that, on average, hospitals don't deviate from the population mean (helps with identifiability).

-   **Variance** $\tau^2$: This represents the variability in hospital-level intercepts. A larger $\tau^2$ implies greater variability between hospitals.

Each hospital $i$ has a **hospital-specific parameter** $\theta_i$, which represents how that hospital's baseline (e.g., health outcomes) deviates from the population average.

## Group-Specific Intercept Model: Conditional Specification

For $i = 1,\ldots,n$ and $j = 1,\ldots,n_i$, \begin{align*}
Y_{ij} | \boldsymbol{\Omega},\theta_i &\stackrel{ind}{\sim} N(\alpha + \mathbf{x}_{ij}\boldsymbol{\beta} + \theta_i,\sigma^2)\\
\theta_i | \tau^2 &\stackrel{iid}{\sim} N(0,\tau^2)\\
\boldsymbol{\Omega} &\sim f(\boldsymbol{\Omega}),
\end{align*} where $\boldsymbol{\Omega} = (\alpha, \boldsymbol{\beta},\sigma,\tau)$ are the population parameters.

## Group-Specific Intercept Model: Conditional Specification

-   Moments for the Conditional Model:

\begin{align*}
\mathbb{E}[Y_{ij} | \boldsymbol{\Omega},\theta_i] &= \alpha + \mathbf{x}_{ij}\boldsymbol{\beta} + \theta_i\\
\mathbb{V}(Y_{ij} | \boldsymbol{\Omega},\theta_i) &= \sigma^2\\
\mathbb{C}(Y_{ij}, Y_{lk} | \boldsymbol{\Omega},\theta_i,\theta_l) &= 0,\quad \forall i,j,l,k.
\end{align*}

## Understanding the Random Intercept

-   $\theta_i$: group-specific parameter captures group-level differences (e.g., hospital level).

-   The intercept $\theta_i$ allows for each group to have its own baseline value.

-   This model introduces **dependence** within groups because observations from the same group share the same intercept $\theta_i$.

\begin{align*}
\mathbb{E}[Y_{ij} | \boldsymbol{\Omega},\theta_i] &= \alpha + \mathbf{x}_{ij} \boldsymbol{\beta} + \theta_i\\
&= (\alpha + \theta_i) + \mathbf{x}_{ij} \boldsymbol{\beta}\\
&= \alpha_i + \mathbf{x}_{ij} \boldsymbol{\beta}.
\end{align*}

## Identifiability Issues

-   **Population Intercept (**$\alpha$): This is the average intercept for the entire population, i.e., the baseline outcome across all hospitals.

-   **Group-Specific Intercept (**$\alpha + \theta_i$): The group-specific intercept, where $\theta_i$ represents the deviation from the population intercept for group $i$.

We face an **identifiability issue** when estimating the population intercept and group-specific intercepts. We could add the same constant to all $\theta_i$'s and subtract that constant from $\alpha$.

-   This is solved by setting $\theta_i$ to be mean zero apriori.

## Group-Specific Intercept Model: Conditional Specification

-   Define $\mathbf{Y}_i = (Y_{i1},\ldots,Y_{in_i})$ and $\mathbf{Y} = (\mathbf{Y}_1,\ldots,\mathbf{Y}_n)$.

-   The posterior for the conditional model can be written as:

\begin{align*}
f(\boldsymbol{\Omega}, \boldsymbol{\theta} | \mathbf{Y}) &\propto f(\mathbf{Y}, \boldsymbol{\Omega}, \boldsymbol{\theta})\\
&= f(\mathbf{Y} | \boldsymbol{\Omega}, \boldsymbol{\theta}) f(\boldsymbol{\theta} | \boldsymbol{\Omega})f(\boldsymbol{\Omega})\\
&=  \prod_{i=1}^n \prod_{j = 1}^{n_i} f(Y_{ij} | \boldsymbol{\Omega}, \boldsymbol{\theta})  \prod_{i=1}^n f(\theta_i | \tau^2) f(\boldsymbol{\Omega}),
\end{align*} where $\boldsymbol{\theta} = (\theta_1,\ldots,\theta_n)$.

## Group-Specific Intercept Model: Marginal Specification {.midi}

To derive a marginal model it is useful to write the model at the level of the independent observations, $\mathbf{Y}_i$.

$$\mathbf{Y}_i = \begin{bmatrix}
    Y_{i1}\\
    Y_{i2}\\
    \vdots\\
    Y_{in_i}
  \end{bmatrix} = 
  \begin{bmatrix}
    \alpha + \mathbf{x}_{i1} \boldsymbol{\beta} + \theta_i + \epsilon_{i1}\\
    \alpha + \mathbf{x}_{i2} \boldsymbol{\beta} + \theta_i + \epsilon_{i2}\\
    \vdots \\
    \alpha + \mathbf{x}_{in_i} \boldsymbol{\beta} + \theta_i + \epsilon_{in_i}
  \end{bmatrix} = \alpha \mathbf{1}_{n_i} + \mathbf{X}_i \boldsymbol{\beta} + \theta_i \mathbf{1}_{n_i} + \boldsymbol{\epsilon}_i,$$ where $\mathbf{1}_{n_i}$ is an $n_i \times 1$ dimensional vector of ones, $\mathbf{X}_i$ is an $n_i \times p$ dimensional matrix with rows $\mathbf{x}_{ij}$.

-   $\boldsymbol{\epsilon}_i = (\epsilon_{i1},\ldots,\epsilon_{in_i}) \stackrel{ind}{\sim} N(\mathbf{0}_{n_i}, \sigma^2 \mathbf{I}_{n_i})$, with $\mathbf{0}_{n_i}$ an $n_i \times 1$ dimensional vector of zeros.

## Group-Specific Intercept Model: Marginal Specification {.midi}

-   Moments for the Marginal Model:

\begin{align*}
\mathbb{E}[\mathbf{Y}_{i} | \boldsymbol{\Omega}] &= \alpha \mathbf{1}_{n_i} + \mathbf{X}_i\boldsymbol{\beta}\\
\mathbb{V}(\mathbf{Y}_{i} | \boldsymbol{\Omega}) &= \tau^2 \mathbf{1}_{n_i} \mathbf{1}_{n_i}^\top + \sigma^2 \mathbf{I}_{n_i} = \boldsymbol{\Upsilon}_i\\
\mathbb{C}(\mathbf{Y}_{i}, \mathbf{Y}_{i'} | \boldsymbol{\Omega}) &= \mathbf{0}_{n_i \times n_i},\quad i \neq i'.
\end{align*}

$$\implies \boldsymbol{\Upsilon}_i = \mathbb{V}(\mathbf{Y}_{i} | \boldsymbol{\Omega}) = \begin{bmatrix}
    \tau^2 + \sigma^2 & \tau^2 & \cdots & \tau^2\\
    \tau^2 & \tau^2 + \sigma^2 & \cdots & \tau^2\\
    \vdots & \vdots & \ddots & \vdots\\
    \tau^2 & \tau^2 & \cdots &\tau^2 + \sigma^2
  \end{bmatrix}.$$

<!-- ## Understanding the Correlation Structure {.midi} -->

<!-- - The random intercept $\theta_i$ introduces correlation between observations within the same group. -->

<!-- - For two observations $Y_{ij}$ and $Y_{ik}$ from the same group $i$, we have: -->

<!-- $$\mathbb{C}(Y_{ij}, Y_{ik} | \boldsymbol{\Omega}) = \tau^2.$$ -->

<!-- - This non-zero covariance reflects the correlation between observations in the same group. -->

<!-- - Note: $\mathbb{C}(Y_{ij}, Y_{i'k} | \boldsymbol{\Omega}) = 0$ for $i \neq i'$. -->

<!-- ## Why Normal Distribution? -->

<!-- - **Natural Assumption**: We assume that the hospital-specific parameters (e.g., hospital intercepts) are normally distributed with a mean of zero because there's no reason to expect systematic deviations from the population average. -->

<!-- - **Flexibility**: The normal distribution allows us to model a wide range of variation in hospital effects, with the variance $\tau^2$ capturing how much hospitals differ from each other. -->

## Covariance Structure

-   The variance $\tau^2$ for $\theta_i$ can be interpreted as the **covariance** between two observations from the same hospital.

-   This reflects how much two observations from the same group are expected to be similar in terms of their outcomes.

\begin{align*}
\mathbb{C}(Y_{ij}, Y_{ik} | \boldsymbol{\Omega}) &= \mathbb{V}(\theta_i)\\
&= \tau^2.
\end{align*}

-   Thus, $\tau^2$ dictates the **within-group correlation** in our model.

-   Note: $\mathbb{C}(Y_{ij}, Y_{i'k} | \boldsymbol{\Omega}) = 0$ for $i \neq i'$.

## Induced Within Correlation

\begin{align*}
\rho (Y_{ij}, Y_{ik} | \boldsymbol{\Omega}) &= \frac{\mathbb{C}(Y_{ij}, Y_{ik} | \boldsymbol{\Omega})}{\sqrt{\mathbb{V}(Y_{ij} |  \boldsymbol{\Omega}) \mathbb{V}(Y_{ik} |  \boldsymbol{\Omega})}}\\
&=\frac{\tau^2}{\tau^2 + \sigma^2}\\
&= \frac{1}{1 + \frac{\sigma^2}{\tau^2}}.
\end{align*}

This model induces positive correlation within group observations.

## Induced Within Correlation

$$\rho (Y_{ij}, Y_{ik} | \alpha,\boldsymbol{\beta},\sigma) = \frac{1}{1 + \frac{\sigma^2}{\tau^2}}$$

```{r, echo = FALSE}
#| echo: false
#| fig-width: 10
#| fig-height: 5
#| fig-align: "center"
correlation <- function(tau2, sigma2 = 1) {
  1 / (1 + (sigma2 / tau2))
}
tau2 <- seq(0.001, 10, length.out = 1000)
dat_fig <- data.frame(
  x = tau2,
  y = correlation(tau2)
)
ggplot(dat_fig, aes(x = x, y = y)) + 
  geom_line(lwd = 2) + 
  geom_vline(aes(xintercept = 1)) + 
  scale_x_continuous(breaks = 0:10) + 
  labs(x = expression(tau^2),
       y = "Correlation",
       subtitle = expression(paste("Induced correlation in the group-intercept model with ", sigma^2, " = 1."))) +
  theme_bw()
```

## Group-Specific Intercept Model: Marginal Specification

For $i = 1,\ldots,n$, \begin{align*}
\mathbf{Y}_{i} | \boldsymbol{\Omega} &\stackrel{ind}{\sim} N(\alpha \mathbf{1}_{n_i}+ \mathbf{X}_i\boldsymbol{\beta},\boldsymbol{\Upsilon}_i)\\
\boldsymbol{\Omega} &\sim f(\boldsymbol{\Omega}),
\end{align*} where $\boldsymbol{\Omega} = (\alpha, \boldsymbol{\beta},\sigma,\tau)$ are the population parameters.

## Group-Specific Intercept Model: Marginal Specification

-   The posterior for the conditional model can be written as:

\begin{align*}
f(\boldsymbol{\Omega} | \mathbf{Y}) &\propto f(\mathbf{Y}, \boldsymbol{\Omega})\\
&= f(\mathbf{Y} | \boldsymbol{\Omega})f(\boldsymbol{\Omega})\\
&=  \prod_{i=1}^n f(\mathbf{Y}_{i} | \boldsymbol{\Omega}) f(\boldsymbol{\Omega}).
\end{align*}

**Why might we be interested in fitting the marginal model?**

## Recovering the Group-Specific Parameters {.midi}

-   We can still recover the $\theta_i$ when we fit the marginal model, we only need to compute $f(\theta_i | \mathbf{Y}_i,\boldsymbol{\Omega})$ for all $i$.

-   We can obtain this full conditional by specifying the joint distribution,

$$f\left(\begin{bmatrix}
    \mathbf{Y}_i\\
    \theta_i
  \end{bmatrix} \Bigg| \boldsymbol{\Omega}\right) = N\left(\begin{bmatrix}
    \alpha \mathbf{1}_{n_i} + \mathbf{X}_i \boldsymbol{\beta} + \theta_i \mathbf{1}_{n_i} + \boldsymbol{\epsilon}_i\\
    0
  \end{bmatrix}, \begin{bmatrix}
    \boldsymbol{\Upsilon}_i & \tau^2 \mathbf{1}_{n_i}\\
    \tau^2 \mathbf{1}_{n_i}^\top & \tau^2
  \end{bmatrix}\right).$$

We can then use the [conditional specification of a multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions) to find, $f(\theta_i | \mathbf{Y}_i, \boldsymbol{\Omega}) = N(\mathbb{E}_{\theta_i},\mathbb{V}_{\theta_i})$, where

\begin{align*}
\mathbb{E}_{\theta_i} &= \mathbf{0}_{n_i} + \tau^2 \mathbf{1}_{n_i}^\top \boldsymbol{\Upsilon}_i^{-1} (\mathbf{Y}_i - \alpha \mathbf{1}_{n_i} - \mathbf{X}_i \boldsymbol{\beta})\\
\mathbb{V}_{\theta_i} &= \tau^2 - \tau^4 \mathbf{1}_{n_i}^\top \boldsymbol{\Upsilon}_i^{-1} \mathbf{1}_{n_i}.
\end{align*}

<!-- ## Why Does This Model Work? -->

<!-- 1. **Within-group correlation**: Observations within the same hospital are more similar due to the shared intercept. -->

<!-- 2. **Between-group differences**: Groups have different intercepts $\alpha_i^*$, reflecting different baseline effects. -->

## Example data: Glucose Measurement in 4 Primary Care Clinics {.midi}

-   We will study glucose values for patients being seen at 4 primary care clinics across the city. The clinics each represent a geographical region: east, west, north, and south.

-   The dataset consists of glucose measurements (mg/dl) from patients, and also risk factors:

    -   Age (years).

    -   BMI ($kg/m^2$).

    -   Sex (0 = male, 1 = female).

    -   Smoking status (0 = non-smoker, 1 = smoker).

    -   Physical activity level (0 = low, 1 = moderate, 2 = high).

    -   Glucose lowering medication (0 = none, 1 = yes).

## Preview the Data {.smallest}

```{r, echo = FALSE}
# Example data: Simulated dataset
set.seed(54)

n <- 500  # Number of observations

# Simulate the dataset
region <- sample(c("North", "South", "East", "West"), size = n, replace = TRUE)
age <- rnorm(n, mean = 55, sd = 10)
bmi <- rnorm(n, mean = 30, sd = 5)
gender <- sample(c(0, 1), size = n, replace = TRUE)  # 0 = Male, 1 = Female
smoking <- sample(c(0, 1), size = n, replace = TRUE)  # 0 = Non-smoker, 1 = Smoker
activity <- sample(0:2, size = n, replace = TRUE)     # 0 = Low, 1 = Moderate, 2 = High
medication <- sample(c(0, 1), size = n, replace = TRUE) # 0 = No, 1 = Yes

# Simulating Blood Glucose Level (as a function of predictors + random intercept for Region)
region_effects <- rnorm(4, 0, 5)  # Random intercepts for 4 regions
region_map <- match(region, c("North", "South", "East", "West"))
glucose <- 100 + (0.3 * age) + (0.5 * bmi) + (10 * gender) + (-5 * smoking) + 
  (-5 * 1 * (activity == 2)) + (-15 * 1 * (activity == 3)) + (-20 * medication) + region_effects[region_map] + rnorm(n, mean = 0, sd = 7.5)

# Create a dataframe
data <- data.frame(
  glucose = glucose,
  age = age,
  bmi = bmi,
  gender = gender,
  smoking = smoking,
  activity = activity,
  medication = medication,
  region = region
)
data <- data[order(data$region), ]
data$glucose <- round(data$glucose, 1)
data$age <- round(data$age, 0)
data$bmi <- round(data$bmi, 1)
rownames(data) <- NULL
```

```{r, message = FALSE, warning = FALSE, echo = FALSE}
library(DT)
DT::datatable(data)
```

## Writing down a model

We would like to fit the following model:

$$Y_{ij} = \alpha + \mathbf{x}_{ij} \boldsymbol{\beta} + \theta_i + \epsilon_{ij},\quad \epsilon_{ij} \stackrel{iid}{\sim}N(0,\sigma^2).$$

-   $Y_{ij}$ is the glucose value for patient $i$ in clinic $j$

-   $\theta_i$ for $i = 1,\ldots,4$ is the clinic-specific intercept deviation. \begin{align*}
    \mathbf{x}_{ij} &= (Age_{ij}, BMI_{ij}, Female_{ij},Smoker_{ij}, \\
    &\quad Moderate\_Activity_{ij}, High\_Activity_{ij}, On\_Meds_{ij}).
    \end{align*}

## Fitting the Conditional Model in Stan

```{stan output.var = "conditional", eval = FALSE}
// conditional-model.stan
data {
  int<lower = 1> n;
  int<lower = 1> N;
  int<lower = 1> p;
  matrix[N, p] X;
  vector[N] Y;
  int<lower = 1, upper = n> Ids[N];
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower = 0> sigma;
  real<lower = 0> tau;
  vector[n] theta;
}
model {
  vector[N] mu = rep_vector(0.0, N);
  mu += alpha;
  for (i in 1:N) {
    mu[i] += X[i, ] * beta + theta[Ids[i]];
  }
  target += normal_lpdf(Y | mu, sigma);
  target += normal_lpdf(theta | 0, tau);
  target += normal_lpdf(alpha | 0, 3);
  target += normal_lpdf(beta | 0, 3);
  target += normal_lpdf(sigma | 0, 3);
  target += normal_lpdf(tau | 0, 3);
}
generated quantities {
  real Intercept_East = alpha + theta[1];
  real Intercept_North = alpha + theta[2];
  real Intercept_South = alpha + theta[3];
  real Intercept_West = alpha + theta[4];
  real rho = 1 / (1 + ((sigma * sigma) / (tau * tau)));
  vector[N] Y_pred;
  vector[N] log_lik;
  vector[N] mu = rep_vector(0.0, N);
  mu += alpha;
  for (i in 1:N) {
    mu[i] += X[i, ] * beta + theta[Ids[i]];
    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);
    Y_pred[i] = normal_rng(mu[i], sigma);
  }
}
```

## Fitting the Model in Stan

```{r, eval = FALSE}
X <- model.matrix(~ age + bmi + gender + smoking + as.factor(activity) + medication, data = data)[, -1]
stan_data <- list(
  N = nrow(data),
  n = length(unique(data$region)),
  p = ncol(X),
  X = X,
  Y = data$glucose,
  Ids = as.numeric(as.factor(data$region))
)
conditional_model <- stan_model(model_code = "conditional-model.stan")
fit_conditional <- sampling(conditional_model, stan_data)
```

```{r, echo = FALSE}
library(rstan)
fit_conditional <- readRDS("/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit-conditional.rds")
fit_lin_reg <- readRDS("/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit-linear-regression.rds")
X <- model.matrix(~ age + bmi + gender + smoking + as.factor(activity) + medication, data = data)[, -1]
stan_data <- list(
  N = nrow(data),
  n = length(unique(data$region)),
  p = ncol(X),
  X = X,
  Y = data$glucose,
  Ids = as.numeric(as.factor(data$region))
)
```

## Assessing Convergence

```{r}
traceplot(fit_conditional, pars = c("alpha", "beta", "sigma", "tau", "rho"))
```

## Assessing Convergence

```{r}
library(bayesplot)
bayesplot::mcmc_acf(fit_conditional, regex_pars = c("alpha", "beta", "sigma", "tau", "rho"))
```

## Posterior Summaries

```{r}
print(fit_conditional, pars = c("alpha", "beta", "sigma", "tau", "rho"))
```

## Comparison to Linear Regression: $\boldsymbol{\beta}$

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 5
#| fig-height: 5
#| layout-ncol: 2
dat_fig <- data.frame(
  MeanHier = apply(rstan::extract(fit_conditional, pars = "beta")$beta, 2, mean), 
  MeanFix = apply(rstan::extract(fit_lin_reg, pars = "beta")$beta, 2, mean),
  VarianceHier = apply(rstan::extract(fit_conditional, pars = "beta")$beta, 2, var), 
  VarianceFix = apply(rstan::extract(fit_lin_reg, pars = "beta")$beta, 2, var)
)  
ggplot(dat_fig, aes(x = MeanFix, y = MeanHier)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
  labs(subtitle = "Posterior Mean",
       x = "Linear Regression",
       y = "Hierarchical Model")
ggplot(dat_fig, aes(x = VarianceFix, y = VarianceHier)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
    labs(subtitle = "Posterior Variance",
       x = "Linear Regression",
       y = "Hierarchical Model")
```

## Model Comparison

```{r}
library(loo)
waic_conditional <- waic(extract_log_lik(fit_conditional))
waic_lin_reg <- waic(extract_log_lik(fit_lin_reg))
comparison <- loo_compare(list("Hierarchical Model" = waic_conditional, "Linear Regression" = waic_lin_reg))
print(comparison, simplify = FALSE)
```

## Explore the Clinic-Specific Variation

```{r}
print(fit_conditional, pars = c("Intercept_East", "Intercept_South", "Intercept_North", "Intercept_West"), probs = c(0.025, 0.975))
```

## Explore the Clinic-Specific Variation

```{r, echo = FALSE}
#| echo: false
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 6
#| layout-ncol: 1
dat_fig <- data.frame(
  North = rstan::extract(fit_conditional, pars = "Intercept_North")$Intercept_North,
  South = rstan::extract(fit_conditional, pars = "Intercept_South")$Intercept_South,
  East = rstan::extract(fit_conditional, pars = "Intercept_East")$Intercept_East,
  West = rstan::extract(fit_conditional, pars = "Intercept_West")$Intercept_West,
  Overall = rstan::extract(fit_conditional, pars = "alpha")$alpha
)
library(tidyr)
library(ggridges)
dat_fig <- pivot_longer(dat_fig, cols = everything(), 
                        names_to = "Intercept", values_to = "Value")
dat_fig$Intercept <- as.factor(dat_fig$Intercept)
dat_fig$Intercept <- relevel(dat_fig$Intercept, ref = "Overall")
ggplot(dat_fig, aes(x = Value, y = Intercept, fill = Intercept, color = Intercept)) +
  geom_density_ridges(alpha = 0.7) + 
  # theme_ridges() +
  theme_minimal() + 
  labs(title = "Posterior Distributions of Intercepts", x = "Glucose Concentration (mg/dL)", y = "Intercept") +
  theme(legend.position = "none")
```

## Fitting the Marginal Model in Stan

Need [ragged data structure](https://mc-stan.org/docs/stan-users-guide/sparse-ragged.html#ragged-data-structs.section).

```{stan output.var = "marginal", eval = FALSE}
// marginal-model.stan
data {
  int<lower = 1> n;
  int<lower = 1> N;
  int<lower = 1> p;
  matrix[N, p] X;
  vector[N] Y;
  int n_is[n];
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower = 0> sigma;
  real<lower = 0> tau;
}
transformed parameters {
  real sigma2 = sigma * sigma;
  real tau2 = tau * tau;
}
model {
  // compute the mean process for the marginal model
  vector[N] mu = rep_vector(0.0, N);
  mu += alpha;
  for (i in 1:N) {
    mu[i] += X[i, ] * beta;
  }
  // evaluate the likelihood for the marginal model using ragged data structure
  int pos;
  pos = 1;
  for (i in 1:n) {
    int n_i = n_is[i];
    matrix[n_i, n_i] Upsilon_i = sigma2 * diag_matrix(rep_vector(1.0, n_i)) + tau2 * rep_matrix(1, n_i, n_i);
    vector[n_i] Y_i = segment(Y, pos, n_i);
    vector[n_i] mu_i = segment(mu, pos, n_i);
    target += multi_normal_lpdf(Y_i | mu_i, Upsilon_i);
    pos = pos + n_i;
  }
  // priors
  target += normal_lpdf(alpha | 0, 3);
  target += normal_lpdf(beta | 0, 3);
  target += normal_lpdf(sigma | 0, 3);
  target += normal_lpdf(tau | 0, 3);
}
generated quantities {
  // compute the mean process for the marginal model
  vector[N] mu = rep_vector(0.0, N);
  mu += alpha;
  for (i in 1:N) {
    mu[i] += X[i, ] * beta;
  }
  // compute theta using the ragged data structure
  int pos;
  pos = 1;
  vector[n] theta;
  for (i in 1:n) {
    int n_i = n_is[i];
    vector[n_i] ones_i = rep_vector(1.0, n_i);
    matrix[n_i, n_i] Upsilon_i = sigma2 * diag_matrix(ones_i) + tau2 * rep_matrix(1, n_i, n_i);
    vector[n_i] Y_i = segment(Y, pos, n_i);
    vector[n_i] mu_i = segment(mu, pos, n_i);
    real mean_theta_i = tau2 * ones_i' * inverse_spd(Upsilon_i) * (Y_i - mu_i);
    real var_theta_i = tau2 - tau2 * tau2 * ones_i' * inverse_spd(Upsilon_i) * ones_i;
    theta[i] = normal_rng(mean_theta_i, sqrt(var_theta_i));
    pos = pos + n_i;
  }
}
```

## Fitting the Model in Stan

```{r, eval = FALSE}
stan_data <- list(
  N = nrow(data),
  n = length(unique(data$region)),
  p = ncol(X),
  X = X,
  Y = data$glucose,
  n_is = as.numeric(table(data$region))
)
marginal_model <- stan_model(model_code = "marginal-model.stan")
fit_marginal <- sampling(marginal_model, stan_data)
```

```{r, echo = FALSE}
library(rstan)
fit_marginal <- readRDS("/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit-marginal.rds")
stan_data <- list(
  N = nrow(data),
  n = length(unique(data$region)),
  p = ncol(X),
  X = X,
  Y = data$glucose,
  n_is = as.numeric(table(data$region))
)
```

## Assessing Convergence

```{r}
traceplot(fit_marginal, pars = c("alpha", "beta", "sigma", "tau"))
```

## Assessing Convergence

```{r}
library(bayesplot)
bayesplot::mcmc_acf(fit_marginal, regex_pars = c("alpha", "beta", "sigma", "tau"))
```

## Posterior Summaries

```{r}
print(fit_marginal, pars = c("alpha", "beta", "sigma", "tau"))
```

## Conclusion

-   By introducing a group-specific intercept, we allow for dependencies between observations within groups, making the model more realistic for real-world clustered or repeated measures data.

-   For the remainder of the class, we will expand upon this hierarchical modeling framework to account for complext data types that are frequently encountered in research, including longitudinal and spatial data.

## Prepare for next class

-   Complete [HW 03](https://biostat725-sp26.netlify.app/hw/hw-03), which is due before Thursday's class.

-   Next Thursday's lecture: Longitudinal Data

-   Exam 1 will be assigned after class on Thursday!