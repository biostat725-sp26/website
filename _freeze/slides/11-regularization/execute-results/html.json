{
  "hash": "34952c3c3964227952486b061f319210",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regularization\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2026-02-12\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2026](https://biostat725-sp26.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n::: {.cell}\n\n:::\n\n\n## Review of last lecture\n\n-   On Tuesday, we learned about robust regression.\n\n    -   Heteroskedasticity\n\n    -   Heavy-tailed distributions\n\n    -   Median regression\n\n-   These were all models for the observed data $Y_i$.\n\n-   Today, we will focus on prior specifications for $\\boldsymbol{\\beta}$.\n\n## Sparsity in regression problems\n\n-   Supervised learning can be cast as the problem of estimating a set of coefficients $\\boldsymbol{\\beta} = \\{\\beta_j\\}_{j=1}^{p}$ that determines some functional relationship between a set of $\\{x_{ij}\\}_{j = 1}^p$ and a target variable $Y_i$.\n\n-   This is a central focus of statistics and machine learning.\n\n-   Challenges arise in \"large-$p$\" problems where, in order to avoid overly complex models that predict poorly, some form of dimension reduction is needed.\n\n-   Finding a sparse solution, where some $\\beta_j$ are zero, is desirable.\n\n## Bayesian sparse estimation\n\n-   From a Bayesian-learning perspective, there are two main sparse-estimation alternatives: discrete mixtures and shrinkage priors.\n\n-   Discrete mixtures have been very popular, with the spike-and-slab prior being the gold standard.\n\n    -   Easy to force $\\beta_j$ to exactly zero, but require discrete parameter specification.\n\n-   Shrinkage priors force $\\beta_j$ to zero using regularization, but struggle to get exact zeros.\n\n    -   In recent years, shrinkage priors have become dominant in Bayesian sparsity priors.\n\n## Global-local shrinkage\n\n-   Let's assume $\\mathbf{Y} \\stackrel{}{\\sim}N\\left(\\alpha\\mathbf{1}_n + \\mathbf{X}\\boldsymbol{\\beta},\\sigma^2\\mathbf{I}_n\\right)$.\n\n-   Sparsity can be induced into $\\boldsymbol{\\beta}$ using a global-local prior,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} f(\\lambda_j).\n\\end{align*}\n\n-   $\\tau^2$ is the global variance term.\n\n-   $\\lambda_j$ is the local term.\n\n-   The degree of sparsity depends on the choice of $f(\\lambda_j)$.\n\n## Spike-and-slab prior\n\n-   Discrete parameter specification,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\text{Bernoulli}(\\pi).\n\\end{align*}\n\n-   $\\lambda_j \\in \\{0,1\\}$, thus this model permits exact zeros.\n\n-   The number of zeros is dictated by $\\pi$, which can either be pre-specified or given a prior.\n\n-   **Discrete parameters can not be specified in Stan!**\n\n<!-- - Instead of giving continuous priors for $\\lambda_j$'s as in the horseshoe, here only two values are allowed (0,1). -->\n\n<!-- -   The shrinkage factor $\\kappa_j$ only has mass at $\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}}$ and $\\kappa_j = 1$ with probabilities $\\pi$ and $1-\\pi$, -->\n\n## Spike-and-slab prior\n\n-   Spike-and-slab can be written generally as a two-component mixture of Gaussians,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau, \\omega &\\stackrel{ind}{\\sim} \\lambda_j N(0, \\tau^2) + (1-\\lambda_j) N(0,\\omega^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\text{Bernoulli}(\\pi).\n\\end{align*}\n\n-   $\\omega \\ll \\tau$ and the indicator variable $\\lambda_j \\in \\{0, 1\\}$ denotes whether $\\beta_j$ is close to zero (comes from the \"spike\", $\\lambda_j = 0$) or non-zero (comes from the \"slab\", $\\lambda_j = 1$).\n\n-   Often $\\omega = 0$ (the spike is a true spike).\n\n## Ridge regression\n\n-   Ridge regression is motivated by extending linear regression to the setting where:\n\n    -   there are too many predictors (sparsity is desired) and/or,\n\n    -   $\\mathbf{X}^\\top \\mathbf{X}$ is ill-conditioned were singular or nearly singular (multicollinearity).\n\n-   The OLS estimate becomes unstable: $$\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1}\\mathbf{X}^\\top \\mathbf{Y}.$$\n\n## Ridge regression\n\nThe ridge estimator minimizes the penalized sum of squares,\n\n$$\\hat{\\boldsymbol{\\beta}}_{\\text{RIDGE}} = \\arg \\min_{\\boldsymbol{\\beta}}\\left||\\mathbf{Y} - \\boldsymbol{\\mu}\\right||_2^2 + \\lambda \\sum_{j=1}^p \\beta_j^2$$\n\n-   $\\boldsymbol{\\mu} = \\alpha\\mathbf{1}_n + \\mathbf{X}\\boldsymbol{\\beta}$.\n\n-   $||\\mathbf{v}||_2 = \\sqrt{\\mathbf{v}^\\top \\mathbf{v}}$ is the L2 norm.\n\n-   $\\hat{\\boldsymbol{\\beta}}_{\\text{RIDGE}} = \\left(\\mathbf{X}^\\top\\mathbf{X} + \\lambda \\mathbf{I}_p\\right)^{-1}\\mathbf{X}^\\top\\mathbf{Y}$\n\n    -   Adding the $\\lambda$ to diagonals of $\\mathbf{X}^\\top\\mathbf{X}$ stabilizes the inverse, which becomes unstable with multicollinearity.\n\n## Bayesian ridge prior\n\nRidge regression can be obtained using the following global-local shrinkage prior,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &= 1 / \\lambda\\\\\n\\tau^2 &= \\sigma^2. \n\\end{align*}\n\n-   This is equivalent to: $f(\\beta_j | \\lambda, \\sigma) \\stackrel{iid}{\\sim} N\\left(0,\\frac{\\sigma^2}{\\lambda}\\right)$.\n\n-   **How is this equivalent to ridge regression?**\n\n## Bayesian ridge prior\n\n-   The negative log-posterior is proportional to,\n\n$$\\frac{||\\mathbf{Y} - \\boldsymbol{\\mu}||_2^2}{2\\sigma^2} + \\frac{\\lambda \\sum_{j=1}^p \\beta_j^2}{2\\sigma^2}.$$\n\n-   The posterior mean and mode are $\\hat{\\boldsymbol{\\beta}}_{\\text{RIDGE}}$.\n\n-   Since $\\lambda$ is applied to the squared norm of the $\\boldsymbol{\\beta}$, people often standardize all of the covariates to make them have a similar scale.\n\n-   Bayesian statistics is inherently performing regularization!\n\n## Lasso regression\n\nThe least absolute shrinkage and selection operator (lasso) estimator minimizes the penalized sum of squares,\n\n$$\\hat{\\boldsymbol{\\beta}}_{\\text{LASSO}} = \\arg \\min_{\\boldsymbol{\\beta}}\\left||\\mathbf{Y} - \\boldsymbol{\\mu}\\right||_2^2 + \\lambda \\sum_{j=1}^p |\\beta_j|$$\n\n-   $\\lambda = 0$ reduces to OLS etimator.\n\n-   $\\lambda = \\infty$ leads to $\\hat{\\boldsymbol{\\beta}}_{\\text{LASSO}} = 0$.\n\n-   Lasso is desirable because it can set some $\\beta_j$ exactly to zero.\n\n## Bayesian lasso prior\n\nLasso regression can be obtained using the following global-local shrinkage prior,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\stackrel{iid}{\\sim} \\text{Exponential}(0.5).\n\\end{align*}\n\n-   This is equivalent to: $f(\\beta_j | \\tau) \\stackrel{iid}{\\sim} \\text{Laplace}\\left(0,\\tau\\right)$.\n\n-   **How is this equivalent to lasso regression?**\n\n## Bayesian lasso prior\n\n-   The negative log-posterior is proportional to,\n\n$$\\frac{||\\mathbf{Y} - \\boldsymbol{\\mu}||_2^2}{2\\sigma^2} + \\frac{\\sum_{j=1}^p |\\beta_j|}{\\tau}.$$\n\n-   Lasso is recovered by specifying: $\\lambda = 1/\\tau$.\n\n-   The posterior mode is $\\hat{\\boldsymbol{\\beta}}_{\\text{LASSO}}$.\n\n-   As $\\lambda$ increases, more coefficients are set to zero (less variables are selected), and among the non-zero coefficients, more shrinkage is employed.\n\n## Bayesian lasso does not work\n\n-   There is a consensus that the Bayesian lasso [does not work well](https://statmodeling.stat.columbia.edu/2017/11/02/king-must-die/).\n\n-   It does not yield $\\beta_j$ that are exactly zero and it can overly shrink non-zero $\\beta_j$.\n\n-   The gold-standard sparsity-inducing prior in Bayesian statistics is the horseshoe prior.\n\n## Relevance vector machine\n\n-   Before we get to the horseshoe, one more global-local prior, called the [relevance vector machine](https://proceedings.neurips.cc/paper_files/paper/1999/file/f3144cefe89a60d6a1afaf7859c5076b-Paper.pdf).\n\n-   This model can be obtained using the following prior,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\n\\end{align*}\n\n-   This is equivalent to: $f(\\beta_j | \\tau) \\stackrel{iid}{\\sim} {t}_{\\nu}\\left(0,\\tau\\right)$.\n\n## Horseshoe prior\n\n-   The horseshoe prior is specified as,\n\n    \\begin{align*}\n    \\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n    \\lambda_j &\\stackrel{iid}{\\sim} \\mathcal C^+(0, 1),\n    \\end{align*} where $\\mathcal C^+(0, 1)$ is a half-Cauchy distribution for the local parameter $\\lambda_j$.\n\n    -   $\\lambda_j$'s are the *local* shrinkage parameters.\n\n    -   $\\tau$ is the *global* shrinkage parameter.\n\n## Half-Cauchy distribution\n\nA random variable $X \\sim \\mathcal C^+(\\mu,\\sigma)$ follows a half-Cauchy distribution with location $\\mu$ and scale $\\sigma > 0$ and has the following density,\n\n$$f(X | \\mu, \\sigma) = \\frac{2}{\\pi \\sigma}\\frac{1}{1 + (X - \\mu)^2 / \\sigma^2},\\quad X \\geq \\mu$$\n\n-   The Half-Cauchy distribution with $\\mu = 0$ is a useful prior for non-negative parameters that may be very large, as allowed by the very heavy tails of the Cauchy distribution.\n\n## Half-Cauchy distribution in Stan\n\nIn Stan, the half-Cauchy distribution can be specified by putting a constraint on the parameter definition.\n\n\n::: {.cell output.var='half-cauchy'}\n\n```{.stan .cell-code}\nparameters {\n  real<lower = 0> lambda;\n}\nmodel {\n  lambda ~ cauchy(0, 1);\n}\n```\n:::\n\n\n## Half-Cauchy distribution\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-regularization_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Horseshoe prior\n\nThe horseshoe prior has two interesting features that make it particularly useful as a shrinkage prior for sparse problems.\n\n1.  It has flat, Cauchy-like tails that allow strong signals to remain large (that is, un-shrunk) a posteriori.\n\n2.  It has an infinitely tall spike at the origin that provides severe shrinkage for the zero elements of $\\boldsymbol{\\beta}$.\n\nAs we will see, these are key elements that make the horseshoe an attractive choice for handling sparse vectors.\n\n## Relation to other shrinkage priors\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\sim f(\\lambda_j)\n\\end{align*}\n\n1.  $\\lambda_j = 1 / \\lambda$, implies ridge regression.\n\n2.  $f(\\lambda_j) = \\text{Exponential}(0.5)$, implies lasso.\n\n3.  $f(\\lambda_j) = \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)$, implies relevance vector machine.\n\n4.  $f(\\lambda_j) = \\mathcal C^+(0,1)$, implies horseshoe.\n\n## Horsehoe density\n\n![](images/10/carvalho1.png){fig-alt=\"workflow\" fig-align=\"center\" height=\"4.5in\"}\n\n[From Carvalho 2009](https://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf)\n\n## Shrinkage of each prior\n\n-   Define the posterior mean of $\\beta_j$ as $\\bar{\\beta}_j$ and the maximum likelihood estimator for $\\beta_j$ as $\\hat{\\beta}_j$.\n\n-   The following relationship holds: $\\bar{\\beta}_j = (1 - \\kappa_j) \\hat{\\beta}_j$,\n\n$$\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}s_j^2\\lambda_j^2}.$$\n\n-   $\\kappa_j$ is called the shrinkage factor for $\\beta_j$.\n\n-   $s_j^2 = \\mathbb{V}(x_j)$ is the variance for each predictor.\n\n## Standardization of predictors {.midi}\n\n-   In regularization problems, predictors are standardized (to mean zero and standard deviation one).\n\n-   This means that so that $s_j = 1$.\n\n-   Shrinkage parameter:\n\n$$\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}\\lambda_j^2}.$$\n\n-   $\\kappa_j = 1$, implies complete shrinkage.\n\n-   $\\kappa_j = 0$, implies no shrinkage.\n\n## Shrinkage parameter\n\n![](images/10/carvalho2.png){fig-alt=\"workflow\" fig-align=\"center\" height=\"5in\"}\n\n[From Carvalho 2009](https://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf)\n\n## Horseshoe shrinkage parameter\n\n-   Choosing $\\lambda_j âˆ¼ \\mathcal C^+(0, 1)$ implies $\\kappa_j âˆ¼ \\text{Beta}(0.5, 0.5)$, a density that is symmetric and unbounded at both 0 and 1.\n\n-   This horseshoe-shaped shrinkage profile expects to see two things a priori:\n\n    1.  Strong signals ($\\kappa \\approx 0$, no shrinkage), and\n\n    2.  Zeros ($\\kappa \\approx 1$, total shrinkage).\n\n## Similarity to spike-and-slab {.small}\n\n-   A horseshoe prior can be considered as a continuous approximation to the spike-and-slab prior.\n\n    -   The spike-and-slab places a discrete probability mass at exactly zero (the \"spike\") and a separate distribution around non-zero values (the \"slab\").\n\n    -   The horseshoe prior smoothly approximates this behavior with a very concentrated distribution near zero.\n\n![](images/10/aki1.png){fig-alt=\"workflow\" fig-align=\"center\" height=\"2.5in\"}\n\n[From Piironena and Vehtari 2017](https://doi.org/10.1214/17-EJS1337SI)\n\n<!-- ## Regularized horseshoe prior -->\n\n<!-- \\begin{align*} -->\n\n<!-- \\beta_j | \\lambda_j, \\tau, c &\\sim N\\left(0, \\tau^2 \\tilde{\\lambda}_j^2\\right),\\quad \\tilde{\\lambda}_j^2 = \\frac{c^2 \\lambda_j^2}{c^2 + \\tau^2 \\lambda_j^2},\\\\ -->\n\n<!-- \\lambda_j &\\sim \\mathcal C^+(0,1). -->\n\n<!-- \\end{align*} -->\n\n<!-- -   When $\\tau^2 \\lambda_j^2 \\ll c^2$ (i.e., $\\beta_j$ close to zero), $\\beta_j \\sim  N\\left(0, \\tau^2\\lambda_j^2\\right)$ -->\n\n<!-- -   When $\\tau^2 \\lambda_j^2 \\gg c^2$, (i.e., $\\beta_j$ far from zero), $\\beta_j \\sim  N\\left(0, c^2\\right)$ -->\n\n<!-- -   $c \\rightarrow \\infty$ recovers the original horseshoe. -->\n\n<!-- **Why is this an appealing extension?** -->\n\n<!-- ## Regularized horseshoe compared to spike-and-slab -->\n\n<!-- -   The regularized horseshoe prior is comparable to the spike-and-slab with finite $c$. -->\n\n<!-- ![](images/10/aki2.png){fig-alt=\"workflow\" fig-align=\"center\" height=\"3in\"} -->\n\n<!-- [From Piironena and Vehtari 2017](https://doi.org/10.1214/17-EJS1337SI) -->\n\n<!-- ## Choosing a prior for $c^2$ -->\n\n<!-- -   Unless substantial knowledge about the scale of the relevant coefficients exists, it is recommended to place a prior for $c$ instead of fixing it. -->\n\n<!-- -   Often a reasonable choice is, $$c^2 \\sim \\text{Inv-Gamma}(\\alpha, \\beta), \\quad \\alpha = \\nu/2, \\beta = \\nu s^2 / 2,$$ -->\n\n<!-- -   This translates to a $t_{\\nu}(0,s^2)$ slab for the coefficients far from 0. -->\n\n<!-- -   Another motivation for using inverse-Gamma is that it has a heavy right tail accompanied by a light left tail thereby preventing much mass from accumulating near zero. -->\n\n## Choosing a prior for $\\tau$\n\n-   Carvalho et al. 2009 suggest $\\tau \\sim \\mathcal C^+(0,1)$.\n\n-   Polson and Scott 2011 recommend $\\tau | \\sigma \\sim \\mathcal C^+(0, \\sigma^2)$.\n\n-   Another prior comes from a quantity called the effective number of nonzero coefficients,\\\n\n    $$m_{eff} = \\sum_{j=1}^p (1 - \\kappa_j).$$\n\n## Global shrinkage parameter $\\tau$\n\n-   The prior mean can be shown to be,\n\n$$\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = \\frac{\\tau \\sigma^{-1} \\sqrt{n}}{1 + \\tau \\sigma^{-1} \\sqrt{n}}p.$$\n\n-   Setting $\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = p_0$ (prior guess for the number of non-zero coefficients) yields for $\\tau$,\n\n$$\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}.$$\n\n## Global shrinkage parameter $\\tau$\n\n![](images/10/aki3.png){fig-alt=\"workflow\" fig-align=\"center\" height=\"5in\"}\n\n[From Piironena and Vehtari 2017](https://doi.org/10.1214/17-EJS1337SI)\n\n## Non-Gaussian observation models\n\n-   The reference value:\n\n$$\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}.$$\n\n-   This framework can be applied to non-Gaussian observation data models using plug-in estimates values for $\\sigma$.\n\n    -   Gaussian approximations to the likelihood.\n\n    -   For example: For logistic regression $\\sigma = 2$.\n\n## Coding up the model in Stan\n\nHorseshoe model has the following form,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\mathcal C^+(0, 1),\\\\\n\\tau &\\sim \\mathcal C^+(0, \\tau_0^2).\n\\end{align*}\n\nEfficient parameter transformation, $$\\beta_j = \\tau \\lambda_j z_j, \\quad z_j \\stackrel{iid}{\\sim} N(0,1).$$\n\n## Horseshoe in Stan\n\n\n::: {.cell output.var='horseshoe'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real<lower = 0> tau0;\n}\nparameters {\n  real alpha;     \n  real<lower = 0> sigma;\n  vector[p] z;\n  vector<lower = 0>[p] lambda;\n  real<lower = 0> tau;\n}\ntransformed parameters {\n  vector[p] beta;\n  beta = tau * lambda .* z;\n}\nmodel {\n  // likelihood\n  target += normal_lpdf(Y | alpha + X * beta, sigma);\n  // population parameters\n  target += normal_lpdf(alpha | 0, 3);\n  sigma ~ normal(0, 3);\n  // horseshoe prior\n  target += std_normal_lpdf(z);\n  lambda ~ cauchy(0, 1);\n  tau ~ cauchy(0, tau0);\n}\n```\n:::\n\n\n## Prepare for next class\n\n-   Work on [HW 03](https://biostat725-sp26.netlify.app/hw/hw-03-blank), which was just assigned.\n\n-   Complete reading to prepare for next Tuesday's lecture\n\n-   Tuesday's lecture: Classification\n",
    "supporting": [
      "11-regularization_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}