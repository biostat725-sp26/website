{
  "hash": "a8d336c1f64dfe7030c8ea6471b1d453",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multiclass Classification\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2026-02-19\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2026](https://biostat725-sp26.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n::: {.cell}\n\n:::\n\n\n## Review of last lecture\n\n-   On Tuesday, we learned about classification using logistic regression.\n\n-   Today, we will focus on multiclass classification: multinomial regression, ordinal regression.\n\n## Multiclass regression\n\n-   Often times one encounters an outcome variable that is nominal and has more than two categories.\n\n-   If there is no inherent rank or order to the variable, we can use multinomial regression. Examples include:\n\n    -   gender (male, female, non-binary),\n\n    -   blood type (A, B, AB, O).\n\n-   If there is an order to the variable, we can use ordinal regression. Examples include:\n\n    -   stages of cancer (stage I, II, III, IV),\n\n    -   pain level (mild, moderate, severe).\n\n## Multinomial random variable\n\n-   Assume an outcome $Y_i \\in \\{1,\\ldots,K\\}$ for $i = 1,\\ldots,n$.\n\n-   The likelihood in multinomial regression can be written as the following categorical likelihood,\n\n$$f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}},$$\n\n-   $\\delta_{ij} = 1(Y_i = j)$ is the Kronecker delta.\n\n-   Since $Y_i$ is discrete, we only need to specify $P(Y_i = j)$ for all $i$ and $j$.\n\n## Log-linear regression\n\n-   One way to motivate multinomial regression is through a log-linear specification:\n\n$$\\log P(Y_i = j) = \\mathbf{x}_i\\boldsymbol{\\beta}_j - \\log Z.$$\n\n-   $\\boldsymbol{\\beta}_j$ is a $j$ specific set of regression parameters.\n\n-   $P(Y_i = j) = \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}/Z$.\n\n-   $Z$ is a normalizing constant that guarentees that $\\sum_{j=1}^K P(Y_i = j) = 1$.\n\n## Finding the normalizing constant\n\n-   We know that,\n\n\\begin{align*}\n1 &= \\sum_{j=1}^K P(Y_i = j) = \\frac{1}{Z}\\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\\\\\n&\\implies Z = \\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\n\\end{align*}\n\n## Multinomial probabilities\n\n-   Thus, we have the following,\n\n$$P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.$$\n\n-   This function is called the **softmax** function.\n\n-   Unfortunately, this specification is not identifiable.\n\n## Identifiability issue\n\n-   We can add a vector $\\mathbf{c}$ to all parameters and get the same result,\n\n\\begin{align*}\n\\frac{\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_k + \\mathbf{c})\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_j + \\mathbf{c})\\}} &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n\\end{align*}\n\n-   A common solution is to set: $\\boldsymbol{\\beta}_K = \\mathbf{0}$.\n\n## Updating the probabilities\n\n-   Using the identifiability constraint of $\\boldsymbol{\\beta}_K = \\mathbf{0}$, the probabilities become, \\begin{align*}\n    P(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\},\\\\\n    P(Y_i = K) &= \\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n    \\end{align*}\n\n-   How to interpret the $\\boldsymbol{\\beta}_k$?\n\n## Deriving the additive log ratio model\n\n-   Using our specification of the probabilities, it can be seen that,\n\n\\begin{align*}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\\\\n&= \\left[\\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\right]\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}\\\\\n&= P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}.\n\\end{align*}\n\n$\\implies \\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots,K-1\\}$\n\n## Additive log ratio model\n\n-   If outcome $K$ is chosen as reference, the $K âˆ’ 1$ regression equations are:\n\n$$\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}$$\n\n-   This formulation is called **additive log ratio**.\n\n-   $\\beta_{jk}$ represent the **log-odds** of being in category $k$ relative to the baseline category $K$ with a one-unit change in $X_{ij}$.\n\n    -   $\\exp (\\beta_{jk})$ is an **odds ratio**.\n\n## Getting back to the likelihood\n\n-   The log-likelihood can be written as,\n\n$$\\log f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\sum_{j=1}^K \\delta_{ij} \\log P(Y_i = j).$$\n\n-   The $P(Y_i = j)$ are given by the additive log ratio model.\n\n-   As Bayesians, we only need to specify priors for $\\boldsymbol{\\beta}_k, k \\in \\{1,\\ldots,K-1\\}$.\n\n## Multinomial regression in Stan {.midi}\n\n-   Hard coding the likelihood.\n\n\n::: {.cell output.var='alr'}\n\n```{.stan .cell-code}\n// additive_log_ratio.stan\nfunctions {\n  matrix compute_alr_probs(int n, int K, int p, matrix X, matrix beta) {\n    matrix[n, K] probs;\n    matrix[n, K - 1] expXbeta = exp(X * beta);\n    for (i in 1:n) {\n      real sum_i = sum(expXbeta[i, ]);\n      for (j in 1:K) {\n        if (j < K) {\n          probs[i, j] = expXbeta[i, j] / (1 + sum_i);\n        }\n        if (j == K) probs[i, j] = 1 - sum(probs[i, 1:(K - 1)]);\n      }\n    }\n    return probs\n  }\n}\ndata {\n  int<lower = 1> K;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  array[n] int<lower = 1, upper = K> Y;\n  matrix[n, p] X;\n  matrix[n, K] delta;\n}\nparameters {\n  matrix[p, K - 1] beta;\n}\nmodel {\n  matrix[n, K] probs = compute_alr_probs(n, K, p, X, beta);\n  for (i in 1:n) {\n    for (j in 1:K) {\n      target += delta[i, j] * log(probs[i, j]);\n    }\n  }\n  target += normal_lpdf(to_vector(beta) | 0, 10);\n}\n```\n:::\n\n\n## Multinomial regression in Stan {.midi}\n\n-   Non-identifiable version.\n\n\n::: {.cell output.var='multi_bad'}\n\n```{.stan .cell-code}\n// multi_logit_bad.stan\ndata {\n  int<lower = 1> K;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  array[n] int<lower = 1, upper = K> Y;\n  matrix[n, p] X;\n}\nparameters {\n  matrix[p, K] beta;\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    target += categorical_logit_lpmf(Y[i] | Xbeta[i]')\n  }\n  target += normal_lpdf(to_vector(beta) | 0, 10);\n}\n```\n:::\n\n\n[categorical_logit](https://mc-stan.org/docs/functions-reference/bounded_discrete_distributions.html#categorical-logit-glm)\n\n## Multinomial regression in Stan {.midi}\n\n-   Zero identifiability constraint.\n\n\n::: {.cell output.var='multi'}\n\n```{.stan .cell-code}\n// multi_logit.stan\ndata {\n  int<lower = 1> K;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  array[n] int<lower = 1, upper = K> Y;\n  matrix[n, p] X;\n}\ntransformed data {\n  vector[p] zeros = rep_vector(0, p);\n}\nparameters {\n  matrix[p, K - 1] beta_raw;\n}\ntransformed parameters {\n  matrix[p, K] beta = append_col(beta_raw, zeros);\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    target += categorical_logit_lpmf(Y[i] | Xbeta[i]')\n  }\n  target += normal_lpdf(to_vector(beta_raw) | 0, 10);\n}\ngenerated quantities {\n  matrix[p, K - 1] ors = exp(beta_raw);\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = categorical_logit_rng(Xbeta[i]');\n    log_lik[i] = categorical_logit_lpmf(Y[i] | Xbeta[i]');\n  }\n}\n```\n:::\n\n\n## Ordinal regression\n\nLet $Y_i \\in \\{1,\\ldots,K\\}$ be an ordinal outcome with $K$ categories.\n\n-   The likelihood in ordinal regression is identical to the one from multinomial regression,\n\n$$f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}}.$$\n\n-   We need to add additional constraints that guarantee ordinality.\n\n## Proportional odds assumption\n\n-   $P(Y_i \\leq k)$ is the cumulative probability of $Y_i$ less than or equal to a specific category $k=1,\\ldots,K-1$.\n\n-   The odds of being less than or equal to a particular category can be defined as, $$\\frac{P(Y\\leq k)}{P(Y > k)} \\text { for } k=1,\\ldots,K-1.$$\n\n-   Not defined for $k = K$, since division by zero is not defined.\n\n## Proportional odds regression\n\nThe log odds can then be modeled as follows, $$\\log \\frac{P(Y_i\\leq k)}{P(Y_i > k)} = \\text{logit}P(Y_i\\leq k) = \\alpha_k - \\mathbf{x}_i \\boldsymbol{\\beta}$$\n\n-   Why $-\\boldsymbol{\\beta}$?\n\n-   $\\boldsymbol{\\beta}$ is a common regression parameter. For a one-unit increase in $x_{ij}$, $\\beta_j$ is the change in log odds of moving to a more severe level of the outcome $Y_i$.\n\n-   $\\alpha_k$ for $k = 1,\\ldots,K-1$ are $k$-specific intercepts that corresponds to the log odds of moving from level $k$ to $k+1$.\n\n## Understanding the probabilities\n\n-   One can solve for $P(Y_i \\leq k), \\quad k = 1,\\ldots,K-1$),\n\n$$P(Y_i \\leq k) = \\frac{\\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}}{1 + \\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}} = \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}).$$\n\n-   $P(Y_i \\leq K) = 1$.\n\nThe individual probabilities are then given by,\n\n\\begin{align*}\nP(Y_i = k) &= P(Y_i \\leq k) - P(Y_i \\leq k-1)\\\\\n&= \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(\\alpha_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n\\end{align*}\n\n## A latent variable representation\n\n-   Define a latent variable,\n\n$$Y_i^* = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim \\text{Logistic}(0, 1).$$\n\n-   $\\mathbb{E}[\\epsilon_i] = 0$.\n\n-   $\\mathbb{V}(\\epsilon_i) = \\pi^2/3$.\n\n-   CDF: $P(\\epsilon_i \\leq x) = \\frac{1}{1 + \\exp\\{-x\\}} = \\frac{\\exp\\{x\\}}{1 + \\exp\\{x\\}} = \\text{expit}(x).$\n\n## Visualizing the latent process\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-multiclass_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Adding thresholds ($c_k$)\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-multiclass_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Getting category probabilities\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-multiclass_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## A latent variable representation\n\n-   Define a set of $K-1$ cut-points, $(c_1,\\ldots,c_{K-1}) \\in \\mathbb{R}^{K-1}$. We also define $c_0 = -\\infty, c_K = \\infty$.\n\n-   Our ordinal random variable can be generated as,\n\n$$Y_i = \\left\\{\n\\begin{matrix*}[l]\n1 & c_0 < Y_i^* \\leq c_1\\\\\n2 & c_1 < Y_i^* \\leq c_2\\\\\n\\vdots & \\\\\nK & c_{K-1} < Y_i^* \\leq c_K\\\\\n\\end{matrix*}\n\\right.$$\n\n## Equivalence of the two specifications\n\n-   Probabilities under the latent specification: \\begin{align*}\n    P(Y_i = k) &= P(c_{k-1} < Y_i^* \\leq c_k)\\\\\n    &= P(c_{k-1} < \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k)\\\\\n    &= P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k) - P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i < c_{k-1})\\\\\n    &= P(\\epsilon_i \\leq c_k - \\mathbf{x}_i \\boldsymbol{\\beta}) - P(\\epsilon_i < c_{k-1} - \\mathbf{x}_i \\boldsymbol{\\beta})\\\\\n    &= \\text{expit}(c_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(c_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n    \\end{align*}\n\n-   Equivalency:\n\n    -   $\\alpha_k = c_k, \\quad k = 1,\\ldots, K-1$, assuming that $\\alpha_k < \\alpha_{k+1}$.\n\n## Ordinal regression using Stan\n\n\n::: {.cell output.var='ordinal'}\n\n```{.stan .cell-code}\n// ordinal.stan\ndata {\n  int<lower = 2> K;\n  int<lower = 0> n;\n  int<lower = 1> p;\n  int<lower = 1, upper = K> Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  ordered[K - 1] alpha;\n}\nmodel {\n  target += ordered_logistic_glm_lpmf(Y | X, beta, alpha);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(alpha | 0, 10);\n}\ngenerated quantities {\n  vector[p] ors = exp(beta);\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = ordered_logistic_rng(X[i, ] * beta, alpha);\n    log_lik[i] = ordered_logistic_glm_lpmf(Y[i] | X[i, ], beta, alpha);\n  }\n}\n```\n:::\n\n\n[ordered_logistic_glm_lpmf](https://mc-stan.org/docs/functions-reference/bounded_discrete_distributions.html#ordered-logistic-generalized-linear-model-ordinal-regression)\n\n## Enforcing order in the cutoffs\n\n-   In Stan, when you define a parameter as `ordered[K-1] alpha;`, the values of `alpha` are automatically constrained to be strictly increasing.\n\n-   This transformation ensures that the `alpha` values follow the required order, i.e., `alpha[1] < alpha[2] < ... < alpha[K-1]`.\n\n-   Stan doesn't sample `alpha` directly but instead works with an **unconstrained** parameter vector, which we will call `gamma`.\n\n## Enforcing order in the cutoffs\n\n\n::: {.cell output.var='order'}\n\n```{.stan .cell-code}\nparameters {\n  vector[K - 1] gamma;\n}\ntransformed parameters {\n  vector[K - 1] alpha;\n  alpha[1] = gamma[1];\n  for (j in 2:K) {\n    alpha[j] = alpha[j - 1] + exp(gamma[j]);\n  }\n}\n```\n:::\n\n\n-   Here, `gamma` represents a vector of independent, unconstrained variables, and the transformation ensures that `alpha` is strictly increasing by construction.\n\n-   Luckily we can use [ordered](https://mc-stan.org/docs/reference-manual/transforms.html#ordered-vector), since Stan takes care of this (including the Jacobian) in the background.\n\n## Prepare for next class\n\n-   Work on [HW 03](https://biostat725-sp26.netlify.app/hw/hw-03).\n\n-   Complete reading to prepare for next Tuesday's lecture\n\n-   Tuesday's lecture: Hierarchical models\n",
    "supporting": [
      "13-multiclass_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}