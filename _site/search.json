[
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you‚Äôre having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you‚Äôll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it‚Äôs been resolved. If there‚Äôs a deadline coming up soon, post on the course forum to let us know that there‚Äôs an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don‚Äôt anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you‚Äôve tried and the errors you see (including verbatim errors and/or screenshots).",
    "crumbs": [
      "Computing",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\nüîó for Duke Container Manager\n\n\nCourse GitHub organization\nüîó for GitHub\n\n\nCourse Canvas site\nüîó for Canvas\n\n\nAssignment submission\nüîó to Gradescope\n\n\nZoom links\nüîó on Canvas",
    "crumbs": [
      "Useful links"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA602 on the Reservations available menu on the right. You only need to do this once, and when you do, you‚Äôll see this container moved to the My reservations menu on the left. We are sharing a container with STA 602 this semester!\nNext, click on STA602 under My reservations to access the RStudio instance you‚Äôll use for the course.",
    "crumbs": [
      "Computing",
      "Access"
    ]
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#lectures",
    "href": "support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#office-hours",
    "href": "support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!\nMake a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone‚Äôs office hours here.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#ed-discussion",
    "href": "support.html#ed-discussion",
    "title": "Course support",
    "section": "Ed Discussion",
    "text": "Ed Discussion\nOutside of class and office hours, any general questions about course content or assignments should be posted on Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters that are not appropriate for the class discussion forum (e.g.¬†illness, accommodations, etc.), you may email me at sib2@duke.edu. If you email me, please include ‚ÄúBIOSTAT 725‚Äù in the subject line. Barring extenuating circumstances, I will respond to BIOSTAT 725 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#mental-health-and-wellness",
    "href": "support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\n\nDukeReach: Provides comprehensive outreach services to identify and support students in managing all aspects of well being. If you have concerns about a student‚Äôs behavior or health visit the website for resources and assistance. Go to studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS): CAPS services include individual, group, and couples counseling services, health coaching, psychiatric services, and workshops and discussions. (919) 660-1000 or students.duke.edu/wellness/caps\nTimelyCare (formerly known as Blue Devils Care): An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#technology-accommodations",
    "href": "support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\n\nWhile we encourage students to use their own laptops for work in this course, we will provide the opportunity to use Duke‚Äôs computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#course-materials-costs",
    "href": "support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#assistance-with-zoom-or-canvas",
    "href": "support.html#assistance-with-zoom-or-canvas",
    "title": "Course support",
    "section": "Assistance with Zoom or Canvas",
    "text": "Assistance with Zoom or Canvas\nFor technical help with Canvas or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Canvas here.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "hw/hw-01-blank.html",
    "href": "hw/hw-01-blank.html",
    "title": "HW 01",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-04-blank.html",
    "href": "hw/hw-04-blank.html",
    "title": "HW 04",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-05-blank.html",
    "href": "hw/hw-05-blank.html",
    "title": "HW 05",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "This course will teach students how to analyze biomedical data from a Bayesian inference perspective with a strong emphasis on using real-world data, including electronic health records, wearables, and imaging data. The course will begin by introducing the machinery of Bayesian statistics through the lens of linear regression, giving enough context for students with no prior experience with Bayesian statistics. A history of computational approaches used in Bayesian statistics will be given before ultimately landing on Stan, a state-of-the-art probabilistic programming language that makes Bayesian inference accessible as a viable data science tool. The course will then branch out from regression and introduce Bayesian versions of machine learning tools, including regularization and classification. The course will then emphasize Bayesian hierarchical models, including Gaussian process models for temporal and spatial data; and clustering. Additional topics may be discussed from the Bayesian perspective, including causal inference, and meta-analysis. While an applied course, the methods will be introduced from a mathematical perspective, allowing students to obtain a fundamental understanding of the introduced models. Students will learn computational skills for implementing Bayesian models using R and Stan. By the end of this course, students will be well-equipped to tackle complex problems in biomedical research using Bayesian inference.\n\n\nBIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#pre-requisites",
    "href": "overview.html#pre-requisites",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "BIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#teaching-assistants",
    "href": "overview.html#teaching-assistants",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\nName\nRole\nOffice Hours\nLocation\n\n\nChristine Shen\nTA\nMon 1:30 - 3:30pm\nOld Chemistry 203A\n\n\nJiang Shu\nTA\nWed 4:45 ‚Äì 6:45pm\nHock 10092",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "hw/hw-03-blank.html",
    "href": "hw/hw-03-blank.html",
    "title": "HW 03",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-02-blank.html",
    "href": "hw/hw-02-blank.html",
    "title": "HW 02",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-00-blank.html",
    "href": "hw/hw-00-blank.html",
    "title": "HW 00",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "Below are freely available resources to learn or review the following in R: data wrangling, data visualization, Quarto basics.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#in-depth-introduction",
    "href": "computing-r-resources.html#in-depth-introduction",
    "title": "Resources for learning R",
    "section": "In-depth introduction",
    "text": "In-depth introduction\nCoursera: Data Visualization and Transformation with R by Mine √áetinkaya-Rundel and Elijah Meyer\n\nIncludes videos, readings, practice exercise, quizzes, and other resources\nYou can select content within the modules you want to complete.\nFocus on Modules 2 and 3. Review the content in Module 1 as needed.\nClick here for instructions to register for Coursera for free as a Duke student",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#in-depth-review",
    "href": "computing-r-resources.html#in-depth-review",
    "title": "Resources for learning R",
    "section": "In-depth review",
    "text": "In-depth review\nData Science with R videos by Mine √áetinkaya-Rundel and Elijah Meyer\n\nVideos from the data science Coursera course\nFocus on videos on visualizing and summarizing data\nYou need to join the Coursera course to access the files from the code along videos.\n\nLearn R: An interactive introduction to data analysis with R\n\nHands-on tutorial that can be completed within the site (no RStudio required)\nFocus on Chapters 4 - 6",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#shorter-review",
    "href": "computing-r-resources.html#shorter-review",
    "title": "Resources for learning R",
    "section": "Shorter review",
    "text": "Shorter review\nR for Data Science (2nd ed) by Hadley Wickham, Mine √áetinkaya-Rundel, and Garrett Grolemund\n\nFocus on Chapters 1 - 3, 10",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nTidy Modeling with R by Max Kuhn & Julia Silge\nPosit Cheatsheets\nR workshops by Duke Center for Data and Visualization Sciences",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#stan-resources",
    "href": "computing-r-resources.html#stan-resources",
    "title": "Resources for learning R",
    "section": "Stan resources",
    "text": "Stan resources\n\nRStan Getting Started by Stan Development Team\nBrief Introduction to Stan by Mark Lai",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\ntopic\nprepare\nslides\nae\nhw\nnotes\n\n\n\n\n1\nTh\nJan 8\nWelcome + What Is Bayesian Health Data Science?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW00 assigned\n\n\n2\nTu\nJan 13\nMonte Carlo Sampling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 15\nMarkov Chain Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 01 assigned, HW00 due\n\n\n3\nTu\nJan 20\nProbabilistic Programming (Intro to Stan!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 22\nPriors, Posteriors, and PPDs!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\nTu\nJan 27\nModel Checking\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 29\nModel Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 02 assigned, HW 01 due\n\n\n5\nTu\nFeb 3\nBayesian Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 5\nNonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\nTu\nFeb 10\nRobust Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 12\nRegularization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 03 assigned, HW 02 due\n\n\n7\nTu\nFeb 17\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 19\nMulticlass Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\nTu\nFeb 24\nHierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 26\nLongitudinal Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExam 01 assigned, HW03 due\n\n\n9\nTu\nMar 3\nExam 1 Office Hours\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 5\nGaussian Processes\n\n\n\n\n\n\n\n\n\n\n\n\nHW04 assigned, Exam 01 due\n\n\n10\nTu\nMar 10\nNO CLASS: Spring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 12\nNO CLASS: Spring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\nTu\nMar 17\nGeospatial Modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 19\nScalable Gaussian Processes #1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\nTu\nMar 24\nScalable Gaussian Processes #2\n\n\n\n\n\n\n\n\n\n\n\n\nHW 05 assigned, HW04 due\n\n\n\nTh\nMar 26\nDisease Mapping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\nTu\nMar 31\nMissing Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nApr 2\nBayesian Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\nTu\nApr 7\nTBD Topic/Live-Coding Review\n\n\n\n\n\n\n\n\n\n\n\n\nHW 05 due, Exam 02 assigned\n\n\n\nTh\nApr 9\nLive-Coding Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\nTu\nApr 14\nExam 02 Office Hours\n\n\n\n\n\n\n\n\n\n\n\n\nExam 02 due for feedback\n\n\nExam period\nTu\nApr 29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExam 02 due by 5:00pm",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "BIOSTAT 725 Syllabus",
    "section": "",
    "text": "Lecture\nTue & Thu 11:45am - 1pm\nHock 10089\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nRole\nOffice Hours\nLocation\n\n\n\n\nProf.¬†Sam Berchuck\nInstructor\nMon 4-6pm\nor by appointment\nHock 9028\n\n\nChristine Shen\nTA\n\nMon 1:30 - 3:30pm\nOld Chemistry 203A\n\n\nJiang Shu\nTA\n\nWed 4:45 ‚Äì 6:45pm\nHock 10092",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "BIOSTAT 725 Syllabus",
    "section": "",
    "text": "Lecture\nTue & Thu 11:45am - 1pm\nHock 10089\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nRole\nOffice Hours\nLocation\n\n\n\n\nProf.¬†Sam Berchuck\nInstructor\nMon 4-6pm\nor by appointment\nHock 9028\n\n\nChristine Shen\nTA\n\nMon 1:30 - 3:30pm\nOld Chemistry 203A\n\n\nJiang Shu\nTA\n\nWed 4:45 ‚Äì 6:45pm\nHock 10092",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course description",
    "text": "Course description\nThis course will teach students how to analyze biomedical data from a Bayesian inference perspective with a strong emphasis on using real-world data, including electronic health records, wearables, and imaging data. The course will begin by introducing the machinery of Bayesian statistics through the lens of linear regression, giving enough context for students with no prior experience with Bayesian statistics. A history of computational approaches used in Bayesian statistics will be given before ultimately landing on Stan, a state-of-the-art probabilistic programming language that makes Bayesian inference accessible as a viable data science tool. The course will then branch out from regression and introduce Bayesian versions of machine learning tools, including regularization and classification. The course will then emphasize Bayesian hierarchical models, including Gaussian process models for temporal and spatial data; and clustering. Additional topics may be discussed from the Bayesian perspective, including causal inference, and meta-analysis. While an applied course, the methods will be introduced from a mathematical perspective, allowing students to obtain a fundamental understanding of the introduced models. Students will learn computational skills for implementing Bayesian models using R and Stan. By the end of this course, students will be well-equipped to tackle complex problems in biomedical research using Bayesian inference.\n\nPrerequisites\nBIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nunderstand fundamental concepts of Bayesian statistics, including prior and posterior, and predictive distributions,\nimplement the Bayesian workflow, including model building, checking, and refinement,\nuse probabilistic programming software for Bayesian analysis (e.g., Stan),\napply Bayesian techniques to real-world health data,\ncommunicate Bayesian analysis results effectively to both technical and non-technical audiences, and\nidentify opportunities for using Bayesian statistics in your research and/or job.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course materials",
    "text": "Course materials\nWhile there is no official textbook for the course; readings will primarily be made available as they are assigned. We will use the statistical software R. Students will be encourage to download the required software on their own laptops. As a courtesy, students will also be able to access R and the required software through Docker containers provided by Duke Office of Information Technology. See the computing page for more information.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course community",
    "text": "Course community\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke‚Äôs Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean or director of graduate studies (DGS) are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nPronouns\nPronouns are meaningful tools to communicate identities and experiences, and using pronouns supports a campus environment where all community members can thrive. Please update your gender pronouns in Duke Hub. You can find instructions to do so here. You can learn more at the Center for Sexual and Gender Diversity‚Äôs website.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, biostat725-sp26.netlify.app.\nLinks to Zoom meetings may be found in Canvas. Periodic announcements will be sent via email and Canvas Announcements. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nEmail\nIf you have questions about assignment extensions, accommodations, or any other matter, please email me directly at sib2@duke.edu. If you email me, please include ‚ÄúBIOSTAT 725‚Äù in the subject line. Barring extenuating circumstances, I will respond to BIOSTAT 725 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#five-tips-for-success",
    "href": "syllabus.html#five-tips-for-success",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success in this course depends very much on you and the effort you put into it. Your TA(s) and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TA, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you‚Äôre not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings and other preparation work.\nDo the homeworks. The earlier you start, the better. It‚Äôs not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example).\nDon‚Äôt procrastinate. The content builds upon what was taught in previous weeks, so if something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, etc. Don‚Äôt let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours and work with a member of the teaching team to help you identify a good (re)starting point.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#getting-help-in-the-course",
    "href": "syllabus.html#getting-help-in-the-course",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Getting help in the course",
    "text": "Getting help in the course\n\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours1 to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them! \n\nCheck out the Support page for more resources.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#what-to-expect-in-the-course",
    "href": "syllabus.html#what-to-expect-in-the-course",
    "title": "BIOSTAT 725 Syllabus",
    "section": "What to expect in the course",
    "text": "What to expect in the course\n\nLectures\nLectures are designed to be interactive, so you gain experience applying new concepts and learning from each other. My role as instructor is to introduce you to new methods, tools, and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities during the lectures. You are expected to prepare for class by completing assigned readings, attend all lecture sessions, and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded based on completing what we do in class.\nYou are expected to bring a laptop, tablet, or any device with internet and a keyboard to each class so that you can participate in the in-class exercises. Please make sure your device is fully charged before you come to class, as the number of outlets in the classroom will not be sufficient to accommodate everyone.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\nYou will be assessed based on four components: homework, exams, live coding, and application exercises.\n\nHomework\nIn homework, you will apply what you‚Äôve learned during lecture to complete data analysis tasks and explain the underlying mathematics, with a focus on the computation and communication. Homework assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and submitted as a PDF for grading in Gradescope. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. HW0 will not be graded for credit.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams\nThere will be two exams in this course. Each exam will be an open-note take-home assessment. Through these exams you have the opportunity to demonstrate what you‚Äôve learned in the course thus far. The exams will focus on both conceptual understanding of the applied and mathematical content and application through analysis and computational tasks. The exams will be based on content in reading assignments, lectures, application exercises, and homework assignments. More detail about the exams will be given during the semester.\n\n\nLive Coding\nThere will be an in-class live coding evaluation in this course. The exercise will take place on Thursday, April 9, during the regularly scheduled class period, and will account for 10% of the final course grade. Students will work independently in a real-time setting to implement and analyze a Bayesian model using Stan, drawing on concepts from lectures and course assignments. This will be an open-book exercise: students may use course materials and external online resources, but may not use AI tools to generate written explanations or narrative interpretations (AI may be used for coding and technical assistance only). The goal of the exercise is to assess students‚Äô ability to apply Bayesian modeling concepts and work effectively in Stan under realistic analytical conditions. Materials will be distributed at the start of the session via a Github repo. Students who cannot attend on this date must contact the instructor in advance to arrange an alternative. More information about the project will be provided during the semester.\n\n\nApplication exercises\nYou will get the most out of the course if you actively participate in class. Parts of some lectures will be dedicated to working on Application Exercises (AEs). AEs are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59p ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59p ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nAEs will be graded based on making a good-faith effort to attempt all questions covered in class. You are welcome to, but not required, to work on AEs beyond lecture.\nSuccessful on-time effort on at least 80% of AEs will result in full credit for AEs in the final course grade.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n40%\n\n\nExam 01\n20%\n\n\nExam 02\n20%\n\n\nLive Coding\n10%\n\n\nApplication exercises\n10%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nDuke Community Standard\nAll students must adhere to the Duke Community Standard (DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard, students agree:\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;and\nI will act if the Standard is compromised.\n\n\n\n\n\n\nAcademic honesty\nTL;DR: Don‚Äôt cheat!\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what‚Äôs the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nYou may not discuss or otherwise work with others on the exams. Unauthorized collaboration or using unauthorized materials will be considered a violation for all students involved. More details will be given closer to the exam date.\n\n\n\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g.¬†StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:2 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate‚Äîrather than hinder‚Äîlearning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\nAI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\nNo AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments. In general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask a member of the teaching team.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including academic integrity (e.g., completing one‚Äôs own work, following proper citation of sources, adhering to guidance around group work projects,and more). Ignoring these requirements is a violation of the Duke Community Standard. Any questions and/or concerns regarding academic integrity can be directed to the Office of Student Conduct and Community Standards at conduct@duke.edu.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback in a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework assignment will be dropped to accommodate such circumstances.\n\nHomework may be submitted up to 2 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThe late work policy for exams will be provided with the exam instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a homework assignment by the stated due date, you may email me at sib2@duke.edu before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your director of graduate studies (DGS) know, as they can be a resource. Please let me know if you need help contacting your DGS.\n\n\nRegrade Requests\nRegrade requests must be submitted on Gradescope within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the last day of classes.\n\n\nAttendance policy\nEvery student is expected to attend and participate in lecture. There may be times, however, when you cannot attend class. Lecture recordings will be made available upon request for students who have an excused absence. If you miss a lecture, make sure to review the material and complete the application exercise, if applicable, before the next lecture.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\n\nAcademic accommodations\nIf you need accommodations for this class, you will need to register with the Student Disability Access Office (SDAO) and provide them with documentation related to your needs. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#academic-and-wellness-support",
    "href": "syllabus.html#academic-and-wellness-support",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Academic and wellness support",
    "text": "Academic and wellness support\n\n\n\nCAPS\nDuke Counseling & Psychological Services (CAPS) helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJanuary 7: Classes begin\nJanuary 19: MLK Jr Day. No classes\nJanuary 21: Drop/Add ends\nMarch 7 - 15: Spring Break. No classes\nApril 15: Graduate classes end.\nApril 16: Reading period begins (no classes or projects are due during the reading period).\nApril 26: Reading period ends.\nApril 27 - May 2: Final exam period\n\nClick here for the full Duke academic calendar.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOffice hours are times the teaching team set aside each week to meet with students. Click here to learn more about how to effectively use office hours.‚Ü©Ô∏é\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.‚Ü©Ô∏é‚Ü©Ô∏é",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-berchuck",
    "href": "slides/01-welcome.html#meet-prof.-berchuck",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Meet Prof.¬†Berchuck!",
    "text": "Meet Prof.¬†Berchuck!\n\nEducation and career journey\n\nBS in Statistical Science from Duke University\nPhD in Biostatistics from University of North Carolina - Chapel Hill\nPostdoc in Duke Forge: Duke‚Äôs Center for Actionable Health Data Science\nNIH/NEI Pathway to Independence Fellow (K99/R00)\nAssistant Professor, Department of Biostatistics & Bioinformatics and Statistical Science at Duke; Faculty Affiliate of Duke AI Health\n\nWork focuses on developing data science tools to improve patient experience using biomedical data (including EHR)\nDad of 5 and 7 year old daughters üôÇ"
  },
  {
    "objectID": "slides/01-welcome.html#teaching-assistants-tas",
    "href": "slides/01-welcome.html#teaching-assistants-tas",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Teaching Assistants (TAs)",
    "text": "Teaching Assistants (TAs)\n\nChristine Shen\n\n5th year PhD student in the Department of Statistical Science at Duke\n\nJiang Shu\n\n1st year PhD student in the Department of Biostatistics & Bioinformatics at Duke"
  },
  {
    "objectID": "slides/01-welcome.html#check-in-on-ed-discussion",
    "href": "slides/01-welcome.html#check-in-on-ed-discussion",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Check-in on Ed Discussion!",
    "text": "Check-in on Ed Discussion!\n\nClick on the link or scan the QR code to answer the Ed Discussion poll\nhttps://forms.office.com/r/QTuhmC0wM3"
  },
  {
    "objectID": "slides/01-welcome.html#topics",
    "href": "slides/01-welcome.html#topics",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Topics",
    "text": "Topics\n\nIntroduction to the course\nSyllabus activity\nReview of probability"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-bayesian-health-data-science",
    "href": "slides/01-welcome.html#what-is-bayesian-health-data-science",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is Bayesian Health Data Science?",
    "text": "What is Bayesian Health Data Science?\n\n\nBayesian Health Data Science involves using Bayesian methods to analyze health data, which can include electronic health records (EHR), clinical trial data, and other health-related datasets. These methods are model-based and can appropriately quantify and propagate uncertainty, making them suitable for tackling challenges in health research.\n\nSource: ChatGPT"
  },
  {
    "objectID": "slides/01-welcome.html#why-data-science",
    "href": "slides/01-welcome.html#why-data-science",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Why Data Science?",
    "text": "Why Data Science?\n\nStatistics versus Data Science?\nIntroductory Bayesian statistics courses are often very mathematical and involve intense computation; thus Bayesian methods are not as frequently used in applied settings.\nModern software now exists to lower the mathematical burden and computational intensity of Bayesian statistics, but courses do not reflect this.\nThis course focuses on teaching students Bayesian statistics as a tool for research; or anywhere data science is practiced."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-biostat-725",
    "href": "slides/01-welcome.html#what-is-biostat-725",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is BIOSTAT 725?",
    "text": "What is BIOSTAT 725?\n\n\n\n\n\n Bayes \n\nModeling\n\n\n\n\n+\n\n\n\n\n\n Stan\n\nProbabilistic Programming\n\n\n\nPrerequisites: BIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission."
  },
  {
    "objectID": "slides/01-welcome.html#course-learning-objectives",
    "href": "slides/01-welcome.html#course-learning-objectives",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nunderstand fundamental concepts of Bayesian statistics, including prior and posterior, and predictive distributions,\nimplement the Bayesian workflow, including model building, checking, and refinement,\nuse probabilistic programming software for Bayesian analysis (e.g., Stan),\napply Bayesian techniques to real-world health data,\ncommunicate Bayesian analysis results effectively to both technical and non-technical audiences, and\nidentify opportunities for using Bayesian statistics in your research and/or job."
  },
  {
    "objectID": "slides/01-welcome.html#course-topics",
    "href": "slides/01-welcome.html#course-topics",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course topics",
    "text": "Course topics\n\n\n\nLinear regression\n\nMethods for inference\nPrior elicitation\nPosterior estimation\nUncertainty quantification\nModel assessment\nBayesian workflow\nPrediction\n\n\n\nHealth Datasets\n\n\n\nExtensions\n\nRobust regression\nRegularization\nClassification\nMissing data\n\n\n\nHierarchical Model\n\nGaussian processes\nLongitudinal data\nSpatial data"
  },
  {
    "objectID": "slides/01-welcome.html#course-toolkit",
    "href": "slides/01-welcome.html#course-toolkit",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nWebsite: https://biostat725-sp26.netlify.app/\n\nCentral hub for the course!\nTour of the website\n\nCanvas: https://canvas.duke.edu/courses/75433\n\nGradebook\nAnnouncements\nGradescope\n\nGitHub: github.com/biostat725-sp26\n\nDistribute assignments\nPlatform for version control and collaboration"
  },
  {
    "objectID": "slides/01-welcome.html#computing-toolkit",
    "href": "slides/01-welcome.html#computing-toolkit",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nInference using Stan, a probabilistic programming language (rstan)\nWrite reproducible reports in Quarto\nAccess RStudio through STA725 (STA602) Docker Containers\n\n\n\n\n\n\n\n\n\n\n\n\nAccess assignments\nFacilitates version control and collaboration\nAll work in BIOSTAT 725 course organization"
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity",
    "href": "slides/01-welcome.html#syllabus-activity",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity",
    "text": "Syllabus activity\n\n\nIntroduce yourself to your group members.\nChoose a reporter. This person will share the group‚Äôs summary with the class.\nRead the portion of the syllabus assigned to your group.\nDiscuss the key points and questions you my have.\nThe reporter will share a summary with the class."
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity-assignments",
    "href": "slides/01-welcome.html#syllabus-activity-assignments",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity assignments",
    "text": "Syllabus activity assignments\n\nGroup 1: What to expect in the course\nGroup 2: Homework and Exams\nGroup 3: Live Coding and Application Exercises\nGroup 4: Academic honesty (except AI policy)\nGroup 5: Artificial intelligence policy\nGroup 6: Late work policy and waiver for extenuating circumstances\nGroup 7: Regrade requests and attendance policy and Getting help in the course"
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity-report-out",
    "href": "slides/01-welcome.html#syllabus-activity-report-out",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity report out",
    "text": "Syllabus activity report out\n\nGroup 1: What to expect in the course\nGroup 2: Homework and Exams\nGroup 3: Live Coding and Application Exercises\nGroup 4: Academic honesty (except AI policy)\nGroup 5: Artificial intelligence policy\nGroup 6: Late work policy and waiver for extenuating circumstances\nGroup 7: Regrade requests and attendance policy and Getting help in the course"
  },
  {
    "objectID": "slides/01-welcome.html#grading",
    "href": "slides/01-welcome.html#grading",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n40%\n\n\nExam 01\n20%\n\n\nExam 02\n20%\n\n\nLive Coding\n10%\n\n\nApplication Exercises\n10%\n\n\nTotal\n100%"
  },
  {
    "objectID": "slides/01-welcome.html#five-tips-for-success-in-biostat-725",
    "href": "slides/01-welcome.html#five-tips-for-success-in-biostat-725",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Five tips for success in BIOSTAT 725",
    "text": "Five tips for success in BIOSTAT 725\n\nComplete all the preparation work before class.\nAsk questions in class, and office hours.\nDo the homework; get started on homework early when possible.\nDon‚Äôt procrastinate and don‚Äôt let a week pass by with lingering questions.\nStay up-to-date on announcements on Canvas and sent via email."
  },
  {
    "objectID": "slides/01-welcome.html#review-of-probability",
    "href": "slides/01-welcome.html#review-of-probability",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Review of probability",
    "text": "Review of probability\n\nThe goal of Bayesian statistics is to compute the posterior distribution (i.e., the uncertainty distribution of the parameters, \\(\\boldsymbol{\\theta}\\), after observing the data, \\(\\mathbf{Y}\\)).\nThis is the conditional distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{Y}\\).\nTherefore, we need to review the probability concepts that lead to the conditional distribution of one variable conditioned on another.\n\nProbability mass (pmf) and density (pdf) functions\nJoint distributions\nMarginal and conditional distributions"
  },
  {
    "objectID": "slides/01-welcome.html#random-variables",
    "href": "slides/01-welcome.html#random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Random variables",
    "text": "Random variables\n\n\\(X\\) (capital) is a random variable.\nWe want to compute the probability that \\(X\\) takes on a specific value \\(x\\) (lowercase).\n\nThis is denoted \\(P(X = x)\\).\n\nWe also might want to compute the probability of \\(X\\) being in a set \\(\\mathcal A\\).\n\nThis is denoted \\(P(X \\in \\mathcal A)\\).\n\nThe set of possible values that \\(X\\) can take on is called its support, \\(\\mathcal S\\)."
  },
  {
    "objectID": "slides/01-welcome.html#random-variables---example",
    "href": "slides/01-welcome.html#random-variables---example",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Random variables - example",
    "text": "Random variables - example\n\nExample 1: \\(X\\) is the roll of a die.\n\nThe support is \\(\\mathcal S = \\{1, 2, 3, 4, 5, 6\\}\\).\n\\(P(X = 1) = 1/6\\).\n\nExample 2: \\(X\\) is a newborn baby‚Äôs weight.\n\nThe support is \\(\\mathcal S = (0, \\infty)\\).\n\\(P(X \\in [0, \\infty]) = 1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-probability",
    "href": "slides/01-welcome.html#what-is-probability",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is probability?",
    "text": "What is probability?\nObjective (associated with frequentist)\n\n\\(P(X = x)\\) as a purely mathematical statement.\nIf we repeatedly sampled \\(X\\), then the proportion of draws equal to \\(x\\) converges to \\(P(X = x)\\).\n\nSubjective (associated with Bayesian)\n\n\\(P(X = x)\\) represents an individual‚Äôs degree of belief.\nOften quantified as the amount an individual would be willing to wager that \\(X\\) will be \\(x\\).\n\nA Bayesian analysis makes use of both of these concepts."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-uncertainty",
    "href": "slides/01-welcome.html#what-is-uncertainty",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is uncertainty?",
    "text": "What is uncertainty?\nAleatoric uncertainty (likelihood)\n\nUncontrollable randomness in the experiment.\nFor example, the results of a fair coin flip can never be predicted with certainty.\n\nEpistemic uncertainty (prior/posterior)\n\nUncertainty about a quantity that could theoretically be known.\nFor example, if we flipped a coin infinitely-many times we could know the true probability of a head.\n\nA Bayesian analysis makes use of both of these concepts."
  },
  {
    "objectID": "slides/01-welcome.html#univariate-distributions",
    "href": "slides/01-welcome.html#univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables.\nThe random variable \\(X\\) is discrete if its support \\(\\mathcal S\\) is countable.\nExamples:\n\n\\(X \\in \\{0, 1, 2, 3\\}\\) is the number of successes in 3 trials.\n\\(X \\in \\{0, 1, 2, \\ldots\\}\\) is the number of patients with COVID in Durham County."
  },
  {
    "objectID": "slides/01-welcome.html#univariate-distributions-1",
    "href": "slides/01-welcome.html#univariate-distributions-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables.\nThe random variable \\(X\\) is continuous if its support \\(\\mathcal S\\) is uncountable.\nExamples with \\(\\mathcal S = (0, \\infty)\\):\n\n\\(X &gt; 0\\) is systolic blood pressure.\n\\(X &gt; 0\\) is a patient‚Äôs BMI."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-univariate-distributions",
    "href": "slides/01-welcome.html#discrete-univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete univariate distributions",
    "text": "Discrete univariate distributions\n\nIf \\(X\\) is discrete we describe its distribution with its probability mass function (pmf).\nThe pmf is \\(f(x) = P(X = x)\\).\nThe domain of \\(X\\) is the set of \\(x\\) with \\(f(x) &gt; 0\\).\nWe must have \\(f(x) \\geq 0\\) and \\(\\sum_x f(x) = 1\\).\nThe mean is \\(\\mathbb E[X] = \\sum_x x f(x)\\).\nThe variance is \\(\\mathbb V(X) = \\sum_x(x ‚àí \\mathbb E[X])^2f(x)\\).\nThe last three sums are over \\(X\\)‚Äôs domain."
  },
  {
    "objectID": "slides/01-welcome.html#parametric-families-of-distributions",
    "href": "slides/01-welcome.html#parametric-families-of-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Parametric families of distributions",
    "text": "Parametric families of distributions\n\nA statistical analysis typically proceeds by selecting a pmf that seems to match the distribution of a sample.\nWe rarely know the pmf exactly, but we assume it is from a parametric family of distributions.\nFor example, Binomial(10, 0.5) and Binomial(4, 0.1) are different but both from the binomial family.\nA family of distributions have the same equation for the pmf but differ by some unknown parameters \\(\\boldsymbol{\\theta}\\).\nWe must estimate these parameters."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-univariate-distributions",
    "href": "slides/01-welcome.html#continuous-univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nIf \\(X\\) is continuous we describe its distribution with the probability density function (pdf) \\(f(x) \\geq 0\\).\nSince there are uncountably many possible values, \\(P(X = x) = 0\\) for all \\(x\\).\nProbabilities are computed as areas under the pdf curve \\[P(a &lt; X &lt; b) = \\int_a^b f(x)dx.\\]\nTherefore, to be valid \\(f(x)\\) must satisfy \\(f(x) \\geq 0\\) and \\[P(‚àí\\infty &lt; X &lt; \\infty) = \\int_{-\\infty}^{\\infty} f(x)dx = 1.\\]"
  },
  {
    "objectID": "slides/01-welcome.html#continuous-univariate-distributions-1",
    "href": "slides/01-welcome.html#continuous-univariate-distributions-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nThe domain is the set of \\(x\\) values with \\(f(x) &gt; 0\\).\nThe mean and the variance are defined similarly to the discrete case but with the sums replaced by integrals.\nThe mean is \\(\\mathbb E[X] = \\int x f(x)dx\\).\nThe variance is \\(\\mathbb V(X) = \\int (x ‚àí \\mathbb E[X])^2 f(x)dx\\)."
  },
  {
    "objectID": "slides/01-welcome.html#joint-distributions",
    "href": "slides/01-welcome.html#joint-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Joint distributions",
    "text": "Joint distributions\n\n\\(\\mathbf{X} = (X_1, \\ldots, X_p)\\) is a random vector (vectors and matrices should be in bold).\nFor notational convenience, let‚Äôs consider only \\(p = 2\\) random variables \\(X\\) and \\(Y\\).\n\\((X, Y)\\) is discrete if it can take on a countable number of values, such as:\n\n\\(X\\) = number of hearts and \\(Y\\) = number of face cards.\n\n\\((X, Y)\\) is continuous if it can take on an uncountable number of values, such as:\n\n\\(X\\) = birthweight and \\(Y\\) = gestational age."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables",
    "href": "slides/01-welcome.html#discrete-random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe joint pmf: \\(f(x, y) = P(X = x, Y = y)\\)\n\n\\(\\sum_x \\sum_y f(x, y) = 1\\)\n\nThe marginal pmf for \\(X\\): \\(f_X(x) = P(X = x) = \\sum_y f(x, y)\\)\nThe marginal pmf for \\(Y\\): \\(f_Y(y) = P(Y = y) = \\sum_x f(x, y)\\)\nThe marginal distribution is the same as univariate distribution as if we ignored the other variable."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables-1",
    "href": "slides/01-welcome.html#discrete-random-variables-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe conditional pmf of \\(Y\\) given \\(X\\) is \\(f(y|x) = P(Y = y|X = x) = \\frac{P(X = x, Y = y)}{P(X = x)} = \\frac{f(x, y)}{f_X (x)}.\\)\n\\(X\\) and \\(Y\\) are independent if \\(f(x, y) = f_X(x)f_Y(y)\\) for all \\(x\\) and \\(y\\).\n\nVariables are dependent if they are not independent.\n\nEquivalently, \\(X\\) and \\(Y\\) are independent if \\(f(x|y) = f_X(x)\\) for all \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables-2",
    "href": "slides/01-welcome.html#discrete-random-variables-2",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nNotation: \\(X_1, \\dots, X_n \\overset{\\mathrm{iid}}{\\sim} f(x)\\) means that \\(X_1, \\ldots, X_n\\) are independent and identically distributed.\nThis implies the joint pmf is \\[P(X_1 = x_1, \\ldots, X_n = x_n) = \\prod_{i=1}^n f(x_i).\\]\nThe same notation and definitions of independence apply to continuous random variables.\nIn this class, assume independence unless otherwise noted."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-random-variables",
    "href": "slides/01-welcome.html#continuous-random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nManipulating joint pdfs is similar to joint pmfs but sums are replaced by integrals.\nThe joint pdf is denoted \\(f(x, y)\\).\nProbabilities are computed as volume under the pdf: \\[P((X, Y) ‚àà A) = \\int_A f(x, y)dxdy\\] where \\(A \\subset \\mathbb{R}^2\\)."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-random-variables-1",
    "href": "slides/01-welcome.html#continuous-random-variables-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nThe marginal pdf of \\(X\\) is \\(f_X(x) = \\int f(x, y)dy\\).\n\\(f_X\\) is the univariate pdf for \\(X\\) as if we never considered \\(Y\\).\nThe conditional pdf of \\(Y\\) given \\(X\\) is \\[f(y|x) = \\frac{f(x, y)}{f_X (x)}.\\]\nProper: \\(\\int f(y|x)dy = \\int \\frac{f(x,y)}{f_X(x)}dy = \\frac{\\int f(x,y)dy}{f_X(x)} = 1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#defining-joint-distributions-conditionally",
    "href": "slides/01-welcome.html#defining-joint-distributions-conditionally",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Defining joint distributions conditionally",
    "text": "Defining joint distributions conditionally\n\nSpecifying joint distributions is hard.\nEvery joint distribution can be written \\(f(x, y) = f(y|x)f(x)\\).\nTherefore, any joint distribution can be defined by,\n\n\\(X\\)‚Äôs marginal distribution\nThe conditional distribution of \\(Y|X\\)\n\nThe joint problem reduces to two univariate problems.\nThis idea forms the basis of hierarchical modeling."
  },
  {
    "objectID": "slides/01-welcome.html#bayes-rule",
    "href": "slides/01-welcome.html#bayes-rule",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Bayes rule",
    "text": "Bayes rule\n\n\n\n\n\nThomas Bayes, 1701-1761\n\n\nPierre-Simon Laplace, 1749-1827\n\n\\[f(\\boldsymbol{\\theta}|\\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{\\int f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}\\]"
  },
  {
    "objectID": "slides/01-welcome.html#prepare-for-next-week",
    "href": "slides/01-welcome.html#prepare-for-next-week",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Prepare for next week",
    "text": "Prepare for next week\n\nComplete HW 00 tasks\nReview syllabus\nComplete reading to prepare for Tuesday‚Äôs lecture\nTuesday‚Äôs lecture: Monte Carlo Sampling"
  },
  {
    "objectID": "slides/01-welcome.html#the-table-game",
    "href": "slides/01-welcome.html#the-table-game",
    "title": "Welcome to BIOSTAT 725!",
    "section": "The Table Game",
    "text": "The Table Game"
  },
  {
    "objectID": "slides/01-welcome.html",
    "href": "slides/01-welcome.html",
    "title": "Welcome to BIOSTAT 725!",
    "section": "",
    "text": "Education and career journey\n\nBS in Statistical Science from Duke University\nPhD in Biostatistics from University of North Carolina - Chapel Hill\nPostdoc in Duke Forge: Duke‚Äôs Center for Actionable Health Data Science\nNIH/NEI Pathway to Independence Fellow (K99/R00)\nAssistant Professor, Department of Biostatistics & Bioinformatics and Statistical Science at Duke; Faculty Affiliate of Duke AI Health\n\nWork focuses on developing data science tools to improve patient experience using biomedical data (including EHR)\nDad of 5 and 7 year old daughters üôÇ\n\n\n\n\n\n\n\nDr.¬†Youngsoo Baek (PhD)\n\nPhd in Statistical Science from Duke University\nPostdoc in Biostatistics & Bioinformatics\n\nBraden Scherting\n\nPhd candidate in Statistical Science\n\n\n\n\n\n\nClick on the link or scan the QR code to answer the Ed Discussion poll\nhttps://edstem.org/us/courses/68995/discussion/5942168\n\n\n\n\n\n\n\n\n\n\nIntroduction to the course\nSyllabus activity\nReview of probability"
  },
  {
    "objectID": "slides/01-welcome.html#translation",
    "href": "slides/01-welcome.html#translation",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Translation",
    "text": "Translation"
  },
  {
    "objectID": "hw/hw-00.html",
    "href": "hw/hw-00.html",
    "title": "HW 00",
    "section": "",
    "text": "Important\n\n\n\nThis first homework will not be graded, however it will be critical that you complete this on time. All of the computing tools we‚Äôll use in the class will be introduced during this assignemnt.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#rstudio",
    "href": "hw/hw-00.html#rstudio",
    "title": "HW 00",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\nIn this class, you have the option to use RStudio on your laptop (i.e., locally) or on through a container hosted by Duke OIT. My suggestion is for everyone to be comfortable using both options, for the following reasons:\n\nFlexibility: If you‚Äôre laptop has problems right before a due date, it will be helpful to be setup in the container.\nIndependence: It is important to be able to compute on your laptop, because when you graduate you will no longer have access to the Duke containers.\n\nThe container is offered as a convenience and you should take advantage of it when needed. We will now give instructions for using both.\n\nInstalling RStudio on your laptop\n\nMost of you probably already have RStudio installed on your laptop. In case you do not, please follow these instructions to install both R and RStudio, Installation instruction.\nWhen given the option, choose the most recent stable version of both.\n\n\n\nReserve RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers. You will log in using your NetID credentials.\nClick ‚ÄúReserve STA602‚Äù to reserve an RStudio container. Be sure you reserve the container labeled STA602 to ensure you have the computing set up you need for the class. This semester, we are sharing a container with STA 602!\n\nYou only need to reserve a container once per semester.\n\n\nOpen RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA602 to log into the Docker container. You should now see the RStudio environment.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#stan",
    "href": "hw/hw-00.html#stan",
    "title": "HW 00",
    "section": "Stan",
    "text": "Stan\nIn this course, we will use the package rstan as our primary tool for conducting Bayesian inference. The container already has rstan installed, so these steps need to be used for installation on your laptop.\n\nFollow the installation guide here: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n\nMake sure to follow these instructions closely, since prior to installing rstan, you need to configure your R installation to be able to compile C++ code.\n\nStan Hello World!\nOnce rstan has been installed, test to make sure we can load the R package.\n\nlibrary(rstan)\n\nNow we will simulate some data that we can fit with linear regression. We will also define a Stan data object (no need to understand this now, we will go into this in detail in future lectures).\n\n###Set a seed for reproducibility\nset.seed(54)\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- 3 # number of covariates\n\n###True parameter values\nbeta &lt;- matrix(c(rnorm(p + 1)), ncol = 1)\nsigma &lt;- 1.5\n\n###Simulate covariates and outcome\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n###Create a Stan data object\nstan_data &lt;- list(\n  n = n,\n  p = p,\n  Y = Y,  \n  X = X\n)\n\nNow, we can define a Stan model (you do not need to understand this yet, we are just testing!). In RStudio, create a new .stan file called test.stan and then copy and paste the following Stan code. To create a .stan file from RStudio, File -&gt; New File -&gt; Stan File.\n\n// Saved in test.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p + 1] X;\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  Y ~ normal(X * beta, sigma);\n}\n\nNext, we test to see if the model can compile. Note compilation can sometimes take a bit of time.\n\nstan_model &lt;- stan_model(file = \"test.stan\")\n\nOK, great. We will now obtain posterior samples, using default specifications for inference.\n\nfit &lt;- sampling(stan_model, data = stan_data, refresh = 0)\n\nFinally, print some summary estimates for the model parameters.\n\nprint(fit)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n           mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\nbeta[1]    1.89    0.00 0.18    1.54    1.77    1.89    2.01    2.23  4666    1\nbeta[2]    0.20    0.00 0.18   -0.15    0.08    0.20    0.32    0.55  4391    1\nbeta[3]   -0.30    0.00 0.16   -0.62   -0.41   -0.31   -0.19    0.01  5189    1\nbeta[4]    1.58    0.00 0.19    1.22    1.46    1.58    1.71    1.95  4983    1\nsigma      1.71    0.00 0.13    1.49    1.63    1.71    1.79    1.98  4479    1\nlp__    -102.49    0.04 1.61 -106.30 -103.33 -102.13 -101.31 -100.34  1907    1\n\nSamples were drawn using NUTS(diag_e) at Mon Jan  6 15:15:47 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#git-and-github",
    "href": "hw/hw-00.html#git-and-github",
    "title": "HW 00",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nIn addition to R and RStudio, we will use git and GitHub for version control and collaboration.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like ‚ÄúTrack Changes‚Äù features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better). Git is important because:\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)\n\n\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, exams, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username.\n\n\n\nIf you already have a GitHub account, you can move on to the next step.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#connect-rstudio-and-github",
    "href": "hw/hw-00.html#connect-rstudio-and-github",
    "title": "HW 00",
    "section": "Connect RStudio and GitHub",
    "text": "Connect RStudio and GitHub\nNow that you have RStudio and a GitHub account, we will configure git so that RStudio and GitHub communicate with one another.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system. So, if you are using both your laptop and the container, you will need to do this process twice.\n\n\n\nStep 0: Open your RStudio (either the STA602 RStudio container or your laptop).\nStep 1: Type credentials::ssh_setup_github() into the console on the bottom left of the RStudio environment.\nStep 2: R will ask ‚ÄúNo SSH key found. Generate one now?‚Äù Click 1 for yes.\nStep 3: You will generate a key. It will begin with ‚Äússh-rsa‚Ä¶.‚Äù R will then ask ‚ÄúWould you like to open a browser now?‚Äù Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used (e.g., biostat725).\n\n\n\nConfigure git\nThe last thing we need to do is configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"berchuck\",\n  user.email = \"sib2@duke.edu\")\n\nIt may look like nothing happened but you are now ready interact between GitHub and RStudio! We will begin working with RStudio and GitHub in lecture this week.\n\n\n\n\n\n\nNote\n\n\n\nYou should be using the email address you used to create your GitHub account, it‚Äôs ok if it isn‚Äôt your Duke email.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "slides/01-welcome.html#office-hours-poll",
    "href": "slides/01-welcome.html#office-hours-poll",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Office Hours Poll!",
    "text": "Office Hours Poll!\n\nClick on the link or scan the QR code to answer the poll\nhttps://forms.office.com/r/QTuhmC0wM3"
  },
  {
    "objectID": "slides/02-monte-carlo.html#simulating-pi-using-monte-carlo",
    "href": "slides/02-monte-carlo.html#simulating-pi-using-monte-carlo",
    "title": "Monte Carlo Sampling",
    "section": "Simulating \\(\\pi\\) using Monte Carlo",
    "text": "Simulating \\(\\pi\\) using Monte Carlo\nSuppose we are interested in estimating \\(\\pi\\).\n\n\n\nWe can formulate \\(\\pi\\) as a function of the area of a square and circle.\nArea of a circle: \\(A_c = \\pi r^2\\)\nArea of a square: \\(A_s = 4 r^2\\)\nThe ratio of the two areas is: \\(\\frac{A_c}{A_s} = \\frac{\\pi r^2}{4 r^2} \\implies \\pi = \\frac{4 A_c}{A_s}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we have an estimate for the ratio we can solve for \\(\\pi\\). The challenge becomes estimating this ratio."
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-sampling-1",
    "href": "slides/02-monte-carlo.html#monte-carlo-sampling-1",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo sampling",
    "text": "Monte Carlo sampling\n\nWe can take advantage of how quickly a computer can generate pseudo-random numbers.\nThere is a class of algorithms called Monte Carlo sampling that exploit randomness to estimate real world scenarios that would otherwise be difficult to explicitly calculate.\nThe name comes from the Monte Carlo Casino in Monaco, where the primary developer of the method, mathematician Stanislaw Ulam, was inspired by his uncle‚Äôs gambling habits. (This is who Stan was named after!)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-estimation-of-the-ratio",
    "href": "slides/02-monte-carlo.html#monte-carlo-estimation-of-the-ratio",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo estimation of the ratio",
    "text": "Monte Carlo estimation of the ratio\n\nWe can use a Monte Carlo simulation to estimate the area ratio of the circle to the square.\nImagine you randomly drop grains of sand into the area of the square. By counting the total number of sand grains in the square (all of them since you‚Äôre an accurate dropper) to the number of sand grains inside the circle we get this estimate.\nMultiply the estimated ratio by 4 and you get an estimate for \\(\\pi\\).\nThe more grains of sand that are used the more accurate your estimate of \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#algorithm-for-estimating-pi",
    "href": "slides/02-monte-carlo.html#algorithm-for-estimating-pi",
    "title": "Monte Carlo Sampling",
    "section": "Algorithm for estimating \\(\\pi\\)",
    "text": "Algorithm for estimating \\(\\pi\\)\n\nGenerate a random point \\((x, y)\\) inside a square centered at the origin with length 2.\n\n\nThis is equivalent to assuming:\n\n\\(f_{X,Y}(x,y) = f_X(x)f_Y(y)\\)\n\\(f_X(x) = Uniform(-1, 1)\\), \\(f_Y(y) = Uniform(-1, 1)\\)\n\n\n\n\nDetermine whether the point falls inside the unit circle inscribed in the square by checking whether \\(x^2 + y^2 \\leq 1\\).\n\n\n\n\nRepeat steps 1 and 2 for a large number of points (\\(S\\))."
  },
  {
    "objectID": "slides/02-monte-carlo.html#algorithm-for-estimating-pi-1",
    "href": "slides/02-monte-carlo.html#algorithm-for-estimating-pi-1",
    "title": "Monte Carlo Sampling",
    "section": "Algorithm for estimating \\(\\pi\\)",
    "text": "Algorithm for estimating \\(\\pi\\)\n\nGenerate a random point \\((x, y)\\) inside a square centered at the origin with length 2.\nDetermine whether the point falls inside the unit circle inscribed in the square by checking whether \\(x^2 + y^2 \\leq 1\\).\nRepeat steps 1 and 2 for a large number of points (\\(S\\)).\nCalculate the ratio of the number of points that fell inside the circle to the total number of points generated.\n\n\n\nMultiply the ratio by 4 to estimate the value of \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-big-of-a-difference-does-s-make",
    "href": "slides/02-monte-carlo.html#how-big-of-a-difference-does-s-make",
    "title": "Monte Carlo Sampling",
    "section": "How big of a difference does \\(S\\) make",
    "text": "How big of a difference does \\(S\\) make"
  },
  {
    "objectID": "slides/02-monte-carlo.html#estimating-pi-with-increasing-s",
    "href": "slides/02-monte-carlo.html#estimating-pi-with-increasing-s",
    "title": "Monte Carlo Sampling",
    "section": "Estimating \\(\\pi\\) with increasing \\(S\\)",
    "text": "Estimating \\(\\pi\\) with increasing \\(S\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#error-in-estimating-pi",
    "href": "slides/02-monte-carlo.html#error-in-estimating-pi",
    "title": "Monte Carlo Sampling",
    "section": "Error in estimating \\(\\pi\\)",
    "text": "Error in estimating \\(\\pi\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling",
    "href": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling",
    "title": "Monte Carlo Sampling",
    "section": "Intuition behing Monte Carlo sampling",
    "text": "Intuition behing Monte Carlo sampling\n\nAssume \\(X \\sim Uniform(-1,1)\\) and \\(Y \\sim Uniform(-1,1)\\).\nWe can write our problem as, \\(\\pi = 4P(X^2 + Y^2 \\leq 1)\\).\nHow could we do this without Monte Carlo?\nDefine, \\(Z = X^2 + Y^2\\). We could then use change-of-variables to compute the density of \\(Z\\) and then compute \\(P(Z \\leq 1)\\).\n\nThis is generally difficult!"
  },
  {
    "objectID": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling-1",
    "href": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling-1",
    "title": "Monte Carlo Sampling",
    "section": "Intuition behing Monte Carlo sampling",
    "text": "Intuition behing Monte Carlo sampling\n\nInstead, we could write our problem as an expectation and use the law of large numbers,\n\n\\[P(X^2 + Y^2 \\leq 1) = \\mathbb{E}_{X,Y}\\left[1\\left(X^2 + Y^2 \\leq 1\\right)\\right].\\]\n\nRecall: \\(\\mathbb{E}_X[1(A)] = \\int_A f_X(x)dx = P(X \\in A)\\).\nWe then have that, \\[\\frac{1}{S}\\sum_{i = 1}^S 1\\left(X_i^2 + Y_i^2 \\leq 1\\right) \\rightarrow \\mathbb{E}_{X,Y}\\left[1\\left(X^2 + Y^2 \\leq 1\\right)\\right],\\]\n\nwhere \\((X_i,Y_i) \\sim f(X,Y)\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#a-motivating-example",
    "href": "slides/02-monte-carlo.html#a-motivating-example",
    "title": "Monte Carlo Sampling",
    "section": "A motivating example",
    "text": "A motivating example\n\nSuppose we are interested in estimating the prevalence of diabetes in Durham County. We aim to estimate this prevalence by taking a sample of \\(n\\) individuals in Durham County and we record whether or not they have diabetes, \\(Y_i\\).\nWe assume that \\(Z = \\sum_{i=1}^n Y_i \\sim Binomial(n, \\pi)\\) for \\(i = 1,\\ldots,n\\).\nOur goal is to estimate \\(\\pi\\) and perform statistical inference (e.g., point estimation, interval estimation, etc.)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#posterior-inference",
    "href": "slides/02-monte-carlo.html#posterior-inference",
    "title": "Monte Carlo Sampling",
    "section": "Posterior inference",
    "text": "Posterior inference\n\nIn Bayesian statistics, inference is encoded through the posterior distribution,\n\n\\[\\begin{aligned}\nf(\\pi | Z) &= \\frac{f(Z | \\pi)f(\\pi)}{f(Z)},\\\\\n&= \\frac{f(Z | \\pi)f(\\pi)}{\\int f(Z | \\pi)f(\\pi)d\\pi}.\n\\end{aligned}\\]\n\nAll we have to do is specify the likelihood and prior."
  },
  {
    "objectID": "slides/02-monte-carlo.html#likelihood-specification",
    "href": "slides/02-monte-carlo.html#likelihood-specification",
    "title": "Monte Carlo Sampling",
    "section": "Likelihood specification",
    "text": "Likelihood specification\n\\(Z\\) is a Binomial distribution with pmf,\n\\[f(Z | \\pi) = P(Z = z) = {n \\choose z} \\pi^z(1-\\pi)^{n-z},\\]\nwhere \\(z \\in \\{0, 1, \\ldots, n\\}\\).\n\n\\({n \\choose z} = \\frac{n!}{z!(n-z)!} = \\frac{\\Gamma(n+1)}{\\Gamma(z+1)\\Gamma(n-z+1)}\\)\n\\(\\Gamma(x) = (x-1)!\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#prior-specification",
    "href": "slides/02-monte-carlo.html#prior-specification",
    "title": "Monte Carlo Sampling",
    "section": "Prior specification",
    "text": "Prior specification\nWhat do we know about \\(\\pi\\)?\n\n\\(\\pi\\) is continuous.\n\\(\\pi \\in (0,1)\\).\n\nWe should place a distribution on \\(\\pi\\) that permits these properties.\n\nOne option is the Beta distribution, \\(\\pi \\sim Beta(\\alpha,\\beta)\\), \\[f(\\pi) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}.\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-marginal-likelihood",
    "href": "slides/02-monte-carlo.html#computing-the-marginal-likelihood",
    "title": "Monte Carlo Sampling",
    "section": "Computing the marginal likelihood",
    "text": "Computing the marginal likelihood\nUnder our prior specification, we can compute the marginal likelihood, \\(f(Z)\\):\n\\[\\begin{aligned}\nf(Z) &= \\int f(Z | \\pi) f(\\pi) d\\pi\\\\\n&= \\int {n \\choose z} \\pi^z(1-\\pi)^{n-z} \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} d\\pi\\\\\n&= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\int \\underbrace{\\pi^{(\\alpha + z) - 1}(1-\\pi)^{(\\beta + n - z) - 1}}_{Beta\\text{ }kernel} d\\pi\n\\end{aligned}\\]\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample: The kernel of the Beta pdf is \\(\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-marginal-likelihood-1",
    "href": "slides/02-monte-carlo.html#computing-the-marginal-likelihood-1",
    "title": "Monte Carlo Sampling",
    "section": "Computing the marginal likelihood",
    "text": "Computing the marginal likelihood\nUnder our prior specification, we can compute the marginal likelihood, \\(f(Z)\\):\n\\[\\begin{aligned}\nf(Z) &= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\int \\pi^{(\\alpha + z) - 1}(1-\\pi)^{(\\beta + n - z) - 1} d\\pi\\\\\n&= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta + n)}{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#we-can-then-compute-the-posterior",
    "href": "slides/02-monte-carlo.html#we-can-then-compute-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "We can then compute the posterior",
    "text": "We can then compute the posterior\n\\[\\begin{aligned}\nf(\\pi | Z) &= \\frac{f(Z | \\pi) f(\\pi)}{f(Z)}\\\\\n&= \\frac{{n \\choose z} \\pi^z(1-\\pi)^{n-z} \\times \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}}{{n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta) }{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}}\\\\\n&=\\frac{\\Gamma(\\alpha+z)\\Gamma(\\beta+n-z)}{\\Gamma(\\alpha + \\beta + n)}\\pi^{(\\alpha + z) - 1} (1 - \\pi)^{(\\beta + n - z) - 1}\\\\\n&=Beta\\left(\\alpha + z, \\beta + n - z\\right).\n\\end{aligned}\\]\n\nA prior that is considered conjugate yields a posterior with the same distribution.\nThe Beta distribution is conjugate for the Bernoulli/Binomial distributions."
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-posterior",
    "href": "slides/02-monte-carlo.html#computing-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Computing the posterior",
    "text": "Computing the posterior\n\nIn general, computing the marginal likelihood, \\(f(Z)\\), is extremely difficulty.\nAn easier approach is to use the kernel trick.\n\n\\[\\begin{aligned}\nf(\\pi | Z) &\\propto f(Z | \\pi) f(\\pi)\\\\\n&\\propto \\pi^z (1-\\pi)^{n - z} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\\\\n&= \\pi^{\\left(\\alpha + z\\right) - 1} (1-\\pi)^{\\left(\\beta + n - z\\right) - 1}\\\\\n&= Beta\\left(\\alpha + z, \\beta + n - z\\right).\n\\end{aligned}\\]\n\nThis only works when a conjugate prior is used."
  },
  {
    "objectID": "slides/02-monte-carlo.html#lets-inspect-the-posterior",
    "href": "slides/02-monte-carlo.html#lets-inspect-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Let‚Äôs inspect the posterior",
    "text": "Let‚Äôs inspect the posterior\n\nSuppose we conducted a simple random sample of 500 individuals in Durham County and 120 responsed that they had diabetes and 380 that they did not.\nThe posterior becomes, \\(Beta\\left(\\alpha + 120, \\beta + 380\\right)\\).\nWe must choose our prior distribution wisely.\nNote that:\n\n\\(\\mathbb{E}[\\pi] = \\alpha/(\\alpha + \\beta)\\)\n\\(\\mathbb{V}(\\pi) = (\\alpha\\beta)/[(\\alpha + \\beta)^2(\\alpha + \\beta + 1)]\\).\n\nTypically, \\(\\alpha = \\beta = 1\\), which corresponds to a uniform prior on \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#lets-inspect-the-posterior-1",
    "href": "slides/02-monte-carlo.html#lets-inspect-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Let‚Äôs inspect the posterior",
    "text": "Let‚Äôs inspect the posterior"
  },
  {
    "objectID": "slides/02-monte-carlo.html#suppose-my-prior-changes",
    "href": "slides/02-monte-carlo.html#suppose-my-prior-changes",
    "title": "Monte Carlo Sampling",
    "section": "Suppose my prior changes",
    "text": "Suppose my prior changes"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarizing-the-posterior",
    "href": "slides/02-monte-carlo.html#summarizing-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nWe now have a posterior distribution. What do we do now?\n\n\nPosterior means, medians, modes, and variances\n\n\n\n\nJoint, conditional, and marginal probabilities, for example: \\(P(\\pi &lt; c | Z)\\)\n\n\n\n\n\\(\\alpha\\)-quantiles: \\(\\{q_{\\alpha} : P(\\pi &lt; q_{\\alpha} | Z) = \\alpha\\}, \\alpha \\in (0,1)\\)\n\n\n\n\n\\(\\ldots\\)\n\n\n\nHow do we go about computing these summaries?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarization-can-be-complex",
    "href": "slides/02-monte-carlo.html#summarization-can-be-complex",
    "title": "Monte Carlo Sampling",
    "section": "Summarization can be complex",
    "text": "Summarization can be complex\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\pi &lt; c | Z)\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pbeta}\\))\n\n\nThese methods work well for standard posterior quantities and distributions."
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarizing-the-posterior-1",
    "href": "slides/02-monte-carlo.html#summarizing-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution.\n\n\\(P(\\pi \\in A| Z)\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\pi\\), \\(g\\left(\\pi\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\pi_1 - \\pi_2|\\), \\(\\pi_1/\\pi_2\\), \\(\\max\\left\\{\\pi_1,\\ldots,\\pi_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible.\n\nWhat are our options?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-mc-sampling",
    "href": "slides/02-monte-carlo.html#monte-carlo-mc-sampling",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo (MC) sampling",
    "text": "Monte Carlo (MC) sampling\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\pi|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\pi|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#mc-approximation",
    "href": "slides/02-monte-carlo.html#mc-approximation",
    "title": "Monte Carlo Sampling",
    "section": "MC approximation",
    "text": "MC approximation\nLet \\(g\\left(\\pi\\right)\\) be (just about) any function of \\(\\pi\\). The law of large numbers says that if,\n\\[\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\pi|\\mathbf{Y}\\right),\\] then, \\[\\frac{1}{S}\\sum_{s=1}^S g\\left(\\pi^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\pi\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\pi\\right)f\\left(\\pi|\\mathbf{Y}\\right)d\\pi,\\] as \\(S\\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#mc-approximation-1",
    "href": "slides/02-monte-carlo.html#mc-approximation-1",
    "title": "Monte Carlo Sampling",
    "section": "MC approximation",
    "text": "MC approximation\n\nImplications (as \\(S\\rightarrow \\infty\\)):\n\n\\(\\overline{\\pi}=\\frac{1}{S}\\sum_{s=1}^S \\pi^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\pi|\\mathbf{Y}\\right]\\)\n\\(\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\pi^{\\left(s\\right)}-\\overline{\\pi}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\pi|\\mathbf{Y}\\right)\\)\n\\(\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\pi^{\\left(s\\right)}\\leq {c}\\right) \\rightarrow P\\left(\\pi\\leq {c} | \\mathbf{Y}\\right)\\)\n\\(\\alpha\\)-quantile of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}\\)\n\nJust about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample"
  },
  {
    "objectID": "slides/02-monte-carlo.html#posterior-inference-for-arbitrary-functions",
    "href": "slides/02-monte-carlo.html#posterior-inference-for-arbitrary-functions",
    "title": "Monte Carlo Sampling",
    "section": "Posterior inference for arbitrary functions",
    "text": "Posterior inference for arbitrary functions\nInterest in the posterior distribution of a function of \\(\\pi\\), \\(g\\left(\\pi\\right)\\)\n\nMC sampling plan:\n\nDraw \\(\\pi^{\\left(1\\right)}\\) from \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\pi^{\\left(1\\right)}\\right)\\)\n\\(\\ldots\\)\nDraw \\(\\pi^{\\left(S\\right)}\\) from \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\pi^{\\left(S\\right)}\\right)\\)\n\n\\(\\Rightarrow g\\left(\\pi^{\\left(1\\right)}\\right),\\ldots,g\\left(\\pi^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\pi\\right)|\\mathbf{Y}\\right)\\)\nTherefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-many-samples-to-take",
    "href": "slides/02-monte-carlo.html#how-many-samples-to-take",
    "title": "Monte Carlo Sampling",
    "section": "How many samples to take?",
    "text": "How many samples to take?\nWe can use a central limit theorem: \\(\\sqrt{S}\\left(\\overline{\\pi}-\\mathbb{E}[\\pi | \\mathbf{Y}]\\right)/\\sigma \\stackrel{d}{\\rightarrow} \\text{N}\\left(0,1\\right)\\),\n\n\\(\\overline{\\pi}=\\frac{1}{S}\\sum_{s=1}^S \\pi^{\\left(s\\right)}\\)\n\\(\\sigma^2 = \\mathbb{V}\\left(\\pi | \\mathbf{Y}\\right), \\quad \\mathbb{V}\\left(\\overline{\\pi}\\right) = \\frac{1}{S^2}\\sum_{s=1}^S\\mathbb{V}\\left(\\pi^{(s)}\\right) = \\frac{1}{S}\\mathbb{V}\\left(\\pi | \\mathbf{Y}\\right) = \\sigma^2/S\\).\n\n\\(\\implies \\overline{\\pi}\\approx N\\left(\\mathbb{E}[\\pi | \\mathbf{Y}],\\sigma^2/S\\right)\\)\nMC standard error: \\(\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\), \\(\\widehat{\\sigma}^2=\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\pi^{\\left(s\\right)}-\\overline{\\pi}\\right)^2\\)\nApproximate 95% MC confidence interval for the posterior mean: \\(\\overline{\\pi} \\pm 2\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\)\n\nChoose \\(S\\) large enough to report the posterior mean with your desired precision\nReporting MC standard errors for the posterior mean is a good way to indicate that \\(S\\) is large enough"
  },
  {
    "objectID": "slides/02-monte-carlo.html#returning-to-our-posterior",
    "href": "slides/02-monte-carlo.html#returning-to-our-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Returning to our posterior",
    "text": "Returning to our posterior\nLet‚Äôs obtain \\(S = 1,000\\) samples from our posterior.\n\npi_samples &lt;- rbeta(1000, 1 + 120, 1 + 380)\n\nWe can compute the posterior mean and variance.\n\nmean(pi_samples)\n\n[1] 0.2399671\n\nvar(pi_samples)\n\n[1] 0.000354165"
  },
  {
    "objectID": "slides/02-monte-carlo.html#assessing-accuracy",
    "href": "slides/02-monte-carlo.html#assessing-accuracy",
    "title": "Monte Carlo Sampling",
    "section": "Assessing accuracy",
    "text": "Assessing accuracy"
  },
  {
    "objectID": "slides/02-monte-carlo.html#additional-posterior-summaries",
    "href": "slides/02-monte-carlo.html#additional-posterior-summaries",
    "title": "Monte Carlo Sampling",
    "section": "Additional posterior summaries",
    "text": "Additional posterior summaries\n\n# median\nmedian(pi_samples)\n\n[1] 0.2405268\n\n# 95% credible intervals\nquantile(pi_samples, probs = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.2042175 0.2794289 \n\n# evaluating probability\nmean(pi_samples &lt; 0.25)\n\n[1] 0.683\n\n# summarizing arbitrary functions of the parameters\npi_new &lt;- pi_samples^3 - pi_samples\nc(mean(pi_new), quantile(pi_new, probs = c(0.025, 0.975)))\n\n                 2.5%      97.5% \n-0.2268650 -0.2576109 -0.1957007"
  },
  {
    "objectID": "slides/02-monte-carlo.html#poisson-random-variable",
    "href": "slides/02-monte-carlo.html#poisson-random-variable",
    "title": "Monte Carlo Sampling",
    "section": "Poisson random variable",
    "text": "Poisson random variable\nSuppose that \\(Y_i \\stackrel{iid}{\\sim} Poisson(\\lambda)\\) for \\(i = 1,\\ldots,n\\). Assume the following conjugate prior, \\(f(\\lambda) \\sim Gamma(a, b)\\), so that \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). We would like to perform inference for the \\(\\lambda\\). We take \\(S=10,000\\) samples.\n\nset.seed(54) # set seed for replicability\nS &lt;- 10000 # number of Monte Carlo samples\nlambda &lt;- 4 # true value of lambda\nn &lt;- 100 # sample size\nY &lt;- rpois(n, lambda)\na &lt;- 3 # hyperprior\nb &lt;- 1 # hyperprior\nsamples &lt;- rgamma(S, a + sum(Y), b + n) # sample from posterior\n\n\n\n\n\n\nmin\nq1\nmedian\nq3\nmax\nmean\nsd\n\n\n\n\n3.269\n3.904\n4.039\n4.173\n4.773\n4.041\n0.201"
  },
  {
    "objectID": "slides/02-monte-carlo.html#sampling-for-any-distribution",
    "href": "slides/02-monte-carlo.html#sampling-for-any-distribution",
    "title": "Monte Carlo Sampling",
    "section": "Sampling for any distribution",
    "text": "Sampling for any distribution\nMonte Carlo sampling does not have to be used solely for posterior inference. Suppose we are interested in computed summaries for \\(X_i \\stackrel{iid}{\\sim} N(3, 4)\\) for \\(i = 1,\\ldots,n\\). We take \\(S = 10,000\\).\n\nS &lt;- 10000\nsamples &lt;- rnorm(S, 3, 2)\nlibrary(moments)\nkurtosis(samples)\n\n[1] 3.041546\n\nskewness(samples)\n\n[1] -0.0109433\n\nmean(samples &gt; 4) # P(X_i &gt; 4)\n\n[1] 0.3087"
  },
  {
    "objectID": "slides/02-monte-carlo.html#combination-of-random-variables",
    "href": "slides/02-monte-carlo.html#combination-of-random-variables",
    "title": "Monte Carlo Sampling",
    "section": "Combination of random variables",
    "text": "Combination of random variables\nSuppose \\(X_i \\stackrel{iid}{\\sim} N(3, 4)\\) and \\(Y_i \\stackrel{iid}{\\sim} \\chi^2(df=3)\\) for \\(i = 1,\\ldots,n\\). \\(X_i\\) and \\(Y_i\\) are independent. We are interested in summaries of \\(Z_i = X_i / Y_i\\). We take \\(S = 10,000\\).\n\nS &lt;- 10000\nx &lt;- rnorm(S, 3, 2)\ny &lt;- rchisq(S, 3)\nz &lt;- x / y\nmean(z)\n\n[1] 2.961057\n\nmedian(z)\n\n[1] 1.146081\n\nmean(z &gt; 1)\n\n[1] 0.5456"
  },
  {
    "objectID": "slides/02-monte-carlo.html#prepare-for-next-class",
    "href": "slides/02-monte-carlo.html#prepare-for-next-class",
    "title": "Monte Carlo Sampling",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nComplete HW 00 which is due before Thursday‚Äôs class\nComplete reading to prepare for Thursday‚Äôs lecture\nThursday‚Äôs lecture: Markov chain Monte Carlo"
  },
  {
    "objectID": "slides/03-mcmc.html#review-of-last-lecture",
    "href": "slides/03-mcmc.html#review-of-last-lecture",
    "title": "Markov chain Monte Carlo",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we performed posterior inference for a Beta-Binomial model using Monte Carlo estimation.\nToday we will discuss Bayesian estimation of linear regression:\n\n\\[\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n).\\]\n\nThere is no closed form posterior, so we cannot directly use Monte Carlo sampling.\nWe need Markov chain Monte Carlo (MCMC)!\nOur goal is to gain intuition behind MCMC, the workhorse behind Stan."
  },
  {
    "objectID": "slides/03-mcmc.html#defining-the-model",
    "href": "slides/03-mcmc.html#defining-the-model",
    "title": "Markov chain Monte Carlo",
    "section": "Defining the model",
    "text": "Defining the model\nSuppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#defining-the-likelihood",
    "href": "slides/03-mcmc.html#defining-the-likelihood",
    "title": "Markov chain Monte Carlo",
    "section": "Defining the likelihood",
    "text": "Defining the likelihood\nThe individual likelihood contribution for subject \\(i\\) is given by,\n\\[Y_i|\\boldsymbol{\\beta},\\sigma^2 \\stackrel{ind}{\\sim} N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta},\\sigma^2) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta},\\sigma^2),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#matrix-likelihood-specification",
    "href": "slides/03-mcmc.html#matrix-likelihood-specification",
    "title": "Markov chain Monte Carlo",
    "section": "Matrix likelihood specification",
    "text": "Matrix likelihood specification\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2 ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-estimation",
    "href": "slides/03-mcmc.html#linear-regression-estimation",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression estimation",
    "text": "Linear regression estimation\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} \\log f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-assumptions",
    "href": "slides/03-mcmc.html#linear-regression-assumptions",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression assumptions",
    "text": "Linear regression assumptions\n\nLinearity between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\): \\(\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\)\nIndependence of errors: \\(\\epsilon_i\\)‚Äôs are independent Gaussian\nHomoskedasticity: constant \\(\\sigma^2\\) across \\(\\mathbf{x}_i\\)\nNormality of errors: \\(\\epsilon_i \\sim N(0,\\sigma^2)\\)\nNo multicollinearity: \\(\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1}\\) exists"
  },
  {
    "objectID": "slides/03-mcmc.html#posterior-for-linear-regression",
    "href": "slides/03-mcmc.html#posterior-for-linear-regression",
    "title": "Markov chain Monte Carlo",
    "section": "Posterior for linear regression",
    "text": "Posterior for linear regression\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)d\\boldsymbol{\\beta}d\\sigma^2}.\n\\end{aligned}\\]\nNo closed form exists for the posterior. \\[f(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#motivation-for-gibbs-sampling",
    "href": "slides/03-mcmc.html#motivation-for-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Motivation for Gibbs sampling",
    "text": "Motivation for Gibbs sampling\n\nSuppose we were given \\(\\sigma^{2(1)}\\), a single sample from the marginal posterior distribution \\(f\\left(\\sigma^2|\\mathbf{Y}\\right)\\) (from where, who knows?)\nUse the sample to generate \\(\\boldsymbol{\\beta}^{(1)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y},\\sigma^{2(1)}\\right)\\)\n\\(\\left(\\boldsymbol{\\beta}^{(1)},\\sigma^{2(1)}\\right)\\) is a sample from \\(f\\left(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}\\right)\\)\n\\(\\boldsymbol{\\beta}^{(1)}\\) is a sample from \\(f\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)\\)\n\n\n\n\n\n\n\n\nRecall\n\n\n\\(f\\left(\\boldsymbol{\\beta}, \\sigma^{2}|\\mathbf{Y}\\right) = f\\left(\\boldsymbol{\\beta} | \\sigma^{2},\\mathbf{Y}\\right)f\\left(\\sigma^{2}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\nSuppose we can sample from the following two distributions,\n\n\\(f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta})\\)\n\\(f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\sigma^2)\\)\n\nThese are called full conditional distributions.\nSet initial values for \\(\\boldsymbol{\\theta}^{(0)} = (\\boldsymbol{\\beta}^{(0)}, \\sigma^{2(0)})\\). Then, given a current state of parameters \\(\\boldsymbol{\\theta}^{(s)}\\), we can generate a new state as follows:\n\nSample \\(\\boldsymbol{\\beta}^{(s + 1)} \\sim f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^{2(s)})\\)\nSample \\(\\sigma^{2(s + 1)} \\sim f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}^{(s + 1)})\\)\nLet \\(\\boldsymbol{\\theta}^{(s+1)} = (\\boldsymbol{\\beta}^{(s + 1)}, \\sigma^{2(s + 1)})\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#why-does-this-work",
    "href": "slides/03-mcmc.html#why-does-this-work",
    "title": "Markov chain Monte Carlo",
    "section": "Why does this work?",
    "text": "Why does this work?\n\n\\(\\boldsymbol{\\theta}^{(0)}\\) isn‚Äôt a sample from the posterior, it is an arbitrarily chosen initial value\n\\(\\boldsymbol{\\theta}^{(1)}\\) likely isn‚Äôt from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\)\n\\(\\boldsymbol{\\theta}^{(2)}\\) likely isn‚Äôt from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\) and \\(\\boldsymbol{\\theta}^{(1)}\\)\nTheorem: For any initial values, the chain will eventually converge to the posterior\nTheorem: If \\(\\boldsymbol{\\theta}^{(s)}\\) is a sample from the posterior, then \\(\\boldsymbol{\\theta}^{(s+1)}\\) is too"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler",
    "href": "slides/03-mcmc.html#gibbs-sampler",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nUnder mild regulatory conditions that are generally satisfied for most statistical models, one can show that the iteration \\(\\boldsymbol{\\theta}^{(s)}\\) converges in distribution to a draw from the true joint posterior distribution\nSo for \\(s\\) sufficiently large (say, bigger than \\(s_0\\)), \\(\\left\\{\\boldsymbol{\\theta}^{(s)}, s=s_0+1,\\ldots,S\\right\\}\\) is a correlated sample from the true joint posterior (and \\(\\boldsymbol{\\beta}^{(s)}\\) and \\(\\sigma^{2(s)}\\) are samples from the marginals)\nSimilar to Monte Carlo approximation, we can use these samples to estimate posterior quantities of interest"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-1",
    "href": "slides/03-mcmc.html#gibbs-sampler-1",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\n\\(\\boldsymbol{\\theta}^{(t)}\\) depends on \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-1)}\\) only through \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\boldsymbol{\\theta}^{(t)}\\) is conditionally independent of \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-2)}\\) given \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\implies\\) Markov property, so the sequence is called a Markov chain\nWe use the samples similar to MC approximation; therefore, Gibbs sampling is a form of Markov chain Monte Carlo (MCMC)\nWe will cover diagnostics for MCMC in another lecture!"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-1",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-1",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nWe need to compute the full conditionals. Before doing this, we require prior distributions.\nLet‚Äôs assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gaussian,\n\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta",
    "href": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\boldsymbol{\\beta}\\)",
    "text": "Full conditional for \\(\\boldsymbol{\\beta}\\)\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta},\\sigma^2 | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{aligned}\\]\nThis is the kernel of a multivariate normal for \\(\\boldsymbol{\\beta}\\), with \\(\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}\\) and \\(\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\). It‚Äôs easy to see then that, \\(f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\\)\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta-1",
    "href": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta-1",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\boldsymbol{\\beta}\\)",
    "text": "Full conditional for \\(\\boldsymbol{\\beta}\\)\nThe full conditional can be found in closed-form and is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-sigma2",
    "href": "slides/03-mcmc.html#full-conditional-for-sigma2",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\sigma^2\\)",
    "text": "Full conditional for \\(\\sigma^2\\)\nFull conditional for \\(\\sigma^2\\), assuming \\(f(\\sigma^2) \\sim IG(a, b)\\):\n\\[\\sigma^2 |  \\mathbf{Y} , \\boldsymbol{\\beta} \\sim IG\\left(a + \\frac{n}{2},b+\\frac{\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)}{2}\\right)\\]\n\nWhy inverse-Gamma (\\(IG\\)) distribution for \\(\\sigma^2\\)?"
  },
  {
    "objectID": "slides/03-mcmc.html#sampling-from-the-posterior",
    "href": "slides/03-mcmc.html#sampling-from-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Sampling from the posterior",
    "text": "Sampling from the posterior\nLet‚Äôs simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/03-mcmc.html#visualize-simulated-data",
    "href": "slides/03-mcmc.html#visualize-simulated-data",
    "title": "Markov chain Monte Carlo",
    "section": "Visualize simulated data",
    "text": "Visualize simulated data"
  },
  {
    "objectID": "slides/03-mcmc.html#inspecting-the-prior",
    "href": "slides/03-mcmc.html#inspecting-the-prior",
    "title": "Markov chain Monte Carlo",
    "section": "Inspecting the prior",
    "text": "Inspecting the prior\n\n###Define hyperparameters\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\na &lt;- 3\nb &lt;- 1"
  },
  {
    "objectID": "slides/03-mcmc.html#perform-gibbs-sampling",
    "href": "slides/03-mcmc.html#perform-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Perform Gibbs sampling",
    "text": "Perform Gibbs sampling\n\nsigma2 &lt;- exp(rnorm(1)) # initial value\nsamples &lt;- NULL\nfor (s in 1:5000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n  \n  ###Sample from full conditional for sigma2\n  quadratic &lt;- as.numeric(t(Y - X %*% beta) %*% (Y - X %*% beta))\n  sigma2 &lt;- 1 / rgamma(1, shape = a + n / 2, rate = b + quadratic / 2)\n  \n  ###Save samples after a burn-in\n  samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-mcmc.html#inspect-results",
    "href": "slides/03-mcmc.html#inspect-results",
    "title": "Markov chain Monte Carlo",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-mcmc.html#summary-of-gibbs-sampling",
    "href": "slides/03-mcmc.html#summary-of-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Summary of Gibbs sampling",
    "text": "Summary of Gibbs sampling\n\nGibbs sampling is great when we are able to sample from the full conditional distributions.\nIt has been the main inference machine for Bayesian inference since the early 1990s.\nComputing full conditionals and coding up a Gibbs sampler can be mathematically and computationally rigorous.\nNew classes of MCMC are becoming more common to make Bayesian inference less rigorous."
  },
  {
    "objectID": "slides/03-mcmc.html#intuition-behind-metropolis-sampling",
    "href": "slides/03-mcmc.html#intuition-behind-metropolis-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Intuition behind Metropolis sampling",
    "text": "Intuition behind Metropolis sampling\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let‚Äôs consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)‚Äôs in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)‚Äôs.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-acceptance-ratio",
    "href": "slides/03-mcmc.html#metropolis-acceptance-ratio",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis acceptance ratio",
    "text": "Metropolis acceptance ratio\n\nFortunately, the comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) can be made even if we cannot compute \\(f(\\boldsymbol{\\theta} | \\mathbf{Y})\\).\n\n\\[\\begin{aligned}\nr &= \\frac{f(\\boldsymbol{\\theta}^* | \\mathbf{Y})}{f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y})}\\frac{f(\\mathbf{Y})}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\n\\end{aligned}\\]\nHaving computed \\(r\\), how should we proceed?"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-intuition",
    "href": "slides/03-mcmc.html#metropolis-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis intuition",
    "text": "Metropolis intuition\nMetropolis ratio: \\(r = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\)\nIf \\(r &gt; 1:\\)\n\nIntuition: Since \\(\\boldsymbol{\\theta}^{(s)}\\) is already in our set, we should include \\(\\boldsymbol{\\theta}^*\\) as it has a higher probability than \\(\\boldsymbol{\\theta}^{(s)}\\)\nProcedure: Accept \\(\\boldsymbol{\\theta}^*\\) into our set (i.e., set \\(\\boldsymbol{\\theta}^{(s + 1)} = \\boldsymbol{\\theta}^*\\))\n\nIf \\(r &lt; 1:\\)\n\nIntuition: The relative frequency of \\(\\boldsymbol{\\theta}\\)-values in our set equal to \\(\\boldsymbol{\\theta}^*\\) compared to those equal to \\(\\boldsymbol{\\theta}^{(s)}\\) should be \\(r\\). This means that for every instance of \\(\\boldsymbol{\\theta}^{(s)}\\), we should have only a ‚Äúfraction‚Äù of an instance of a \\(\\boldsymbol{\\theta}^*\\) value.\nProcedure: Set \\(\\boldsymbol{\\theta}^{(s + 1)}\\) equal to either \\(\\boldsymbol{\\theta}^*\\) or \\(\\boldsymbol{\\theta}^{(s)}\\), with probability \\(r\\) and \\(1 ‚àí r\\) respectively."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-update",
    "href": "slides/03-mcmc.html#metropolis-update",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis update",
    "text": "Metropolis update\nGiven \\(\\boldsymbol{\\theta}^{(s)}\\), the Metropolis algorithm generates a value \\(\\boldsymbol{\\theta}^{(s + 1)}\\) as follows:\n\nSample \\(\\boldsymbol{\\theta}^*\\) from a proposal distribution, \\(\\boldsymbol{\\theta}^* ‚àº J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\)\nCompute the acceptance ratio \\(r\\)\nLet \\[\\boldsymbol{\\theta}^{(s + 1)} =\n\\left\\{\n  \\begin{array}{ll}\n\\boldsymbol{\\theta}^* & \\text{with probability }\\min(r, 1) \\\\\n\\boldsymbol{\\theta}^{(s)} & \\text{with probability }1 -\\min(r, 1)\n  \\end{array}\n\\right.\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-proposal-distribution",
    "href": "slides/03-mcmc.html#metropolis-proposal-distribution",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis proposal distribution",
    "text": "Metropolis proposal distribution\n\nThe proposal distribution is symmetric (i.e., \\(J(\\boldsymbol{\\theta}_a | \\boldsymbol{\\theta}_b) = J(\\boldsymbol{\\theta}_b | \\boldsymbol{\\theta}_a)\\)\nUsually \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) is very simple, with samples from \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) being near \\(\\boldsymbol{\\theta}\\) with high probability.\nThe most common proposal is a normal distribution\n\n\\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)}) = N(\\boldsymbol{\\theta}^{(s)}, \\boldsymbol{\\Delta})\\)\n\nThe value of the parameter \\(\\boldsymbol{\\Delta}\\) is generally chosen to make the approximation algorithm run efficiently"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-and-gibbs-combined",
    "href": "slides/03-mcmc.html#metropolis-and-gibbs-combined",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis and Gibbs combined",
    "text": "Metropolis and Gibbs combined\n\nThe Gibbs and Metropolis samplers are actually both algorithms within a larger class of Metropolis-Hastings algorithms\nWhen performing MCMC, one can actually choose to update a parameter using either a Gibbs or Metropolis update\nLet‚Äôs see this in action using our linear regression example"
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs",
    "href": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nIn this example, we will use a Metropolis update for \\(\\sigma^2\\), however we will actually focus on \\(\\log\\sigma^2\\).\n\nMetropolis requires a symmetric proposal, so it is often easier to transform parameters to be on the real line and use a normal proposal.\n\nWe will use the following proposal, \\(\\log\\sigma^{2*} \\sim N\\left(\\log\\sigma^{2(s)}, \\delta\\right)\\), where \\(\\delta = 1\\).\nWe will place the prior: \\(\\log\\sigma^2 \\sim N(0,1)\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs-1",
    "href": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs-1",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nsigma2 &lt;- exp(rnorm(1))\nsamples &lt;- NULL\ndelta &lt;- 1\nfor (s in 1:10000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n\n  ###Metropolis update for sigma2\n  # Sample a proposal value\n  log_sigma2_proposal &lt;- rnorm(1, log(sigma2), delta)\n  # Compute the ratio r on the log scale for numeric stability\n  # Also, I've decided to update log(sigma2) instead of sigma2, so I can use a normal proposal distribution\n  # I've placed a normal prior on log(sigma2)\n  likelihood_proposal &lt;- sum(dnorm(Y, X %*% beta, sqrt(exp(log_sigma2_proposal)), log = TRUE))\n  likelihood_current &lt;- sum(dnorm(Y, X %*% beta, sqrt(sigma2), log = TRUE))\n  prior_proposal &lt;- dnorm(log_sigma2_proposal, 0, 1, log = TRUE)\n  prior_current &lt;- dnorm(log(sigma2), 0, 1, log = TRUE)\n  log_r &lt;- (likelihood_proposal + prior_proposal) - (likelihood_current + prior_current)\n  # Update beta using Metropolis ratio\n  if (log(runif(1)) &lt; log_r) sigma2 &lt;- exp(log_sigma2_proposal)\n\n  ###Save samples after a burn-in\n  if (s &gt; 5000) samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-mcmc.html#inspect-results-1",
    "href": "slides/03-mcmc.html#inspect-results-1",
    "title": "Markov chain Monte Carlo",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-mcmc.html#summary-of-metropolis-algorithm",
    "href": "slides/03-mcmc.html#summary-of-metropolis-algorithm",
    "title": "Markov chain Monte Carlo",
    "section": "Summary of Metropolis algorithm",
    "text": "Summary of Metropolis algorithm\n\nMore flexible than Gibbs sampling, because we are no longer required to compute the full conditional distribution analytically.\nPosterior samples can be obtained, however the algorithm must be properly tuned (i.e., choosing \\(\\delta\\)) and the samples may take longer to converge.\nFurthermore, choosing a proper proposal distribution can be difficult in practice.\nIn more recent years, Hamiltonian Monte Carlo has emerged as an new MCMC approach that alleviates the aforementioned issues."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-hastings-mh-algorithm",
    "href": "slides/03-mcmc.html#metropolis-hastings-mh-algorithm",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis-Hastings (MH) algorithm",
    "text": "Metropolis-Hastings (MH) algorithm\nThe proposal distribution is no longer assumed to be symmetric, so the acceptance ratio is, \\[\\begin{aligned}\nr &= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}^*, \\sigma^{2(s)}) f(\\boldsymbol{\\beta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\beta}^{(s)},\\sigma^{2(s)})f(\\boldsymbol{\\beta}^{(s)})} \\frac{J(\\boldsymbol{\\beta}^* | \\boldsymbol{\\beta}^{(s)})}{J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*)}.\n\\end{aligned}\\]\nBoth Metropolis and Gibbs can easily be seen as subcases of Metropolis Hastings.\n\nMetropolis assumes a symmetric proposal, so the proposal terms cancel.\nGibbs assumes that \\(J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*) = f(\\boldsymbol{\\beta}^* | \\mathbf{Y}, \\sigma^{2(s)})\\), which implies that \\(r=1\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#hamiltonian-monte-carlo-hmc-intuition",
    "href": "slides/03-mcmc.html#hamiltonian-monte-carlo-hmc-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "Hamiltonian Monte Carlo (HMC) intuition",
    "text": "Hamiltonian Monte Carlo (HMC) intuition\n\nHMC is a new MCMC approach that has been shown to work better than the usual MH algorithm.\nIt is based on the idea of Hamiltonian dynamics (a physical concept)\n\n\n\n\n\n\n\nRollercoaster Metaphor\n\n\nImagine you‚Äôre on a roller coaster at an amusement park. As the roller coaster moves along the track, it goes up and down hills. When the roller coaster is at the top of a hill, it has a lot of potential energy (like stored energy). When it goes down the hill, that potential energy turns into kinetic energy (energy of motion), making the roller coaster go faster. Hamiltonian dynamics is like a set of rules that tells us how the roller coaster‚Äôs energy changes as it moves along the track."
  },
  {
    "objectID": "slides/03-mcmc.html#hmc-intuition",
    "href": "slides/03-mcmc.html#hmc-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "HMC intuition",
    "text": "HMC intuition\n\nHamiltonian dynamics is used to generate a proposal from a better proposal distribution, \\(J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*)\\), and modifies the acceptance part so the it has a higher acceptance rate.\nJust like the roller coaster follows the track smoothly, Hamiltonian Monte Carlo (HMC) helps us explore different possibilities smoothly and efficiently. This way, we can make more efficient samples from the posterior, just like how the roller coaster moves quickly and smoothly along its track.\nHMC requires evaluations of \\(\\log f(\\boldsymbol{\\theta} | \\mathbf{Y})\\) and \\(\\nabla_{\\boldsymbol{\\theta}} \\log f(\\boldsymbol{\\theta} | \\mathbf{Y})\\),\n\n\\[\\log f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto \\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) + \\log f(\\boldsymbol{\\theta})\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#prepare-for-next-class",
    "href": "slides/03-mcmc.html#prepare-for-next-class",
    "title": "Markov chain Monte Carlo",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nBegin HW 01 which is due January 29\nComplete reading to prepare for next Tuesday‚Äôs lecture\nTuesday‚Äôs lecture: Probabilistic Programming (Intro to Stan!)"
  },
  {
    "objectID": "prepare/prepare-jan13.html",
    "href": "prepare/prepare-jan13.html",
    "title": "Prepare for January 13 lecture",
    "section": "",
    "text": "Occasionally during the semester we will reference material from the text book A First Course in Bayesian Statistical Methods by Peter Hoff. As a Duke student, an electronic version of the book is freely available to you through the Duke Library. We will refer to this textbook as Hoff.\nüìñ Read about Bayesian inference and probability in Hoff Chapter 1 and 2\nüìñ Read about Monte Carlo sampling in Hoff Chapter 4, Sections 4.1 and 4.2\n‚úÖ Complete HW 00 tasks"
  },
  {
    "objectID": "prepare/prepare-jan15.html",
    "href": "prepare/prepare-jan15.html",
    "title": "Prepare for January 15 lecture",
    "section": "",
    "text": "üìñ Read Hoff Chapter 6, Sections 6.1-6.5 to learn about Gibbs sampling\nüìñ Read Hoff Chapter 10, Sections 10.2-10.4 to learn about Metropolis sampling\nüìñ Review simple linear regression\n‚úÖ Complete HW 00 tasks before class"
  },
  {
    "objectID": "hw/hw-01.html",
    "href": "hw/hw-01.html",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "",
    "text": "ImportantDue date\n\n\n\nThis assignment is due on Thursday, January 29 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercises-1-7",
    "href": "hw/hw-01.html#exercises-1-7",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercises 1-7",
    "text": "Exercises 1-7\nDefine a random variable \\(Y_i\\) that represents the number of hospital visits during pregnancy for each woman \\(i\\), for \\(i = 1,\\ldots,n\\). Assume that this random variable follows a Poisson distribution with rate \\(\\lambda\\), such that \\(Y_i \\stackrel{iid}{\\sim} \\text{Poisson}(\\lambda)\\). For a Poisson distribution, the mean and variance are equal to \\(\\lambda\\). We are interested in performing statistical inference on \\(\\lambda\\) using a Bayesian approach. A frequently used prior for \\(\\lambda\\) is \\(\\text{Gamma}(\\text{shape = }a, \\text{rate = }b)\\), where \\(\\mathbb{E}[\\lambda] = a/b\\) and \\(\\mathbb{V}(\\lambda) = a/b^2\\).\n\nExercise 1\nThe researchers have prior knowledge that leads them to believe that \\(\\lambda\\) should have mean 8 and variance 4. What values of \\(a\\) and \\(b\\) should they specify?\n\n\nExercise 2\nUsing the prior specified in Exercise 1, compute the probability that \\(\\lambda\\) is greater than 11? For this computation compute the exact probability using the pgamma function in R. This is equivalent to computing \\(P(\\lambda &gt; 11)\\).\n\n\nExercise 3\nCompute the same probability as in Exercise 2, this time using Monte Carlo sampling. Report your Monte Carlo standard error and make sure it is less than 0.01.\n\n\nExercise 4\nSuppose the researchers are interested in the quantity, \\(\\alpha = \\sqrt{\\lambda}\\). Compute the probability that \\(\\alpha\\) is greater than 2.5. Use the same number of Monte Carlo samples as in Exercise 3 and describe why Monte Carlo sampling makes this computation much more efficient than computing the exact probability.\n\n\nExercise 5\nUsing the prior specified in Exercise 1, compute the posterior distribution for \\(\\lambda\\), \\(f(\\lambda | \\mathbf{Y})\\), where \\(\\mathbf{Y}_i = (Y_1,\\ldots,Y_n).\\) Recall that the Gamma prior for \\(\\lambda\\) is a conjugate prior, so that the posterior is given by: \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). Visualize the posterior distribution and report the posterior mean and a 95% credible interval. Provide an interpretation of the posterior summaries within the context of the US births data.\n\n\nExercise 6\nWhat is the posterior probability that \\(\\lambda\\) is greater than 11? This is equivalent to computing \\(P(\\lambda &gt; 11 | \\mathbf{Y})\\). Again, use Monte Carlo sampling. Provide an interpretation for this probability in the context of hospital visits.\n\n\nExercise 7\nCreate a figure that includes both the prior and posterior distributions for \\(\\lambda\\). Also, include a figure of the observed data. Use these figures to make a comparison of the prior and posterior probabilities found in Exercise 3 and Exercise 6, respectively. Describe any changes in these two probabilities and how they relate to the observed data.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-8-10",
    "href": "hw/hw-01.html#exercise-8-10",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercise 8-10",
    "text": "Exercise 8-10\nDefine a random variable \\(weight_i\\) that represents the weight of the baby at birth in pounds for pregnancy \\(i\\). We are interested in learning the association between birth weight and the smoking habit, \\(habit_i\\), of the mother. Fit the following Bayesian linear regression model using Gibbs sampling,\n\\[\\begin{align*}\nweight_i &= \\beta_0 + \\beta_1 \\times 1(habit_i = \\text{smoker}) + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2),\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\\\\n\\boldsymbol{\\beta} &\\sim N(\\mathbf{0}, 100 \\mathbf{I})\\\\\n\\sigma^2 &\\sim \\text{Inv-Gamma}(3,1).\n\\end{align*}\\]\n\nExercise 8\nObtain samples from the posterior distribution of \\((\\boldsymbol{\\beta},\\sigma^2)\\) given the observed data. Visualize the posterior distributions and provide justification that the Gibbs sampler has converged.\n\n\nExercise 9\nReport the posterior mean, standard deviation, and 95% credible intervals for each parameter.\n\n\nExercise 10\nIf someone were to fit the same regression using a frequentist approach the resulting model would look like the following.\n\nmod &lt;- lm(weight ~ habit, data = births14)\nres &lt;- summary(mod)\nprint(res)\n\n\nCall:\nlm(formula = weight ~ habit, data = births14)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4965 -0.6865  0.0635  0.8150  3.1135 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.30654    0.04586 159.317  &lt; 2e-16 ***\nhabitsmoker -0.75203    0.16412  -4.582 5.34e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.241 on 792 degrees of freedom\nMultiple R-squared:  0.02583,   Adjusted R-squared:  0.0246 \nF-statistic:    21 on 1 and 792 DF,  p-value: 5.345e-06\n\n\nSuppose researchers are interested in testing the following hypothesis test: \\(H_0: \\beta_1 = 0, H_1: \\beta_1 &lt; 0\\). We can compute this p-value from the frequentist model.\n\npvalue &lt;- pt(coef(res)[, 3], mod$df, lower = TRUE)[2]\n\nThe resulting p-value is &lt;0.001. Compute the Bayesian p-value that corresponds to the same hypothesis test, \\(P(\\beta_1 &lt; 0 | \\mathbf{Y})\\). Interpret both p-values at a Type-I error rate of 0.05 and compare and contrast their interpretations in the context of the association between smoking and low birth weight.\n\nYou‚Äôre done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message ‚ÄúDone with Homework 1!‚Äù, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html",
    "href": "ae/ae-01-monte-carlo.html",
    "title": "AE 01: Monte Carlo sampling",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE is a demonstration and you do not have to turn anything in!\n\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#learning-goals",
    "href": "ae/ae-01-monte-carlo.html#learning-goals",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will‚Ä¶\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nPerform some Monte Carlo estimation"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-01-monte-carlo.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp26 organization on GitHub.\nClick on the repo with the prefix ae-01-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select SSH (this might already be selected by default, and if it is, you‚Äôll see the text Use a password-protected SSH key.). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#r-and-r-studio",
    "href": "ae/ae-01-monte-carlo.html#r-and-r-studio",
    "title": "AE 01: Monte Carlo sampling",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for ‚ÄúYAML Ain‚Äôt Markup Language‚Äù. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you‚Äôre happy with these changes, we‚Äôll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don‚Äôt have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we may tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\n\n\nPush changes\nNow that you have made an update and committed this change, it‚Äôs time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. Click on Push.\nNow let‚Äôs make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#exercise-1",
    "href": "ae/ae-01-monte-carlo.html#exercise-1",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Exercise 1",
    "text": "Exercise 1\nCompute a one-sided Bayesian p-value for \\(\\mu\\): \\(P(\\mu &lt; 14 | \\mathbf{Y})\\) using Monte Carlo sampling. Interpret the results in plain English.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#exercise-2",
    "href": "ae/ae-01-monte-carlo.html#exercise-2",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe can also compute \\(P(\\mu &lt; 14 | \\mathbf{Y})\\) using pnorm, since we have the posterior in closed form. Compute the exact probability and compare it to the Monte Carlo estimate given in Exercise 1.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#exercise-3",
    "href": "ae/ae-01-monte-carlo.html#exercise-3",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute a 95% confidence interval for \\(\\mu\\). Provide an interpretation for this confidence interval.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your AE repo on GitHub. You‚Äôre done! üéâ\n\nRecall: This AE is a demonstration and nothing needs to be turned in!"
  },
  {
    "objectID": "ae/ae-02-mcmc.html",
    "href": "ae/ae-02-mcmc.html",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE is a demonstration and you do not have to turn anything in!\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#learning-goals",
    "href": "ae/ae-02-mcmc.html#learning-goals",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will‚Ä¶\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nPerform Gibbs sampling for Bayesian linear regression and compute some basic summaries"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-02-mcmc.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp26 organization on GitHub.\nClick on the repo with the prefix ae-02-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select SSH (this might already be selected by default, and if it is, you‚Äôll see the text Use a password-protected SSH key.). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-02.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-02-mcmc.html#exercise-1",
    "href": "ae/ae-02-mcmc.html#exercise-1",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Exercise 1",
    "text": "Exercise 1\nCompute the posterior mean and standard deviation for the intercept, slope, and measurement error. Provide an interpretation for each of these parameter estimates.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#exercise-2",
    "href": "ae/ae-02-mcmc.html#exercise-2",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Exercise 2",
    "text": "Exercise 2\nCompute a 95% confidence interval for the regression slope. Provide an interpretation for this confidence interval.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#exercise-3",
    "href": "ae/ae-02-mcmc.html#exercise-3",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute a one-sided Bayesian p-value for the regression slope: \\(P(\\beta_1 &lt; 0)\\). Interpret the results in plain English. Is intraocular pressure associated with disease progression?\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your AE repo on GitHub. You‚Äôre done! üéâ\n\nThis AE is a demonstration and you do not have to turn anything in!"
  },
  {
    "objectID": "slides/04-stan.html#review-of-last-lecture",
    "href": "slides/04-stan.html#review-of-last-lecture",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Thursday, we performed posterior inference for Bayesian linear regression using Gibbs and Metropolis sampling.\n\nWe obtained correlated samples from the posterior using MCMC.\nGibbs required a lot math!\nMetropolis required tuning!\n\nToday we will introduce Stan, a probabilistic programming language that uses Hamiltonian Monte Carlo to perform general Bayesian inference."
  },
  {
    "objectID": "slides/04-stan.html#learning-objectives",
    "href": "slides/04-stan.html#learning-objectives",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this lecture you should:\n\nKnow how to start coding up a model in Stan.\nAppreciate how easy Stan makes things for us compared to coding up the algorithm ourselves.\nBe able to fit a basic linear regression in Stan."
  },
  {
    "objectID": "slides/04-stan.html#what-is-stan-and-how-do-we-use-it",
    "href": "slides/04-stan.html#what-is-stan-and-how-do-we-use-it",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "What is Stan and how do we use it?",
    "text": "What is Stan and how do we use it?\n\nStan is an intuitive yet sophisticated programming language that does the hard work for us.\nProgramming language like R, Python, Matlab, C++‚Ä¶\nWorks like most other languages: can use loops, conditional statements, and functions.\nCode up a model in Stan and then it implements HMC (actually something called NUTS) for us."
  },
  {
    "objectID": "slides/04-stan.html#why-should-we-use-stan",
    "href": "slides/04-stan.html#why-should-we-use-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Why should we use Stan?",
    "text": "Why should we use Stan?\n\nStan is the brainchild of Andrew Gelman at Columbia.\nStan uses an extension of HMC called NUTS that automatically tunes. It is fast.\nStan is simple to learn.\nStan has excellent documentation (a manual full of extensive examples).\nMost important: Stan has a very active and helpful user forum and development team; for example, typical question answered in less than a couple of hours."
  },
  {
    "objectID": "slides/04-stan.html#how-do-we-use-it",
    "href": "slides/04-stan.html#how-do-we-use-it",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "How do we use it?",
    "text": "How do we use it?\nCode up model in Stan code in a text editor and save as .stan file.\n\nCall Stan to run the model from:\n\nR, python, the command line, Matlab, Stata, Julia\n\nUse one of the above to analyse the data (of course you can export to another one)."
  },
  {
    "objectID": "slides/04-stan.html#a-straightforward-example",
    "href": "slides/04-stan.html#a-straightforward-example",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "A straightforward example",
    "text": "A straightforward example\nSuppose:\n\nWe record the height, \\(Y_i\\), of 10 people.\nWe want a model to explain the variation, and choose a normal likelihood: \\[Y_i \\sim N(\\mu, \\sigma^2)\\]\nWe choose the following (independent) priors on each parameter:\n\n\\(\\mu \\sim N(0, 1)\\)\n\\(\\sigma^2 \\sim IG(1, 1)\\)\n\nQuestion: How do we code this up in Stan?"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program",
    "href": "slides/04-stan.html#an-example-stan-program",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program",
    "text": "An example Stan program\n\ndata {\n  real Y[10]; // height for 10 people\n}\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma2;\n}\nmodel {\n  Y ~ normal(mu, sqrt(sigma2)); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma2 ~ inv_gamma(1, 1); // prior for sigma\n}"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-data-block",
    "href": "slides/04-stan.html#an-example-stan-program-data-block",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: data block",
    "text": "An example Stan program: data block\n\ndata {\n  real Y[10]; // height for 10 people\n}\n\n\nDeclare all data that you will pass to Stan to estimate your model.\nTerminate all statements with a semi-colon ;.\nUse ## or // for comments."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-data-block-1",
    "href": "slides/04-stan.html#an-example-stan-program-data-block-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: data block",
    "text": "An example Stan program: data block\n\ndata {\n  real Y[10]; // height for 10 people\n}\n\n\nWe need to tell Stan the type of data variable. For example:\n\nreal for continuous data.\nint for discrete data.\nArrays: above we specified Y as an array of continuous data of length 10."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-data-block-2",
    "href": "slides/04-stan.html#an-example-stan-program-data-block-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: data block",
    "text": "An example Stan program: data block\n\ndata {\n  real Y[10]; // height for 10 people\n}\n\nCan place limits on data, for example:\n\nreal&lt;lower = 0, upper = 1&gt; X;\nreal&lt;lower = 0&gt; Z;\n\nVectors and matrices; only contain reals and can be used for matrix operations.\n\nreal Y[10]; // array representation\nvector[10] Y; // vector representation"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-parameter-block",
    "href": "slides/04-stan.html#an-example-stan-program-parameter-block",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: parameter block",
    "text": "An example Stan program: parameter block\n\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma2;\n}\n\n\nDeclare all parameters that you use in your model.\nPlace limits on variables, for example:\n\nreal&lt;lower = 0&gt; sigma2;\n\n\nA multitude of parameter types including some of the aforementioned:\n\nreal for continuous parameters.\nArrays of types, for example real beta[10];"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-parameter-block-1",
    "href": "slides/04-stan.html#an-example-stan-program-parameter-block-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: parameter block",
    "text": "An example Stan program: parameter block\n\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma2;\n}\n\n\nvector or matrix, specified by:\n\nvector[5] beta;\nmatrix[5, 3] gamma;\n\nsimplex for a parameter vector that must sum to 1.\nMore exotic types like corr_matrix, or ordered."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-parameter-block-2",
    "href": "slides/04-stan.html#an-example-stan-program-parameter-block-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: parameter block",
    "text": "An example Stan program: parameter block\n\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma2;\n}\n\nImportant: Stan is not developed yet to work with discrete parameters. Options for discrete parameters in Stan:\n\nMarginalize out the parameter. For example, suppose we have \\(f(\\boldsymbol{\\beta}, \\theta)\\), where \\(\\boldsymbol{\\beta}\\) is continuous and \\(\\theta\\) is discrete:\n\n\n\\(f(\\boldsymbol{\\beta}) = \\sum_{i = 1}^K f(\\boldsymbol{\\beta}, \\theta_i)\\)\n\n\nSome models can be reformulated without discrete parameters."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-model-block",
    "href": "slides/04-stan.html#an-example-stan-program-model-block",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: model block",
    "text": "An example Stan program: model block\n\nmodel {\n  Y ~ normal(mu, sqrt(sigma2)); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma2 ~ inv_gamma(1, 1); // prior for sigma2\n}\n\n\nUsed to define:\n\nLikelihood.\nPriors on parameters.\n\n\nIf don‚Äôt specify priors on parameters Stan assumes you are using flat priors (which can be improper)."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-model-block-1",
    "href": "slides/04-stan.html#an-example-stan-program-model-block-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: model block",
    "text": "An example Stan program: model block\n\nmodel {\n  Y ~ normal(mu, sqrt(sigma2)); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma2 ~ inv_gamma(1, 1); // prior for sigma2\n}\n\nHuge range of probability distributions covered, across a range of parameterizations. For example:\n\nDiscrete: Bernoulli, binomial, Poisson, beta-binomial, negative-binomial, categorical, multinomial.\nContinuous unbounded: normal, skew-normal, student-t, Cauchy, logistic."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-model-block-2",
    "href": "slides/04-stan.html#an-example-stan-program-model-block-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: model block",
    "text": "An example Stan program: model block\n\nmodel {\n  Y ~ normal(mu, sqrt(sigma2)); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma2 ~ inv_gamma(1, 1); // prior for sigma2\n}\n\n\nContinuous bounded: uniform, beta, log-normal, exponential, gamma, chi-squared, inverse-chi-squared, Weibull, Wiener diffusion, Pareto.\nMultivariate continuous: normal, student-t, Gaussian process.\nExotics: Dirichlet, LKJ correlation distribution, Wishart and its inverse, Von-Mises."
  },
  {
    "objectID": "slides/04-stan.html#running-stan",
    "href": "slides/04-stan.html#running-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Running Stan",
    "text": "Running Stan\nWrite model in a text editing program and save as a .stan file.\n\nTo create a .stan file from RStudio, File -&gt; New File -&gt; Stan File.\n\n\n###Load packages\nlibrary(rstan)\n\n###Generate fake data\nY &lt;- rnorm(10, mean = 0, sd = 1)\n\n###Compile and run model, and save in fit\nfit &lt;- stan(file = 'straightforward.stan', data = list(Y = Y), \n            iter = 1000, chains = 4, seed = 1)"
  },
  {
    "objectID": "slides/04-stan.html#running-stan-on-example-model",
    "href": "slides/04-stan.html#running-stan-on-example-model",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Running Stan on example model",
    "text": "Running Stan on example model\n\n###Compile and run model, and save in fit\nfit &lt;- stan(file = 'straightforward.stan', data = list(Y = Y), \n            iter = 1000, chains = 4, seed = 1)\n\nThe above R code runs NUTS for our model with the following options:\n\n\\(S=1,000\\) MCMC samples of which 500 are discarded as warm-up.\nAcross 4 chains.\nUsing a random number seed of 1 (good to ensure you can reproduce results)."
  },
  {
    "objectID": "slides/04-stan.html#example-model-results",
    "href": "slides/04-stan.html#example-model-results",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Example model: results",
    "text": "Example model: results\n\n###Print summary statistics\nprint(fit, probs = c(0.25, 0.5, 0.75))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n        mean se_mean   sd   25%   50%   75% n_eff Rhat\nmu     -0.45    0.01 0.32 -0.66 -0.46 -0.26  1361    1\nsigma2  1.23    0.02 0.62  0.81  1.09  1.48   865    1\nlp__   -6.80    0.04 1.03 -7.21 -6.49 -6.06   808    1\n\nSamples were drawn using NUTS(diag_e) at Tue Jan 21 14:36:33 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/04-stan.html#example-model-results-1",
    "href": "slides/04-stan.html#example-model-results-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Example model: results",
    "text": "Example model: results\n\n###Extract posterior samples\npars &lt;- extract(fit)\nclass(pars)\n\n[1] \"list\"\n\nnames(pars)\n\n[1] \"mu\"     \"sigma2\" \"lp__\"  \n\n###Extract samples for particular parameters\npars &lt;- extract(fit, pars = \"mu\")\nclass(pars$mu)\n\n[1] \"array\"\n\ndim(pars$mu)\n\n[1] 2000"
  },
  {
    "objectID": "slides/04-stan.html#visualize-posterior",
    "href": "slides/04-stan.html#visualize-posterior",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Visualize posterior",
    "text": "Visualize posterior\n###Extract samples for particular parameters\nlibrary(ggplot2)\ndata.frame(mu = pars$mu) |&gt;\n  ggplot(aes(x = mu)) +\n  geom_histogram() +\n  labs(x = expression(mu), y = \"Count\", \n       subtitle = bquote(\"Posterior distribution for \" ~ mu))"
  },
  {
    "objectID": "slides/04-stan.html#quick-note-what-does-sim-mean",
    "href": "slides/04-stan.html#quick-note-what-does-sim-mean",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Quick note: what does \\(\\sim\\) mean?",
    "text": "Quick note: what does \\(\\sim\\) mean?\n\nmodel {\n  Y ~ normal(mu, sigma); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma ~ inv_gamma(1, 1); // prior for sigma\n}\n\n\n\\(\\sim\\) doesn‚Äôt mean sampling, although often times it can be thought of as sampling\nMCMC/HMC makes use of the log-posterior\n\n\\[\\log f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto \\log f(\\boldsymbol{\\theta}) + \\sum_{i=1}^n \\log f({Y}_i | \\boldsymbol{\\theta})\\]\n\nAs such \\(\\sim\\) really means increment log probability\nAll we have to do in Stan is specify the log-posterior!"
  },
  {
    "objectID": "slides/04-stan.html#alternate-way-of-specifying-stan-models",
    "href": "slides/04-stan.html#alternate-way-of-specifying-stan-models",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Alternate way of specifying Stan models",
    "text": "Alternate way of specifying Stan models\n\nmodel {\n  target += normal_lpdf(Y | mu, sqrt(sigma2)); // likelihood\n  target += normal_lpdf(mu | 0, 1); // prior for mu\n  target += inv_gamma_lpdf(sigma2 | 1, 1); // prior for sigma\n}\n\n\ntarget is a not a variable, but a special object that represents incremental log probability.\ntarget is initialized to zero.\nnormal_lpdf is the log of the normal density of y given location mu and scale sigma:\nStan documentation for normal distribution\n\n\ntarget += std_normal_lpdf(mu) // prior for mu using standard normal"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan-data-and-parameter-chunks",
    "href": "slides/04-stan.html#linear-regression-using-stan-data-and-parameter-chunks",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan: data and parameter chunks",
    "text": "Linear regression using Stan: data and parameter chunks\n\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates\n  vector[n] Y; // outcome vector\n  matrix[n, p + 1] X; // covariate vector\n  real beta0; // location hyperparameter for beta\n  real&lt;lower = 0&gt; sigma_beta; // scale hyperparameter for beta\n  real&lt;lower = 0&gt; a; // shape hyperparameter for sigma2\n  real&lt;lower = 0&gt; b; // scale hyperparameter for sigma2\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma2;\n}"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan-model-chunk",
    "href": "slides/04-stan.html#linear-regression-using-stan-model-chunk",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan: model chunk",
    "text": "Linear regression using Stan: model chunk\n\nmodel {\n  for (i in 1:n) {\n    target += normal_lpdf(Y[i] | X[i, ] * beta, sqrt(sigma2)); // likelihood\n  }\n  target += normal_lpdf(beta | beta0, sigma_beta); // prior for beta\n  target += inv_gamma_lpdf(sigma2 | a, b); // prior for sigma2\n}"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan-vectorization",
    "href": "slides/04-stan.html#linear-regression-using-stan-vectorization",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan: vectorization",
    "text": "Linear regression using Stan: vectorization\nIt is always a good idea to vectorize Stan code for faster and more efficient inference\n\nmodel {\n  target += normal_lpdf(Y | X * beta, sqrt(sigma2)); // likelihood\n  target += normal_lpdf(beta | beta0, sigma_beta); // prior for beta\n  target += inv_gamma_lpdf(sigma2 | a, b); // prior for sigma2\n}"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan",
    "href": "slides/04-stan.html#linear-regression-using-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan",
    "text": "Linear regression using Stan\n\n// saved in linear_regression.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates\n  vector[n] Y; // outcome vector\n  matrix[n, p + 1] X; // covariate vector\n  real beta0; // location hyperparameter for beta\n  real&lt;lower = 0&gt; sigma_beta; // scale hyperparameter for beta\n  real&lt;lower = 0&gt; a; // shape hyperparameter for sigma2\n  real&lt;lower = 0&gt; b; // scale hyperparameter for sigma2\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma2;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, sqrt(sigma2)); // likelihood\n  target += normal_lpdf(beta | beta0, sigma_beta); // prior for beta\n  target += inv_gamma_lpdf(sigma2 | a, b); // prior for sigma2\n}"
  },
  {
    "objectID": "slides/04-stan.html#lets-simulate-some-data-again",
    "href": "slides/04-stan.html#lets-simulate-some-data-again",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Let‚Äôs simulate some data again",
    "text": "Let‚Äôs simulate some data again\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/04-stan.html#fit-linear-regression-using-stan",
    "href": "slides/04-stan.html#fit-linear-regression-using-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Fit linear regression using Stan",
    "text": "Fit linear regression using Stan\n\n###Load packages\nlibrary(rstan)\n\n###Create stan data object\nstan_data &lt;- list(n = n,\n                  p = p,\n                  Y = Y,\n                  X = X,\n                  beta0 = 0,\n                  sigma_beta = 10,\n                  a = 3, \n                  b = 1)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_fit.rds\")"
  },
  {
    "objectID": "slides/04-stan.html#example-model-results-2",
    "href": "slides/04-stan.html#example-model-results-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Example model: results",
    "text": "Example model: results\n\n###Print summary statistics\nprint(fit, probs = c(0.25, 0.5, 0.75))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n           mean se_mean   sd     25%     50%     75% n_eff Rhat\nbeta[1]   -1.48    0.00 0.15   -1.58   -1.48   -1.37  1951    1\nbeta[2]    3.30    0.00 0.15    3.20    3.30    3.40  1674    1\nsigma2     2.35    0.01 0.33    2.12    2.32    2.55  1679    1\nlp__    -196.98    0.04 1.29 -197.58 -196.63 -196.04   899    1\n\nSamples were drawn using NUTS(diag_e) at Tue Jan 21 14:38:36 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/04-stan.html#stan-plots-point-estimate-and-intervals",
    "href": "slides/04-stan.html#stan-plots-point-estimate-and-intervals",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan plots: point estimate and intervals",
    "text": "Stan plots: point estimate and intervals\n\nstan_plot(fit, pars = c(\"beta\", \"sigma2\"), include_warmup = FALSE,\n          point_est = \"median\", ci_level = 0.8, outer_level = 0.95)"
  },
  {
    "objectID": "slides/04-stan.html#stan-plots-histogram",
    "href": "slides/04-stan.html#stan-plots-histogram",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan plots: histogram",
    "text": "Stan plots: histogram\n\nstan_hist(fit)"
  },
  {
    "objectID": "slides/04-stan.html#stan-plots-density",
    "href": "slides/04-stan.html#stan-plots-density",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan plots: density",
    "text": "Stan plots: density\n\nstan_dens(fit)"
  },
  {
    "objectID": "slides/04-stan.html#stan-a-few-of-the-loops-and-conditions",
    "href": "slides/04-stan.html#stan-a-few-of-the-loops-and-conditions",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan: a few of the loops and conditions",
    "text": "Stan: a few of the loops and conditions\nStan has pretty much the full range of language constructs to allow pretty much any model to be coded.\nfor (i in 1:10) {something;}\n\n\nwhile (i &gt; 1) {something;}\n\n\nif (i &gt; 1) {something 1;}\nelse if (i == 0) {something2;}\nelse {something 3;}"
  },
  {
    "objectID": "slides/04-stan.html#stan-speed-concerns",
    "href": "slides/04-stan.html#stan-speed-concerns",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan speed concerns",
    "text": "Stan speed concerns\nWhile Stan is fast it pays to know the importance of each code block for efficiency.\n\ndata: called once at beginning of execution.\ntransformed data: called once at beginning of execution.\nparameters: every log probability evaluation!\ntransformed parameters: every log probability evaluation!\nmodel: every log probability evaluation!\ngenerated quantities: once per sample.\nfunctions: how many times it is called depends on the function‚Äôs nature."
  },
  {
    "objectID": "slides/04-stan.html#stan-in-parallel",
    "href": "slides/04-stan.html#stan-in-parallel",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan in parallel",
    "text": "Stan in parallel\nIn R can run chains in parallel easily using:\n\nlibrary(rstan)\noptions(mc.cores = 8)"
  },
  {
    "objectID": "slides/04-stan.html#stan-summary",
    "href": "slides/04-stan.html#stan-summary",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan summary",
    "text": "Stan summary\n\nStan works by default with a HMC-like algorithm called NUTS.\nThe Stan language is similar in nature to other common languages with loops, conditional statements and user-definable functions (didn‚Äôt cover here).\nStan makes life easier for us than coding up the MCMC algorithms ourselves."
  },
  {
    "objectID": "slides/04-stan.html#r-packages-that-interface-with-stan",
    "href": "slides/04-stan.html#r-packages-that-interface-with-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "R packages that interface with Stan",
    "text": "R packages that interface with Stan\n\nrstan, brms, cmdstanr, rstanarm\nrstan and cmdstanr you write the Stan code, which gives you the most options.\n\nrstan has a more intuitive user interface.\ncmdstanr is more memory efficient and a lightweight interface to Stan.\n\nrstanarm and brms you don‚Äôt need to write the Stan code yourself, which makes it easier to use Stan, but is limiting.\n\nrstanarm‚Äôs biggest advantage is that the models are pre-compiled, but this is also it‚Äôs biggest limitation.\nbrms writes Stan code on the fly, so has many more models, some that are pretty advanced."
  },
  {
    "objectID": "slides/04-stan.html#prepare-for-next-class",
    "href": "slides/04-stan.html#prepare-for-next-class",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due January 29\nComplete reading to prepare for next Thursday‚Äôs lecture\nThursday‚Äôs lecture: Priors, Posteriors, and PPDs!"
  },
  {
    "objectID": "slides/05-priors-ppds.html#review-of-last-lecture",
    "href": "slides/05-priors-ppds.html#review-of-last-lecture",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Tuesday, we learned about Stan\n\nA probabilistic programming language for Bayesian inference\nWe learned about the data, parameter, and model code chunks\nWe used Stan to fit a Bayesian linear regression\n\nToday, we will dive into priors, posterior summaries, and posterior predictive distributions (PPDs)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-elicitation",
    "href": "slides/05-priors-ppds.html#prior-elicitation",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior elicitation",
    "text": "Prior elicitation\n\nSelecting the prior is one of the most important steps in a Bayesian analysis.\nThere is no ‚Äúright‚Äù way to select a prior.\nThe choices often depend on the objective of the study and the nature of the data.\n\nConjugate versus non-conjugate\nInformative versus uninformative\nProper versus improper\nSubjective versus objective"
  },
  {
    "objectID": "slides/05-priors-ppds.html#conjugate-priors",
    "href": "slides/05-priors-ppds.html#conjugate-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Conjugate priors",
    "text": "Conjugate priors\n\nA prior is conjugate if the posterior is a member of the same parametric family.\nWe have seen that if the response is normal and we use a normal prior on the regression parameter, the posterior is also a normal (if we use an inverse gamma distribution for the variance, the posterior is also inverse gamma).\n\nThis requires a pairing of the likelihood and prior.\nThere is a long list of conjugate priors.\n\nThe advantage of a conjugate prior is that the posterior is available in closed form.\nNo longer critical with Stan!"
  },
  {
    "objectID": "slides/05-priors-ppds.html#informative-versus-uninformative-priors",
    "href": "slides/05-priors-ppds.html#informative-versus-uninformative-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Informative versus uninformative priors",
    "text": "Informative versus uninformative priors\n\nIn some cases informative priors are available.\nPotential sources include: literature reviews; pilot studies; expert opinions; etc.\nPrior elicitation is the process of converting expert information to prior distribution.\nFor example, the expert might not comprehend an inverse gamma pdf, but if they give you an estimate and a spread you can back out \\(a\\) and \\(b\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-1",
    "href": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Informative versus uninformative priors",
    "text": "Informative versus uninformative priors\n\nStrong priors for the parameters of interest can be hard to defend.\nStrong priors for nuisance parameters are more common.\nFor example, say you are doing a Bayesian t-test to study the mean \\(\\mu\\), you might use an informative prior for the nuisance parameter \\(\\sigma^2\\).\nAny time informative priors are used you should conduct a sensitivity analysis.\n\nThat is, compare the posterior for several priors."
  },
  {
    "objectID": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-2",
    "href": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-2",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Informative versus uninformative priors",
    "text": "Informative versus uninformative priors\n\nIn most cases prior information is not available and so uninformative priors are used.\nOther names: vague, weak, flat, diffuse, etc.\n\nThese all refer to priors with large variance.\n\nExamples: \\(\\theta \\sim Uniform(0, 1)\\) or \\(\\mu ‚àº N(0, 1000^2)\\)\nUninformative priors can be conjugate or not conjugate.\nThe idea is that the likelihood overwhelms the prior.\nYou should verify this with a sensitivity analysis."
  },
  {
    "objectID": "slides/05-priors-ppds.html#improper-priors",
    "href": "slides/05-priors-ppds.html#improper-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Improper priors",
    "text": "Improper priors\n\nExtreme case: \\(\\mu \\sim N(0, \\tau^2)\\) and we set \\(\\tau = \\infty\\).\nA ‚Äúprior‚Äù that doesn‚Äôt integrate to one is called improper.\nExample: \\(f(\\mu) = 1\\) for all \\(\\mu \\in \\mathbb{R}\\).\nIt‚Äôs OK to use an improper prior so long as you verify that the posterior integrates to one.\nFor example, in linear regression an improper prior can be used for the slopes as long as the number of observations exceeds the number of covariates and there are no redundant predictors."
  },
  {
    "objectID": "slides/05-priors-ppds.html#subjective-versus-objective-priors",
    "href": "slides/05-priors-ppds.html#subjective-versus-objective-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Subjective versus objective priors",
    "text": "Subjective versus objective priors\n\nA subjective Bayesian picks a prior that corresponds to their current state of knowledge before collecting data.\nOf course, if the reader does not share this prior then they might not accept the analysis, and so sensitivity analysis is common.\nAn objective analysis is one that requires no subjective decisions by the analyst.\nSubjective decisions include picking the likelihood, treatment of outliers, transformations, ‚Ä¶ and prior specification.\nA completely objective analysis may be feasible in tightly controlled experiments, but is impossible in many analyses."
  },
  {
    "objectID": "slides/05-priors-ppds.html#objective-bayes",
    "href": "slides/05-priors-ppds.html#objective-bayes",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Objective Bayes",
    "text": "Objective Bayes\n\nAn objective Bayesian attempts to replace the subjective choice of prior with an algorithm that determines the prior.\nThere are many approaches: Jeffreys, reference, probability matching, maximum entropy, empirical Bayes, penalized complexity, etc.\nJeffreys priors are the most common: \\(f(\\boldsymbol{\\theta}) \\propto \\sqrt{\\det[I(\\boldsymbol{\\theta})]}\\), where \\(I(\\boldsymbol{\\theta})_{ij} = \\mathbb{E}\\left[\\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})}{\\partial \\theta_i}\\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})}{\\partial \\theta_j}\\right]\\) is the Fisher Information matrix.\nMany of these priors are improper and so you have to check that the posterior is proper."
  },
  {
    "objectID": "slides/05-priors-ppds.html#hyperpriors",
    "href": "slides/05-priors-ppds.html#hyperpriors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Hyperpriors",
    "text": "Hyperpriors\nPrior is the name for the distribution of model parameters that show up in the likelihood.\n\nFor example: In linear regression, \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are parameters in the likelihood, thus \\(f(\\boldsymbol{\\beta})\\) and \\(f(\\sigma^2)\\) are called priors.\n\nHyperprior is the name for the distribution of model parameters not in the likelihood.\nHyperparameter is the name for the parameters in a hyperprior."
  },
  {
    "objectID": "slides/05-priors-ppds.html#hyperpriors-1",
    "href": "slides/05-priors-ppds.html#hyperpriors-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Hyperpriors",
    "text": "Hyperpriors\n\nFor example: Suppose in linear regression, we place the following prior for \\(\\boldsymbol{\\beta}\\), \\(f(\\boldsymbol{\\beta} | \\boldsymbol{\\beta}_0, \\sigma_{\\beta}^2) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I})\\) and\n\n\\(f(\\boldsymbol{\\beta}_0) = N(\\mathbf{0}, \\mathbf{I})\\)\n\\(f(\\sigma_{\\beta}^2) = IG(a_{\\beta}, b_{\\beta})\\)\n\n\\(f(\\boldsymbol{\\beta}_0)\\) and \\(f(\\sigma_{\\beta}^2)\\) are hyperpriors.\n\\(a_{\\beta}, b_{\\beta}\\) are hyperparameters.\n\n\nQuestion: Why would a researcher be interested in a hyperprior?"
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-predictive-checks",
    "href": "slides/05-priors-ppds.html#prior-predictive-checks",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\n\nUse the prior distribution to obtain samples of the data, \\(\\mathbf{Y}\\).\n\n\\[\\begin{aligned}\nf(\\mathbf{Y}) &= \\int f(\\mathbf{Y}, \\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\\\\\n&= \\int f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\n\\end{aligned}\\]\n\nThis is easy to sample from using the following steps, 1. \\(\\boldsymbol{\\theta}^{sim} \\sim f(\\boldsymbol{\\theta})\\) and 2. \\(\\mathbf{Y}^{sim} \\sim f(\\mathbf{Y} | \\boldsymbol{\\theta}^{sim})\\).\nSimilar to Gibbs sampling, this gives a sample from the joint \\((\\mathbf{Y}^{sim}, \\boldsymbol{\\theta}^{sim})\\) and also the marginal \\(\\mathbf{Y}^{sim}\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-predictive-checks-1",
    "href": "slides/05-priors-ppds.html#prior-predictive-checks-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\n\nSuppose that I am interested in modeling the number of patients who are in the waiting room during an hour in the Duke ED, \\(Y_i\\).\nWe can model this random variable as \\(Y_i \\stackrel{iid}{\\sim}Poisson(\\lambda)\\) for \\(i = 1,\\ldots,n\\). A conjugate prior for \\(\\lambda\\) is \\(\\lambda \\sim Gamma(a,b)\\).\nWe know from experience that on average there are 20 patients waiting during an hour, so we want \\(\\mathbb{E}[Y_i]=\\lambda = 20\\).\nThus, we place a prior on \\(\\lambda\\) that is centered at 20, which requires that \\(a/b=20\\).\nThere are infinitely many priors specifications."
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-predictive-checks-2",
    "href": "slides/05-priors-ppds.html#prior-predictive-checks-2",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\n\nPrior predictive checks can be used to determine a realistic specification for \\(a\\) and \\(b\\).\n\n\ndata {\n  int&lt;lower = 1&gt; n;\n  real&lt;lower = 0&gt; a;\n  real&lt;lower = 0&gt; b;\n}\ngenerated quantities {\n  real lambda = gamma_rng(a, b);\n  vector[n] y_sim;\n  for (i in 1:n) y_sim[i] = poisson_rng(lambda);\n}\n\n\nWhen running Stan for prior predictive checks you must specify algorithm = \"Fixed_param\""
  },
  {
    "objectID": "slides/05-priors-ppds.html#visualizing-prior-predictive-checks",
    "href": "slides/05-priors-ppds.html#visualizing-prior-predictive-checks",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Visualizing prior predictive checks",
    "text": "Visualizing prior predictive checks\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the samples of \\(Y_i\\) under different prior specifications to compute a summary statstic, for example for the maximum value."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-point-estimates",
    "href": "slides/05-priors-ppds.html#posterior-summaries-point-estimates",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: point estimates",
    "text": "Posterior summaries: point estimates\n\nThe output of Bayesian inference is a probability distribution. It is often convenient to summarize the posterior in various ways.\nUsually summaries are computed for individual parameters using the marginal distributions,\n\n\\[f(\\theta_i|\\mathbf{Y}) = \\int f(\\boldsymbol{\\theta} | \\mathbf{Y}) d\\boldsymbol{\\theta}_{-i}.\\]\n\nThe posterior mean is defined by,\n\n\\[\\int_{-\\infty}^{\\infty}\\theta_i f(\\theta_i|\\mathbf{Y}) d\\theta_i.\\]"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-point-estimates-1",
    "href": "slides/05-priors-ppds.html#posterior-summaries-point-estimates-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: point estimates",
    "text": "Posterior summaries: point estimates\n\nThe posterior median m is defined by,\n\n\\[\\int_{-\\infty}^{m} f(\\theta_i|\\mathbf{Y}) d\\theta_i = 0.5 = \\int_m^{\\infty} f(\\theta_i|\\mathbf{Y}) d\\theta_i.\\]\n\nThe posterior mode is given by,\n\n\\[M=\\text{argmax} f(\\theta_i|\\mathbf{Y}) \\]\n\nThe mean and mode are also well defined for the joint posterior distribution."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-intervals",
    "href": "slides/05-priors-ppds.html#posterior-summaries-intervals",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: intervals",
    "text": "Posterior summaries: intervals\n\nThe Bayesian analogue of a frequentist confidence interval is a credible interval.\nAn interval \\((a,b)\\) is a \\(100(1-\\alpha)\\%\\) posterior credible interval for \\(\\theta_i\\) if\n\n\\[\\int_a^b f(\\theta_i|\\mathbf{Y}) d\\theta_i = (1-\\alpha),\\quad 0\\leq \\alpha \\leq 1.\\]\n\nA credible region can be defined similarly for a joint distribution."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-intervals-1",
    "href": "slides/05-priors-ppds.html#posterior-summaries-intervals-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: intervals",
    "text": "Posterior summaries: intervals\n\nCredible intervals are not unique. The two most common are symmetric and highest posterior density (HPD).\n\nSymmetric: An interval \\((a,b)\\) is a symmetric \\(100(1-\\alpha)\\%\\) credible interval if,\n\n\\[\\int_{-\\infty}^a f(\\theta_i|\\mathbf{Y}) d\\theta_i = \\frac{\\alpha}{2} = \\int_b^{\\infty} f(\\theta_i|\\mathbf{Y}) d\\theta_i.\\]\n\nHighest posterior density (HPD): An interval \\((a,b)\\) is a \\(100(1-\\alpha)\\%\\) HPD interval if,\n\n\\([a,b]\\) is a \\(100(1-\\alpha)\\%\\) credible interval for \\(\\theta_i\\)\nFor all \\(\\theta_i \\in [a,b]\\) and \\(\\theta_i^* \\notin [a,b]\\), \\(f(\\theta_i|\\mathbf{Y}) \\geq f(\\theta_i^*|\\mathbf{Y})\\)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval",
    "href": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: HPD credible interval",
    "text": "Posterior summaries: HPD credible interval"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval-1",
    "href": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: HPD credible interval",
    "text": "Posterior summaries: HPD credible interval"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-probability",
    "href": "slides/05-priors-ppds.html#posterior-summaries-probability",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: probability",
    "text": "Posterior summaries: probability\n\nWe may be interested in a hypothesis test: \\(H_0: \\theta \\leq c\\) versus \\(H_1: \\theta \\geq c\\)\nWe can report the posterior probability of the null hypothesis\n\n\\(P(\\theta \\leq c | \\mathbf{Y}) = \\mathbb{E}[1(\\theta \\leq c) | \\mathbf{Y}]\\)\n\nInterpretation of the posteriour probability:\n\nProbability that the null is true \\(P(\\theta \\leq c | \\mathbf{Y})\\)\n\nInterpretation of the p-value:\n\nProbability of observing a test-statistic as or more extreme given that the null is true\nEvidence for or against the null (reject or fail to reject)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-estimation",
    "href": "slides/05-priors-ppds.html#posterior-summaries-estimation",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: estimation",
    "text": "Posterior summaries: estimation\n\nWe have already seen that we can use MC or MCMC to estimate these posterior summaries!\nTo compute the HPD interval we can use the hdi function from the ggdist R package\n\n\nlibrary(ggdist)\ny &lt;- rgamma(10000, 3, 1)\nquantile(y, probs = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.6184613 7.2143297 \n\nggdist::hdi(y)\n\n          [,1]     [,2]\n[1,] 0.2539334 6.291464"
  },
  {
    "objectID": "slides/05-priors-ppds.html#linear-regression-recall",
    "href": "slides/05-priors-ppds.html#linear-regression-recall",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Linear regression recall",
    "text": "Linear regression recall\n\nAssume we observe \\((Y_i,\\mathbf{x}_i)\\) for \\(i = 1,\\ldots,n\\), where \\[Y_i|\\boldsymbol{\\beta},\\sigma^2 \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2).\\]\nThe full data likelihood is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta},\\sigma^2),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\).\nWe have parameters \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\beta}^\\top,\\sigma^2)\\) and \\(f(\\boldsymbol{\\theta}) = f(\\boldsymbol{\\beta})f(\\sigma^2)\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-definition",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-definition",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: definition",
    "text": "Posterior predictive distribution: definition\n\nAssume we observe a new \\(\\mathbf{x}'\\) and we would like to make some type of prediction about \\(Y'\\) given the data we have already observed, \\(\\mathbf{Y}\\).\nThe posterior predictive distribution is defined as \\(f(Y'|\\mathbf{Y})\\) and can be written as, \\[\\begin{aligned}\nf(Y' | \\mathbf{Y}) &= \\int f(Y' , \\boldsymbol{\\theta} | \\mathbf{Y}) d\\boldsymbol{\\theta},\\quad\\text{(marginal)}\\\\\n&= \\int f(Y' | \\boldsymbol{\\theta},\\mathbf{Y}) f(\\boldsymbol{\\theta} | \\mathbf{Y})d\\boldsymbol{\\theta},\\quad\\text{(conditional)}\\\\\n&= \\int \\underbrace{f(Y' | \\boldsymbol{\\theta})}_{likelihood} \\underbrace{f(\\boldsymbol{\\theta} | \\mathbf{Y})}_{posterior}d\\boldsymbol{\\theta}.\\quad\\text{(independence)}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-estimation",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-estimation",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: estimation",
    "text": "Posterior predictive distribution: estimation\n\nThe PPD can be written as an expectation,\n\n\\[f(Y' | \\mathbf{Y})  = \\int f(Y' | \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\mathbf{Y})d\\boldsymbol{\\theta} = \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[f(Y' | \\boldsymbol{\\theta})\\right].\\]\n\nThus, we can estimate the PPD using a Monte Carlo estimate,\n\n\\[\\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[f(Y' | \\boldsymbol{\\theta})\\right] \\approx \\frac{1}{S} \\sum_{s = 1}^S f\\left(Y' | \\boldsymbol{\\theta}^{(s)}\\right),\\]\nwhere \\(\\left\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(S)}\\right\\}\\) are samples from the posterior."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: Stan",
    "text": "Posterior predictive distribution: Stan\n\nWe want to compute the posterior predictive distribution.\nUse generated quantities block.\n\n\ngenerated quantities {\n  vector[n] in_sample;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(X[i] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-1",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: Stan",
    "text": "Posterior predictive distribution: Stan\n\ngenerated quantities {\n  vector[n] in_sample;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(X[i] * beta, sigma);\n  }\n}\n\nThe function normal_rng generates a single independent sample from a normal distribution with parameters:\n\nmean = X[i, ] * beta, where beta is a sample from the estimated posterior.\nsd = sigma, where sigma is a sample from the estimated posterior.\n\nThis computes the posterior predictive distribution for the original data."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-2",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-2",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: Stan",
    "text": "Posterior predictive distribution: Stan\nThe following additions are added to the linear regression Stan code.\n\n// saved in linear_regression_ppd.stan\ndata {\n  ...\n  int&lt;lower = 1&gt; n_pred; // number of new observations\n  matrix[n_pred, p + 1] X_pred; // covariate matrix for new observations\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n_pred] out_sample;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(X[i, ] * beta, sigma);\n  }\n  for (i in 1:n_pred) {\n    out_sample[i] = normal_rng(X_pred[i, ] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/05-priors-ppds.html#lets-simulate-some-data-again",
    "href": "slides/05-priors-ppds.html#lets-simulate-some-data-again",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Let‚Äôs simulate some data again",
    "text": "Let‚Äôs simulate some data again\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\nn_pred &lt;- 10\n  \n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\nX_pred &lt;- cbind(1, matrix(rnorm(n_pred * p), ncol = p))\nY_pred &lt;- as.numeric(X_pred %*% beta + rnorm(n_pred, 0, sigma))"
  },
  {
    "objectID": "slides/05-priors-ppds.html#fit-linear-regression-using-stan",
    "href": "slides/05-priors-ppds.html#fit-linear-regression-using-stan",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Fit linear regression using Stan",
    "text": "Fit linear regression using Stan\n\n###Load packages\nlibrary(rstan)\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_fit.rds\")"
  },
  {
    "objectID": "slides/05-priors-ppds.html#examining-prediction-performance",
    "href": "slides/05-priors-ppds.html#examining-prediction-performance",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Examining prediction performance",
    "text": "Examining prediction performance\n\n###Inspect the ppd\nppds &lt;- extract(fit, pars = c(\"in_sample\", \"out_sample\"))\nlapply(ppds, dim)\n\n$in_sample\n[1] 2000  100\n\n$out_sample\n[1] 2000   10"
  },
  {
    "objectID": "slides/05-priors-ppds.html#prepare-for-next-class",
    "href": "slides/05-priors-ppds.html#prepare-for-next-class",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due next Thursday before class\nComplete reading to prepare for next Tuesday‚Äôs lecture\nTuesday‚Äôs lecture: Model checking"
  },
  {
    "objectID": "prepare/prepare-jan20.html",
    "href": "prepare/prepare-jan20.html",
    "title": "Prepare for January 20 lecture",
    "section": "",
    "text": "üìñ Read Getting started with Stan\nüìñ Read Linear regression in Stan\n‚úÖ Work on HW 01"
  },
  {
    "objectID": "prepare/prepare-jan22.html",
    "href": "prepare/prepare-jan22.html",
    "title": "Prepare for January 22 lecture",
    "section": "",
    "text": "üìñ Read about the posterior predictive distribution in Hoff Chapter 4, Section 4.3\nüìñ Review prior choice recommendations\n‚úÖ Work on HW 01"
  },
  {
    "objectID": "ae/ae-03-stan.html",
    "href": "ae/ae-03-stan.html",
    "title": "AE 03: Introduction to Stan",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-03-stan.html#learning-goals",
    "href": "ae/ae-03-stan.html#learning-goals",
    "title": "AE 03: Introduction to Stan",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will‚Ä¶\n\nBe familiar with the workflow using RStudio and GitHub\nPerform Bayesian linear regression using HMC\nPrepare data for a regression task in Stan\nPrint posterior results from Stan"
  },
  {
    "objectID": "ae/ae-03-stan.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-03-stan.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 03: Introduction to Stan",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp26 organization on GitHub.\nClick on the repo with the prefix ae-03-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select SSH (this might already be selected by default, and if it is, you‚Äôll see the text Use a password-protected SSH key.). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-03.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-03-stan.html#exercise-1",
    "href": "ae/ae-03-stan.html#exercise-1",
    "title": "AE 03: Introduction to Stan",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit the following model in Stan and present posterior summaries.\n\\[\nY_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2),\n\\] where \\(\\mathbf{x}_i = (Age_i, 1(Sex_i = Male), BMI_i, BP_i)\\) and flat priors for all parameters: \\(f(\\alpha,\\boldsymbol{\\beta},\\sigma) \\propto c.\\) Flat priors are specified by default, so we can omit any prior specification.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-03-stan.html#exercise-2",
    "href": "ae/ae-03-stan.html#exercise-2",
    "title": "AE 03: Introduction to Stan",
    "section": "Exercise 2",
    "text": "Exercise 2\nFit the same model using the lm function in R. Compare the Bayesian and OLS/MLE results.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-03-stan.html#exercise-3",
    "href": "ae/ae-03-stan.html#exercise-3",
    "title": "AE 03: Introduction to Stan",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit the same regression model as in Exercise 1, but with the following priors:\n\\[\\begin{align*}\n\\alpha &\\sim N(0,10)\\\\\n\\beta_j &\\sim N(0,10),\\quad j=1,\\ldots,p\\\\\n\\sigma^2 &\\sim \\text{Inv-Gamma}(3,1).\n\\end{align*}\\]\nCompare the results from this model to the previous two models.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your AE repo on GitHub. You‚Äôre done! üéâ"
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html",
    "href": "ae/ae-04-priors-ppd.html",
    "title": "AE 04: Priors in Stan",
    "section": "",
    "text": "ImportantDue date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html#learning-goals",
    "href": "ae/ae-04-priors-ppd.html#learning-goals",
    "title": "AE 04: Priors in Stan",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will‚Ä¶\n\nUnderstand the idea of centering data for stabilizing inference\nGain knowledge on prior specification and its sometimes unintended impact\nBe able to compute and interpret a posterior predictive distribution"
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-04-priors-ppd.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 04: Priors in Stan",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp26 organization on GitHub.\nClick on the repo with the prefix ae-04-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select SSH (this might already be selected by default, and if it is, you‚Äôll see the text Use a password-protected SSH key.). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-04.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html#exercise-1",
    "href": "ae/ae-04-priors-ppd.html#exercise-1",
    "title": "AE 04: Priors in Stan",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit the centered regression model detailed above with the following priors: \\(\\alpha \\sim N(0,10^2)\\), \\(\\beta_j \\sim N(0,10^2)\\), and \\(\\sigma^2 \\sim \\text{Inv-Gamma}(3,1).\\) Obtain posterior samples for this model using Stan. Be sure to present posterior samples for \\(\\alpha^*\\).\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html#exercise-2",
    "href": "ae/ae-04-priors-ppd.html#exercise-2",
    "title": "AE 04: Priors in Stan",
    "section": "Exercise 2",
    "text": "Exercise 2\nStan is extremely flexible in terms of the priors that can be used for parameters. Using the centered data specification, change the priors for all three parameters. Report how sensitive the results are.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html#exercise-3",
    "href": "ae/ae-04-priors-ppd.html#exercise-3",
    "title": "AE 04: Priors in Stan",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute the posterior predictive distribution for a 60 year old male with a BMI of 25 and average blood pressure of 85.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your AE repo on GitHub. You‚Äôre done! üéâ"
  },
  {
    "objectID": "prepare/prepare-jan27.html",
    "href": "prepare/prepare-jan27.html",
    "title": "Prepare for January 27 lecture",
    "section": "",
    "text": "Occasionally during the semester we will reference material from the text book Bayesian Data Analysis Third Edition by Andrew Gelman et al.¬†A PDF of the textbook is available for download at Andrew Gelman‚Äôs website. We will refer to this textbook as BDA3.\nüìñ Read BDA3 Chapter 6 to learn about posterior predictive checking\nüìñ Read BDA3 Chapter 11.4 and 11.5 to learn about model convergence metrics\n‚úÖ Work on HW 01 which is due Thursday before class"
  },
  {
    "objectID": "prepare/prepare-jan29.html",
    "href": "prepare/prepare-jan29.html",
    "title": "Prepare for January 29 lecture",
    "section": "",
    "text": "üìñ Read sections 9.1-9.4 of Mark Lai‚Äôs course notes for an introduction to model comparison\nüìñ Review BDA3 Chapter 7.1-7.3 to learn more about model comparison metrics\n‚úÖ Work on HW 01 which is due Thursday before class"
  },
  {
    "objectID": "slides/06-model-checking.html#review-of-last-lecture",
    "href": "slides/06-model-checking.html#review-of-last-lecture",
    "title": "Model checking",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Thursday, we learned about\n\nDifferent types of priors that can be specified\nPosterior summaries: point estimates, intervals, and probabilities\nPosterior predictive distributions\nWe learned about the generated quantities code chunk\n\nToday, we will dive into (1) methods for assessing MCMC convergence, and (2) model performance techniques."
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-in-a-few-iterations",
    "href": "slides/06-model-checking.html#convergence-in-a-few-iterations",
    "title": "Model checking",
    "section": "Convergence in a few iterations",
    "text": "Convergence in a few iterations"
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-in-a-few-hundred-iterations",
    "href": "slides/06-model-checking.html#convergence-in-a-few-hundred-iterations",
    "title": "Model checking",
    "section": "Convergence in a few hundred iterations",
    "text": "Convergence in a few hundred iterations"
  },
  {
    "objectID": "slides/06-model-checking.html#this-one-never-converged",
    "href": "slides/06-model-checking.html#this-one-never-converged",
    "title": "Model checking",
    "section": "This one never converged",
    "text": "This one never converged"
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-is-questionable",
    "href": "slides/06-model-checking.html#convergence-is-questionable",
    "title": "Model checking",
    "section": "Convergence is questionable",
    "text": "Convergence is questionable"
  },
  {
    "objectID": "slides/06-model-checking.html#traceplots-for-linear-regression",
    "href": "slides/06-model-checking.html#traceplots-for-linear-regression",
    "title": "Model checking",
    "section": "Traceplots for linear regression",
    "text": "Traceplots for linear regression\n\nlibrary(rstan)\nfit &lt;- readRDS(file = \"linear_regression_ppd_fit.rds\")\nrstan::traceplot(fit, pars = c(\"beta\", \"sigma\"), \n                 inc_warmup = TRUE)"
  },
  {
    "objectID": "slides/06-model-checking.html#autocorrelation",
    "href": "slides/06-model-checking.html#autocorrelation",
    "title": "Model checking",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nIdeally the samples would be independent across iteration.\nWhen using MCMC we are obtaining dependent samples from the posterior.\nThe autocorrelation function \\(\\rho(h)\\) is the correlation between samples \\(h\\) iterations apart.\nLower values are better, but if the chains are long enough even large values can be OK.\nHighly correlated samples have less information than independent samples."
  },
  {
    "objectID": "slides/06-model-checking.html#autocorrelation-for-linear-regression",
    "href": "slides/06-model-checking.html#autocorrelation-for-linear-regression",
    "title": "Model checking",
    "section": "Autocorrelation for linear regression",
    "text": "Autocorrelation for linear regression\n\nstan_ac(fit, pars = c(\"beta\", \"sigma\"), \n        separate_chains = TRUE, lags = 25)"
  },
  {
    "objectID": "slides/06-model-checking.html#autocorrelation-for-linear-regression-1",
    "href": "slides/06-model-checking.html#autocorrelation-for-linear-regression-1",
    "title": "Model checking",
    "section": "Autocorrelation for linear regression",
    "text": "Autocorrelation for linear regression\n\nlibrary(bayesplot)\nmcmc_acf(fit, regex_pars = c(\"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/06-model-checking.html#effective-sample-size",
    "href": "slides/06-model-checking.html#effective-sample-size",
    "title": "Model checking",
    "section": "Effective sample size",
    "text": "Effective sample size\nThe effective samples size is,\n\\[n_{eff}=ESS(\\theta_i) = \\frac{mS}{1 + 2 \\sum_{h = 1}^{\\infty} \\rho (h)},\\] where \\(m\\) is the number of chains, \\(S\\) is the number of MCMC samples, and \\(\\rho(h)\\) is the \\(h\\)th order autocorrelation for \\(\\theta_i\\).\n\nThe correlated MCMC sample of length \\(mS\\) has the same information as \\(n_{eff}\\) independent samples.\nRule of thumb: \\(n_{eff}\\) should be at least a thousand for all parameters."
  },
  {
    "objectID": "slides/06-model-checking.html#effective-sample-size-for-linear-regression",
    "href": "slides/06-model-checking.html#effective-sample-size-for-linear-regression",
    "title": "Model checking",
    "section": "Effective sample size for linear regression",
    "text": "Effective sample size for linear regression\n\nprint(fit, pars = c(\"beta\", \"sigma\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -1.48       0 0.16 -1.79 -1.58 -1.48 -1.37 -1.16  2002    1\nbeta[2]  3.30       0 0.15  3.01  3.19  3.29  3.40  3.59  1980    1\nsigma    1.55       0 0.11  1.36  1.47  1.54  1.62  1.78  1909    1\n\nSamples were drawn using NUTS(diag_e) at Fri Nov 22 10:44:51 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/06-model-checking.html#standard-errors-of-posterior-mean-estimates",
    "href": "slides/06-model-checking.html#standard-errors-of-posterior-mean-estimates",
    "title": "Model checking",
    "section": "Standard errors of posterior mean estimates",
    "text": "Standard errors of posterior mean estimates\n\nThe sample mean from our MCMC draws is a point estimate for the posterior mean.\nThe standard error (SE) of this estimate can be used as a diagnostic.\nAssuming independence, the Monte Carlo standard error is \\(\\text{MCSE} = \\frac{s}{\\sqrt{S}},\\) where \\(s\\) is the sample standard deviation and \\(S\\) is the number of samples.\nA more realistic standard error is \\(\\text{MCSE} = \\frac{s}{\\sqrt{n_{eff}}}.\\)\n\\(\\text{MCSE} \\rightarrow 0\\) as \\(S \\rightarrow \\infty\\), whereas the standard deviation of the posterior draws approaches the standard deviation of the posterior distribution."
  },
  {
    "objectID": "slides/06-model-checking.html#assessing-mixing-using-between--and-within-sequence-variances",
    "href": "slides/06-model-checking.html#assessing-mixing-using-between--and-within-sequence-variances",
    "title": "Model checking",
    "section": "Assessing mixing using between- and within-sequence variances",
    "text": "Assessing mixing using between- and within-sequence variances\nFor a scalar parameter, \\(\\theta\\), define the MCMC samples as \\(\\theta_{ij}\\) for chain \\(j=1,\\ldots,m\\) and simulations \\(i = 1,\\ldots,n\\). We can compute the between- and within-sequence variances:\n\\[\\begin{aligned}\nB &= \\frac{n}{m-1}\\sum_{j=1}^m \\left(\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot}\\right)^2,\\quad \\bar{\\theta}_{\\cdot j} = \\frac{1}{n}\\sum_{i=1}^n \\theta_{ij},\\quad\\bar{\\theta}_{\\cdot \\cdot} = \\frac{1}{m} \\sum_{j=1}^m \\bar{\\theta}_{\\cdot j}\\\\\nW &= \\frac{1}{m}\\sum_{j=1}^m s_j^2,\\quad s_j^2=\\frac{1}{n-1}\\sum_{i=1}^n \\left(\\theta_{ij} - \\bar{\\theta}_{\\cdot j}\\right)^2\n\\end{aligned}\\]\nThe between-sequence variance, \\(B\\) contains a factor of \\(n\\) because it is based on the variance of the within-sequence means, \\(\\bar{\\theta}_{\\cdot j}\\), each of which is an average of \\(n\\) values \\(\\theta_{ij}\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-metric-widehatr",
    "href": "slides/06-model-checking.html#convergence-metric-widehatr",
    "title": "Model checking",
    "section": "Convergence metric: \\(\\widehat{R}\\)",
    "text": "Convergence metric: \\(\\widehat{R}\\)\nEstimate a total variance \\(\\mathbb{V}(\\theta | \\mathbf{Y})\\) as a weighted mean of \\(W\\), \\(B\\) \\[\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y}) = \\frac{n-1}{n}W + \\frac{1}{n}B\\]\n\nThis overestimates marginal posterior variance if starting points are overdispersed.\nGiven finite \\(n\\), \\(W\\) underestimates marginal posterior variance.\n\nSingle chains have not yet visited all points in the distribution.\nWhen \\(n \\rightarrow \\infty\\), \\(\\mathbb{E}[W] \\rightarrow \\mathbb{V}(\\theta | \\mathbf{Y})\\)\n\nAs \\(\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})\\) overestimates and \\(W\\) underestimates, we can compute\n\n\\[\\widehat{R} = \\sqrt{\\frac{\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})}{W}}, \\quad \\widehat{R} \\rightarrow 1 \\text{ as }n \\rightarrow \\infty.\\]"
  },
  {
    "objectID": "slides/06-model-checking.html#widehatr-for-linear-regression",
    "href": "slides/06-model-checking.html#widehatr-for-linear-regression",
    "title": "Model checking",
    "section": "\\(\\widehat{R}\\) for linear regression",
    "text": "\\(\\widehat{R}\\) for linear regression\n\nprint(fit, pars = c(\"beta\", \"sigma\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -1.48       0 0.16 -1.79 -1.58 -1.48 -1.37 -1.16  2002    1\nbeta[2]  3.30       0 0.15  3.01  3.19  3.29  3.40  3.59  1980    1\nsigma    1.55       0 0.11  1.36  1.47  1.54  1.62  1.78  1909    1\n\nSamples were drawn using NUTS(diag_e) at Fri Nov 22 10:44:51 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nA good rule of thumb is to want \\(\\widehat{R} \\leq 1.1\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#what-to-do-if-your-mcmc-does-not-converge",
    "href": "slides/06-model-checking.html#what-to-do-if-your-mcmc-does-not-converge",
    "title": "Model checking",
    "section": "What to do if your MCMC does not converge?",
    "text": "What to do if your MCMC does not converge?\nGelman‚Äôs Folk Theorem:\n\n\nWhen you have computational problems, often there‚Äôs a problem with your model.\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-slow-convergence",
    "href": "slides/06-model-checking.html#sampling-issues-slow-convergence",
    "title": "Model checking",
    "section": "Sampling issues: slow convergence",
    "text": "Sampling issues: slow convergence\nPoor chain mixing is usually due to lack of parameter identification.\n\nA parameter is identified if it has some unique effect on the data generating process that can be separated from the effect of the other parameters.\n\nSolution: use simulated data where you know the true parameter values.\n\nInformative as to whether the model is sufficient to estimate a parameter‚Äôs value."
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-divergent-iterations",
    "href": "slides/06-model-checking.html#sampling-issues-divergent-iterations",
    "title": "Model checking",
    "section": "Sampling issues: divergent iterations",
    "text": "Sampling issues: divergent iterations\nYou may get a warning in output from Stan with the number of iterations where the NUTS sampler has terminated prematurely.\n\nWarning messages:\n1: There were 62 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n2: There were 8 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttps://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded \n\nSolution: If this warning appears, you can try:\n\nIncrease adapt_delta (double, between 0 and 1, defaults to 0.8).\nIncrease max_treedepth (integer, positive, defaults to 10).\nPlay with stepsize (double, positive, defaults to 1)."
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-divergent-iterations-1",
    "href": "slides/06-model-checking.html#sampling-issues-divergent-iterations-1",
    "title": "Model checking",
    "section": "Sampling issues: divergent iterations",
    "text": "Sampling issues: divergent iterations\n\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000,\n                control = list(adapt_delta = 0.8, \n                               max_treedepth = 10,\n                               stepsize = 1))\n\n\nSee help(stan) for a full list of options that can be set in control.\nComplete set of recommendations: runtime warnings and convergence problems\n\nIf the above doesn‚Äôt help change priors then likelihood!"
  },
  {
    "objectID": "slides/06-model-checking.html#coding-errors",
    "href": "slides/06-model-checking.html#coding-errors",
    "title": "Model checking",
    "section": "Coding errors",
    "text": "Coding errors\nStan error messages are generally quite informative, however inevitably there are times when it is less clear why code fails.\n\nOne option is to debug by print.\n\n\nmodel {\n  ...\n  print(theta);\n}\n\nIn R this prints (neatly) to the console output.\nMore details on printing can be found on Stan: print statements"
  },
  {
    "objectID": "slides/06-model-checking.html#coding-errors-1",
    "href": "slides/06-model-checking.html#coding-errors-1",
    "title": "Model checking",
    "section": "Coding errors",
    "text": "Coding errors\nImportant: failing a resolution via the above go to http://mc-stan.org/ and do:\n\nLook through manual for a solution.\nLook through user forum for previous answers to similar problems.\nAsk a question; be clear, and thorough - post as simple a model that replicates the issue.\n\nOutside of this, you have an endless source of resources using Google/stackoverflow/stackexchange/ChatGPT. You can also go to office hours, and ask in lecture!"
  },
  {
    "objectID": "slides/06-model-checking.html#what-to-do-when-things-go-wrong-summary",
    "href": "slides/06-model-checking.html#what-to-do-when-things-go-wrong-summary",
    "title": "Model checking",
    "section": "What to do when things go wrong: summary",
    "text": "What to do when things go wrong: summary\n\nProblems with sampling are almost invariably problems with the underlying model not the sampling algorithm.\nUse fake data with all models to test for parameter identification (and that you‚Äôve coded up correctly).\nTo debug a model that fails read error messages carefully, then try print statements.\nStan has an active developer and user forum, great documentation, and an extensive answer bank."
  },
  {
    "objectID": "slides/06-model-checking.html#posterior-predictive-checks-1",
    "href": "slides/06-model-checking.html#posterior-predictive-checks-1",
    "title": "Model checking",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\n\nLast lecture we learned about posterior predictive distributions.\nThese can be used to check the model fit in our observed data.\nThe goal is to check how well our model can generate data that matches the observed data.\nIf our model is ‚Äúgood‚Äù, it should be able to generate new observations that resemble the observed data.\nTo perform the posterior predictive check, we must include the generated quantities code chunk."
  },
  {
    "objectID": "slides/06-model-checking.html#comparing-the-ppd-to-the-observed-data-distribution",
    "href": "slides/06-model-checking.html#comparing-the-ppd-to-the-observed-data-distribution",
    "title": "Model checking",
    "section": "Comparing the PPD to the observed data distribution",
    "text": "Comparing the PPD to the observed data distribution\nlibrary(rstan)\nlibrary(bayesplot)\nY_in_sample &lt;- extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(Y, Y_in_sample)"
  },
  {
    "objectID": "slides/06-model-checking.html#ppd-test-statistics",
    "href": "slides/06-model-checking.html#ppd-test-statistics",
    "title": "Model checking",
    "section": "PPD test statistics",
    "text": "PPD test statistics\n\nThe procedure for carrying out a posterior predictive check requires a test quantity, \\(T(\\mathbf{Y})\\), for our observed data.\nSuppose that we have samples from the posterior predictive distribution, \\(\\mathbf{Y}^{(s)} = \\left\\{Y_1^{(s)},\\ldots,Y_n^{(s)}\\right\\}\\) for \\(s = 1,\\ldots,S\\).\nWe can compute: \\(T\\left(\\mathbf{Y}^{(s)}\\right) = \\left\\{T\\left(Y_1^{(s)}\\right),\\ldots,\\left(Y_n^{(s)}\\right)\\right\\}\\)\nOur posterior predictive check will then compare the distribution of \\(T\\left(\\mathbf{Y}^{(s)}\\right)\\) to the value from our observed data, \\(T(\\mathbf{Y})\\).\n\\(T(\\cdot)\\) can be any statistics, including mean, median, etc.\nWhen the predictive distribution is not consistent with the observed statistics it indicates poor model fit."
  },
  {
    "objectID": "slides/06-model-checking.html#visualizing-posterior-predictive-check",
    "href": "slides/06-model-checking.html#visualizing-posterior-predictive-check",
    "title": "Model checking",
    "section": "Visualizing posterior predictive check",
    "text": "Visualizing posterior predictive check\nppc_stat(Y, Y_in_sample, stat = \"mean\") # from bayesplot\nppc_stat(Y, Y_in_sample, stat = \"sd\")\nq25 &lt;- function(y) quantile(y, 0.25)\nq75 &lt;- function(y) quantile(y, 0.75)\nppc_stat(Y, Y_in_sample, stat = \"q25\")\nppc_stat(Y, Y_in_sample, stat = \"q75\")"
  },
  {
    "objectID": "slides/06-model-checking.html#posterior-predictive-p-values",
    "href": "slides/06-model-checking.html#posterior-predictive-p-values",
    "title": "Model checking",
    "section": "Posterior predictive p-values",
    "text": "Posterior predictive p-values\nplot &lt;- ppc_stat(Y, Y_in_sample, stat = \"median\") # from bayesplot\npvalue &lt;- mean(apply(Y_in_sample, 1, median) &gt; median(Y))\nplot + yaxis_text() + # just so I can see y-axis values for specifying them in annotate() below, but can remove this if you don't want the useless y-axis values displayed \n  annotate(\"text\", x = -0.75, y = 150, label = paste(\"p =\", pvalue))\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian p-value: \\(p_B = P\\left(T\\left(\\mathbf{Y}^{(s)}\\right) \\geq T\\left(\\mathbf{Y}\\right) | \\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/06-model-checking.html#debugging-with-shinystan",
    "href": "slides/06-model-checking.html#debugging-with-shinystan",
    "title": "Model checking",
    "section": "Debugging with shinystan",
    "text": "Debugging with shinystan\n\nlibrary(shinystan)\nmy_sso &lt;- launch_shinystan(fit)"
  },
  {
    "objectID": "slides/06-model-checking.html#prepare-for-next-class",
    "href": "slides/06-model-checking.html#prepare-for-next-class",
    "title": "Model checking",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due before next class\nComplete reading to prepare for next Thursday‚Äôs lecture\nThursday‚Äôs lecture: Model Comparison"
  },
  {
    "objectID": "slides/07-model-comparison.html#review-of-last-lecture",
    "href": "slides/07-model-comparison.html#review-of-last-lecture",
    "title": "Model Comparison",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Tuesday, we learned about various ways to check MCMC convergence and model fit.\n\nTraceplots, effective sample size (\\(n_{eff}\\)), MC standard error, \\(\\hat{R}\\), sampling issues\nPosterior predictive checks\nModel checks using shinystan\n\nToday, we will learn about model comparisons."
  },
  {
    "objectID": "slides/07-model-comparison.html#model-comparison",
    "href": "slides/07-model-comparison.html#model-comparison",
    "title": "Model Comparison",
    "section": "Model comparison",
    "text": "Model comparison\n\nIn statistical modeling, a more complex model almost always results in a better fit to the data.\n\nA more complex model means one with more parameters.\n\nIf one has 10 observations, one can have a model with 10 parameters that can perfectly predict every single data point (by just having a parameter to predict each data point).\nThere are two problems with overly complex models.\n\nThey become increasingly hard to interpret (think a straight line versus a polynomial).\nThey are more at risk of overfitting, such that it does not work for future observations."
  },
  {
    "objectID": "slides/07-model-comparison.html#model-fit-an-example-data-set",
    "href": "slides/07-model-comparison.html#model-fit-an-example-data-set",
    "title": "Model Comparison",
    "section": "Model fit: an example data set",
    "text": "Model fit: an example data set\n\nLet‚Äôs explore the idea of model fit using an example dataset from the openintro package called bdims.\nThis dataset contains body girth measurements and skeletal diameter measurements.\nToday we will explore the association between height and weight."
  },
  {
    "objectID": "slides/07-model-comparison.html#model-fit-an-example-data-set-1",
    "href": "slides/07-model-comparison.html#model-fit-an-example-data-set-1",
    "title": "Model Comparison",
    "section": "Model fit: an example data set",
    "text": "Model fit: an example data set\n\nlibrary(openintro)\ndat &lt;- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))"
  },
  {
    "objectID": "slides/07-model-comparison.html#models-of-increasing-complexity",
    "href": "slides/07-model-comparison.html#models-of-increasing-complexity",
    "title": "Model Comparison",
    "section": "Models of increasing complexity",
    "text": "Models of increasing complexity\n\nWhen using height to predict weight, we can use models of increasing complexity using higher order polynomials.\nLet‚Äôs fit the following models to the subset of 10 data points:\n\n\\[\\begin{aligned}\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2 + \\beta_3 height_i^3\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2 + \\beta_3 height_i^3 + \\beta_4 height_i^4\n\\end{aligned}\\]\n\nWe can compare these models using standard measures of goodness-of-fit, including \\(R^2\\) and root mean squared error (RMSE)."
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting"
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting-1",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting-1",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting"
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting-2",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting-2",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\nWith more complex models, out-of-sample prediction becomes worse.\nThis is because when you use a complex model in a data set, it tailors the coefficients to any sampling errors and noise in the data such that it will not generalize to new observations.\nTherefore, our goal in model comparison is to choose a model with the following two properties:\n\nIt is complex enough to capture the essence of the data generation process (and thus avoid underfitting),\nIt avoids overfitting to make the model usefull for predicting new observations."
  },
  {
    "objectID": "slides/07-model-comparison.html#finding-an-optimal-model",
    "href": "slides/07-model-comparison.html#finding-an-optimal-model",
    "title": "Model Comparison",
    "section": "Finding an optimal model",
    "text": "Finding an optimal model\n\nTrade-off between overfitting and underfitting (in machine learning this is commonly called bias-variance trade-off).\n\nA simple model tends to produce biased predictions because it does not capture the essence of the data generating process.\nA model that is overly complex is unbiased but results in a lot of uncertainty in the prediction.\n\nPolynomials are merely one example of comparing simple to complex models. You can think about:\n\nModels with and without interactions,\nModels with a few predictors versus hundreds of predictors,\nRegression analyses versus hierarchical models, etc."
  },
  {
    "objectID": "slides/07-model-comparison.html#model-comparison-1",
    "href": "slides/07-model-comparison.html#model-comparison-1",
    "title": "Model Comparison",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nWhen comparing models, we prefer models that are closer to the true data-generating process.\nWe need some ways to quantify the degree of closeness to the true model. Note that in this context models refer to the distributional family as well as the parameter values.\nFor example, the model \\(Y_i \\sim N(5,2)\\) is a different model than \\(Y_i \\sim N(3,2)\\), which is a different model than \\(Y_i \\sim Gamma(2,2)\\).\n\nThe first two have the same family but different parameter values (different means, same SD), whereas the last two have different distributional families (Normal vs.¬†Gamma).\n\nOne way to quantify the degree of closeness to the true model is using Kullback-Leibler (KL) divergence."
  },
  {
    "objectID": "slides/07-model-comparison.html#kullback-leibler-divergence",
    "href": "slides/07-model-comparison.html#kullback-leibler-divergence",
    "title": "Model Comparison",
    "section": "Kullback-Leibler divergence",
    "text": "Kullback-Leibler divergence\n\nFor two models, \\(M_0\\) and \\(M_1\\), the KL divergence is given by,\n\n\\[\\begin{aligned}\nD_{KL}\\left(M_0 | M_1\\right) &= \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log\\frac{f_{M_0}(\\mathbf{Y})}{f_{M_1}(\\mathbf{Y})} d\\mathbf{Y}\\\\\n&\\hspace{-1.5in}= \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_0}(\\mathbf{Y})d\\mathbf{Y} - \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_1}(\\mathbf{Y})d\\mathbf{Y}\n\\end{aligned}\\]\n\nNote that \\(D_{KL}\\) is not considered a distance, because it is not strictly symmetric, \\(D_{KL}\\left(M_0 | M_1\\right) \\neq D_{KL}\\left(M_1 | M_0\\right)\\)."
  },
  {
    "objectID": "slides/07-model-comparison.html#kullback-leibler-divergence-1",
    "href": "slides/07-model-comparison.html#kullback-leibler-divergence-1",
    "title": "Model Comparison",
    "section": "Kullback-Leibler divergence",
    "text": "Kullback-Leibler divergence\nAs an example, assume that the data are generated by a true model \\(M_0\\), and we have two candidate models \\(M_1\\) and \\(M_2\\), where\n\n\n\n\\(M_0: Y_i \\sim N(3,2)\\)\n\\(M_1: Y_i \\sim N(3.5, 2.5)\\)\n\\(M_2: Y_i \\sim Cauchy(3,2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{KL}(M_0 |M_1) = 0.063\\), \\(D_{KL}(M_0 | M_2) = 0.259\\), so \\(M_1\\) is a better model than \\(M_2\\)."
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nNote that in the expression of \\(D_{KL}\\), when talking about the same target model, the first term is always the same and describes the true model, \\(M_0\\).\nTherefore, it is sufficient to compare models on the second term,\n\\[\\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_1}(\\mathbf{Y})d\\mathbf{Y},\\] which can also be written as, \\(\\mathbb{E}_{M_0}\\left[\\log f_{M_1}(\\mathbf{Y})\\right].\\)\nThis term is the expected log predictive density (elpd).\nA larger elpd is preferred. Why?"
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-1",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-1",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nIn the real world, we do not know \\(M_0\\).\n\nIf we knew, then we would just need to choose \\(M_0\\) as our model and there will be no problem about model comparisons.\nEven if we knew the true model, we would still need to estimate the parameter values.\n\nThus, we cannot compute elpd, since the expectation is over \\(f_{M_0}(\\mathbf{Y})\\).\nWe need to estimate elpd!"
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-2",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-2",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nelpd is an expectation, so we can think about estimating it using Monte Carlo sampling, \\[\\frac{1}{S}\\sum_{s = 1}^S\\log f_{M_1}\\left(\\mathbf{Y}^{(s)}\\right)\\rightarrow \\mathbb{E}_{M_0}\\left[\\log f_{M_1}(\\mathbf{Y})\\right], \\quad \\mathbf{Y}^{(s)} \\sim f_{M_0}(\\mathbf{Y}).\\]\n\nWe need to find a way to approximate, \\(f_{M_0}\\left(\\mathbf{Y}^{(s)}\\right)\\).\n\nA naive way to approximate \\(f(\\mathbf{Y}^{(s)})\\) is to assume that the distribution of the observed data is the true model.\n\nThis is equivalent to assuming that \\(\\mathbf{Y}^{(s)} \\sim \\{\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n\\}\\).\nThis leads to an overly optimistic estimate and favors complex models."
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-3",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-3",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nA better way to estimate elpd is to collect data on a new independent sample that is believed to share the same data generating process as the current sample, and estimate elpd on the new sample.\n\nThis is called out-of-sample validation.\nThe problem, of course, is we usually do not have the resources to collect a new sample.\n\nTherefore, statisticians have worked hard to find ways to estimate elpd from the current sample, and there are two broad approaches, information criteria and cross-validation."
  },
  {
    "objectID": "slides/07-model-comparison.html#overview-of-comparison-methods",
    "href": "slides/07-model-comparison.html#overview-of-comparison-methods",
    "title": "Model Comparison",
    "section": "Overview of comparison methods",
    "text": "Overview of comparison methods\n\nInformation criteria: AIC, DIC, and WAIC, which estimate the elpd in the current sample, minus a correction factor.\nCross validation: A method that splits the current sample into \\(K\\) parts, estimates the parameters in \\(K ‚àí 1\\) parts, and estimates the elpd in the remaining 1 part.\n\n\nA special case is when \\(K = n\\) so that each time one uses \\(n-1\\) data points to estimate the model parameters, and estimates the elpd for the observation that was left out. This is called leave-one-out cross-validation (LOO-CV)."
  },
  {
    "objectID": "slides/07-model-comparison.html#information-criteria",
    "href": "slides/07-model-comparison.html#information-criteria",
    "title": "Model Comparison",
    "section": "Information criteria",
    "text": "Information criteria\n\nSeveral information criteria have been proposed that do not require fitting the model several times, including AIC, DIC, and WAIC.\nWe will introduce the information criteria, assuming a likelihood \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})\\) for observed data \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\) with population parameter \\(\\boldsymbol{\\theta}\\).\nInformation criteria are often presented as deviance, defined as, \\(D(\\mathbf{Y}|\\boldsymbol{\\theta}) = ‚àí2 \\log f(\\mathbf{Y}|\\boldsymbol{\\theta})\\).\nIdeally, models will have small deviance.\nHowever, if a model is too complex it will have small deviance but be unstable (overfitting)."
  },
  {
    "objectID": "slides/07-model-comparison.html#akaike-information-criteria-aic",
    "href": "slides/07-model-comparison.html#akaike-information-criteria-aic",
    "title": "Model Comparison",
    "section": "Akaike information criteria (AIC)",
    "text": "Akaike information criteria (AIC)\nAkaike information criteria (AIC) estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{AIC}} = \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}) - p,\\] where \\(p\\) is the number of parameters estimated in the model and \\(\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}\\) is the MLE point estimate.\n\n\\(\\text{AIC} = -2\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}) + 2p\\)\n\\(p\\) is an adjustment for overfitting, but once we go beyond linear models, we cannot simply add \\(p\\).\nInformative priors tend to reduce the amount of overfitting.\nModel with smaller AIC are preferred."
  },
  {
    "objectID": "slides/07-model-comparison.html#deviance-information-criteria-dic",
    "href": "slides/07-model-comparison.html#deviance-information-criteria-dic",
    "title": "Model Comparison",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\nDeviance information criteria (DIC) estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{DIC}} = \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) - p_{\\text{DIC}},\\] where \\(\\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}\\) is a Bayesian point estimate, typically a posterior mean, and \\(p_{\\text{DIC}}\\) is an estimate of the complexity penalty,\n\\[p_{\\text{DIC}} = 2 \\left(\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) - \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[\\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) \\right]\\right).\\]\n\nThe second term can be estimated as a MC integral.\n\\(\\text{DIC} = -2\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) + 2p_{\\text{DIC}}.\\)"
  },
  {
    "objectID": "slides/07-model-comparison.html#deviance-information-criteria-dic-1",
    "href": "slides/07-model-comparison.html#deviance-information-criteria-dic-1",
    "title": "Model Comparison",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nAdvantages of DIC:\n\nThe effective number of parameters is a useful measure of model complexity.\nIntuitively, if there are \\(p\\) parameters and we have uninformative priors then \\(p_D \\approx p\\).\nHowever, \\(p_D \\ll p\\) if there are strong priors.\n\nDisadvantages of DIC:\n\nDIC can only be used to compare models with the same likelihood.\nDIC really only applies when the posterior is approximately normal, and will give misleading results when the posterior is far from normality (e.g., bimodal)."
  },
  {
    "objectID": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic",
    "href": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic",
    "title": "Model Comparison",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\nWatanabe-Akaike or widely available information criteria (WAIC) estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{WAIC}} = \\text{lppd} - p_{\\text{WAIC}}.\\]\n\nThe log pointwise predictive density (lppd) is given by, \\[\\text{lppd} = \\log \\prod_{i=1}^n f(Y_i | \\mathbf{Y}) =  \\sum_{i=1}^n \\log \\int f\\left(Y_i | \\boldsymbol{\\theta}\\right)f(\\boldsymbol{\\theta}| \\mathbf{Y}) d\\boldsymbol{\\theta}.\\]\nlppd can be estimated as, \\(\\sum_{i=1}^n \\log \\left(\\frac{1}{S} \\sum_{s = 1}^S f\\left(Y_i | \\boldsymbol{\\theta}^{(s)}\\right)\\right)\\), where \\(\\boldsymbol{\\theta}^{(s)}\\) are drawn from the posterior."
  },
  {
    "objectID": "slides/07-model-comparison.html#waic",
    "href": "slides/07-model-comparison.html#waic",
    "title": "Model Comparison",
    "section": "WAIC",
    "text": "WAIC\nThere are two common estimates of \\(p_{\\text{WAIC}}\\), both of which can be estimated using MC samples of the posterior. \\[\\begin{aligned}\np_{\\text{WAIC}_1} &= 2 \\sum_{i=1}^n\\left(\\log \\left( \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[f(Y_i | \\boldsymbol{\\theta})\\right]\\right) - \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[\\log f(Y_i | \\boldsymbol{\\theta}) \\right]\\right)\\\\\np_{\\text{WAIC}_2} &= \\sum_{i=1}^n \\mathbb{V}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left(\\log f\\left(Y_i | \\mathbf{\\theta}\\right)\\right)\n\\end{aligned}\\]\n\n\\(\\text{WAIC} = -2 \\text{lppd} + 2p_{\\text{WAIC}}.\\)"
  },
  {
    "objectID": "slides/07-model-comparison.html#waic-1",
    "href": "slides/07-model-comparison.html#waic-1",
    "title": "Model Comparison",
    "section": "WAIC",
    "text": "WAIC\n\nWAIC has the desirable property of averaging over the posterior distribution, instead of conditioning on a point estimate.\n\\(p_{\\text{WAIC}}\\) can be thought of as an approximation to the number of unconstrained parameters in the model.\nIn practice, \\(p_{\\text{WAIC}_2}\\) is often used, since it is theoretically closer to LOO-CV."
  },
  {
    "objectID": "slides/07-model-comparison.html#cross-validation",
    "href": "slides/07-model-comparison.html#cross-validation",
    "title": "Model Comparison",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nA common approach to compare models is using cross-validation.\nThis is exactly the same procedure used in classical statistics.\nThis operates under the assumption that the true model likely produces better out-of-sample predictions than competing models.\nAdvantages: Simple, intuitive, and broadly applicable.\nDisadvantages: Slow because it requires several model fits and it is hard to say a difference is statistically significant."
  },
  {
    "objectID": "slides/07-model-comparison.html#k-fold-cross-validation",
    "href": "slides/07-model-comparison.html#k-fold-cross-validation",
    "title": "Model Comparison",
    "section": "K-fold cross-validation",
    "text": "K-fold cross-validation\n\nSplit the data into \\(K\\) equally-sized groups.\nSet aside group \\(k\\) as test set and fit the model to the remaining \\(K ‚àí 1\\) groups.\nMake predictions for the test set \\(k\\) based on the model fit to the training data.\nRepeat steps 1 and 2 for \\(k = 1, \\dots, K\\) giving a predicted value \\(\\widehat{Y}_i\\) for all \\(n\\) observations.\nMeasure prediction accuracy, e.g.,\n\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2.\\]"
  },
  {
    "objectID": "slides/07-model-comparison.html#variants-of-cross-validation",
    "href": "slides/07-model-comparison.html#variants-of-cross-validation",
    "title": "Model Comparison",
    "section": "Variants of cross-validation",
    "text": "Variants of cross-validation\n\nUsually \\(K\\) is either 5 or 10.\n\\(K = n\\) is called leave-one-out cross-validation (LOO-CV), which is great but slow.\nThe predicted value \\(\\widehat{Y}_i\\) can be either the posterior predictive mean or median.\nMean squared error (MSE) can be replaced with mean absolute deviation (MAD),\n\n\\[MAD = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\widehat{Y}_i|.\\]"
  },
  {
    "objectID": "slides/07-model-comparison.html#leave-one-out-cross-validation-loo-cv",
    "href": "slides/07-model-comparison.html#leave-one-out-cross-validation-loo-cv",
    "title": "Model Comparison",
    "section": "Leave-one-out cross-validation (LOO-CV)",
    "text": "Leave-one-out cross-validation (LOO-CV)\n\nAssume the data are partitioned into a training set, \\(\\mathbf{Y}_{\\text{train}}\\) and a holdout set \\(\\mathbf{Y}_{\\text{test}}\\), thus yielding a posterior distribution \\(f(\\boldsymbol{\\theta} | \\mathbf{Y}_{\\text{train}})\\).\nIn the setting of LOO-CV, we have \\(n\\) different \\(f\\left(\\boldsymbol{\\theta} | \\mathbf{Y}_{-i}\\right)\\).\nThe Bayesian LOO-CV estimate of out-of-sample predictive fit is \\[\\widehat{\\text{elpd}}_{\\text{LOO-CV}} = \\sum_{i=1}^n \\log f\\left(Y_i|\\mathbf{Y}_{-i}\\right),\\] calculated as \\[\\sum_{i=1}^n \\log \\left(\\frac{1}{S} \\sum_{s=1}^S f\\left(Y_i|\\boldsymbol{\\theta}^{(is)}\\right)\\right),\\quad \\boldsymbol{\\theta}^{(is)} \\sim f(\\boldsymbol{\\theta}|\\mathbf{Y}_{-i}).\\]"
  },
  {
    "objectID": "slides/07-model-comparison.html#loo-cv",
    "href": "slides/07-model-comparison.html#loo-cv",
    "title": "Model Comparison",
    "section": "LOO-CV",
    "text": "LOO-CV\n\nWe can write LOO-CV as an information criterion\n\n\\[\\widehat{\\text{elpd}}_{\\text{LOO-CV}} = \\text{lppd} - p_{LOO-CV},\\]\nwhere \\[p_{\\text{LOO-CV}} = \\text{lppd} - \\widehat{\\text{elpd}}_{\\text{LOO-CV}}.\\]\n\nThe estimated number of parameters measures how much in-sample predictive fit overstates out-of-sample performance.\nThis formulation is often called the leave-one-out information criterion (LOO-IC)."
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-and-loo-cv-using-stan",
    "href": "slides/07-model-comparison.html#computing-waic-and-loo-cv-using-stan",
    "title": "Model Comparison",
    "section": "Computing WAIC and LOO-CV using Stan",
    "text": "Computing WAIC and LOO-CV using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/07-model-comparison.html#lets-simulate-some-data",
    "href": "slides/07-model-comparison.html#lets-simulate-some-data",
    "title": "Model Comparison",
    "section": "Let‚Äôs simulate some data:",
    "text": "Let‚Äôs simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3, 1), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\nn_pred &lt;- 10 # number of predicted observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\nX_pred &lt;- cbind(1, matrix(rnorm(n_pred * p), ncol = p))\nY_pred &lt;- as.numeric(X_pred %*% beta + rnorm(n_pred, 0, sigma))"
  },
  {
    "objectID": "slides/07-model-comparison.html#an-example-model-comparison",
    "href": "slides/07-model-comparison.html#an-example-model-comparison",
    "title": "Model Comparison",
    "section": "An example model comparison",
    "text": "An example model comparison\nTrue Model: \\(\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}\\)\nModel 1: \\(\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1}\\)\nModel 2: \\(\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}\\)"
  },
  {
    "objectID": "slides/07-model-comparison.html#fit-model-1",
    "href": "slides/07-model-comparison.html#fit-model-1",
    "title": "Model Comparison",
    "section": "Fit model 1",
    "text": "Fit model 1\n\n###Create stan data object\nstan_data_model1 &lt;- list(n = n, p = p - 1, Y = Y, X = X[, -3],\n                         beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                         n_pred = n_pred, X_pred = X_pred[, -3])\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model 1 and save\nfit_model1 &lt;- sampling(stan_model, data = stan_data_model1, \n                chains = 4, iter = 1000)\nsaveRDS(fit_model1, file = \"linear_regression_ppd_log_lik_fit_model1.rds\")"
  },
  {
    "objectID": "slides/07-model-comparison.html#fit-model-2",
    "href": "slides/07-model-comparison.html#fit-model-2",
    "title": "Model Comparison",
    "section": "Fit model 2",
    "text": "Fit model 2\n\n###Create stan data object\nstan_data_model2 &lt;- list(n = n, p = p, Y = Y, X = X,\n                         beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                         n_pred = n_pred, X_pred = X_pred)\n\n###Run model 2 and save\nfit_model2 &lt;- sampling(stan_model, data = stan_data_model2, \n                chains = 4, iter = 1000)\nsaveRDS(fit_model2, file = \"linear_regression_ppd_log_lik_fit_model2.rds\")"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic",
    "href": "slides/07-model-comparison.html#computing-waic",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\nBegin by extracting the log-likelihood values from the model.\nWe will use the loo package.\n\n\n###Load loo package\nlibrary(loo)\n\n###Extract log likelihood\nlog_lik_model1 &lt;- loo::extract_log_lik(fit_model1, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model2 &lt;- loo::extract_log_lik(fit_model2, parameter_name = \"log_lik\", merge_chains = TRUE)\n\n###Explore the object\nclass(log_lik_model1)\n\n[1] \"matrix\" \"array\" \n\ndim(log_lik_model1)\n\n[1] 2000  100"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-1",
    "href": "slides/07-model-comparison.html#computing-waic-1",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\n###Compute WAIC for the two models\nwaic_model1 &lt;- loo::waic(log_lik_model1)\nwaic_model2 &lt;- loo::waic(log_lik_model2)\n\n###Inspect WAIC for model 1\nwaic_model1\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -201.0  6.6\np_waic         3.0  0.5\nwaic         401.9 13.1"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-2",
    "href": "slides/07-model-comparison.html#computing-waic-2",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\n###Inspect WAIC for model 2\nwaic_model2\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -178.9  6.6\np_waic         3.9  0.6\nwaic         357.7 13.1"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-3",
    "href": "slides/07-model-comparison.html#computing-waic-3",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"true\" = waic_model2, \"misspec\" = waic_model1))\nprint(comp_waic, digits = 2)\n\n        elpd_diff se_diff\ntrue      0.00      0.00 \nmisspec -22.09      5.76 \n\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \ntrue       0.00      0.00 -178.86      6.57         3.88    0.58    357.72\nmisspec  -22.09      5.76 -200.95      6.56         2.99    0.52    401.90\n        se_waic\ntrue      13.14\nmisspec   13.12"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-loo-cvloo-ci",
    "href": "slides/07-model-comparison.html#computing-loo-cvloo-ci",
    "title": "Model Comparison",
    "section": "Computing LOO-CV/LOO-CI",
    "text": "Computing LOO-CV/LOO-CI\n\n###Compute LOO-IC for the two models\nloo_model1 &lt;- loo::loo(log_lik_model1)\nloo_model2 &lt;- loo::loo(log_lik_model2)\n\n###Make a comparison\ncomp &lt;- loo::loo_compare(list(\"true\" = loo_model2, \"misspec\" = loo_model1))\nprint(comp, digits = 2)\n\n        elpd_diff se_diff\ntrue      0.00      0.00 \nmisspec -22.08      5.76 \n\nprint(comp, digits = 2, simplify = FALSE)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic  \ntrue       0.00      0.00 -178.88     6.57        3.90    0.58   357.77\nmisspec  -22.08      5.76 -200.96     6.56        3.00    0.52   401.93\n        se_looic\ntrue      13.14 \nmisspec   13.12"
  },
  {
    "objectID": "slides/07-model-comparison.html#prepare-for-next-class",
    "href": "slides/07-model-comparison.html#prepare-for-next-class",
    "title": "Model Comparison",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02, which was just assigned\nComplete reading to prepare for next Tuesday‚Äôs lecture\nTuesday‚Äôs lecture: Bayesian Workflow"
  },
  {
    "objectID": "hw/hw-02.html",
    "href": "hw/hw-02.html",
    "title": "HW 02: Bayesian linear regression",
    "section": "",
    "text": "ImportantDue date\n\n\n\nThis assignment is due on Thursday, February 12 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-1",
    "href": "hw/hw-02.html#exercise-1",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nSetup a multivariable linear regression to estimate the association between access to recreational facilities and exercise, making sure to allow for this relationship to change based on crime. Be sure to control for the following confounders: age, marital status, and race. Define the random variable \\(Y_i\\) as the exercise in MET minutes per week for women \\(i\\) and assume that \\(Y_i \\stackrel{ind}{\\sim} N(\\mu_i, \\sigma^2)\\) for \\(i = 1,\\ldots,n\\) where \\[\\begin{align*}\n\\mu_i &= \\alpha + recreation_i \\beta_1 + crime_i \\beta_2+(recreation_i \\times crime_i) \\beta_3\\\\\n&\\quad+ age_i \\beta_4 + black_i \\beta_5 + asian_i \\beta_6 + married_i \\beta_7\\\\\n&= \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta}.\n\\end{align*}\\]\nFit this regression using a Bayesian framework in Stan to estimate \\((\\alpha, \\boldsymbol{\\beta},\\sigma)\\). For all model parameters, choose weakly-informative priors, \\(\\alpha \\sim N(0,100)\\), \\(\\beta_j \\sim N(0,100)\\) for \\(j=1,\\ldots,p\\), and \\(\\sigma \\sim \\text{Half-Normal}(0,100)\\). Recall, that in this class, normal distributions are parameterized using variances (i.e., the standard deviations in these priors is 10).\nEvaluate model convergence and present posterior predictive checks. Provide an argument for whether the model fits the data well.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-2",
    "href": "hw/hw-02.html#exercise-2",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nRefit the model in Exercise 1, this time using centered outcome and predictor variables, \\(Y_i^* \\stackrel{ind}{\\sim} N(\\mu_i, \\sigma^2)\\), where \\(\\mu_i^* = \\alpha + \\mathbf{x}_i^*\\boldsymbol{\\beta}\\). The centered data are defined as, \\(Y_i^* = Y_i - \\overline{Y}\\), where \\(\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\) and \\(\\mathbf{x}_i^* = \\mathbf{x}_i-\\bar{\\mathbf{x}}\\), where \\(\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\\). Use the same priors as before. Once again, evaluate model convergence and present posterior predictive checks. Provide an argument for whether the model fits the data well and make a comparison to the model from Exercise 1. Make an argument for why this model may have improved model fit.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-3",
    "href": "hw/hw-02.html#exercise-3",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nFor the model from Exercise 2, present the posterior means, standard deviations, and 95% credible intervals for all model parameters. What is interpretation of the slope main effect corresponding to recreation in your model? Within the context of the association between access to recreational facilities and exercise, is this main effect parameter useful to interpret?",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-4",
    "href": "hw/hw-02.html#exercise-4",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the association between access to recreational facilities and exercise, for a pregnant women living in an area with 5 annual crimes/1,000 people? What about for 15 annual crimes/1,000 people? Provide posterior mean and standard deviations for both quantities.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-5",
    "href": "hw/hw-02.html#exercise-5",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret the posterior mean and standard deviations from Exercise 4 and compare and contrast them. What do these posterior slopes say about the impact of crime on the relationship between access to recreational facilities and exercise.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-6",
    "href": "hw/hw-02.html#exercise-6",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nResearchers are interested in the level of crime where the association between recreational facilities and exercise disappears. Present the posterior median and interquartile range (i.e., 25% and 75% percentiles) for this quantity.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-7",
    "href": "hw/hw-02.html#exercise-7",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nCompute the posterior predictive distribution for a patient with 10 recreational facilities within a one-mile radius, 5 crimes within a one-mile buffer per 1,000 people, is 40 years old, married, and white race. Report the posterior mean and 95% credible intervals.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-8",
    "href": "hw/hw-02.html#exercise-8",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nResearchers are interested in comparing their original model (i.e., the one from Exercise 2) with a model that does not contain an interaction term between recreational facility access and crime. Fit the model without the interaction term and perform a model comparison between the two models using an information criteria. Which model would you suggest as more scientifically plausible?",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-9",
    "href": "hw/hw-02.html#exercise-9",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nPerform a sensitivity analysis to the choice of prior for \\((\\alpha,\\boldsymbol{\\beta},\\sigma)\\). Make sure to change the family of priors for each parameter. Are your results robust to the choice of prior?\n\nYou‚Äôre done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message ‚ÄúDone with Homework 2!‚Äù, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "slides/07-model-comparison.html#loo-cv-1",
    "href": "slides/07-model-comparison.html#loo-cv-1",
    "title": "Model Comparison",
    "section": "LOO-CV",
    "text": "LOO-CV\n\nComputing LOO-CV/LOO-IC is computationally demanding.\nUnder some common models there are shortcuts for computing it, however in general these do not exist.\nWAIC can be treated as a fast approximation of LOO-CV.\nIn Stan, LOO-CV is approximated using the Pareto smoothed importance sampling (PSIS) to make the process faster, without having to repeat the process \\(n\\) times."
  },
  {
    "objectID": "prepare/prepare-feb03.html",
    "href": "prepare/prepare-feb03.html",
    "title": "Prepare for February 3 lecture",
    "section": "",
    "text": "üìñ Review Bayesian Workflow by Gelman et al.\n‚úÖ Work on HW 02"
  },
  {
    "objectID": "slides/09-nonlinear.html",
    "href": "slides/09-nonlinear.html",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nLoaded lars 1.3\n\n\n\nAttaching package: 'LaplacesDemon'\n\n\nThe following objects are masked from 'package:mvtnorm':\n\n    dmvt, logdet, rmvt\n\n\nThe following objects are masked from 'package:lubridate':\n\n    dst, interval\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\n\nnimble version 1.2.1 is loaded.\nFor more information on NIMBLE and a User Manual,\nplease visit https://R-nimble.org.\n\nNote for advanced users who have written their own MCMC samplers:\n  As of version 0.13.0, NIMBLE's protocol for handling posterior\n  predictive nodes has changed in a way that could affect user-defined\n  samplers in some situations. Please see Section 15.5.1 of the User Manual.\n\n\nAttaching package: 'nimble'\n\n\nThe following objects are masked from 'package:LaplacesDemon':\n\n    cloglog, dcat, dinvgamma, is.model, logdet, logit, rcat, rinvgamma\n\n\nThe following object is masked from 'package:mvtnorm':\n\n    logdet\n\n\nThe following object is masked from 'package:stats':\n\n    simulate\n\n\nThe following object is masked from 'package:base':\n\n    declare"
  },
  {
    "objectID": "slides/09-nonlinear.html#review-of-last-lecture",
    "href": "slides/09-nonlinear.html#review-of-last-lecture",
    "title": "Nonlinear Regression",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we put all of our skills together and learned about the Bayesian workflow.\nWe have now learned all the skills needed to perform Bayesian inference.\nThe rest of the course we will introduce new models and data types that are useful for performing biomedical data science."
  },
  {
    "objectID": "slides/09-nonlinear.html#learning-objectives-today",
    "href": "slides/09-nonlinear.html#learning-objectives-today",
    "title": "Nonlinear Regression",
    "section": "Learning objectives today",
    "text": "Learning objectives today\n\nThus far, we have focused on linear regression models.\nToday we will focus on approaches that use linear regression to build nonlinear associations. For example: polynomial regression and b-splines.\nThese approaches work by transforming a single predictor variable into several synthetic variables.\nWe will also look at a change point model, that encodes clinical context into a nonlinear framework."
  },
  {
    "objectID": "slides/09-nonlinear.html#linear-regression",
    "href": "slides/09-nonlinear.html#linear-regression",
    "title": "Nonlinear Regression",
    "section": "Linear regression",
    "text": "Linear regression\n\nConsider the classic parametric model:\n\n\\[Y_i = \\alpha + X_i \\beta + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2).\\]\n\nAssumptions:\n\n\\(\\epsilon_i\\) are independent.\n\\(\\epsilon_i\\) are Gaussian.\nThe mean of \\(Y_i\\) is linear in \\(X_i\\).\nThe residual distribution does not depend on \\(X_i\\).\n\n\nToday we will generalize the linearity assumption."
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression",
    "href": "slides/09-nonlinear.html#nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nDefine: \\(\\mu_i = \\mathbb{E}[Y_i] = \\alpha + X_i\\beta.\\)\nThe mean process can be modeled flexibly, \\(\\mu_i = g(X_i)\\), where \\(g\\) is some function that relates \\(X_i\\) to \\(\\mathbb{E}[Y_i].\\)\nA form of nonlinear regression approximates the function \\(g\\) using a finite basis expansion, \\[g(X_i) = \\alpha + \\sum_{j=1}^J B_j(X_i)\\beta_j,\\] where \\(B_j(X)\\) are known basis functions and \\(\\beta_j\\) are unknown parameters that determine the shape of \\(g\\)."
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression-1",
    "href": "slides/09-nonlinear.html#nonlinear-regression-1",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nExample: Polynomial regression takes \\(B_j(X_i) = X_i^j\\).\nExample: Gaussian radial basis functions: \\[B_j(X_i) = \\exp\\left\\{-\\frac{|X_i - \\nu_j|^2}{l^2}\\right\\},\\] where \\(\\nu_j\\) are centers of the basis functions and \\(l\\) is a common width parameter.\nThe number of of basis functions and the width parameter \\(l\\) controls the scale at which the model can vary as a function of \\(X_i\\)."
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression-2",
    "href": "slides/09-nonlinear.html#nonlinear-regression-2",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nExample: The cubic B-spline basis function is the following piecewise cubic polynomial:\n\n\\[B_j(X_i) = \\left\\{\n\\begin{matrix*}[l]\n\\frac{1}{6}u^3 & \\text{for }X_i \\in (\\nu_j,\\nu_{j+1}), & u = (X_i - \\nu_j) / \\delta\\\\\n\\frac{1}{6}(1 + 3u + 3u^2 - 3u^3) & \\text{for }X_i \\in (\\nu_{j+1},\\nu_{j+2}), & u = (X_i - \\nu_{j+1}) / \\delta\\\\\n\\frac{1}{6}(4 - 6u^2 + 3u^3) & \\text{for }X_i \\in (\\nu_{j+2},\\nu_{j+3}), & u = (X_i - \\nu_{j+2}) / \\delta\\\\\n\\frac{1}{6}(1 - 3u + 3u^2 - u^3) & \\text{for }X_i \\in (\\nu_{j+3},\\nu_{j+4}), & u = (X_i - \\nu_{j+3}) / \\delta\\\\\n0 & \\text{otherwise.}\n\\end{matrix*}\n\\right.\\]\n\nB-splines are a piecewise continuous function defined conditional on some set of knots.\nHere we assume a uniform knot locations \\(\\nu_{j + k} = \\nu_j + \\delta k\\).\nB-splines have compact support, so the design matrix is sparse."
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression-3",
    "href": "slides/09-nonlinear.html#nonlinear-regression-3",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nConditionally on the selected bases \\(B\\), the model is linear in the parameters. Hence we can write, \\[Y_i = \\mu_i + \\epsilon_i = \\mathbf{w}_i \\boldsymbol{\\beta} + \\epsilon_i,\\] with \\(\\mathbf{w}_i = (B_1(X_i),\\ldots,B_J(X_i))\\).\nModel fitting can proceed as in linear regression models, since the resulting model is linear in \\(\\boldsymbol{\\beta}\\).\nIt is often useful to center the basis function model around the linear model, \\(\\mu_i = \\alpha + X_i \\beta + \\mathbf{w}_i\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/09-nonlinear.html#glaucoma-disease-progression",
    "href": "slides/09-nonlinear.html#glaucoma-disease-progression",
    "title": "Nonlinear Regression",
    "section": "Glaucoma disease progression",
    "text": "Glaucoma disease progression\n\nToday we will use data from the Rotterdam Ophthalmic Data Repository.\nGlaucoma is the leading cause of irreversible blindness world wide with over 60 million glaucoma patients as of 2012. Since impairment caused by glaucoma is irreversible, early detection of disease progression is crucial for effective treatment.\nPatients with glaucoma are routinely followed up and administered visual fields, a functional assessment of their vision.\nAfter each visual field test their current disease status is reported as a mean deviation (MD) value, measured in decibels (dB). A lower mean deviation indicates worse vision.\nCentral clinical challenges are i) identifying disease progression of MD, and ii) predicting future MD."
  },
  {
    "objectID": "slides/09-nonlinear.html#glaucoma-data",
    "href": "slides/09-nonlinear.html#glaucoma-data",
    "title": "Nonlinear Regression",
    "section": "Glaucoma data",
    "text": "Glaucoma data\n\n### Load and process data to obtain data for an example patient\ndat &lt;- read.csv(file = \"LongGlaucVF_20150216/VisualFields.csv\")\ndat &lt;- dat[order(dat$STUDY_ID, dat$SITE), ]\ndat$EYE_ID &lt;- cumsum(!duplicated(dat[, c(\"STUDY_ID\", \"SITE\")]))\ndat_pat &lt;- dat[dat$EYE_ID == \"4\", ] # 4\ndat_pat$time &lt;- (dat_pat$AGE - dat_pat$AGE[1]) / 365\ndat_pat &lt;- dat_pat[, c(\"time\", \"MD\")]\ncolnames(dat_pat) &lt;- c(\"X\", \"Y\")\nglimpse(dat_pat)\n\n\n\nRows: 18\nColumns: 2\n$ X &lt;dbl&gt; 0.0000000, 0.6136986, 1.1315068, 1.6520548, 2.1671233, 2.6794521, 3.‚Ä¶\n$ Y &lt;dbl&gt; -2.76, -2.08, -1.91, -2.63, -5.13, -2.14, -1.97, -0.83, -1.75, -1.61‚Ä¶"
  },
  {
    "objectID": "slides/09-nonlinear.html#an-example-patient",
    "href": "slides/09-nonlinear.html#an-example-patient",
    "title": "Nonlinear Regression",
    "section": "An example patient",
    "text": "An example patient"
  },
  {
    "objectID": "slides/09-nonlinear.html#linear-regression-1",
    "href": "slides/09-nonlinear.html#linear-regression-1",
    "title": "Nonlinear Regression",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#linear-regression-2",
    "href": "slides/09-nonlinear.html#linear-regression-2",
    "title": "Nonlinear Regression",
    "section": "Linear regression",
    "text": "Linear regression\n\nLinear regression is simple.\nLinear regression is highly interpretable. It encodes disease progression into a slope, which is the amount of MD loss (dB) per year.\n\nInterpretability is important!\n\nA linear relationship may be an oversimplification.\nOften in prediction contexts, a nonlinear approach is preferred."
  },
  {
    "objectID": "slides/09-nonlinear.html#polynomials",
    "href": "slides/09-nonlinear.html#polynomials",
    "title": "Nonlinear Regression",
    "section": "Polynomials",
    "text": "Polynomials\n\nModel for the mean process becomes nonlinear:\n\n\\[\\mu_i = \\alpha + \\beta_1 X_i + \\cdots + \\beta_J X_i^J\\]\n\n\\(X_i\\) is years from baseline visit.\n\\(J\\) is chosen depending on the degree of non-linearity.\nWhen fitting non-linear regression in Bayesian context it is useful to standardize the data."
  },
  {
    "objectID": "slides/09-nonlinear.html#polynomial-regression-in-stan",
    "href": "slides/09-nonlinear.html#polynomial-regression-in-stan",
    "title": "Nonlinear Regression",
    "section": "Polynomial regression in Stan",
    "text": "Polynomial regression in Stan\n\ndat_poly &lt;- data.frame(\n  Y = scale(dat_pat$Y),\n  X = scale(dat_pat$X)\n)\ndat_poly$X2 &lt;- dat_poly$X^2\nstan_data &lt;- list(\n  n = nrow(dat_pat),\n  p = 2,\n  Y = dat_poly$Y,\n  X = cbind(dat_poly$X, dat_poly$X2),\n)\ncompile_model &lt;- stan_model(file = \"nonlinear_linear.stan\")\nfit_quadratic &lt;- sampling(compile_model, data = stan_data)"
  },
  {
    "objectID": "slides/09-nonlinear.html#polynomial-regression-in-stan-1",
    "href": "slides/09-nonlinear.html#polynomial-regression-in-stan-1",
    "title": "Nonlinear Regression",
    "section": "Polynomial regression in Stan",
    "text": "Polynomial regression in Stan\n\n// saved in nonlinear_linear.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates\n  vector[n] Y; // outcome vector\n  matrix[n, p] X; // covariate vector\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, sigma); // likelihood\n  target += normal_lpdf(alpha | 0, 1); // prior for alpha\n  target += normal_lpdf(beta | 0, 1); // prior for beta\n  target += inv_gamma_lpdf(sigma | 3, 1); // prior for sigma\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n] log_lik;\n  vector[n] mu;\n  for (i in 1:n) {\n    mu[i] = alpha + X[i, ] * beta;\n    in_sample[i] = normal_rng(mu[i], sigma);\n    log_lik[i] = normal_lpdf(Y[i] |  mu[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/09-nonlinear.html#quadratic-regression",
    "href": "slides/09-nonlinear.html#quadratic-regression",
    "title": "Nonlinear Regression",
    "section": "Quadratic regression",
    "text": "Quadratic regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#extract-posterior-mean-for-mu",
    "href": "slides/09-nonlinear.html#extract-posterior-mean-for-mu",
    "title": "Nonlinear Regression",
    "section": "Extract posterior mean for \\(\\mu\\)",
    "text": "Extract posterior mean for \\(\\mu\\)\n\nmu &lt;- rstan::extract(fit_quadratic, pars = \"mu\")$mu\nmu &lt;- mu * sd(dat_pat$Y) + mean(dat_pat$Y) # transform to original unstandardized Y_i\nmu_mean &lt;- apply(mu, 2, mean)\nmu_lower &lt;- apply(mu, 2, function(x) quantile(x, probs = 0.025))\nmu_upper &lt;- apply(mu, 2, function(x) quantile(x, probs = 0.975))"
  },
  {
    "objectID": "slides/09-nonlinear.html#cubic-regression",
    "href": "slides/09-nonlinear.html#cubic-regression",
    "title": "Nonlinear Regression",
    "section": "Cubic regression",
    "text": "Cubic regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-with-5-knots",
    "href": "slides/09-nonlinear.html#b-spline-regression-with-5-knots",
    "title": "Nonlinear Regression",
    "section": "B-spline regression with 5 knots",
    "text": "B-spline regression with 5 knots"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-with-10-knots",
    "href": "slides/09-nonlinear.html#b-spline-regression-with-10-knots",
    "title": "Nonlinear Regression",
    "section": "B-spline regression with 10 knots",
    "text": "B-spline regression with 10 knots"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression",
    "href": "slides/09-nonlinear.html#b-spline-regression",
    "title": "Nonlinear Regression",
    "section": "B-spline regression",
    "text": "B-spline regression\n\nlibrary(splines)\nnum_knots &lt;- 5\nknot_list &lt;- quantile(dat_pat$X, probs = seq(from = 0, to = 1, length.out = num_knots))\nB &lt;- bs(dat_pat$X,\n        knots = knot_list[-c(1, num_knots)], \n        degree = 3, \n        intercept = TRUE)\nB\n\n                 1            2            3           4            5\n [1,] 1.0000000000 0.0000000000 0.0000000000 0.000000000 0.0000000000\n [2,] 0.3932160225 0.5205689817 0.0833170637 0.002897932 0.0000000000\n [3,] 0.1303337386 0.6136422453 0.2378607351 0.018163281 0.0000000000\n [4,] 0.0220025742 0.5116331668 0.4098320335 0.056532225 0.0000000000\n [5,] 0.0001737807 0.3325401051 0.5396793792 0.127606735 0.0000000000\n [6,] 0.0000000000 0.1814804087 0.5792969517 0.238573820 0.0006488199\n [7,] 0.0000000000 0.0883598392 0.5363295039 0.367729756 0.0075809005\n [8,] 0.0000000000 0.0097143260 0.3289042281 0.593688910 0.0676925363\n [9,] 0.0000000000 0.0003772242 0.1863761908 0.659307134 0.1539394508\n[10,] 0.0000000000 0.0000000000 0.0788259578 0.624941237 0.2955691681\n[11,] 0.0000000000 0.0000000000 0.0357900241 0.542804472 0.4124556582\n[12,] 0.0000000000 0.0000000000 0.0107764123 0.421496598 0.5286019082\n[13,] 0.0000000000 0.0000000000 0.0006952375 0.263269190 0.6110273290\n[14,] 0.0000000000 0.0000000000 0.0000000000 0.145095180 0.5888020313\n[15,] 0.0000000000 0.0000000000 0.0000000000 0.060977201 0.4468755310\n[16,] 0.0000000000 0.0000000000 0.0000000000 0.016061781 0.2349278937\n[17,] 0.0000000000 0.0000000000 0.0000000000 0.002189635 0.0740048857\n[18,] 0.0000000000 0.0000000000 0.0000000000 0.000000000 0.0000000000\n                 6            7\n [1,] 0.0000000000 0.0000000000\n [2,] 0.0000000000 0.0000000000\n [3,] 0.0000000000 0.0000000000\n [4,] 0.0000000000 0.0000000000\n [5,] 0.0000000000 0.0000000000\n [6,] 0.0000000000 0.0000000000\n [7,] 0.0000000000 0.0000000000\n [8,] 0.0000000000 0.0000000000\n [9,] 0.0000000000 0.0000000000\n[10,] 0.0006636368 0.0000000000\n[11,] 0.0089498455 0.0000000000\n[12,] 0.0391250811 0.0000000000\n[13,] 0.1250082431 0.0000000000\n[14,] 0.2659535203 0.0001492682\n[15,] 0.4675827148 0.0245645535\n[16,] 0.5868492773 0.1621610484\n[17,] 0.4743685242 0.4494369547\n[18,] 0.0000000000 1.0000000000\nattr(,\"degree\")\n[1] 3\nattr(,\"knots\")\n     25%      50%      75% \n2.295205 4.965753 6.997945 \nattr(,\"Boundary.knots\")\n[1] 0.000000 9.257534\nattr(,\"intercept\")\n[1] TRUE\nattr(,\"class\")\n[1] \"bs\"     \"basis\"  \"matrix\""
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-1",
    "href": "slides/09-nonlinear.html#b-spline-regression-1",
    "title": "Nonlinear Regression",
    "section": "B-spline regression",
    "text": "B-spline regression\n\nstan_data &lt;- list(\n  n = nrow(dat_pat),\n  p = ncol(B),\n  Y = dat_poly$Y,\n  X = B\n)\nfit_bspline &lt;- sampling(compile_model, data = stan_data)"
  },
  {
    "objectID": "slides/09-nonlinear.html#model-comparison",
    "href": "slides/09-nonlinear.html#model-comparison",
    "title": "Nonlinear Regression",
    "section": "Model comparison",
    "text": "Model comparison"
  },
  {
    "objectID": "slides/09-nonlinear.html#what-is-the-point",
    "href": "slides/09-nonlinear.html#what-is-the-point",
    "title": "Nonlinear Regression",
    "section": "What is the point?",
    "text": "What is the point?\n\nChoice of model is highly dependent on the context.\nAs we learned in the model comparison lecture, a better fit to the sample might not actually be a better model.\nThese basis models are difficult to interpret and are not particularly useful for a clinical setting (they may be useful for prediction!)."
  },
  {
    "objectID": "slides/09-nonlinear.html#change-point-motivation",
    "href": "slides/09-nonlinear.html#change-point-motivation",
    "title": "Nonlinear Regression",
    "section": "Change point motivation",
    "text": "Change point motivation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgression is defined by slow (or stable) deterioration, followed by a rapid decrease.\nFlexible modeling of MD across time.\nBiological representation of progression through the change point.\nChange points are a framework for inherently parameterizing progression."
  },
  {
    "objectID": "slides/09-nonlinear.html#writing-down-a-model",
    "href": "slides/09-nonlinear.html#writing-down-a-model",
    "title": "Nonlinear Regression",
    "section": "Writing down a model",
    "text": "Writing down a model\n\nModel for the observed data:\n\n\\[Y_i = \\mu_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0,\\sigma^2).\\]\n\n\nModel for the mean process:\n\n\\[\\mu_i =\\left\\{ \\begin{array}{ll}\n        {\\beta}_0 + \\beta_1 X_i & \\text{ } \\mbox{$X_i \\leq \\theta$},\\\\\n        {\\beta}_0 + \\beta_1 \\theta + {\\beta}_2(X_i - \\theta)& \\text{ } \\mbox{$X_i &gt; \\theta.$}\\end{array} \\right.\\]\n\n\\(\\theta \\in (\\min X_i, \\max X_i)\\) represents a change point."
  },
  {
    "objectID": "slides/09-nonlinear.html#change-point-model-in-stan",
    "href": "slides/09-nonlinear.html#change-point-model-in-stan",
    "title": "Nonlinear Regression",
    "section": "Change point model in Stan",
    "text": "Change point model in Stan\n\n// saved in change_points.stan\nfunctions {\n  vector compute_mean(vector X, real beta0, real beta1, real beta2, real theta) {\n    int n = size(X);\n    vector[n] mu;\n    for (t in 1:n) {\n      if (X[t] &lt;= theta) mu[t] = beta0 + beta1 * X[t];\n      if (X[t] &gt; theta) mu[t] = beta0 + beta1 * theta + beta2 * (X[t] - theta);\n  }\n  return mu;\n  }\n}\ndata {\n  int&lt;lower=1&gt; n;\n  vector[n] Y;\n  vector[n] X;\n  int n_pred;\n  vector[n_pred] X_pred;\n}\ntransformed data {\n  real min_X = min(X);\n  real max_X = max(X);\n}\nparameters {\n  real beta0;\n  real beta1;\n  real beta2;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = min_X, upper = max_X&gt; theta;\n}\nmodel {\n  vector[n] mu = compute_mean(X, beta0, beta1, beta2, theta);\n  target += normal_lpdf(Y | mu, sigma);\n  sigma ~ normal(0, 1);\n  target += normal_lpdf(beta0 | 0, 1);\n  target += normal_lpdf(beta1 | 0, 1);\n  target += normal_lpdf(beta2 | 0, 1);\n}\ngenerated quantities {\n  vector[n_pred] mu_pred = compute_mean(X_pred, beta0, beta1, beta2, theta);\n  array[n_pred] real Y_pred_out = normal_rng(mu_pred, sigma);\n  vector[n] mu = compute_mean(X, beta0, beta1, beta2, theta);\n  array[n] real Y_pred_in = normal_rng(mu, sigma);\n}"
  },
  {
    "objectID": "slides/09-nonlinear.html#change-point-regression",
    "href": "slides/09-nonlinear.html#change-point-regression",
    "title": "Nonlinear Regression",
    "section": "Change point regression",
    "text": "Change point regression\n\nstan_model &lt;- stan_model(file = \"change_points.stan\")\nn_pred &lt;- 1000\nstan_data &lt;- list(Y = dat_pat$Y, \n                  X = dat_pat$X,\n                  n = nrow(dat_pat),\n                  n_pred = n_pred,\n                  X_pred = seq(0, max(dat_pat$X) + 2, length.out = n_pred))\nfit_cp &lt;- sampling(stan_model, data = stan_data)\nprint(fit_cp, probs = c(0.25, 0.5, 0.75), pars = c(\"beta0\", \"beta1\", \"beta2\", \"sigma\", \"theta\"))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n       mean se_mean   sd   25%   50%   75% n_eff Rhat\nbeta0 -1.82    0.02 0.66 -2.29 -1.85 -1.40   749 1.00\nbeta1 -0.05    0.01 0.29 -0.19 -0.04  0.10   560 1.01\nbeta2 -0.84    0.02 0.45 -1.12 -0.87 -0.58   889 1.00\nsigma  1.27    0.01 0.26  1.08  1.23  1.41   792 1.00\ntheta  5.36    0.07 1.79  4.69  5.57  6.34   749 1.00\n\nSamples were drawn using NUTS(diag_e) at Wed Jan  1 17:29:30 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/09-nonlinear.html#diagnostics",
    "href": "slides/09-nonlinear.html#diagnostics",
    "title": "Nonlinear Regression",
    "section": "Diagnostics",
    "text": "Diagnostics"
  },
  {
    "objectID": "slides/09-nonlinear.html#posterior-histograms",
    "href": "slides/09-nonlinear.html#posterior-histograms",
    "title": "Nonlinear Regression",
    "section": "Posterior histograms",
    "text": "Posterior histograms"
  },
  {
    "objectID": "slides/09-nonlinear.html#posterior-fit",
    "href": "slides/09-nonlinear.html#posterior-fit",
    "title": "Nonlinear Regression",
    "section": "Posterior fit",
    "text": "Posterior fit"
  },
  {
    "objectID": "slides/09-nonlinear.html#posterior-fit-1",
    "href": "slides/09-nonlinear.html#posterior-fit-1",
    "title": "Nonlinear Regression",
    "section": "Posterior fit",
    "text": "Posterior fit"
  },
  {
    "objectID": "slides/09-nonlinear.html#prediction",
    "href": "slides/09-nonlinear.html#prediction",
    "title": "Nonlinear Regression",
    "section": "Prediction",
    "text": "Prediction"
  },
  {
    "objectID": "slides/09-nonlinear.html#prepare-for-next-class",
    "href": "slides/09-nonlinear.html#prepare-for-next-class",
    "title": "Nonlinear Regression",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02.\nComplete reading to prepare for next Tuesday‚Äôs lecture\nTuesday‚Äôs lecture: Robust regression"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html",
    "href": "ae/ae-05-nonlinear.html",
    "title": "AE 05: Change point regression",
    "section": "",
    "text": "ImportantDue date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-05-nonlinear.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 05: Change point regression",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp26 organization on GitHub.\nClick on the repo with the prefix ae-05-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-05.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#exercise-1",
    "href": "ae/ae-05-nonlinear.html#exercise-1",
    "title": "AE 05: Change point regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nFor eye 4, fit a change point regression model. Present MCMC convergence diagnostics. Did the model converge?\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#exercise-2",
    "href": "ae/ae-05-nonlinear.html#exercise-2",
    "title": "AE 05: Change point regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nUsing the model from Exercise 1, present posterior estimates for model parameters. Provide an point and interval estimate for when this eye progressed.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#exercise-3",
    "href": "ae/ae-05-nonlinear.html#exercise-3",
    "title": "AE 05: Change point regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nCreate a figure that plots the time since first visual field visit versus the mean deviation. Overlay the posterior mean process with a 95% credible band (similar to the figure on slide 32 from today). Also include a vertical line for the posterior mean estimate of the change point.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#exercise-4",
    "href": "ae/ae-05-nonlinear.html#exercise-4",
    "title": "AE 05: Change point regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nNow fit the change point model to eye 30. Did the MCMC sampler converge? To help answer this question, visualize the observed data for eye 30.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#optional-exercise-5",
    "href": "ae/ae-05-nonlinear.html#optional-exercise-5",
    "title": "AE 05: Change point regression",
    "section": "(Optional) Exercise 5",
    "text": "(Optional) Exercise 5\nGoing back to eye 4, fit a new change point model where both the mean and variance process are modeled as having a change point, so that \\(Y_i \\stackrel{ind}{\\sim} N(\\mu(X_i),\\sigma(X_i)^2)\\), where\n\\[\\log \\sigma (X_i) =\\left\\{ \\begin{array}{ll}\n        {\\gamma}_0 + \\gamma_1 X_i & \\text{ } \\mbox{$X_i \\leq \\theta$},\\\\\n        {\\gamma}_0 + \\gamma_1 \\theta + {\\gamma}_2(X_i - \\theta)& \\text{ } \\mbox{$X_i &gt; \\theta.$}\\end{array} \\right.\\]\nPresent posterior summaries for each parameter and create a visualization of the posterior standard deviations across time.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your AE repo on GitHub. You‚Äôre done! üéâ"
  },
  {
    "objectID": "prepare/prepare-feb05.html",
    "href": "prepare/prepare-feb05.html",
    "title": "Prepare for February 5 lecture",
    "section": "",
    "text": "üìñ Review Curve fitting with B-splines\n‚úÖ Work on HW 02"
  },
  {
    "objectID": "slides/08-workflow.html",
    "href": "slides/08-workflow.html",
    "title": "Bayesian Workflow",
    "section": "",
    "text": "‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine"
  },
  {
    "objectID": "slides/08-workflow.html#review-of-last-lecture",
    "href": "slides/08-workflow.html#review-of-last-lecture",
    "title": "Bayesian Workflow",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Thursday, we learned about various ways compare models.\n\nAIC, DIC, WAIC\nLOO-CV/LOO-IC\n\nToday, we will put these concepts within the larger framework of the Bayesian workflow."
  },
  {
    "objectID": "slides/08-workflow.html#bayes-theorem",
    "href": "slides/08-workflow.html#bayes-theorem",
    "title": "Bayesian Workflow",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{f(\\mathbf{Y})}\\]\n\n\nRethinking Bayes theorem:\n\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto f(\\mathbf{Y}, \\boldsymbol{\\theta}) = f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) \\]\n\n\n\nIn Stan:\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) + \\log f(\\boldsymbol{\\theta})\\]"
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow",
    "href": "slides/08-workflow.html#bayesian-workflow",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\n\n\n\n\nGelman A., Vehtari A., Simpson D., Margossian, C., Carpenter, B. and Yao, Y., Kennedy, L., Gabry, J., B√ºrkner P. C., & Modr√°k M. (2020). Bayesian Workflow."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow-1",
    "href": "slides/08-workflow.html#bayesian-workflow-1",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\n\n\nTaken from Bayesian workflow by Francesca Capel\n\n\n\nToday we will talk about a general strategy for taking a question and data to a robust conclusion."
  },
  {
    "objectID": "slides/08-workflow.html#a-simplified-workflow",
    "href": "slides/08-workflow.html#a-simplified-workflow",
    "title": "Bayesian Workflow",
    "section": "A simplified workflow",
    "text": "A simplified workflow\n\nSetting up a full probability model: a joint probability distribution for all observable and unobservable quantities in a problem. The model should be consistent with knowledge about the underlying scientific problem and the data collection process.\nConditioning on observed data: calculating and interpreting the appropriate posterior distribution ‚Äî the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data.\nEvaluating the fit of the model and the implications of the resulting posterior distribution: how well does the model fit the data, are the substantive conclusions reasonable, and how sensitive are the results to the modeling assumptions in step 1? In response, one can alter or expand the model and repeat the three steps.\n\nFrom BDA3."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow-2",
    "href": "slides/08-workflow.html#bayesian-workflow-2",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\nResearch question: What are your dependent and indepednent variables? What associations are you interested in? EDA.\n\n\n\nSpecify likelihood & priors: Use knowledge of the problem to construct a generative model.\n\n\n\n\nCheck the model with simulated data: Generate data from the model and evaluate fit as a sanity check (prior predictive checks).\n\n\n\n\nFit the model to real data: Estimate parameters using MCMC."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow-3",
    "href": "slides/08-workflow.html#bayesian-workflow-3",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\nCheck diagnostics: Use MCMC diagnostics to guarentee that the algorithm converged.\n\n\n\nExamine posterior fit: Create posterior summaries that are relevant to the research question.\n\n\n\n\nCheck predictions: Examing posterior predictive checks.\n\n\n\n\nCompare models: Iterate on model design and choose a model."
  },
  {
    "objectID": "slides/08-workflow.html#motivating-example-predicting-weight-from-height",
    "href": "slides/08-workflow.html#motivating-example-predicting-weight-from-height",
    "title": "Bayesian Workflow",
    "section": "Motivating example: predicting weight from height",
    "text": "Motivating example: predicting weight from height\nResearch question: We would like to understand the relationship between a person‚Äôs height and weight. A few particular questions we have are:\n\nHow much does a person‚Äôs weight increase when their height increases?\nHow certain can we be about the magnitude of the increase?\nCan we predict a person‚Äôs weight based on their height?\n\nData: We will use the bdims dataset from the openintro package. This dataset contains body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender."
  },
  {
    "objectID": "slides/08-workflow.html#prepare-data",
    "href": "slides/08-workflow.html#prepare-data",
    "title": "Bayesian Workflow",
    "section": "Prepare data",
    "text": "Prepare data\n\nlibrary(openintro)\ndat &lt;- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))\nhead(dat)\n\n    weight   height  sex\n1 144.6231 68.50397 Male\n2 158.2917 69.01579 Male\n3 177.9128 76.18114 Male\n4 160.0554 73.42524 Male\n5 173.7241 73.70083 Male\n6 164.9056 71.45673 Male"
  },
  {
    "objectID": "slides/08-workflow.html#research-question",
    "href": "slides/08-workflow.html#research-question",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/08-workflow.html#research-question-1",
    "href": "slides/08-workflow.html#research-question-1",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/08-workflow.html#research-question-2",
    "href": "slides/08-workflow.html#research-question-2",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors",
    "href": "slides/08-workflow.html#specify-likelihood-priors",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\nDefine, \\(Y_i\\) as the weight of observation \\(i\\) and \\(\\mathbf{x}_i\\) as a vector of covariates (here only height).\n\n\\[Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates (excluding intercept)\n  vector[n] Y;      // outcome variable\n  matrix[n, p] X;   // covariate matrix\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-1",
    "href": "slides/08-workflow.html#specify-likelihood-priors-1",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\nparameter {\n  real alpha;            // intercept on the original scale\n  vector[p] beta;        // regression parameters\n  real&lt;lower = 0&gt; sigma; // measurement error\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-2",
    "href": "slides/08-workflow.html#specify-likelihood-priors-2",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-3",
    "href": "slides/08-workflow.html#specify-likelihood-priors-3",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha\\) is the intercept, or average weight for someone who is zero inches (not a particularly useful number on its own)\n\\(\\beta\\) measures the association between weight and height, in pounds/inch\n\\(\\sigma\\) is the measurement error for the population\n\n\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, sigma);\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n  sigma ~ normal(0, 10);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-4",
    "href": "slides/08-workflow.html#specify-likelihood-priors-4",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[Y_i] = \\alpha^+ + (\\mathbf{x}_i - \\bar{\\mathbf{x}}) \\boldsymbol{\\beta},\\quad\\bar{\\mathbf{x}}=\\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha^+\\) is the intercept, or average weight for someone who is an average height\n\n\ntransformed data {\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-5",
    "href": "slides/08-workflow.html#specify-likelihood-priors-5",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[Y_i] = \\alpha^+ + (\\mathbf{x}_i - \\bar{\\mathbf{x}}) \\boldsymbol{\\beta},\\quad\\bar{\\mathbf{x}}=\\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha^+\\) is the intercept, or average weight for someone who is an average height\n\n\nmodel {\n  target += normal_lpdf(Y | alpha_plus + X_centered * beta, sigma);\n  target += normal_lpdf(alpha_plus | 150, 10);\n  target += normal_lpdf(beta | 0, 10);\n  sigma ~ normal(0, 10);\n}\n\n\nHard to put a weakly informative prior on \\(\\alpha^+\\)."
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-6",
    "href": "slides/08-workflow.html#specify-likelihood-priors-6",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[Y_i - \\bar{Y}] = \\alpha^* + (\\mathbf{x}_i - \\bar{\\mathbf{x}}) \\boldsymbol{\\beta},\\quad\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^n Y_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha^*\\) is the intercept for the centered data, should be zero.\n\n\ntransformed data {\n  vector[n] Y_centered;\n  real Y_bar;\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n  Y_bar = mean(Y);\n  Y_centered = Y - Y_bar;\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-7",
    "href": "slides/08-workflow.html#specify-likelihood-priors-7",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[Y_i - \\bar{Y}] = \\alpha^* + (\\mathbf{x}_i - \\bar{\\mathbf{x}}) \\boldsymbol{\\beta},\\quad\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^n Y_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha^*\\) is the intercept for the centered data, should be zero.\n\n\nmodel {\n  target += normal_lpdf(Y_centered | alpha_star + X_centered * beta, sigma);\n  target += normal_lpdf(alpha_star | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n  sigma ~ normal(0, 10);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#quick-aside",
    "href": "slides/08-workflow.html#quick-aside",
    "title": "Bayesian Workflow",
    "section": "Quick aside",
    "text": "Quick aside\nWhat does it mean to use the prior sigma ~ normal(0, 10)?\n\nWhen a parameter is truncated, for example real&lt;lower = 0&gt; sigma, priors can still be placed across the real line, \\(\\mathbb{R}\\).\n\n\nparameters {\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  sigma ~ normal(0, 10);\n}\n\n\nThis specification induces a prior on the truncated space \\(\\mathbb{R}^+\\).\nThe induced prior for sigma is a half-normal distribution."
  },
  {
    "objectID": "slides/08-workflow.html#quick-aside-1",
    "href": "slides/08-workflow.html#quick-aside-1",
    "title": "Bayesian Workflow",
    "section": "Quick aside",
    "text": "Quick aside\n\nThe half-normal is a useful prior for nonnegative parameters that should not be too large and may be very close to zero.\nSimilar distributions for scale parameters are half-t and half-Cauchy priors, these have heavier tales."
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\n\nDraw parameter values from priors.\nGenerate data based on those parameter values.\nCheck simulated data summaries and compare to observed data."
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-1",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-1",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\n\n// stored in workflow_prior_pred_check.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  real Y_bar;\n  matrix[n, p] X;\n  real&lt;lower = 0&gt; sigma_alpha;\n  real&lt;lower = 0&gt; sigma_beta;\n  real&lt;lower = 0&gt; sigma_sigma;\n}\ntransformed data {\n  row_vector[p] X_bar;\n  for (i in 1:p) X_bar[i] = mean(X[, i]);\n}\ngenerated quantities {\n  // Sample from the priors\n  real alpha_star = normal_rng(0, sigma_alpha);\n  real alpha_plus = alpha_star + Y_bar;\n  real alpha = alpha_plus - X_bar * beta;\n  vector[p] beta;\n  for (i in 1:p) beta[i] = normal_rng(0, sigma_beta);\n  real sigma = fabs(normal_rng(0, sigma_sigma));\n  // Simulate data from the prior\n  vector[n] Y;\n  for (i in 1:n) {\n    Y[i] = normal_rng(alpha + X[i, ] * beta, sigma);\n  }\n  // Compute summaries from the prior\n  real Y_min = min(Y);\n  real Y_max = max(Y);\n  real Y_mean = mean(Y);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-2",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-2",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\n\n###Compile the Stan code\nprior_check &lt;- stan_model(file = \"workflow_prior_pred_check.stan\")\n\n###Define the Stan data object\nY &lt;- dat$weight\nX &lt;- matrix(dat$height)\nstan_data &lt;- list(\n  n = nrow(dat), \n  p = ncol(X),\n  Y_bar = mean(Y),\n  X = X,\n  sigma_alpha = 10,\n  sigma_beta = 10,\n  sigma_sigma = 10)\n\n###Simulate data from the prior\nprior_check1 &lt;- sampling(prior_check, data = stan_data, \n                         algorithm = \"Fixed_param\", chains = 1, iter = 1000)"
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-3",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-3",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:"
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-4",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-4",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\n\n###Compile the Stan code\nprior_check &lt;- stan_model(file = \"workflow_prior_pred_check.stan\")\n\n###Define the Stan data object\nY &lt;- dat$weight\nX &lt;- matrix(dat$height)\nstan_data &lt;- list(\n  n = nrow(dat), \n  p = ncol(X),\n  Y_bar = mean(Y),\n  X = X,\n  sigma_alpha = 10,\n  sigma_beta = 5,\n  sigma_sigma = 4)\n\n###Simulate data from the prior\nprior_check2 &lt;- sampling(prior_check, data = stan_data, \n                         algorithm = \"Fixed_param\", chains = 1, iter = 1000)"
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-5",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-5",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:"
  },
  {
    "objectID": "slides/08-workflow.html#fit-the-model-to-real-data",
    "href": "slides/08-workflow.html#fit-the-model-to-real-data",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\n// saved in linear_regression_workflow.stan\ndata {\n  int&lt;lower = 1&gt; n;        // number of observations\n  int&lt;lower = 1&gt; p;        // number of covariates (excluding intercept)\n  vector[n] Y;             // outcome vector\n  matrix[n, p] X;          // covariate matrix\n  int&lt;lower = 1&gt; n_pred;   // number of new observations to predict\n  matrix[n_pred, p] X_new; // covariate matrix for new observations\n}\ntransformed data {\n  vector[n] Y_centered;\n  real Y_bar;\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n  Y_bar = mean(Y);\n  Y_centered = Y - Y_bar;\n}\nparameters {\n  real alpha_star;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(Y_centered | alpha_star + X_centered * beta, sigma); // likelihood\n  target += normal_lpdf(alpha_star | 0, 10);\n  target += normal_lpdf(beta | 0, 5);\n  sigma ~ normal(0, 4);\n}\ngenerated quantities {\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  vector[n_pred] Y_new;\n  real alpha = Y_bar + alpha_star - X_bar * beta;\n  for (i in 1:n) {\n    Y_pred[i] = normal_rng(alpha + X[i, ] * beta, sigma);\n    log_lik[i] = normal_lpdf(Y_centered[i] | alpha_star + X_centered[i, ] * beta, sigma);\n  }\n  for (i in 1:n_pred) Y_new[i] = normal_rng(alpha + X_new[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#fit-the-model-to-real-data-1",
    "href": "slides/08-workflow.html#fit-the-model-to-real-data-1",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\n###Compile model\nregression_model &lt;- stan_model(file = \"linear_regression_workflow.stan\")\n\n###Create data\nY &lt;- dat$weight\nX &lt;- matrix(dat$height)\nn_new &lt;- 1000\nX_new &lt;- matrix(seq(min(dat$height), max(dat$height), length.out = n_new))\nstan_data &lt;- list(\n  n = nrow(dat), \n  p = ncol(X),\n  Y = Y,\n  X = X,\n  n_new = n_new,\n  X_new = X_new\n)"
  },
  {
    "objectID": "slides/08-workflow.html#fit-the-model-to-real-data-2",
    "href": "slides/08-workflow.html#fit-the-model-to-real-data-2",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\n###Fit the model\nfit_workflow &lt;- sampling(regression_model, data = stan_data)\nprint(fit_workflow, pars = c(\"alpha\", \"beta\", \"sigma\"), probs = c(0.025, 0.5, 0.975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n           mean se_mean    sd    2.5%     50%   97.5% n_eff Rhat\nalpha   -230.32    0.25 16.71 -263.60 -230.48 -198.47  4349    1\nbeta[1]    5.68    0.00  0.25    5.20    5.68    6.17  4367    1\nsigma     20.09    0.01  0.61   18.93   20.07   21.32  4148    1\n\nSamples were drawn using NUTS(diag_e) at Mon Feb  3 13:39:30 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/08-workflow.html#check-diagnostics",
    "href": "slides/08-workflow.html#check-diagnostics",
    "title": "Bayesian Workflow",
    "section": "5. Check diagnostics:",
    "text": "5. Check diagnostics:\n\nrstan::traceplot(fit_workflow, pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/08-workflow.html#check-diagnostics-1",
    "href": "slides/08-workflow.html#check-diagnostics-1",
    "title": "Bayesian Workflow",
    "section": "5. Check diagnostics:",
    "text": "5. Check diagnostics:\n\nlibrary(bayesplot)\nmcmc_acf(fit_workflow, regex_pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/08-workflow.html#examine-posterior-fit",
    "href": "slides/08-workflow.html#examine-posterior-fit",
    "title": "Bayesian Workflow",
    "section": "6. Examine posterior fit:",
    "text": "6. Examine posterior fit:"
  },
  {
    "objectID": "slides/08-workflow.html#examine-posterior-fit-1",
    "href": "slides/08-workflow.html#examine-posterior-fit-1",
    "title": "Bayesian Workflow",
    "section": "6. Examine posterior fit:",
    "text": "6. Examine posterior fit:\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior mean and 95% credible interval for \\(\\mu = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/08-workflow.html#check-predictions",
    "href": "slides/08-workflow.html#check-predictions",
    "title": "Bayesian Workflow",
    "section": "7.Check predictions:",
    "text": "7.Check predictions:\nY_pred &lt;- rstan::extract(fit_workflow, pars = \"Y_pred\")$Y_pred\nppc_dens_overlay(Y, Y_pred[1:100, ])"
  },
  {
    "objectID": "slides/08-workflow.html#check-predictions-1",
    "href": "slides/08-workflow.html#check-predictions-1",
    "title": "Bayesian Workflow",
    "section": "7. Check predictions:",
    "text": "7. Check predictions:\nppc_stat(Y, Y_pred, stat = \"mean\") # from bayesplot\nppc_stat(Y, Y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(Y, Y_pred, stat = \"q025\")\nppc_stat(Y, Y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/08-workflow.html#check-predictions-2",
    "href": "slides/08-workflow.html#check-predictions-2",
    "title": "Bayesian Workflow",
    "section": "7. Check predictions:",
    "text": "7. Check predictions:\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior predictive distribution mean and 95% credible interval, \\(f(Y_{i'} | Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/08-workflow.html#shinystan",
    "href": "slides/08-workflow.html#shinystan",
    "title": "Bayesian Workflow",
    "section": "shinystan",
    "text": "shinystan\n\nlibrary(shinystan)\nY &lt;- dat$weight # need to define outcome as a global variable to be accessible\nsso &lt;- shinystan::launch_shinystan(fit_workflow)"
  },
  {
    "objectID": "slides/08-workflow.html#compare-models",
    "href": "slides/08-workflow.html#compare-models",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\nSuppose we would like to compare our original model with models that also includes sex and an interaction between sex and height.\n\n\\[\\begin{aligned}\n\\mathbb{E}[weight_i] &= \\alpha + \\beta_1 height_i\\\\\n\\mathbb{E}[weight_i] &= \\alpha + \\beta_1 height_i + \\beta_2 sex_i\\\\\n\\mathbb{E}[weight_i] &= \\alpha + \\beta_1 height_i + \\beta_2 sex_i + \\beta_3 height_i sex_i\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08-workflow.html#compare-models-1",
    "href": "slides/08-workflow.html#compare-models-1",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\n###Compute individual WAIC\nlibrary(loo)\nlog_lik_model1 &lt;- loo::extract_log_lik(fit_model1, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model2 &lt;- loo::extract_log_lik(fit_model2, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model3 &lt;- loo::extract_log_lik(fit_model3, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model1 &lt;- loo::waic(log_lik_model1)\nwaic_model2 &lt;- loo::waic(log_lik_model2)\nwaic_model3 &lt;- loo::waic(log_lik_model3)\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"height_only\" = waic_model1, \"height_sex\" = waic_model2, \"interaction\" = waic_model3))\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n            elpd_diff se_diff  elpd_waic se_elpd_waic p_waic   se_p_waic\ninteraction     0.00      0.00 -2225.62     24.22         5.08     0.96 \nheight_sex     -1.19      1.66 -2226.81     23.78         4.80     0.87 \nheight_only   -28.15      8.43 -2253.77     21.55         3.57     0.63 \n            waic     se_waic \ninteraction  4451.23    48.44\nheight_sex   4453.62    47.55\nheight_only  4507.53    43.10"
  },
  {
    "objectID": "slides/08-workflow.html#compare-models-2",
    "href": "slides/08-workflow.html#compare-models-2",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\n###Compute individual LOO-CV/LOO-IC\nloo_model1 &lt;- loo::loo(log_lik_model1)\nloo_model2 &lt;- loo::loo(log_lik_model2)\nloo_model3 &lt;- loo::loo(log_lik_model3)\n\n###Make a comparison\ncomp_loo &lt;- loo::loo_compare(list(\"height_only\" = loo_model1, \"height_sex\" = loo_model2, \"interaction\" = loo_model3))\nprint(comp_loo, digits = 2, simplify = FALSE)\n\n            elpd_diff se_diff  elpd_loo se_elpd_loo p_loo    se_p_loo looic   \ninteraction     0.00      0.00 -2225.63    24.23        5.10     0.96  4451.27\nheight_sex     -1.18      1.66 -2226.81    23.78        4.80     0.87  4453.63\nheight_only   -28.13      8.43 -2253.77    21.55        3.57     0.63  4507.54\n            se_looic\ninteraction    48.46\nheight_sex     47.56\nheight_only    43.10"
  },
  {
    "objectID": "slides/08-workflow.html#the-plan-moving-forward",
    "href": "slides/08-workflow.html#the-plan-moving-forward",
    "title": "Bayesian Workflow",
    "section": "The plan moving forward",
    "text": "The plan moving forward\n\nWe have now learned all of the skills needed to implement a Bayesian workflow.\nThe remainder of the course will be focused on introducing new models for types of data that are common when working in the biomedical data setting."
  },
  {
    "objectID": "slides/08-workflow.html#prepare-for-next-class",
    "href": "slides/08-workflow.html#prepare-for-next-class",
    "title": "Bayesian Workflow",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02\nComplete reading to prepare for next Thursday‚Äôs lecture\nThursday‚Äôs lecture: Nonlinear Regression"
  },
  {
    "objectID": "slides/08-workflow.html#examine-posterior-fit-2",
    "href": "slides/08-workflow.html#examine-posterior-fit-2",
    "title": "Bayesian Workflow",
    "section": "6. Examine posterior fit:",
    "text": "6. Examine posterior fit:\n\n\n\n\n\n\n\n\n\n\n\n\nvector[n] mu;\nfor (i in 1:n) mu[i] = alpha + X[i, ] * beta;"
  },
  {
    "objectID": "hw/hw-03.html",
    "href": "hw/hw-03.html",
    "title": "HW 03: Going beyond linear regression",
    "section": "",
    "text": "ImportantDue date\n\n\n\nThis assignment is due on Thursday, February 26 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-1",
    "href": "hw/hw-03.html#exercise-1",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nFormulate this regression problem within the framework of a Bayesian model (hint: Laplace distribution!). Fit this regression using Stan to estimate \\((\\alpha, \\boldsymbol{\\beta})\\) and any other parameters that arise in the model. For all model parameters, choose weakly-informative priors. Evaluate model convergence.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-2",
    "href": "hw/hw-03.html#exercise-2",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nPerform a posterior predictive check using the median as a test statistic. Be sure to present a posterior predictive p-value and use it to describe the model fit.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-3",
    "href": "hw/hw-03.html#exercise-3",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nPerform a posterior predictive check for the 2.5th and 97.5th quantiles. Comment on the difference between the result from Exercise 2.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-4",
    "href": "hw/hw-03.html#exercise-4",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nPresent posterior summaries for all population parameters. For all predictors with a significant association (i.e., 95% credible interval does not include zero), provide an interpretation within the context of the problem.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-5",
    "href": "hw/hw-03.html#exercise-5",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nFit the same model as in Exercise 1 using linear regression. Compare the posterior mean estimates of \\(\\boldsymbol{\\beta}\\) between the two models. Do they correspond?",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-6",
    "href": "hw/hw-03.html#exercise-6",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nPerform a model comparison between the models in Exercise 1 and Exercise 5. Which model is preferred? Provide intuition for why one model may be preferred over the other.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-7",
    "href": "hw/hw-03.html#exercise-7",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nBefore fitting the horseshoe regression, a realistic value of \\(\\tau_0\\) must be determined. Compute a realistic value for \\(\\tau_0\\) based on the effective number of non-zero coefficients. Researchers have a prior belief that the number of non-zero coefficients will be equal to 1 (i.e., \\(q_0 = 1\\)). When computing \\(\\tau_0\\) be sure to provide a visual justification by plotting the effective number of non-zero coefficients for your choice of \\(\\tau_0\\).",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-8",
    "href": "hw/hw-03.html#exercise-8",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit the logistic regression model using the horseshoe prior above. Present model convergence diagnostics and make a statement about whether the MCMC sampler has converged.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-9",
    "href": "hw/hw-03.html#exercise-9",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nVisualize the posterior distributions for the population parameters \\((\\alpha, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma})\\). \\(\\alpha\\) is the intercept on the scale of the original data. Make a statement about the impact of the horseshoe prior on the posterior shape for \\(\\boldsymbol{\\gamma}\\).",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-10",
    "href": "hw/hw-03.html#exercise-10",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nPresent posterior summaries for \\((\\alpha, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma})\\). Choose one predictor that is significant and provide an interpretation of the posterior mean.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-11",
    "href": "hw/hw-03.html#exercise-11",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 11",
    "text": "Exercise 11\nVisualize the posterior distribution of \\(\\pi_2\\) and \\(\\pi_4\\), which are the probability of having a length of stay greater than 5 days for observation \\(Y_2\\) and \\(Y_4\\), respectively (i.e., the second and fourth rows of the hdp_hw3 dataset). Compute \\(P(\\pi_4 &gt; \\pi_2 | \\mathbf{Y})\\) and make a statement about which patient is more likely to have a longer length of stay.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "prepare/prepare-feb10.html",
    "href": "prepare/prepare-feb10.html",
    "title": "Prepare for February 10 lecture",
    "section": "",
    "text": "üìñ Read BDA3 Chapter 17.1-17.2 to be introduced to robust regression\nüìñ Review Robust regression\n‚úÖ Work on HW 02"
  },
  {
    "objectID": "slides/11-regularization.html",
    "href": "slides/11-regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nLoaded lars 1.3\n\n\n\nAttaching package: 'LaplacesDemon'\n\n\nThe following objects are masked from 'package:mvtnorm':\n\n    dmvt, logdet, rmvt\n\n\nThe following objects are masked from 'package:lubridate':\n\n    dst, interval\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\n\nnimble version 1.2.1 is loaded.\nFor more information on NIMBLE and a User Manual,\nplease visit https://R-nimble.org.\n\nNote for advanced users who have written their own MCMC samplers:\n  As of version 0.13.0, NIMBLE's protocol for handling posterior\n  predictive nodes has changed in a way that could affect user-defined\n  samplers in some situations. Please see Section 15.5.1 of the User Manual.\n\n\nAttaching package: 'nimble'\n\n\nThe following objects are masked from 'package:LaplacesDemon':\n\n    cloglog, dcat, dinvgamma, is.model, logdet, logit, rcat, rinvgamma\n\n\nThe following object is masked from 'package:mvtnorm':\n\n    logdet\n\n\nThe following object is masked from 'package:stats':\n\n    simulate\n\n\nThe following object is masked from 'package:base':\n\n    declare"
  },
  {
    "objectID": "slides/11-regularization.html#review-of-last-lecture",
    "href": "slides/11-regularization.html#review-of-last-lecture",
    "title": "Regularization",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we learned about robust regression.\n\nHeteroskedasticity\nHeavy-tailed distributions\nMedian regression\n\nThese were all models for the observed data \\(Y_i\\).\nToday, we will focus on prior specifications for \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/11-regularization.html#sparsity-in-regression-problems",
    "href": "slides/11-regularization.html#sparsity-in-regression-problems",
    "title": "Regularization",
    "section": "Sparsity in regression problems",
    "text": "Sparsity in regression problems\n\nSupervised learning can be cast as the problem of estimating a set of coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_j\\}_{j=1}^{p}\\) that determines some functional relationship between a set of \\(\\{x_{ij}\\}_{j = 1}^p\\) and a target variable \\(Y_i\\).\nThis is a central focus of statistics and machine learning.\nChallenges arise in ‚Äúlarge-\\(p\\)‚Äù problems where, in order to avoid overly complex models that predict poorly, some form of dimension reduction is needed.\nFinding a sparse solution, where some \\(\\beta_j\\) are zero, is desirable."
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-sparse-estimation",
    "href": "slides/11-regularization.html#bayesian-sparse-estimation",
    "title": "Regularization",
    "section": "Bayesian sparse estimation",
    "text": "Bayesian sparse estimation\n\nFrom a Bayesian-learning perspective, there are two main sparse-estimation alternatives: discrete mixtures and shrinkage priors.\nDiscrete mixtures have been very popular, with the spike-and-slab prior being the gold standard.\n\nEasy to force \\(\\beta_j\\) to exactly zero, but require discrete parameter specification.\n\nShrinkage priors force \\(\\beta_j\\) to zero using regularization, but struggle to get exact zeros.\n\nIn recent years, shrinkage priors have become dominant in Bayesian sparsity priors."
  },
  {
    "objectID": "slides/11-regularization.html#global-local-shrinkage",
    "href": "slides/11-regularization.html#global-local-shrinkage",
    "title": "Regularization",
    "section": "Global-local shrinkage",
    "text": "Global-local shrinkage\n\nLet‚Äôs assume \\(\\mathbf{Y} \\stackrel{}{\\sim}N\\left(\\alpha\\mathbf{1}_n + \\mathbf{X}\\boldsymbol{\\beta},\\sigma^2\\mathbf{I}_n\\right)\\).\nSparsity can be induced into \\(\\boldsymbol{\\beta}\\) using a global-local prior,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} f(\\lambda_j).\n\\end{aligned}\\]\n\n\\(\\tau^2\\) is the global variance term.\n\\(\\lambda_j\\) is the local term.\nThe degree of sparsity depends on the choice of \\(f(\\lambda_j)\\)."
  },
  {
    "objectID": "slides/11-regularization.html#spike-and-slab-prior",
    "href": "slides/11-regularization.html#spike-and-slab-prior",
    "title": "Regularization",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nDiscrete parameter specification,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\text{Bernoulli}(\\pi).\n\\end{aligned}\\]\n\n\\(\\lambda_j \\in \\{0,1\\}\\), thus this model permits exact zeros.\nThe number of zeros is dictated by \\(\\pi\\), which can either be pre-specified or given a prior.\nDiscrete parameters can not be specified in Stan!"
  },
  {
    "objectID": "slides/11-regularization.html#spike-and-slab-prior-1",
    "href": "slides/11-regularization.html#spike-and-slab-prior-1",
    "title": "Regularization",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nSpike-and-slab can be written generally as a two-component mixture of Gaussians,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau, \\omega &\\stackrel{ind}{\\sim} \\lambda_j N(0, \\tau^2) + (1-\\lambda_j) N(0,\\omega^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\text{Bernoulli}(\\pi).\n\\end{aligned}\\]\n\n\\(\\omega \\ll \\tau\\) and the indicator variable \\(\\lambda_j \\in \\{0, 1\\}\\) denotes whether \\(\\beta_j\\) is close to zero (comes from the ‚Äúspike‚Äù, \\(\\lambda_j = 0\\)) or non-zero (comes from the ‚Äúslab‚Äù, \\(\\lambda_j = 1\\)).\nOften \\(\\omega = 0\\) (the spike is a true spike)."
  },
  {
    "objectID": "slides/11-regularization.html#ridge-regression",
    "href": "slides/11-regularization.html#ridge-regression",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nRidge regression is motivated by extending linear regression to the setting where:\n\nthere are too many predictors (sparsity is desired) and/or,\n\\(\\mathbf{X}^\\top \\mathbf{X}\\) is ill-conditioned and singular or nearly singular (multicollinearity).\n\nThe OLS estimate becomes unstable: \\[\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1}\\mathbf{X}^\\top \\mathbf{Y}.\\]"
  },
  {
    "objectID": "slides/11-regularization.html#ridge-regression-1",
    "href": "slides/11-regularization.html#ridge-regression-1",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\nThe ridge estimator minimizes the penalized sum of squares,\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{RIDGE}} = \\arg \\min_{\\boldsymbol{\\beta}}\\left||\\mathbf{Y} - \\boldsymbol{\\mu}\\right||_2^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\\]\n\n\\(\\boldsymbol{\\mu} = \\alpha\\mathbf{1}_n + \\mathbf{X}\\boldsymbol{\\beta}\\).\n\\(||\\mathbf{v}||_2 = \\sqrt{\\mathbf{v}^\\top \\mathbf{v}}\\) is the L2 norm.\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{RIDGE}} = \\left(\\mathbf{X}^\\top\\mathbf{X} + \\lambda \\mathbf{I}_p\\right)^{-1}\\mathbf{X}^\\top\\mathbf{Y}\\)\n\nAdding the \\(\\lambda\\) to diagonals of \\(\\mathbf{X}^\\top\\mathbf{X}\\) stabilizes the inverse, which becomes unstable with multicollinearity."
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-ridge-prior",
    "href": "slides/11-regularization.html#bayesian-ridge-prior",
    "title": "Regularization",
    "section": "Bayesian ridge prior",
    "text": "Bayesian ridge prior\nRidge regression can be obtained using the following global-local shrinkage prior,\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &= 1 / \\lambda\\\\\n\\tau^2 &= \\sigma^2.\n\\end{aligned}\\]\n\nThis is equivalent to: \\(f(\\beta_j | \\lambda, \\sigma) \\stackrel{iid}{\\sim} N\\left(0,\\frac{\\sigma^2}{\\lambda}\\right)\\).\nHow is this equivalent to ridge regression?"
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-ridge-prior-1",
    "href": "slides/11-regularization.html#bayesian-ridge-prior-1",
    "title": "Regularization",
    "section": "Bayesian ridge prior",
    "text": "Bayesian ridge prior\n\nThe negative log-posterior is proportional to,\n\n\\[\\frac{||\\mathbf{Y} - \\boldsymbol{\\mu}||_2^2}{2\\sigma^2} + \\frac{\\lambda \\sum_{j=1}^p \\beta_j^2}{2\\sigma^2}.\\]\n\nThe posterior mean and mode are \\(\\hat{\\boldsymbol{\\beta}}_{\\text{RIDGE}}\\).\nSince \\(\\lambda\\) is applied to the squared norm of the \\(\\boldsymbol{\\beta}\\), people often standardize all of the covariates to make them have a similar scale.\nBayesian statistics is inherently performing regularization!"
  },
  {
    "objectID": "slides/11-regularization.html#lasso-regression",
    "href": "slides/11-regularization.html#lasso-regression",
    "title": "Regularization",
    "section": "Lasso regression",
    "text": "Lasso regression\nThe least absolute shrinkage and selection operator (lasso) estimator minimizes the penalized sum of squares,\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{LASSO}} = \\arg \\min_{\\boldsymbol{\\beta}}\\left||\\mathbf{Y} - \\boldsymbol{\\mu}\\right||_2^2 + \\lambda \\sum_{j=1}^p |\\beta_j|\\]\n\n\\(\\lambda = 0\\) reduces to OLS etimator.\n\\(\\lambda = \\infty\\) leads to \\(\\hat{\\boldsymbol{\\beta}}_{\\text{LASSO}} = 0\\).\nLasso is desirable because it can set some \\(\\beta_j\\) exactly to zero."
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-lasso-prior",
    "href": "slides/11-regularization.html#bayesian-lasso-prior",
    "title": "Regularization",
    "section": "Bayesian lasso prior",
    "text": "Bayesian lasso prior\nLasso regression can be obtained using the following global-local shrinkage prior,\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\stackrel{iid}{\\sim} \\text{Exponential}(0.5).\n\\end{aligned}\\]\n\nThis is equivalent to: \\(f(\\beta_j | \\tau) \\stackrel{iid}{\\sim} \\text{Laplace}\\left(0,\\tau\\right)\\).\nHow is this equivalent to lasso regression?"
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-lasso-prior-1",
    "href": "slides/11-regularization.html#bayesian-lasso-prior-1",
    "title": "Regularization",
    "section": "Bayesian lasso prior",
    "text": "Bayesian lasso prior\n\nThe negative log-posterior is proportional to,\n\n\\[\\frac{||\\mathbf{Y} - \\boldsymbol{\\mu}||_2^2}{2\\sigma^2} + \\frac{\\sum_{j=1}^p |\\beta_j|}{\\tau}.\\]\n\nLasso is recovered by specifying: \\(\\lambda = 1/\\tau\\).\nThe posterior mode is \\(\\hat{\\boldsymbol{\\beta}}_{\\text{LASSO}}\\).\nAs \\(\\lambda\\) increases, more coefficients are set to zero (less variables are selected), and among the non-zero coefficients, more shrinkage is employed."
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-lasso-does-not-work",
    "href": "slides/11-regularization.html#bayesian-lasso-does-not-work",
    "title": "Regularization",
    "section": "Bayesian lasso does not work",
    "text": "Bayesian lasso does not work\n\nThere is a consensus that the Bayesian lasso does not work well.\nIt does not yield \\(\\beta_j\\) that are exactly zero and it can overly shrink non-zero \\(\\beta_j\\).\nThe gold-standard sparsity-inducing prior in Bayesian statistics is the horseshoe prior."
  },
  {
    "objectID": "slides/11-regularization.html#relevance-vector-machine",
    "href": "slides/11-regularization.html#relevance-vector-machine",
    "title": "Regularization",
    "section": "Relevance vector machine",
    "text": "Relevance vector machine\n\nBefore we get to the horseshoe, one more global-local prior, called the relevance vector machine.\nThis model can be obtained using the following prior,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\n\\end{aligned}\\]\n\nThis is equivalent to: \\(f(\\beta_j | \\tau) \\stackrel{iid}{\\sim} {t}_{\\nu}\\left(0,\\tau\\right)\\)."
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-prior",
    "href": "slides/11-regularization.html#horseshoe-prior",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\n\nThe horseshoe prior is specified as,\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\mathcal C^+(0, 1),\n\\end{aligned}\\] where \\(\\mathcal C^+(0, 1)\\) is a half-Cauchy distribution for the local parameter \\(\\lambda_j\\).\n\n\\(\\lambda_j\\)‚Äôs are the local shrinkage parameters.\n\\(\\tau\\) is the global shrinkage parameter."
  },
  {
    "objectID": "slides/11-regularization.html#half-cauchy-distribution",
    "href": "slides/11-regularization.html#half-cauchy-distribution",
    "title": "Regularization",
    "section": "Half-Cauchy distribution",
    "text": "Half-Cauchy distribution\nA random variable \\(X \\sim \\mathcal C^+(\\mu,\\sigma)\\) follows a half-Cauchy distribution with location \\(\\mu\\) and scale \\(\\sigma &gt; 0\\) and has the following density,\n\\[f(X | \\mu, \\sigma) = \\frac{2}{\\pi \\sigma}\\frac{1}{1 + (X - \\mu)^2 / \\sigma^2},\\quad X \\geq \\mu\\]\n\nThe Half-Cauchy distribution with \\(\\mu = 0\\) is a useful prior for non-negative parameters that may be very large, as allowed by the very heavy tails of the Cauchy distribution."
  },
  {
    "objectID": "slides/11-regularization.html#half-cauchy-distribution-in-stan",
    "href": "slides/11-regularization.html#half-cauchy-distribution-in-stan",
    "title": "Regularization",
    "section": "Half-Cauchy distribution in Stan",
    "text": "Half-Cauchy distribution in Stan\nIn Stan, the half-Cauchy distribution can be specified by putting a constraint on the parameter definition.\n\nparameters {\n  real&lt;lower = 0&gt; lambda;\n}\nmodel {\n  lambda ~ cauchy(0, 1);\n}"
  },
  {
    "objectID": "slides/11-regularization.html#half-cauchy-distribution-1",
    "href": "slides/11-regularization.html#half-cauchy-distribution-1",
    "title": "Regularization",
    "section": "Half-Cauchy distribution",
    "text": "Half-Cauchy distribution"
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-prior-1",
    "href": "slides/11-regularization.html#horseshoe-prior-1",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\nThe horseshoe prior has two interesting features that make it particularly useful as a shrinkage prior for sparse problems.\n\nIt has flat, Cauchy-like tails that allow strong signals to remain large (that is, un-shrunk) a posteriori.\nIt has an infinitely tall spike at the origin that provides severe shrinkage for the zero elements of \\(\\boldsymbol{\\beta}\\).\n\nAs we will see, these are key elements that make the horseshoe an attractive choice for handling sparse vectors."
  },
  {
    "objectID": "slides/11-regularization.html#relation-to-other-shrinkage-priors",
    "href": "slides/11-regularization.html#relation-to-other-shrinkage-priors",
    "title": "Regularization",
    "section": "Relation to other shrinkage priors",
    "text": "Relation to other shrinkage priors\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\sim f(\\lambda_j)\n\\end{aligned}\\]\n\n\\(\\lambda_j = 1 / \\lambda\\), implies ridge regression.\n\\(f(\\lambda_j) = \\text{Exponential}(0.5)\\), implies lasso.\n\\(f(\\lambda_j) = \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\\), implies relevance vector machine.\n\\(f(\\lambda_j) = \\mathcal C^+(0,1)\\), implies horseshoe."
  },
  {
    "objectID": "slides/11-regularization.html#horsehoe-density",
    "href": "slides/11-regularization.html#horsehoe-density",
    "title": "Regularization",
    "section": "Horsehoe density",
    "text": "Horsehoe density\n\n\n\n\n\nFrom Carvalho 2009"
  },
  {
    "objectID": "slides/11-regularization.html#shrinkage-of-each-prior",
    "href": "slides/11-regularization.html#shrinkage-of-each-prior",
    "title": "Regularization",
    "section": "Shrinkage of each prior",
    "text": "Shrinkage of each prior\n\nDefine the posterior mean of \\(\\beta_j\\) as \\(\\bar{\\beta}_j\\) and the maximum likelihood estimator for \\(\\beta_j\\) as \\(\\hat{\\beta}_j\\).\nThe following relationship holds: \\(\\bar{\\beta}_j = (1 - \\kappa_j) \\hat{\\beta}_j\\),\n\n\\[\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}s_j^2\\lambda_j^2}.\\]\n\n\\(\\kappa_j\\) is called the shrinkage factor for \\(\\beta_j\\).\n\\(s_j^2 = \\mathbb{V}(x_j)\\) is the variance for each predictor."
  },
  {
    "objectID": "slides/11-regularization.html#standardization-of-predictors",
    "href": "slides/11-regularization.html#standardization-of-predictors",
    "title": "Regularization",
    "section": "Standardization of predictors",
    "text": "Standardization of predictors\n\nIn regularization problems, predictors are standardized (to mean zero and standard deviation one).\nThis means that \\(s_j = 1\\).\nShrinkage parameter:\n\n\\[\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}\\lambda_j^2}.\\]\n\n\\(\\kappa_j = 1\\), implies complete shrinkage.\n\\(\\kappa_j = 0\\), implies no shrinkage."
  },
  {
    "objectID": "slides/11-regularization.html#shrinkage-parameter",
    "href": "slides/11-regularization.html#shrinkage-parameter",
    "title": "Regularization",
    "section": "Shrinkage parameter",
    "text": "Shrinkage parameter\n\n\n\n\n\nFrom Carvalho 2009"
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-shrinkage-parameter",
    "href": "slides/11-regularization.html#horseshoe-shrinkage-parameter",
    "title": "Regularization",
    "section": "Horseshoe shrinkage parameter",
    "text": "Horseshoe shrinkage parameter\n\nChoosing \\(\\lambda_j ‚àº \\mathcal C^+(0, 1)\\) implies \\(\\kappa_j ‚àº \\text{Beta}(0.5, 0.5)\\), a density that is symmetric and unbounded at both 0 and 1.\nThis horseshoe-shaped shrinkage profile expects to see two things a priori:\n\nStrong signals (\\(\\kappa \\approx 0\\), no shrinkage), and\nZeros (\\(\\kappa \\approx 1\\), total shrinkage)."
  },
  {
    "objectID": "slides/11-regularization.html#similarity-to-spike-and-slab",
    "href": "slides/11-regularization.html#similarity-to-spike-and-slab",
    "title": "Regularization",
    "section": "Similarity to spike-and-slab",
    "text": "Similarity to spike-and-slab\n\nA horseshoe prior can be considered as a continuous approximation to the spike-and-slab prior.\n\nThe spike-and-slab places a discrete probability mass at exactly zero (the ‚Äúspike‚Äù) and a separate distribution around non-zero values (the ‚Äúslab‚Äù).\nThe horseshoe prior smoothly approximates this behavior with a very concentrated distribution near zero.\n\n\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/11-regularization.html#choosing-a-prior-for-tau",
    "href": "slides/11-regularization.html#choosing-a-prior-for-tau",
    "title": "Regularization",
    "section": "Choosing a prior for \\(\\tau\\)",
    "text": "Choosing a prior for \\(\\tau\\)\n\nCarvalho et al.¬†2009 suggest \\(\\tau \\sim \\mathcal C^+(0,1)\\).\nPolson and Scott 2011 recommend \\(\\tau | \\sigma \\sim \\mathcal C^+(0, \\sigma^2)\\).\nAnother prior comes from a quantity called the effective number of nonzero coefficients,\n\n\\[m_{eff} = \\sum_{j=1}^p (1 - \\kappa_j).\\]"
  },
  {
    "objectID": "slides/11-regularization.html#global-shrinkage-parameter-tau",
    "href": "slides/11-regularization.html#global-shrinkage-parameter-tau",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\nThe prior mean can be shown to be,\n\n\\[\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = \\frac{\\tau \\sigma^{-1} \\sqrt{n}}{1 + \\tau \\sigma^{-1} \\sqrt{n}}p.\\]\n\nSetting \\(\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = p_0\\) (prior guess for the number of non-zero coefficients) yields for \\(\\tau\\),\n\n\\[\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}.\\]"
  },
  {
    "objectID": "slides/11-regularization.html#global-shrinkage-parameter-tau-1",
    "href": "slides/11-regularization.html#global-shrinkage-parameter-tau-1",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/11-regularization.html#non-gaussian-observation-models",
    "href": "slides/11-regularization.html#non-gaussian-observation-models",
    "title": "Regularization",
    "section": "Non-Gaussian observation models",
    "text": "Non-Gaussian observation models\n\nThe reference value:\n\n\\[\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}.\\]\n\nThis framework can be applied to non-Gaussian observation data models using plug-in estimates values for \\(\\sigma\\).\n\nGaussian approximations to the likelihood.\nFor example: For logistic regression \\(\\sigma = 2\\)."
  },
  {
    "objectID": "slides/11-regularization.html#coding-up-the-model-in-stan",
    "href": "slides/11-regularization.html#coding-up-the-model-in-stan",
    "title": "Regularization",
    "section": "Coding up the model in Stan",
    "text": "Coding up the model in Stan\nHorseshoe model has the following form,\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\mathcal C^+(0, 1),\\\\\n\\tau &\\sim \\mathcal C^+(0, \\tau_0^2).\n\\end{aligned}\\]\nEfficient parameter transformation, \\[\\beta_j = \\tau \\lambda_j z_j, \\quad z_j \\stackrel{iid}{\\sim} N(0,1).\\]"
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-in-stan",
    "href": "slides/11-regularization.html#horseshoe-in-stan",
    "title": "Regularization",
    "section": "Horseshoe in Stan",
    "text": "Horseshoe in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0&gt; tau0;\n}\nparameters {\n  real alpha;     \n  real&lt;lower = 0&gt; sigma;\n  vector[p] z;\n  vector&lt;lower = 0&gt;[p] lambda;\n  real&lt;lower = 0&gt; tau;\n}\ntransformed parameters {\n  vector[p] beta;\n  beta = tau * lambda .* z;\n}\nmodel {\n  // likelihood\n  target += normal_lpdf(Y | alpha + X * beta, sigma);\n  // population parameters\n  target += normal_lpdf(alpha | 0, 3);\n  sigma ~ normal(0, 3);\n  // horseshoe prior\n  target += std_normal_lpdf(z);\n  lambda ~ cauchy(0, 1);\n  tau ~ cauchy(0, tau0);\n}"
  },
  {
    "objectID": "slides/11-regularization.html#prepare-for-next-class",
    "href": "slides/11-regularization.html#prepare-for-next-class",
    "title": "Regularization",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03, which was just assigned.\nComplete reading to prepare for next Tuesday‚Äôs lecture\nTuesday‚Äôs lecture: Classification"
  },
  {
    "objectID": "prepare/prepare-feb12.html",
    "href": "prepare/prepare-feb12.html",
    "title": "Prepare for February 12 lecture",
    "section": "",
    "text": "üìñ Read BDA3 Chapter 14.6 about regularization in Bayesian models\nüìñ Read Handling Sparsity via the Horseshoe by Carvalho, Polson, and Scott.\n‚úÖ Work on HW 02"
  },
  {
    "objectID": "slides/10-robust.html",
    "href": "slides/10-robust.html",
    "title": "Robust Regression",
    "section": "",
    "text": "‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine"
  },
  {
    "objectID": "slides/10-robust.html#review-of-last-lecture",
    "href": "slides/10-robust.html#review-of-last-lecture",
    "title": "Robust Regression",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Thursday, we started to branch out from linear regression.\nWe learned about approaches for nonlinear regression.\nToday we will address approaches for robust regression, which will generalize the assumption of homoskedasticity (and also the normality assumption)."
  },
  {
    "objectID": "slides/10-robust.html#a-motivating-research-question",
    "href": "slides/10-robust.html#a-motivating-research-question",
    "title": "Robust Regression",
    "section": "A motivating research question",
    "text": "A motivating research question\n\nIn today‚Äôs lecture, we will look at data on serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years.\n\nA detailed discussion of this data set may be found in Isaacs et al.¬†(1983) and Royston and Altman (1994).\n\nFor an example patient, we define \\(Y_i\\) as the serum concentration value and \\(X_i\\) as a child‚Äôs age, given in years."
  },
  {
    "objectID": "slides/10-robust.html#pulling-the-data",
    "href": "slides/10-robust.html#pulling-the-data",
    "title": "Robust Regression",
    "section": "Pulling the data",
    "text": "Pulling the data\n\nlibrary(Brq)\ndata(\"ImmunogG\")\nhead(ImmunogG)\n\n  IgG Age\n1 1.5 0.5\n2 2.7 0.5\n3 1.9 0.5\n4 4.0 0.5\n5 1.9 0.5\n6 4.4 0.5"
  },
  {
    "objectID": "slides/10-robust.html#visualizing-igg-data",
    "href": "slides/10-robust.html#visualizing-igg-data",
    "title": "Robust Regression",
    "section": "Visualizing IgG data",
    "text": "Visualizing IgG data"
  },
  {
    "objectID": "slides/10-robust.html#modeling-the-association-between-age-and-igg",
    "href": "slides/10-robust.html#modeling-the-association-between-age-and-igg",
    "title": "Robust Regression",
    "section": "Modeling the association between age and IgG",
    "text": "Modeling the association between age and IgG\n\nLinear regression can be written as follows for \\(i = 1,\\ldots,n\\),\n\n\\[Y_i = \\alpha + \\beta X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2).\\]\n\n\\(\\beta\\) represent the change in IgG serum concentration with a one year increase in age."
  },
  {
    "objectID": "slides/10-robust.html#linear-regression-assumptions",
    "href": "slides/10-robust.html#linear-regression-assumptions",
    "title": "Robust Regression",
    "section": "Linear regression assumptions",
    "text": "Linear regression assumptions\n\\[\\begin{aligned}\nY_i &= \\alpha + \\beta X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(Y_i\\) are independent observations (independence).\n\\(Y_i\\) is linearly related to \\(X_i\\) (linearity).\n\\(\\epsilon_i = Y_i - \\mu_i\\) is normally distributed (normality).\n\\(\\epsilon_i\\) has constant variance across \\(X_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/10-robust.html#assessing-assumptions",
    "href": "slides/10-robust.html#assessing-assumptions",
    "title": "Robust Regression",
    "section": "Assessing assumptions",
    "text": "Assessing assumptions"
  },
  {
    "objectID": "slides/10-robust.html#robust-regression",
    "href": "slides/10-robust.html#robust-regression",
    "title": "Robust Regression",
    "section": "Robust regression",
    "text": "Robust regression\n\nToday we will learn about regression techniques that are robust to the assumptions of linear regression.\nWe will introduce the idea of robust regression by exploring ways to generalize the homoskedastic variance assumption in linear regression.\nWe will touch on heteroskedasticity, heavy-tailed distributions, and median regression (more generally quantile regression)."
  },
  {
    "objectID": "slides/10-robust.html#why-is-robust-regression-not-more-comomon",
    "href": "slides/10-robust.html#why-is-robust-regression-not-more-comomon",
    "title": "Robust Regression",
    "section": "Why is robust regression not more comomon?",
    "text": "Why is robust regression not more comomon?\n\nDespite their desirable properties, robust methods are not widely used. Why?\n\nHistorically computationally complex.\nNot available in statistical software packages.\n\nBayesian modeling using Stan alleviates these bottlenecks!"
  },
  {
    "objectID": "slides/10-robust.html#heteroskedasticity",
    "href": "slides/10-robust.html#heteroskedasticity",
    "title": "Robust Regression",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nHeteroskedasticity is the violation of the assumption of constant variance.\nHow can we handle this?\nIn OLS, there are approaches like heteroskedastic consistent errors, but this is not a generative model.\nIn the Bayesian framework, we generally like to write down generative models."
  },
  {
    "objectID": "slides/10-robust.html#modeling-the-scale-with-covariates",
    "href": "slides/10-robust.html#modeling-the-scale-with-covariates",
    "title": "Robust Regression",
    "section": "Modeling the scale with covariates",
    "text": "Modeling the scale with covariates\n\nOne option is to allow the sale to be modeled as a function of covariates.\nIt is common to model the log-transformation of the scale or variance to transform it to \\(\\mathbb{R}\\),\n\n\\[\\log \\tau_i = \\mathbf{z}_i \\boldsymbol{\\gamma},\\]\nwhere \\(\\mathbf{z}_i = (z_{i1},\\ldots,z_{ip})\\) are a \\(p\\)-dimensional vector of covariates and \\(\\boldsymbol{\\gamma}\\) are parameters that regress the covariates onto the log standard deviation.\n\nOther options include: \\(\\log \\tau_i = \\mathbf{z}_i \\boldsymbol{\\gamma} + \\nu_i,\\quad \\nu_i \\sim N(0, \\sigma^2)\\)\nOther options include: \\(\\log \\tau_i = f(\\mu_i)\\)\nAny plausible generative model can be specified!"
  },
  {
    "objectID": "slides/10-robust.html#modeling-the-scale-with-covariates-1",
    "href": "slides/10-robust.html#modeling-the-scale-with-covariates-1",
    "title": "Robust Regression",
    "section": "Modeling the scale with covariates",
    "text": "Modeling the scale with covariates\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1&gt; q;\n  vector[n] Y;\n  matrix[n, p] X;\n  matrix[n, q] Z;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  vector[q] gamma;\n}\ntransformed parameters {\n  vector[n] tau = exp(Z * gamma);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n}"
  },
  {
    "objectID": "slides/10-robust.html#heteroskedastic-variance",
    "href": "slides/10-robust.html#heteroskedastic-variance",
    "title": "Robust Regression",
    "section": "Heteroskedastic variance",
    "text": "Heteroskedastic variance\n\nWe can write the regression model using an observation specific variance, \\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\tau_i^2).\\]\nOne way of writing the variance is: \\(\\tau_i^2 = \\sigma^2 \\lambda_i\\).\n\n\\(\\sigma^2\\) is a global scale parameter.\n\\(\\lambda_i\\) is an observation specific scale parameter.\n\nIn the Bayesian framework, we must place a prior on \\(\\lambda_i\\)."
  },
  {
    "objectID": "slides/10-robust.html#a-prior-to-induce-a-heavy-tail",
    "href": "slides/10-robust.html#a-prior-to-induce-a-heavy-tail",
    "title": "Robust Regression",
    "section": "A prior to induce a heavy-tail",
    "text": "A prior to induce a heavy-tail\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\\]"
  },
  {
    "objectID": "slides/10-robust.html#a-prior-to-induce-a-heavy-tail-1",
    "href": "slides/10-robust.html#a-prior-to-induce-a-heavy-tail-1",
    "title": "Robust Regression",
    "section": "A prior to induce a heavy-tail",
    "text": "A prior to induce a heavy-tail\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\\]\n\nUnder this prior, the marginal likelihood for \\(Y_i\\) is equivalent to a Student-t distribution,\n\n\\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} t_{\\nu}\\left(0, \\sigma\\right).\\]"
  },
  {
    "objectID": "slides/10-robust.html#understanding-the-equivalence",
    "href": "slides/10-robust.html#understanding-the-equivalence",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\nHeteroskedastic variances assumption is equivalent to assuming a heavy-tailed distribution.\n\n\\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} t_{\\nu}\\left(0, \\sigma\\right).\\]\n\\[\\iff\\]\n\\[\\begin{aligned}\nY_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\nNote that since the number of \\(\\lambda_i\\) parameters is equal to the number of observations, this model will not have a proper posterior distribution without a proper prior distribution."
  },
  {
    "objectID": "slides/10-robust.html#understanding-the-equivalence-1",
    "href": "slides/10-robust.html#understanding-the-equivalence-1",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\\[\\begin{aligned}\nf(Y_i) &= \\int_0^{\\infty} f(Y_i , \\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} N(Y_i ; \\mu_i, \\sigma^2 \\lambda_i) \\text{Inverse-Gamma}\\left(\\lambda_i ; \\frac{\\nu}{2},\\frac{\\nu}{2}\\right) d\\lambda_i\\\\\n&= t_{\\nu}\\left(\\mu_i,\\sigma\\right).\n\\end{aligned}\\]\n\nThe marginal likelihood can be viewed as a mixture of a Gaussian likelihood with an Inverse-Gamma scale parameter."
  },
  {
    "objectID": "slides/10-robust.html#understanding-the-equivalence-2",
    "href": "slides/10-robust.html#understanding-the-equivalence-2",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\nA random variable \\(T_i \\stackrel{iid}{\\sim} t_{\\nu}\\) can be written as a function of Gaussian and \\(\\chi^2\\) random variables, \\[\\begin{aligned}\nT_i &= \\frac{Z_i}{\\sqrt{\\frac{W_i}{\\nu}}},\\quad Z_i \\stackrel{iid}{\\sim} N(0,1), \\quad W_i \\stackrel{iid}{\\sim}\\chi^2_{\\nu}\\\\\n&= \\frac{Z_i}{\\sqrt{\\frac{1}{\\nu V_i}}},\\quad V_i \\stackrel{iid}{\\sim} \\text{Inv-}\\chi^2_{\\nu},\\quad V_i=W_i^{-1}\\\\\n&= \\sqrt{\\nu V_i} Z_i,\\quad \\lambda_i = \\nu V_i\\\\\n&= \\sqrt{\\lambda_i} Z_i, \\quad \\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\n\\end{aligned}\\]\n\nWe then have: \\(Y_i = \\mu_i + \\sigma T_i \\sim t_{\\nu}(\\mu_i, \\sigma).\\)"
  },
  {
    "objectID": "slides/10-robust.html#student-t-in-stan",
    "href": "slides/10-robust.html#student-t-in-stan",
    "title": "Robust Regression",
    "section": "Student-t in Stan",
    "text": "Student-t in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; nu;\n}\nmodel {\n  target += student_t_lpdf(Y | nu, alpha + X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/10-robust.html#student-t-in-stan-mixture",
    "href": "slides/10-robust.html#student-t-in-stan-mixture",
    "title": "Robust Regression",
    "section": "Student-t in Stan: mixture",
    "text": "Student-t in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n  real&lt;lower = 0&gt; nu;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n  target += inv_gamma_lpdf(lambda | 0.5 * nu, 0.5 * nu);\n}"
  },
  {
    "objectID": "slides/10-robust.html#why-heavy-tailed-distributions",
    "href": "slides/10-robust.html#why-heavy-tailed-distributions",
    "title": "Robust Regression",
    "section": "Why heavy-tailed distributions?",
    "text": "Why heavy-tailed distributions?\n\nReplacing the normal distribution with a distribution with heavy-tails (e.g., Student-t, Laplace) is a common approach to robust regression.\nRobust regression refers to regression methods which are less sensitive to outliers or small sample sizes.\nLinear regression, including Bayesian regression with normally distributed errors is sensitive to outliers, because the normal distribution has narrow tail probabilities.\nOur heteroskedastic model that we just explored is only one example of a robust regression model."
  },
  {
    "objectID": "slides/10-robust.html#vizualizing-heavy-tail-distributions",
    "href": "slides/10-robust.html#vizualizing-heavy-tail-distributions",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/10-robust.html#vizualizing-heavy-tail-distributions-1",
    "href": "slides/10-robust.html#vizualizing-heavy-tail-distributions-1",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/10-robust.html#vizualizing-heavy-tail-distributions-2",
    "href": "slides/10-robust.html#vizualizing-heavy-tail-distributions-2",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/10-robust.html#another-example-of-robust-regression",
    "href": "slides/10-robust.html#another-example-of-robust-regression",
    "title": "Robust Regression",
    "section": "Another example of robust regression",
    "text": "Another example of robust regression\n\nLet‚Äôs revisit our general heteroskedastic regression, \\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\sigma^2 \\lambda_i).\\]\nWe can induce another form of robust regression using the following prior for \\(\\lambda_i\\), \\(\\lambda_i \\sim \\text{Exponential}(1/2)\\)."
  },
  {
    "objectID": "slides/10-robust.html#another-example-of-robust-regression-1",
    "href": "slides/10-robust.html#another-example-of-robust-regression-1",
    "title": "Robust Regression",
    "section": "Another example of robust regression",
    "text": "Another example of robust regression\n\nLet‚Äôs revisit our general heteroskedastic regression, \\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\sigma^2 \\lambda_i).\\]\nWe can induce another form of robust regression using the following prior for \\(\\lambda_i\\), \\(\\lambda_i \\sim \\text{Exponential}(1/2)\\).\nUnder this prior, the induced marginal model is, \\[Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\stackrel{iid}{\\sim} \\text{Laplace}(\\mu = 0, \\sigma).\\]\nThis has a really nice interpretation!"
  },
  {
    "objectID": "slides/10-robust.html#laplace-distribution",
    "href": "slides/10-robust.html#laplace-distribution",
    "title": "Robust Regression",
    "section": "Laplace distribution",
    "text": "Laplace distribution\nSuppse a variable \\(Y_i\\) follows a Laplace (or double exponential) distribution, then the pdf is given by,\n\\[f(Y_i | \\mu, \\sigma) = \\frac{1}{2\\sigma} \\exp\\left\\{-\\frac{|Y_i - \\mu|}{\\sigma}\\right\\}\\]\n\n\\(\\mathbb{E}[Y_i] = \\mu\\)\n\\(\\mathbb{V}(Y_i) = 2 \\sigma^2\\)\nUnder the Laplace likelihood, estimation of \\(\\mu\\) is equivalent to estimating the population median of \\(Y_i\\)."
  },
  {
    "objectID": "slides/10-robust.html#median-regression-using-laplace",
    "href": "slides/10-robust.html#median-regression-using-laplace",
    "title": "Robust Regression",
    "section": "Median regression using Laplace",
    "text": "Median regression using Laplace\nLeast absolute deviation (LAD) regression minimizes the following objective function,\n\\[\\hat{{\\alpha}}_{\\text{LAD}},\\hat{\\boldsymbol{\\beta}}_{\\text{LAD}} = \\arg \\min_{\\alpha,\\boldsymbol{\\beta}} \\sum_{i=1}^n |Y_i - \\mu_i|, \\quad \\mu_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta}.\\]\nThe Bayesian analog is the Laplace distribution,\n\\[f(\\mathbf{Y} | \\alpha, \\boldsymbol{\\beta}, \\sigma) = \\left(\\frac{1}{2\\sigma}\\right)^n \\exp\\left\\{-\\sum_{i=1}^n\\frac{|Y_i - \\mu_i|}{\\sigma}\\right\\}.\\]"
  },
  {
    "objectID": "slides/10-robust.html#median-regression-using-laplace-1",
    "href": "slides/10-robust.html#median-regression-using-laplace-1",
    "title": "Robust Regression",
    "section": "Median regression using Laplace",
    "text": "Median regression using Laplace\n\nThe Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is \\(|x‚àí\\mu|\\), so minimizing the likelihood will also minimize the least absolute distances.\nLaplace distribution is also known as the double-exponential distribution (symmetric exponential distributions around \\(\\mu\\) with scale \\(\\sigma\\)).\nThus, a linear regression with Laplace errors is analogous to a median regression,\nWhy is median regression considered more robust than regression of the mean?"
  },
  {
    "objectID": "slides/10-robust.html#laplace-regression-in-stan",
    "href": "slides/10-robust.html#laplace-regression-in-stan",
    "title": "Robust Regression",
    "section": "Laplace regression in Stan",
    "text": "Laplace regression in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += double_exponential_lpdf(Y | alpha + X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/10-robust.html#laplace-regression-in-stan-mixture",
    "href": "slides/10-robust.html#laplace-regression-in-stan-mixture",
    "title": "Robust Regression",
    "section": "Laplace regression in Stan: mixture",
    "text": "Laplace regression in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n  target += exponential_lpdf(lambda | 0.5);\n}"
  },
  {
    "objectID": "slides/10-robust.html#returning-to-igg",
    "href": "slides/10-robust.html#returning-to-igg",
    "title": "Robust Regression",
    "section": "Returning to IgG",
    "text": "Returning to IgG"
  },
  {
    "objectID": "slides/10-robust.html#posterior-of-beta",
    "href": "slides/10-robust.html#posterior-of-beta",
    "title": "Robust Regression",
    "section": "Posterior of \\(\\beta\\)",
    "text": "Posterior of \\(\\beta\\)\n\n\n\n\n\nModel\nMean\nLower\nUpper\n\n\n\n\nLaplace\n0.73\n0.56\n0.89\n\n\nStudent-t\n0.69\n0.55\n0.82\n\n\nGaussian\n0.69\n0.56\n0.83\n\n\nGaussian with Covariates in Variance\n0.76\n0.62\n0.89"
  },
  {
    "objectID": "slides/10-robust.html#model-comparison",
    "href": "slides/10-robust.html#model-comparison",
    "title": "Robust Regression",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\n\n\n\n\nelpd_diff\nelpd_loo\nlooic\n\n\n\n\nStudent-t\n0.00\n-624.42\n1248.84\n\n\nGaussian\n-2.01\n-626.43\n1252.86\n\n\nGaussian with Covariates in Variance\n-6.09\n-630.51\n1261.02\n\n\nLaplace\n-13.10\n-637.52\n1275.05"
  },
  {
    "objectID": "slides/10-robust.html#examining-the-student-t-model-fit",
    "href": "slides/10-robust.html#examining-the-student-t-model-fit",
    "title": "Robust Regression",
    "section": "Examining the Student-t model fit",
    "text": "Examining the Student-t model fit\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd 2.5% 97.5% n_eff Rhat\nalpha   3.32    0.00 0.21 2.91  3.74  2084    1\nbeta[1] 0.69    0.00 0.07 0.55  0.82  2122    1\nsigma   1.72    0.00 0.10 1.53  1.91  2744    1\nnu      7.80    0.05 2.35 4.40 13.29  2617    1\n\nSamples were drawn using NUTS(diag_e) at Sun Feb  9 14:54:53 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/10-robust.html#examining-the-student-t-model-fit-1",
    "href": "slides/10-robust.html#examining-the-student-t-model-fit-1",
    "title": "Robust Regression",
    "section": "Examining the Student-t model fit",
    "text": "Examining the Student-t model fit"
  },
  {
    "objectID": "slides/10-robust.html#examining-the-student-t-model-fit-2",
    "href": "slides/10-robust.html#examining-the-student-t-model-fit-2",
    "title": "Robust Regression",
    "section": "Examining the Student-t model fit",
    "text": "Examining the Student-t model fit"
  },
  {
    "objectID": "slides/10-robust.html#summary-of-robust-regression",
    "href": "slides/10-robust.html#summary-of-robust-regression",
    "title": "Robust Regression",
    "section": "Summary of robust regression",
    "text": "Summary of robust regression\n\nRobust regression techniques can be used when the assumptions of constant variance and/or normality of the residuals do not hold.\nHeteroskedastic variance can viewed as inducing extreme value distributions.\nExtreme value regression using Student-t and Laplace distributions are robust to outliers.\nLaplace regression is equivalent to median regression."
  },
  {
    "objectID": "slides/10-robust.html#prepare-for-next-class",
    "href": "slides/10-robust.html#prepare-for-next-class",
    "title": "Robust Regression",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02, which is due before next class.\nComplete reading to prepare for next Thursday‚Äôs lecture\nThursday‚Äôs lecture: Regularization"
  },
  {
    "objectID": "prepare/prepare-feb17.html",
    "href": "prepare/prepare-feb17.html",
    "title": "Prepare for February 17 lecture",
    "section": "",
    "text": "üìñ Read AE 06: Classification. This AE contains an introduction to an EHR dataset we will use for a few assignments in the course. Please read the introduction of the data through the exercises and be ready to work with the data on Tuesday.\nüìñ Review Stan documentation on logistic and probit regression.\n‚úÖ Work on HW 03"
  },
  {
    "objectID": "slides/12-classification.html",
    "href": "slides/12-classification.html",
    "title": "Classification",
    "section": "",
    "text": "‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nLoaded lars 1.3\n\n\n\nAttaching package: 'LaplacesDemon'\n\n\nThe following objects are masked from 'package:mvtnorm':\n\n    dmvt, logdet, rmvt\n\n\nThe following objects are masked from 'package:lubridate':\n\n    dst, interval\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\n\nnimble version 1.2.1 is loaded.\nFor more information on NIMBLE and a User Manual,\nplease visit https://R-nimble.org.\n\nNote for advanced users who have written their own MCMC samplers:\n  As of version 0.13.0, NIMBLE's protocol for handling posterior\n  predictive nodes has changed in a way that could affect user-defined\n  samplers in some situations. Please see Section 15.5.1 of the User Manual.\n\n\nAttaching package: 'nimble'\n\n\nThe following objects are masked from 'package:LaplacesDemon':\n\n    cloglog, dcat, dinvgamma, is.model, logdet, logit, rcat, rinvgamma\n\n\nThe following object is masked from 'package:mvtnorm':\n\n    logdet\n\n\nThe following object is masked from 'package:stats':\n\n    simulate\n\n\nThe following object is masked from 'package:base':\n\n    declare"
  },
  {
    "objectID": "slides/12-classification.html#review-of-last-lecture",
    "href": "slides/12-classification.html#review-of-last-lecture",
    "title": "Classification",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nLast week, we learned about Bayesian approaches to robust regression and regularization.\n\nGlobal-local shrinkage priors.\n\nThis week, we will focus on classification models.\n\nToday: Binary classification (logistic regression).\nThursday: Multiclass classification (multinomial, ordinal regression)."
  },
  {
    "objectID": "slides/12-classification.html#models-for-binary-outcomes",
    "href": "slides/12-classification.html#models-for-binary-outcomes",
    "title": "Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nBernoulli random variable: Used for binary outcomes (success/failure), e.g., whether a patient responds to a treatment (yes/no).\nBinomial random variable: Used when there are multiple trials (e.g., 10 patients), and you want to model the number of successes (e.g., how many out of 10 patients experience a treatment response)."
  },
  {
    "objectID": "slides/12-classification.html#bernoulli-random-variable-example",
    "href": "slides/12-classification.html#bernoulli-random-variable-example",
    "title": "Classification",
    "section": "Bernoulli random variable example",
    "text": "Bernoulli random variable example\nA Bernoulli random variable represents a random variable with two possible outcomes: 0 or 1.\nScenario:\nImagine a medical study on a new drug for hypertension (high blood pressure). You want to model whether a patient responds positively to the treatment.\n\nSuccess (1): The patient‚Äôs blood pressure decreases significantly (e.g., more than 10% reduction).\nFailure (0): The patient does not experience a significant decrease in blood pressure."
  },
  {
    "objectID": "slides/12-classification.html#binomial-random-variable-example",
    "href": "slides/12-classification.html#binomial-random-variable-example",
    "title": "Classification",
    "section": "Binomial random variable example",
    "text": "Binomial random variable example\nA Binomial random variable represents the number of successes in a fixed number of independent Bernoulli trials.\nScenario:\nA clinical trial is conducted where 10 patients are given a new drug for diabetes. You want to model how many of these 10 patients experience a significant reduction in their blood sugar levels (e.g., a decrease by at least 20%).\n\nEach patient‚Äôs outcome is a Bernoulli random variable: success (1) if their blood sugar level decreases, failure (0) if it does not.\nThe total number of successes (patients who experience a reduction) is modeled as a Binomial random variable."
  },
  {
    "objectID": "slides/12-classification.html#models-for-binary-outcomes-1",
    "href": "slides/12-classification.html#models-for-binary-outcomes-1",
    "title": "Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nSuppose \\(Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\) for \\(i = 1,\\ldots,n\\). The pmf is,\n\n\\[f(Y_i) = P(Y_i = y) = \\pi_i^y (1 - \\pi_i)^{1 - y}, \\quad y \\in\\{0,1\\}.\\]\n\nWe only need to specify \\(\\pi_i = P(Y_i = 1)\\).\nOne strategy might be to simply fit a linear regression model,\n\n\\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\quad\\epsilon_i \\sim N(0, \\sigma^2).\\]\n\nWe can set \\(P(Y_i = 1) = \\hat{Y}_i\\)."
  },
  {
    "objectID": "slides/12-classification.html#primary-biliary-cirrhosis",
    "href": "slides/12-classification.html#primary-biliary-cirrhosis",
    "title": "Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nThe Mayo Clinic conducted a trial for primary biliary cirrhosis, comparing the drug D-penicillamine vs.¬†placebo. Patients were followed for a specified duration, and their status at the end of follow-up (whether they died) was recorded.\nResearchers are interested in predicting whether a patient died based on the following variables:\n\nascites: whether the patient had ascites (1 = yes, 0 = no)\nbilirubin: serum bilirubin in mg/dL\nstage: histologic stage of disease (ordinal categorical variable with stages 1, 2, 3, and 4)"
  },
  {
    "objectID": "slides/12-classification.html#what-can-go-wrong",
    "href": "slides/12-classification.html#what-can-go-wrong",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nSuppose we fit the following model:\n\n\\[\\begin{aligned}\nY_i &= \\alpha + \\beta_1(ascites)_i + \\beta_2(bilirubin)_i\\\\\n&\\quad+\\beta_3(stage = 2)_i + \\beta_4(stage = 3)_i\\\\\n&\\quad+\\beta_5(stage = 4)_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\n\\end{aligned}\\]\n\nWhat can go wrong?"
  },
  {
    "objectID": "slides/12-classification.html#what-can-go-wrong-1",
    "href": "slides/12-classification.html#what-can-go-wrong-1",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "slides/12-classification.html#what-can-go-wrong-2",
    "href": "slides/12-classification.html#what-can-go-wrong-2",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nAdditionally, as a probability, \\(P(Y_i = 1)\\) must be in the interval [0, 1], but there is nothing in the model that enforces this constraint, so that you could be estimating probabilities that are negative or that are greater than 1!"
  },
  {
    "objectID": "slides/12-classification.html#from-probabilities-to-log-odds",
    "href": "slides/12-classification.html#from-probabilities-to-log-odds",
    "title": "Classification",
    "section": "From probabilities to log-odds",
    "text": "From probabilities to log-odds\n\nSuppose the probability of an event is \\(\\pi\\).\nThen the odds that the event occurs is \\(\\frac{\\pi}{1 - \\pi}\\).\nTaking the (natural) log of the odds, we have the logit of \\(\\pi\\): the log-odds:\n\n\\[\\text{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).\\]\n\nNote that although \\(\\pi\\) is constrained to lie between 0 and 1, the logit of \\(\\pi\\) is unconstrained - it can be anything from \\(-\\infty\\) to \\(\\infty\\)."
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression-model",
    "href": "slides/12-classification.html#logistic-regression-model",
    "title": "Classification",
    "section": "Logistic regression model",
    "text": "Logistic regression model\n\nLet‚Äôs create a model for the logit of \\(\\pi\\): \\(\\text{logit}(\\pi_i)= \\eta_i\\), where \\(\\eta_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\\)\nThis is a linear model for a transformation of the outcome of interest, and is also equivalent to,\n\n\\[\\pi_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\text{expit}(\\eta_i).\\]\n\nThe expression on the right is called a logistic function and cannot yield a value that is negative or a value that is &gt;1. Fitting a model of this form is known as logistic regression."
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression",
    "href": "slides/12-classification.html#logistic-regression",
    "title": "Classification",
    "section": "Logistic regression",
    "text": "Logistic regression\n\\[\\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\eta_i\\]\n\nNegative logits represent probabilities less than one-half.\n\n\\(\\eta_i &lt; 0 \\implies \\pi_i &lt; 0.5\\).\n\nPositive logits represent probabilities greater than one-half.\n\n\\(\\eta_i &gt; 0 \\implies \\pi_i &gt; 0.5\\)."
  },
  {
    "objectID": "slides/12-classification.html#interpreting-parameters-in-logistic-regression",
    "href": "slides/12-classification.html#interpreting-parameters-in-logistic-regression",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\nTypically we interpret functions of parameters in logistic regression rather than the parameters themselves.\nFor the simple model: \\(\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\beta X_{i},\\) we note that the probability that \\(Y_i = 1\\) when \\(X_i = 0\\) is\n\\[P(Y_i = 1 | X_{i} = 0) = \\frac{\\exp(\\alpha)}{1 + \\exp(\\alpha)}.\\]"
  },
  {
    "objectID": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-1",
    "href": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-1",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nSuppose that \\(X\\) is a binary (0/1) variable (e.g., \\(X = 1\\) for males and 0 for non-males).\n\nIn this case, we interpret \\(\\exp(\\beta)\\) as the odds ratio (OR) of the response for the two possible levels of \\(X\\).\nFor \\(X\\) on other scales, \\(\\exp(\\beta)\\) is interpreted as the odds ratio of the response comparing two values of \\(X\\) one unit apart.\n\nWhy?"
  },
  {
    "objectID": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-2",
    "href": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-2",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nThe log odds of response for \\(X = 1\\) is given by \\(\\alpha + \\beta\\), and the log odds of response for \\(X = 0\\) is \\(\\alpha\\).\nSo the odds ratio of response comparing \\(X = 1\\) to \\(X = 0\\) is given by \\(\\frac{\\exp(\\alpha + \\beta)}{\\exp(\\alpha)} = \\exp(\\beta)\\).\nIn a multivariable logistic regression model with more than one predictor, this OR is interpreted conditionally on values of other variables (i.e., controlling for them)."
  },
  {
    "objectID": "slides/12-classification.html#bayesian-logistic-regression",
    "href": "slides/12-classification.html#bayesian-logistic-regression",
    "title": "Classification",
    "section": "Bayesian logistic regression",
    "text": "Bayesian logistic regression\n\nWe start with observations \\(Y_i \\in \\{0,1\\}\\) for \\(i = 1,\\ldots,n\\), where \\(Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\), \\(\\pi_i = P(Y_i = 1)\\).\nThe log-odds are modeled as \\(\\text{logit}(\\pi_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} = \\eta_i\\).\nTo complete the Bayesian model specification, we must place priors on \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\).\n\nAll priors we have discussed up-to-this point apply!\n\nHistorically, this was a difficult model to fit, but can be easily implemented in Stan."
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression-in-stan",
    "href": "slides/12-classification.html#logistic-regression-in-stan",
    "title": "Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\n// Saved in logistic_regression.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int Y[n];                              // Y is now type int\n  matrix[n, p] X;\n}\ntransformed data {\n  matrix[n, p] X_centered;               // We are only centering X!\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n}\nmodel {\n  target += bernoulli_logit_lpmf(Y | alpha + X_centered * beta); // bernoulli likelihood parameterized in logits\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n}\ngenerated quantities {\n  real pi_average = exp(alpha) / (1 + exp(alpha));\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = bernoulli_logit_rng(alpha + X_centered[i, ] * beta);\n    log_lik[i] = bernoulli_logit_lpmf(Y[i] | alpha + X_centered[i, ] * beta);\n  }\n}\n\nbernoulli_logit_lpmf"
  },
  {
    "objectID": "slides/12-classification.html#primary-biliary-cirrhosis-1",
    "href": "slides/12-classification.html#primary-biliary-cirrhosis-1",
    "title": "Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nhead(pbc)\n\n  id trt      age sex ascites hepato spiders edema bili chol albumin copper\n1  1   1 58.76523   f       1      1       1   1.0 14.5  261    2.60    156\n2  2   1 56.44627   f       0      1       1   0.0  1.1  302    4.14     54\n3  3   1 70.07255   m       0      0       0   0.5  1.4  176    3.48    210\n4  4   1 54.74059   f       0      1       1   0.5  1.8  244    2.54     64\n5  5   2 38.10541   f       0      1       1   0.0  3.4  279    3.53    143\n6  7   2 55.53457   f       0      1       0   0.0  1.0  322    4.09     52\n  alk.phos    ast trig platelet protime stage outcome\n1   1718.0 137.95  172      190    12.2     4       1\n2   7394.8 113.52   88      221    10.6     3       0\n3    516.0  96.10   55      151    12.0     4       1\n4   6121.8  60.63   92      183    10.3     4       1\n5    671.0 113.15   72      136    10.9     3       1\n6    824.0  60.45  213      204     9.7     3       0"
  },
  {
    "objectID": "slides/12-classification.html#prepare-data-for-stan",
    "href": "slides/12-classification.html#prepare-data-for-stan",
    "title": "Classification",
    "section": "Prepare data for Stan",
    "text": "Prepare data for Stan\n\nX &lt;- model.matrix(object = ~ ascites + bili + as.factor(stage), data = pbc)[, -1]\nY &lt;- pbc$outcome\nstan_data &lt;- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X)\nhead(X)\n\n  ascites bili as.factor(stage)2 as.factor(stage)3 as.factor(stage)4\n1       1 14.5                 0                 0                 1\n2       0  1.1                 0                 1                 0\n3       0  1.4                 0                 0                 1\n4       0  1.8                 0                 0                 1\n5       0  3.4                 0                 1                 0\n6       0  1.0                 0                 1                 0"
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression-in-stan-1",
    "href": "slides/12-classification.html#logistic-regression-in-stan-1",
    "title": "Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\nlibrary(rstan)\ncompiled_model &lt;- stan_model(file = \"logistic_regression.stan\")\nfit &lt;- sampling(compiled_model, data = stan_data)\nprint(fit, pars = c(\"alpha\", \"beta\", \"pi_average\"), probs = c(0.025, 0.5, 0.975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n           mean se_mean   sd  2.5%  50% 97.5% n_eff Rhat\nalpha      0.21    0.00 0.19 -0.14 0.20  0.60  1819 1.00\nbeta[1]    2.24    0.03 1.32  0.12 2.12  5.30  2328 1.00\nbeta[2]    0.38    0.00 0.08  0.24 0.38  0.55  2027 1.00\nbeta[3]    1.79    0.04 1.31 -0.43 1.65  4.86  1097 1.01\nbeta[4]    2.26    0.04 1.30  0.10 2.14  5.27  1073 1.01\nbeta[5]    2.69    0.04 1.31  0.52 2.54  5.74  1069 1.01\npi_average 0.55    0.00 0.05  0.47 0.55  0.65  1826 1.00\n\nSamples were drawn using NUTS(diag_e) at Mon Feb 17 09:17:32 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/12-classification.html#convergence-diagnostics",
    "href": "slides/12-classification.html#convergence-diagnostics",
    "title": "Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/12-classification.html#convergence-diagnostics-1",
    "href": "slides/12-classification.html#convergence-diagnostics-1",
    "title": "Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/12-classification.html#back-to-the-pbc-data",
    "href": "slides/12-classification.html#back-to-the-pbc-data",
    "title": "Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nFitting a logistic regression model, we obtain\n\n\n\n\n\n\n\nvariable\nmean\nsd\n2.5%\n97.5%\n\n\n\n\nalpha\nintercept\n0.21\n0.00\n-0.14\n0.60\n\n\nbeta[1]\nascites\n2.24\n0.03\n0.12\n5.30\n\n\nbeta[2]\nbilirubin\n0.38\n0.00\n0.24\n0.55\n\n\nbeta[3]\nstage == 2\n1.79\n0.04\n-0.43\n4.86\n\n\nbeta[4]\nstage == 3\n2.26\n0.04\n0.10\n5.27\n\n\nbeta[5]\nstage == 4\n2.69\n0.04\n0.52\n5.74\n\n\n\n\n\n\nHow might we interpret these coefficients as odds ratios?"
  },
  {
    "objectID": "slides/12-classification.html#back-to-the-pbc-data-1",
    "href": "slides/12-classification.html#back-to-the-pbc-data-1",
    "title": "Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nRemember, we are interested in the probability that a patient died during follow-up (a ‚Äúsuccess‚Äù). We are predicting the log-odds of this event happening.\n\nThe posterior mean for ascites was 2.24. Thus, the odds ratio for dying is \\(\\exp(2.24) \\approx 9.40\\). That is, patients with ascites have 9 times the odds of dying compared to patients that do not, holding all other variables constant.\nThe posterior mean for bilirubin was 0.38. Thus, the odds ratio for dying for a patient with 1 additional mg/dL serum bilirubin compared to another is \\(\\exp(0.38) \\approx 1.46\\), holding all other variables constant.\nThe baseline stage was 1. The posterior mean for stage 3 was 2.26. Thus, patients in stage 3 have approximately 9.58 times the odds of dying compared to patients that do not, holding all other variables constant."
  },
  {
    "objectID": "slides/12-classification.html#predicted-probabilities",
    "href": "slides/12-classification.html#predicted-probabilities",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nThere is a one-to-one relationship between \\(\\pi\\) and \\(\\text{logit}(\\pi)\\). So, if we predict \\(\\text{logit}(\\pi)\\), we can ‚Äúback-transform‚Äù to get back to a predicted probability.\n\n\n// stored in logistic_regression_new.stan\ndata {\n  row_vector[p] X_new;\n}\ngenerated quantities {\n  real eta_new = (alpha + (X_new - X_bar) * beta);\n  real pi_new = inv_logit(eta_new); // expit function\n  real Y_new = bernoulli_logit_rng(eta_new); // posterior predictive distribution\n}"
  },
  {
    "objectID": "slides/12-classification.html#predicted-probabilities-1",
    "href": "slides/12-classification.html#predicted-probabilities-1",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nFor instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\ncompiled_model &lt;- stan_model(file = \"logistic_regression_new.stan\")\nstan_data &lt;- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X,\n                  X_new = c(0, 5, 1, 0, 0))\nfit &lt;- sampling(compiled_model, data = stan_data)"
  },
  {
    "objectID": "slides/12-classification.html#predicted-probabilities-2",
    "href": "slides/12-classification.html#predicted-probabilities-2",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nFor instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd  2.5%  50% 97.5% n_eff Rhat\neta_new 0.28    0.01 0.40 -0.48 0.27  1.07  2754    1\npi_new  0.57    0.00 0.09  0.38 0.57  0.75  2780    1\nY_new   0.57    0.01 0.49  0.00 1.00  1.00  4013    1\n\nSamples were drawn using NUTS(diag_e) at Mon Feb 17 09:17:32 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/12-classification.html#predicted-probabilities-3",
    "href": "slides/12-classification.html#predicted-probabilities-3",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior mean of the predicted probabilities is 0.57."
  },
  {
    "objectID": "slides/12-classification.html#posterior-predictive-checks",
    "href": "slides/12-classification.html#posterior-predictive-checks",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\ny_pred &lt;- rstan::extract(fit, pars = \"Y_pred\")$Y_pred\nppc_dens_overlay(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/12-classification.html#posterior-predictive-checks-1",
    "href": "slides/12-classification.html#posterior-predictive-checks-1",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_bars(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/12-classification.html#posterior-predictive-checks-2",
    "href": "slides/12-classification.html#posterior-predictive-checks-2",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_stat(stan_data$Y, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(stan_data$Y, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(stan_data$Y, y_pred, stat = \"q025\")\nppc_stat(stan_data$Y, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/12-classification.html#model-comparison",
    "href": "slides/12-classification.html#model-comparison",
    "title": "Classification",
    "section": "Model comparison",
    "text": "Model comparison\n\nComparing our model to a baseline that removed stage.\n\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_baseline &lt;- loo::extract_log_lik(fit_baseline, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model &lt;- loo::waic(log_lik)\nwaic_model_baseline &lt;- loo::waic(log_lik_baseline)\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"full\" = waic_model, \"baseline\" = waic_model_baseline))\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nfull        0.00      0.00 -156.08      9.60         7.83    2.20    312.17\nbaseline   -1.38      3.15 -157.46      9.29         4.68    1.82    314.92\n         se_waic\nfull       19.21\nbaseline   18.58"
  },
  {
    "objectID": "slides/12-classification.html#other-models-for-binary-data",
    "href": "slides/12-classification.html#other-models-for-binary-data",
    "title": "Classification",
    "section": "Other models for binary data",
    "text": "Other models for binary data\nAn alternative approach is Probit regression, where we use the CDF of the standard normal distribution instead of the logit link: \\(\\Phi^{-1}(\\pi) = \\alpha + \\beta X\\)\nWhere \\(\\Phi^{-1}\\) is the inverse normal CDF (also called the probit link function).\n\ndata {\n  int&lt;lower = 1&gt; n;               // number of observations\n  int&lt;lower = 1&gt; p;               // number of predictors\n  int&lt;lower = 0, upper = 1&gt; Y[n]; // binary outcome (0 or 1)\n  matrix[n, p] X;                 // design matrix (predictors)\n}\nparameters {\n  real alpha;               // intercept\n  vector[p] beta;           // coefficients\n}\n\nmodel {\n  target += bernoulli_lpmf(Y | Phi(alpha + X * beta)); // Probit model\n}"
  },
  {
    "objectID": "slides/12-classification.html#steps-to-selecting-a-bayesian-glm",
    "href": "slides/12-classification.html#steps-to-selecting-a-bayesian-glm",
    "title": "Classification",
    "section": "Steps to selecting a Bayesian GLM",
    "text": "Steps to selecting a Bayesian GLM\n\nIdentify the support of the response distribution.\nSelect the likelihood by picking a parametric family of distributions with this support.\nChoose a link function \\(g\\) that transforms the range of parameters to the whole real line.\nSpecify a linear model on the transformed parameters.\nSelect priors for the regression coefficients."
  },
  {
    "objectID": "slides/12-classification.html#example-of-selecting-a-bayesian-glm",
    "href": "slides/12-classification.html#example-of-selecting-a-bayesian-glm",
    "title": "Classification",
    "section": "Example of selecting a Bayesian GLM",
    "text": "Example of selecting a Bayesian GLM\n\nSupport: \\(Y_i \\in \\{0, 1, 2, \\ldots\\}\\).\nLikelihood family: \\(Y_i \\stackrel{ind}{\\sim} \\text{Poisson}(\\lambda_i)\\).\nLink: \\(g(\\lambda_i) = \\log(\\lambda_i) \\in (‚àí\\infty, \\infty)\\).\nRegression model: \\(\\log(\\lambda_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nPriors: \\(\\alpha, \\beta_j \\sim N(0, 10^2)\\)."
  },
  {
    "objectID": "slides/12-classification.html#prepare-for-next-class",
    "href": "slides/12-classification.html#prepare-for-next-class",
    "title": "Classification",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03.\nComplete reading to prepare for next Thursday‚Äôs lecture\nThursday‚Äôs lecture: Multiclass classification"
  },
  {
    "objectID": "ae/ae-06-classification.html",
    "href": "ae/ae-06-classification.html",
    "title": "AE 06: Classification",
    "section": "",
    "text": "ImportantDue date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE spans the lectures on February 17 and 19, so this is considered a Thursday AE!\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-06-classification.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-06-classification.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 06: Classification",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp26 organization on GitHub.\nClick on the repo with the prefix ae-06-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-06.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-06-classification.html#mimic-iv-emergency-department-ed-data",
    "href": "ae/ae-06-classification.html#mimic-iv-emergency-department-ed-data",
    "title": "AE 06: Classification",
    "section": "MIMIC-IV Emergency Department (ED) Data",
    "text": "MIMIC-IV Emergency Department (ED) Data\nThe MIMIC-IV-ED database, which includes over 400,000 emergency department (ED) admissions to the Beth Israel Deaconess Medical Center between 2011 and 2019. We will use the MIMIC-IV-ED demo dataset, which is an educational version containing data for only 100 patients, making it a perfect tool for exploring electronic health records (EHR) and learning how to analyze real-world clinical data.\n\nAbout MIMIC-IV-ED\nThe MIMIC-IV-ED dataset is a large, publicly accessible resource intended to facilitate data analysis and research in emergency care. The demo dataset maintains the same structure as the original MIMIC-IV-ED, but with the protected health information (PHI) removed to ensure deidentification and privacy.\n\n\nDataset Access and Ethics\nAccess to the full MIMIC-IV-ED dataset requires registration, identity verification, completion of human participant training, and a signed data use agreement via PhysioNet. However, this demo dataset is publicly available, so you can start exploring it right away!\nThe project has been approved by the Institutional Review Boards (IRBs) of Beth Israel Deaconess Medical Center and the Massachusetts Institute of Technology (MIT), and all patient data is deidentified in compliance with HIPAA regulations."
  },
  {
    "objectID": "ae/ae-06-classification.html#what-is-ehr-data",
    "href": "ae/ae-06-classification.html#what-is-ehr-data",
    "title": "AE 06: Classification",
    "section": "What is EHR Data?",
    "text": "What is EHR Data?\n\nElectronic Health Records (EHR) Overview\nEHR data refers to digital records of a patient‚Äôs medical history, typically collected during healthcare encounters such as doctor visits, hospital stays, and emergency department admissions. This data is real-world data that is collected during routine medical care, and it is used by healthcare providers to manage patient care, track diagnoses, and prescribe treatments.\nEHRs are made up of a variety of information, such as:\n\nDemographics: Information about the patient, including age, gender, and race.\nClinical Data: Diagnoses, medications, lab test results, and other medical conditions.\nVitals: Measurements of important bodily functions, such as heart rate, blood pressure, and temperature.\nProcedures: Records of surgeries, imaging tests, and other medical interventions.\nBilling and Insurance Information: Data related to how healthcare services are paid for.\n\nEHR data is crucial in modern healthcare, enabling both providers and researchers to track health outcomes, improve care quality, and conduct studies on medical practices and disease progression.\n\n\nEHR Data in the Context of MIMIC-IV-ED\nThe MIMIC-IV-ED dataset is an example of EHR data, specifically from emergency department (ED) visits. It contains a wide variety of patient-related information, from demographics to vital signs, to medications and diagnostic codes. By analyzing this type of data, researchers can identify patterns, predict outcomes, and improve healthcare delivery.\nHowever, it‚Äôs important to understand that EHR data is far from perfect. Here are a few challenges and considerations when working with this type of data:"
  },
  {
    "objectID": "ae/ae-06-classification.html#challenges-of-ehr-data",
    "href": "ae/ae-06-classification.html#challenges-of-ehr-data",
    "title": "AE 06: Classification",
    "section": "Challenges of EHR Data",
    "text": "Challenges of EHR Data\n\n1. Biases in Data Collection\nEHR data reflects the way care is actually provided in the real world, meaning it can carry systematic biases. For example:\n\nSociodemographic Bias: Certain groups of people (e.g., those from lower socioeconomic backgrounds) may experience different levels of access to healthcare, leading to underrepresentation of these groups in the data.\nProvider Bias: Clinicians may make diagnostic or treatment decisions that are influenced by their experiences, leading to biases in how certain conditions are treated or recorded.\nSelection Bias: Only patients who visit the ED are included in the dataset. This excludes people who might have similar health conditions but do not seek emergency care.\n\n\n\n2. Missing Data\nSince EHR data is collected during routine medical care, it often contains missing data. Some reasons for missing data include:\n\nIncomplete records: Not every patient will have all the data points recorded. For example, certain tests or treatments may not be administered to every patient.\nVariable documentation: Different providers may record information inconsistently or leave fields blank.\nPatient noncompliance: Some patients might not provide full information during visits, leading to gaps in the data.\n\nHandling missing data is a key challenge when working with EHR data, and it can impact the accuracy of any analysis performed.\n\n\n3. Data Quality and Inconsistencies\nEHR data is typically entered manually by clinicians or extracted from various systems. As a result, there may be data quality issues such as:\n\nTypos or errors in data entry (e.g., incorrect medication dosages).\nInconsistencies across systems, especially when data is pulled from multiple healthcare organizations or devices.\nCoding errors: Diagnoses and procedures are often coded using standard systems like ICD codes, which can sometimes be misapplied.\n\n\n\n4. Billing-Related Data\nSome elements of EHR data are heavily influenced by billing processes. For example, certain procedures or diagnoses might be recorded in the system primarily for reimbursement purposes, rather than being a true reflection of the patient‚Äôs condition. This can introduce confounding factors in analysis, as billing codes may not always align with clinical realities."
  },
  {
    "objectID": "ae/ae-06-classification.html#analyzing-ehr-data",
    "href": "ae/ae-06-classification.html#analyzing-ehr-data",
    "title": "AE 06: Classification",
    "section": "Analyzing EHR Data",
    "text": "Analyzing EHR Data\nWhile these challenges can make EHR data more complex to work with, they also present opportunities for learning and improvement. By using appropriate methods, researchers can account for biases and missingness, and still gain valuable insights from the data.\nIn the case of the MIMIC-IV-ED dataset, these challenges are present, but the data is carefully deidentified and has been cleaned for use in research and education. It‚Äôs important to acknowledge these limitations and use statistical techniques to account for them when analyzing the data."
  },
  {
    "objectID": "ae/ae-06-classification.html#structure-of-the-mimic-iv-ed-demo-dataset",
    "href": "ae/ae-06-classification.html#structure-of-the-mimic-iv-ed-demo-dataset",
    "title": "AE 06: Classification",
    "section": "Structure of the MIMIC-IV-ED Demo Dataset",
    "text": "Structure of the MIMIC-IV-ED Demo Dataset\nEHR data is a relational database, which means the data is organized into tables that are linked by common identifiers (in this case, subject_id). Here‚Äôs an overview of the key tables in the dataset:\n\nedstays: A patient tracking table that includes the unique ID for each patient visit.\ndiagnosis: Contains diagnostic codes and descriptions for each ED visit.\nmedrecon: Records the medications administered during the patient‚Äôs ED visit.\npyxis: Provides information on the medications dispensed from the hospital‚Äôs automated medication dispensing system.\ntriage: Contains triage data, which includes the assessment of patients when they first arrive in the ED.\nvitalsign: Includes measurements of vital signs taken during the visit, such as heart rate, blood pressure, and temperature.\n\nEach table is designed to capture different aspects of the patient‚Äôs visit to the ED, giving you the ability to explore patient demographics, diagnoses, treatments, and outcomes."
  },
  {
    "objectID": "ae/ae-06-classification.html#mimic-iv-ed-demo-exploring-key-datasets",
    "href": "ae/ae-06-classification.html#mimic-iv-ed-demo-exploring-key-datasets",
    "title": "AE 06: Classification",
    "section": "MIMIC-IV-ED Demo: Exploring Key Datasets",
    "text": "MIMIC-IV-ED Demo: Exploring Key Datasets\nIn this course, we will work most with the edstays and triage datasets. The edstays dataset contains important information, including the start and end time of the ED encounter, demographics (including gender and race), the mode of arrival, and importantly the discharge disposition. Discharge disposition is a critical outcome for ED encounters that indicates how a patient left the encounter and can include, among others, going home, being admitted to the hospital.\n\nedstays &lt;- read_csv(\"mimic_ed/edstays.csv\")\n\n\n\n\n\n\n\nWhen a patient enters the ED, they are triaged, meaning a medical professional will quickly assess their condition to determine the severity of their illness or injury and prioritize them in line for treatment based on how urgently they need care, essentially sorting patients based on their need for immediate attention; the most critical cases will be seen first. The data in the triage dataset comes from this process and includes vital measurements, including, among others, temperature, oxygen saturation, and pain; an assessment of acuity, and also a chief complaint variable. Vitals in the triage dataset are different from those in the vitals dataset, which are collected during the ED stay and are time stamped.\n\ntriage &lt;- read_csv(\"mimic_ed/triage.csv\")\n\n\n\n\n\n\n\n\nData is Messy: Race\nWorking with any real-world data is messy and requires substantial data processing. To illustrate this, let‚Äôs consider the variable race from the edstays dataset.\n\n\n\n\n\nCategory\nn\n\n\n\n\nWHITE\n138\n\n\nBLACK/AFRICAN AMERICAN\n46\n\n\nHISPANIC/LATINO - CUBAN\n11\n\n\nPORTUGUESE\n9\n\n\nUNKNOWN\n6\n\n\nWHITE - BRAZILIAN\n3\n\n\nHISPANIC/LATINO - SALVADORAN\n3\n\n\nOTHER\n2\n\n\nWHITE - OTHER EUROPEAN\n1\n\n\nUNABLE TO OBTAIN\n1\n\n\nPATIENT DECLINED TO ANSWER\n1\n\n\nMULTIPLE RACE/ETHNICITY\n1\n\n\n\n\n\nThere are 12 categories of race in only 100 patients. Imagine what this would look like in 400,000 patients and with racial categories changing over time! When working with EHR data, variables should be processed into forms that are appropriate for the research question. Typically this means grouping categorical variables like race into meaningful categories. The following is an example.\n\nedstays &lt;- edstays %&gt;%\n  mutate(race2 = case_when(\n    race == \"BLACK/AFRICAN AMERICAN\" ~ \"black\",\n    race == \"WHITE - BRAZILIAN\" ~ \"white\",\n    race == \"WHITE - OTHER EUROPEAN\" ~ \"white\",\n    race == \"WHITE\" ~ \"white\",\n    race == \"UNKNOWN\" ~ \"other\",\n    race == \"UNABLE TO OBTAIN\" ~ \"other\",\n    race == \"PATIENT DECLINED TO ANSWER\" ~ \"other\",\n    race == \"OTHER\" ~ \"other\",\n    race == \"PORTUGUESE\" ~ \"other\",\n    race == \"MULTIPLE RACE/ETHNICITY\" ~ \"other\",\n    race == \"HISPANIC/LATINO - SALVADORAN\" ~ \"other\",\n    race == \"HISPANIC/LATINO - CUBAN\" ~ \"other\",\n    TRUE ~ NA_character_  # handles any other cases that don't match\n  )) %&gt;%\n  mutate(race2 = relevel(factor(race2), ref = \"white\"))\n\nThis yields the following categories.\n\n\n\n\n\nCategory\nn\n\n\n\n\nwhite\n142\n\n\nblack\n46\n\n\nother\n34"
  },
  {
    "objectID": "ae/ae-06-classification.html#data-is-messy-ed-length-of-stay",
    "href": "ae/ae-06-classification.html#data-is-messy-ed-length-of-stay",
    "title": "AE 06: Classification",
    "section": "Data is Messy: ED Length of Stay",
    "text": "Data is Messy: ED Length of Stay\nAnother example of data processing in EHR data is computing ED length of stay. Length of stay is a critical outcome in healthcare applications and can be computed as the time of admission to discharge. In real-world datasets this is not pre-computed, but must be derived from date-time variables. We first check to see if the admission and discharge times are date-time data objects.\n\nclass(edstays$outtime)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nclass(edstays$intime)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nIf they are not, they can be converted.\n\nedstays$outtime &lt;- as.POSIXct(edstays$outtime)\nedstays$intime &lt;- as.POSIXct(edstays$intime)\n\nWe can then compute the length of stay by computing the time differences. It is important to specify the units of time for consistency. We then convert to a numeric and visualize.\n\nedstays$los &lt;- difftime(as.POSIXct(edstays$outtime), as.POSIXct(edstays$intime), units = \"hours\")\nedstays$los &lt;- as.numeric(edstays$los)\nggplot(edstays, aes(x = los)) +\n  geom_histogram() +\n  labs(x = \"ED Length of Stay (hours)\",\n       y = \"Count\")"
  },
  {
    "objectID": "ae/ae-06-classification.html#exercise-1",
    "href": "ae/ae-06-classification.html#exercise-1",
    "title": "AE 06: Classification",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit the logistic regression model: \\[\\begin{align*}\nY_i &\\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\\\\n\\text{logit}(\\pi_i) &= \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta},\n\\end{align*}\\] where \\(Y_i\\) is the binary indicator for a length of stay being greater than 6 hours and \\(\\mathbf{x}_i = (black_i, other_i)\\) contains the covariates for race, with white as the reference category. Place weakly-informative priors. Evaluate model convergence, check model fit with posterior predictive checks. Present the posterior mean and 95% credible interval for the odds ratio for black race.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-06-classification.html#exercise-2",
    "href": "ae/ae-06-classification.html#exercise-2",
    "title": "AE 06: Classification",
    "section": "Exercise 2",
    "text": "Exercise 2\nFit the additive log ratio regression model for \\(Y_i \\in \\{1,\\ldots,K\\}\\) for \\(i = 1,\\ldots,n\\), \\[\\log \\left(\\frac{P(Y_i = k)}{P(Y_i = K)} \\right)= \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\},\\] where \\(K\\) is chosen as reference. Place weakly-informative priors. Evaluate model convergence, check model fit with posterior predictive checks. Present the posterior mean and 95% credible interval for the odds ratio for black race.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-06-classification.html#exercise-3",
    "href": "ae/ae-06-classification.html#exercise-3",
    "title": "AE 06: Classification",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit the proportional odds regression model for ordinal \\(Y_i \\in \\{1,\\ldots,K\\}\\) for \\(i = 1,\\ldots,n\\), \\[\\log \\frac{P(Y_i\\leq k)}{P(Y_i &gt; k)} = \\text{logit}P(Y_i\\leq k) = \\alpha_k - \\mathbf{x}_i \\boldsymbol{\\beta}.\\] Place weakly-informative priors. Evaluate model convergence, check model fit with posterior predictive checks. Present the posterior mean and 95% credible interval for the odds ratio for black race.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your AE repo on GitHub. You‚Äôre done! üéâ"
  },
  {
    "objectID": "prepare/prepare-feb19.html",
    "href": "prepare/prepare-feb19.html",
    "title": "Prepare for February 19 lecture",
    "section": "",
    "text": "üìñ If you have not already, familizarize yourself with the data from AE 06: Classification.\nüìñ Read about multinomial regression.\nüìñ Read about ordinal regression.\n‚úÖ Work on HW 03"
  },
  {
    "objectID": "slides/13-multiclass.html",
    "href": "slides/13-multiclass.html",
    "title": "Multiclass Classification",
    "section": "",
    "text": "‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nLoaded lars 1.3\n\n\n\nAttaching package: 'LaplacesDemon'\n\n\nThe following objects are masked from 'package:mvtnorm':\n\n    dmvt, logdet, rmvt\n\n\nThe following objects are masked from 'package:lubridate':\n\n    dst, interval\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\n\nnimble version 1.2.1 is loaded.\nFor more information on NIMBLE and a User Manual,\nplease visit https://R-nimble.org.\n\nNote for advanced users who have written their own MCMC samplers:\n  As of version 0.13.0, NIMBLE's protocol for handling posterior\n  predictive nodes has changed in a way that could affect user-defined\n  samplers in some situations. Please see Section 15.5.1 of the User Manual.\n\n\nAttaching package: 'nimble'\n\n\nThe following objects are masked from 'package:LaplacesDemon':\n\n    cloglog, dcat, dinvgamma, is.model, logdet, logit, rcat, rinvgamma\n\n\nThe following object is masked from 'package:mvtnorm':\n\n    logdet\n\n\nThe following object is masked from 'package:stats':\n\n    simulate\n\n\nThe following object is masked from 'package:base':\n\n    declare"
  },
  {
    "objectID": "slides/13-multiclass.html#review-of-last-lecture",
    "href": "slides/13-multiclass.html#review-of-last-lecture",
    "title": "Multiclass Classification",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we learned about classification using logistic regression.\nToday, we will focus on multiclass classification: multinomial regression, ordinal regression."
  },
  {
    "objectID": "slides/13-multiclass.html#multiclass-regression",
    "href": "slides/13-multiclass.html#multiclass-regression",
    "title": "Multiclass Classification",
    "section": "Multiclass regression",
    "text": "Multiclass regression\n\nOften times one encounters an outcome variable that is nominal and has more than two categories.\nIf there is no inherent rank or order to the variable, we can use multinomial regression. Examples include:\n\ngender (male, female, non-binary),\nblood type (A, B, AB, O).\n\nIf there is an order to the variable, we can use ordinal regression. Examples include:\n\nstages of cancer (stage I, II, III, IV),\npain level (mild, moderate, severe)."
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-random-variable",
    "href": "slides/13-multiclass.html#multinomial-random-variable",
    "title": "Multiclass Classification",
    "section": "Multinomial random variable",
    "text": "Multinomial random variable\n\nAssume an outcome \\(Y_i \\in \\{1,\\ldots,K\\}\\) for \\(i = 1,\\ldots,n\\).\nThe likelihood in multinomial regression can be written as the following categorical likelihood,\n\n\\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}},\\]\n\n\\(\\delta_{ij} = 1(Y_i = j)\\) is the Kronecker delta.\nSince \\(Y_i\\) is discrete, we only need to specify \\(P(Y_i = j)\\) for all \\(i\\) and \\(j\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#log-linear-regression",
    "href": "slides/13-multiclass.html#log-linear-regression",
    "title": "Multiclass Classification",
    "section": "Log-linear regression",
    "text": "Log-linear regression\n\nOne way to motivate multinomial regression is through a log-linear specification:\n\n\\[\\log P(Y_i = j) = \\mathbf{x}_i\\boldsymbol{\\beta}_j - \\log Z.\\]\n\n\\(\\boldsymbol{\\beta}_j\\) is a \\(j\\) specific set of regression parameters.\n\\(P(Y_i = j) = \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}/Z\\).\n\\(Z\\) is a normalizing constant that guarentees that \\(\\sum_{j=1}^K P(Y_i = j) = 1\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#finding-the-normalizing-constant",
    "href": "slides/13-multiclass.html#finding-the-normalizing-constant",
    "title": "Multiclass Classification",
    "section": "Finding the normalizing constant",
    "text": "Finding the normalizing constant\n\nWe know that,\n\n\\[\\begin{aligned}\n1 &= \\sum_{j=1}^K P(Y_i = j) = \\frac{1}{Z}\\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\\\\\n&\\implies Z = \\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-probabilities",
    "href": "slides/13-multiclass.html#multinomial-probabilities",
    "title": "Multiclass Classification",
    "section": "Multinomial probabilities",
    "text": "Multinomial probabilities\n\nThus, we have the following,\n\n\\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\\]\n\nThis function is called the softmax function.\nUnfortunately, this specification is not identifiable."
  },
  {
    "objectID": "slides/13-multiclass.html#identifiability-issue",
    "href": "slides/13-multiclass.html#identifiability-issue",
    "title": "Multiclass Classification",
    "section": "Identifiability issue",
    "text": "Identifiability issue\n\nWe can add a vector \\(\\mathbf{c}\\) to all parameters and get the same result,\n\n\\[\\begin{aligned}\n\\frac{\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_k + \\mathbf{c})\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_j + \\mathbf{c})\\}} &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n\\end{aligned}\\]\n\nA common solution is to set: \\(\\boldsymbol{\\beta}_K = \\mathbf{0}\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#updating-the-probabilities",
    "href": "slides/13-multiclass.html#updating-the-probabilities",
    "title": "Multiclass Classification",
    "section": "Updating the probabilities",
    "text": "Updating the probabilities\n\nUsing the identifiability constraint of \\(\\boldsymbol{\\beta}_K = \\mathbf{0}\\), the probabilities become, \\[\\begin{aligned}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\},\\\\\nP(Y_i = K) &= \\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n\\end{aligned}\\]\nHow to interpret the \\(\\boldsymbol{\\beta}_k\\)?"
  },
  {
    "objectID": "slides/13-multiclass.html#deriving-the-additive-log-ratio-model",
    "href": "slides/13-multiclass.html#deriving-the-additive-log-ratio-model",
    "title": "Multiclass Classification",
    "section": "Deriving the additive log ratio model",
    "text": "Deriving the additive log ratio model\n\nUsing our specification of the probabilities, it can be seen that,\n\n\\[\\begin{aligned}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\\\\n&= \\left[\\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\right]\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}\\\\\n&= P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}.\n\\end{aligned}\\]\n\\(\\implies \\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots,K-1\\}\\)"
  },
  {
    "objectID": "slides/13-multiclass.html#additive-log-ratio-model",
    "href": "slides/13-multiclass.html#additive-log-ratio-model",
    "title": "Multiclass Classification",
    "section": "Additive log ratio model",
    "text": "Additive log ratio model\n\nIf outcome \\(K\\) is chosen as reference, the \\(K ‚àí 1\\) regression equations are:\n\n\\[\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}\\]\n\nThis formulation is called additive log ratio.\n\\(\\beta_{jk}\\) represent the log-odds of being in category \\(k\\) relative to the baseline category \\(K\\) with a one-unit change in \\(X_{ij}\\).\n\n\\(\\exp (\\beta_{jk})\\) is an odds ratio."
  },
  {
    "objectID": "slides/13-multiclass.html#getting-back-to-the-likelihood",
    "href": "slides/13-multiclass.html#getting-back-to-the-likelihood",
    "title": "Multiclass Classification",
    "section": "Getting back to the likelihood",
    "text": "Getting back to the likelihood\n\nThe log-likelihood can be written as,\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\sum_{j=1}^K \\delta_{ij} \\log P(Y_i = j).\\]\n\nThe \\(P(Y_i = j)\\) are given by the additive log ratio model.\nAs Bayesians, we only need to specify priors for \\(\\boldsymbol{\\beta}_k, k \\in \\{1,\\ldots,K-1\\}\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-in-stan",
    "href": "slides/13-multiclass.html#multinomial-regression-in-stan",
    "title": "Multiclass Classification",
    "section": "Multinomial regression in Stan",
    "text": "Multinomial regression in Stan\n\nHard coding the likelihood.\n\n\n// additive_log_ratio.stan\nfunctions {\n  matrix compute_alr_probs(int n, int K, int p, matrix X, matrix beta) {\n    matrix[n, K] probs;\n    matrix[n, K - 1] expXbeta = exp(X * beta);\n    for (i in 1:n) {\n      real sum_i = sum(expXbeta[i, ]);\n      for (j in 1:K) {\n        if (j &lt; K) {\n          probs[i, j] = expXbeta[i, j] / (1 + sum_i);\n        }\n        if (j == K) probs[i, j] = 1 - sum(probs[i, 1:(K - 1)]);\n      }\n    }\n    return probs\n  }\n}\ndata {\n  int&lt;lower = 1&gt; K;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  array[n] int&lt;lower = 1, upper = K&gt; Y;\n  matrix[n, p] X;\n  matrix[n, K] delta;\n}\nparameters {\n  matrix[p, K - 1] beta;\n}\nmodel {\n  matrix[n, K] probs = compute_alr_probs(n, K, p, X, beta);\n  for (i in 1:n) {\n    for (j in 1:K) {\n      target += delta[i, j] * log(probs[i, j]);\n    }\n  }\n  target += normal_lpdf(to_vector(beta) | 0, 10);\n}"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-in-stan-1",
    "href": "slides/13-multiclass.html#multinomial-regression-in-stan-1",
    "title": "Multiclass Classification",
    "section": "Multinomial regression in Stan",
    "text": "Multinomial regression in Stan\n\nNon-identifiable version.\n\n\n// multi_logit_bad.stan\ndata {\n  int&lt;lower = 1&gt; K;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  array[n] int&lt;lower = 1, upper = K&gt; Y;\n  matrix[n, p] X;\n}\nparameters {\n  matrix[p, K] beta;\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    target += categorical_logit_lpmf(Y[i] | Xbeta[i]')\n  }\n  target += normal_lpdf(to_vector(beta) | 0, 10);\n}\n\ncategorical_logit"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-in-stan-2",
    "href": "slides/13-multiclass.html#multinomial-regression-in-stan-2",
    "title": "Multiclass Classification",
    "section": "Multinomial regression in Stan",
    "text": "Multinomial regression in Stan\n\nZero identifiability constraint.\n\n\n// multi_logit.stan\ndata {\n  int&lt;lower = 1&gt; K;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  array[n] int&lt;lower = 1, upper = K&gt; Y;\n  matrix[n, p] X;\n}\ntransformed data {\n  vector[p] zeros = rep_vector(0, p);\n}\nparameters {\n  matrix[p, K - 1] beta_raw;\n}\ntransformed parameters {\n  matrix[p, K] beta = append_col(beta_raw, zeros);\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    target += categorical_logit_lpmf(Y[i] | Xbeta[i]')\n  }\n  target += normal_lpdf(to_vector(beta_raw) | 0, 10);\n}\ngenerated quantities {\n  matrix[p, K - 1] ors = exp(beta_raw);\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = categorical_logit_rng(Xbeta[i]');\n    log_lik[i] = categorical_logit_lpmf(Y[i] | Xbeta[i]');\n  }\n}"
  },
  {
    "objectID": "slides/13-multiclass.html#ordinal-regression",
    "href": "slides/13-multiclass.html#ordinal-regression",
    "title": "Multiclass Classification",
    "section": "Ordinal regression",
    "text": "Ordinal regression\nLet \\(Y_i \\in \\{1,\\ldots,K\\}\\) be an ordinal outcome with \\(K\\) categories.\n\nThe likelihood in ordinal regression is identical to the one from multinomial regression,\n\n\\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}}.\\]\n\nWe need to add additional constraints that guarantee ordinality."
  },
  {
    "objectID": "slides/13-multiclass.html#proportional-odds-assumption",
    "href": "slides/13-multiclass.html#proportional-odds-assumption",
    "title": "Multiclass Classification",
    "section": "Proportional odds assumption",
    "text": "Proportional odds assumption\n\n\\(P(Y_i \\leq k)\\) is the cumulative probability of \\(Y_i\\) less than or equal to a specific category \\(k=1,\\ldots,K-1\\).\nThe odds of being less than or equal to a particular category can be defined as, \\[\\frac{P(Y\\leq k)}{P(Y &gt; k)} \\text { for } k=1,\\ldots,K-1.\\]\nNot defined for \\(k = K\\), since division by zero is not defined."
  },
  {
    "objectID": "slides/13-multiclass.html#proportional-odds-regression",
    "href": "slides/13-multiclass.html#proportional-odds-regression",
    "title": "Multiclass Classification",
    "section": "Proportional odds regression",
    "text": "Proportional odds regression\nThe log odds can then be modeled as follows, \\[\\log \\frac{P(Y_i\\leq k)}{P(Y_i &gt; k)} = \\text{logit}P(Y_i\\leq k) = \\alpha_k - \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nWhy \\(-\\boldsymbol{\\beta}\\)?\n\\(\\boldsymbol{\\beta}\\) is a common regression parameter. For a one-unit increase in \\(x_{ij}\\), \\(\\beta_j\\) is the change in log odds of moving to a more severe level of the outcome \\(Y_i\\).\n\\(\\alpha_k\\) for \\(k = 1,\\ldots,K-1\\) are \\(k\\)-specific intercepts that corresponds to the log odds of moving from level \\(k\\) to \\(k+1\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#understanding-the-probabilities",
    "href": "slides/13-multiclass.html#understanding-the-probabilities",
    "title": "Multiclass Classification",
    "section": "Understanding the probabilities",
    "text": "Understanding the probabilities\n\nOne can solve for \\(P(Y_i \\leq k), \\quad k = 1,\\ldots,K-1\\)),\n\n\\[P(Y_i \\leq k) = \\frac{\\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}}{1 + \\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}} = \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}).\\]\n\n\\(P(Y_i \\leq K) = 1\\).\n\nThe individual probabilities are then given by,\n\\[\\begin{aligned}\nP(Y_i = k) &= P(Y_i \\leq k) - P(Y_i \\leq k-1)\\\\\n&= \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(\\alpha_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#a-latent-variable-representation",
    "href": "slides/13-multiclass.html#a-latent-variable-representation",
    "title": "Multiclass Classification",
    "section": "A latent variable representation",
    "text": "A latent variable representation\n\nDefine a latent variable,\n\n\\[Y_i^* = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim \\text{Logistic}(0, 1).\\]\n\n\\(\\mathbb{E}[\\epsilon_i] = 0\\).\n\\(\\mathbb{V}(\\epsilon_i) = \\pi^2/3\\).\nCDF: \\(P(\\epsilon_i \\leq x) = \\frac{1}{1 + \\exp\\{-x\\}} = \\frac{\\exp\\{x\\}}{1 + \\exp\\{x\\}} = \\text{expit}(x).\\)"
  },
  {
    "objectID": "slides/13-multiclass.html#visualizing-the-latent-process",
    "href": "slides/13-multiclass.html#visualizing-the-latent-process",
    "title": "Multiclass Classification",
    "section": "Visualizing the latent process",
    "text": "Visualizing the latent process"
  },
  {
    "objectID": "slides/13-multiclass.html#adding-thresholds-c_k",
    "href": "slides/13-multiclass.html#adding-thresholds-c_k",
    "title": "Multiclass Classification",
    "section": "Adding thresholds (\\(c_k\\))",
    "text": "Adding thresholds (\\(c_k\\))"
  },
  {
    "objectID": "slides/13-multiclass.html#getting-category-probabilities",
    "href": "slides/13-multiclass.html#getting-category-probabilities",
    "title": "Multiclass Classification",
    "section": "Getting category probabilities",
    "text": "Getting category probabilities"
  },
  {
    "objectID": "slides/13-multiclass.html#a-latent-variable-representation-1",
    "href": "slides/13-multiclass.html#a-latent-variable-representation-1",
    "title": "Multiclass Classification",
    "section": "A latent variable representation",
    "text": "A latent variable representation\n\nDefine a set of \\(K-1\\) cut-points, \\((c_1,\\ldots,c_{K-1}) \\in \\mathbb{R}^{K-1}\\). We also define \\(c_0 = -\\infty, c_K = \\infty\\).\nOur ordinal random variable can be generated as,\n\n\\[Y_i = \\left\\{\n\\begin{matrix*}[l]\n1 & c_0 &lt; Y_i^* \\leq c_1\\\\\n2 & c_1 &lt; Y_i^* \\leq c_2\\\\\n\\vdots & \\\\\nK & c_{K-1} &lt; Y_i^* \\leq c_K\\\\\n\\end{matrix*}\n\\right.\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#equivalence-of-the-two-specifications",
    "href": "slides/13-multiclass.html#equivalence-of-the-two-specifications",
    "title": "Multiclass Classification",
    "section": "Equivalence of the two specifications",
    "text": "Equivalence of the two specifications\n\nProbabilities under the latent specification: \\[\\begin{aligned}\nP(Y_i = k) &= P(c_{k-1} &lt; Y_i^* \\leq c_k)\\\\\n&= P(c_{k-1} &lt; \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k)\\\\\n&= P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k) - P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i &lt; c_{k-1})\\\\\n&= P(\\epsilon_i \\leq c_k - \\mathbf{x}_i \\boldsymbol{\\beta}) - P(\\epsilon_i &lt; c_{k-1} - \\mathbf{x}_i \\boldsymbol{\\beta})\\\\\n&= \\text{expit}(c_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(c_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n\\end{aligned}\\]\nEquivalency:\n\n\\(\\alpha_k = c_k, \\quad k = 1,\\ldots, K-1\\), assuming that \\(\\alpha_k &lt; \\alpha_{k+1}\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#ordinal-regression-using-stan",
    "href": "slides/13-multiclass.html#ordinal-regression-using-stan",
    "title": "Multiclass Classification",
    "section": "Ordinal regression using Stan",
    "text": "Ordinal regression using Stan\n\n// ordinal.stan\ndata {\n  int&lt;lower = 2&gt; K;\n  int&lt;lower = 0&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1, upper = K&gt; Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  ordered[K - 1] alpha;\n}\nmodel {\n  target += ordered_logistic_glm_lpmf(Y | X, beta, alpha);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(alpha | 0, 10);\n}\ngenerated quantities {\n  vector[p] ors = exp(beta);\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = ordered_logistic_rng(X[i, ] * beta, alpha);\n    log_lik[i] = ordered_logistic_glm_lpmf(Y[i] | X[i, ], beta, alpha);\n  }\n}\n\nordered_logistic_glm_lpmf"
  },
  {
    "objectID": "slides/13-multiclass.html#enforcing-order-in-the-cutoffs",
    "href": "slides/13-multiclass.html#enforcing-order-in-the-cutoffs",
    "title": "Multiclass Classification",
    "section": "Enforcing order in the cutoffs",
    "text": "Enforcing order in the cutoffs\n\nIn Stan, when you define a parameter as ordered[K-1] alpha;, the values of alpha are automatically constrained to be strictly increasing.\nThis transformation ensures that the alpha values follow the required order, i.e., alpha[1] &lt; alpha[2] &lt; ... &lt; alpha[K-1].\nStan doesn‚Äôt sample alpha directly but instead works with an unconstrained parameter vector, which we will call gamma."
  },
  {
    "objectID": "slides/13-multiclass.html#enforcing-order-in-the-cutoffs-1",
    "href": "slides/13-multiclass.html#enforcing-order-in-the-cutoffs-1",
    "title": "Multiclass Classification",
    "section": "Enforcing order in the cutoffs",
    "text": "Enforcing order in the cutoffs\n\nparameters {\n  vector[K - 1] gamma;\n}\ntransformed parameters {\n  vector[K - 1] alpha;\n  alpha[1] = gamma[1];\n  for (j in 2:K) {\n    alpha[j] = alpha[j - 1] + exp(gamma[j]);\n  }\n}\n\n\nHere, gamma represents a vector of independent, unconstrained variables, and the transformation ensures that alpha is strictly increasing by construction.\nLuckily we can use ordered, since Stan takes care of this (including the Jacobian) in the background."
  },
  {
    "objectID": "slides/13-multiclass.html#prepare-for-next-class",
    "href": "slides/13-multiclass.html#prepare-for-next-class",
    "title": "Multiclass Classification",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03.\nComplete reading to prepare for next Tuesday‚Äôs lecture\nTuesday‚Äôs lecture: Hierarchical models"
  },
  {
    "objectID": "prepare/prepare-feb24.html",
    "href": "prepare/prepare-feb24.html",
    "title": "Prepare for February 24 lecture",
    "section": "",
    "text": "üìñ Read pages 175-182 from Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists.\n‚úÖ Finish HW 03, which is due before class on Thursday."
  },
  {
    "objectID": "slides/14-hierarchical.html",
    "href": "slides/14-hierarchical.html",
    "title": "Hierarchical Models",
    "section": "",
    "text": "‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nLoaded lars 1.3\n\n\n\nAttaching package: 'LaplacesDemon'\n\n\nThe following objects are masked from 'package:mvtnorm':\n\n    dmvt, logdet, rmvt\n\n\nThe following objects are masked from 'package:lubridate':\n\n    dst, interval\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\n\nnimble version 1.2.1 is loaded.\nFor more information on NIMBLE and a User Manual,\nplease visit https://R-nimble.org.\n\nNote for advanced users who have written their own MCMC samplers:\n  As of version 0.13.0, NIMBLE's protocol for handling posterior\n  predictive nodes has changed in a way that could affect user-defined\n  samplers in some situations. Please see Section 15.5.1 of the User Manual.\n\n\nAttaching package: 'nimble'\n\n\nThe following objects are masked from 'package:LaplacesDemon':\n\n    cloglog, dcat, dinvgamma, is.model, logdet, logit, rcat, rinvgamma\n\n\nThe following object is masked from 'package:mvtnorm':\n\n    logdet\n\n\nThe following object is masked from 'package:stats':\n\n    simulate\n\n\nThe following object is masked from 'package:base':\n\n    declare\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n\nThis is bayesplot version 1.11.1\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n- bayesplot theme set to bayesplot::theme_default()\n\n   * Does _not_ affect other ggplot2 plots\n\n   * See ?bayesplot_theme_set for details on theme setting"
  },
  {
    "objectID": "slides/14-hierarchical.html#review-of-last-lecture",
    "href": "slides/14-hierarchical.html#review-of-last-lecture",
    "title": "Hierarchical Models",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nLast week, we learned about classification for binary and multiclass problems.\nMoving forward: Up until today, we have dealt with independent data. Today, we will look at our first example of dependent data!"
  },
  {
    "objectID": "slides/14-hierarchical.html#linear-regression-assumptions",
    "href": "slides/14-hierarchical.html#linear-regression-assumptions",
    "title": "Hierarchical Models",
    "section": "Linear regression assumptions",
    "text": "Linear regression assumptions\n\\[\\begin{aligned}\nY_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(Y_i\\) are independent observations (independence).\n\\(Y_i\\) is linearly related to \\(\\mathbf{x}_i\\) (linearity).\n\\(\\epsilon_i = Y_i - \\mu_i\\) is normally distributed (normality).\n\\(\\epsilon_i\\) has constant variance across \\(\\mathbf{x}_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/14-hierarchical.html#independence-assumption-in-linear-regression",
    "href": "slides/14-hierarchical.html#independence-assumption-in-linear-regression",
    "title": "Hierarchical Models",
    "section": "Independence Assumption in Linear Regression",
    "text": "Independence Assumption in Linear Regression\nWe assume that the residuals \\(\\epsilon_i\\) are independent:\n\\[\\mathbb{C}(\\epsilon_i, \\epsilon_j) = 0, \\quad \\text{for} \\quad i \\neq j,\\] where \\(\\mathbb{C}(X, Y)\\) is the covariance between two random variables \\(X\\) and \\(Y\\). As a note: \\(\\mathbb{C}(X, X) = \\mathbb{V}(X)\\).\n\nThis implies that the observations \\(Y_i\\) and \\(Y_j\\) are independent, and their correlation is zero.\n\nCorrelation: \\(\\rho(X,Y) = \\frac{\\mathbb{C}(X, Y)}{\\sqrt{\\mathbb{V}(X)\\mathbb{V}(Y)}}\\)."
  },
  {
    "objectID": "slides/14-hierarchical.html#real-world-dependent-observations",
    "href": "slides/14-hierarchical.html#real-world-dependent-observations",
    "title": "Hierarchical Models",
    "section": "Real-World: Dependent Observations",
    "text": "Real-World: Dependent Observations\nHowever, in real-world data, the independence assumption often does not hold:\n\nRepeated measures data (e.g., same individual over time).\nClustered data (e.g., patients within a hospital).\nLongitudinal data (e.g., disease severity measures over time).\nSpatial data (e.g., disease counts observed across zip codes)."
  },
  {
    "objectID": "slides/14-hierarchical.html#the-challenge",
    "href": "slides/14-hierarchical.html#the-challenge",
    "title": "Hierarchical Models",
    "section": "The Challenge",
    "text": "The Challenge\n\nIf we assume independence in the presence of correlation:\n\nBiased parameter estimates: Parameter estimation will be biased due to group-level dependencies that effect the outcome.\nUnderestimated uncertainty: The model will not account for the true variability, leading to narrower confidence intervals.\nInaccurate Predictions: Predictions for new groups may be biased because the model doesn‚Äôt properly account for group-level variability.\n\nThus, we need a way to account for dependencies between observations, especially when data are grouped or clustered."
  },
  {
    "objectID": "slides/14-hierarchical.html#example-of-hierarchical-data",
    "href": "slides/14-hierarchical.html#example-of-hierarchical-data",
    "title": "Hierarchical Models",
    "section": "Example of Hierarchical Data",
    "text": "Example of Hierarchical Data\n\nHierarchical data refers to data that is organized into groups or clusters, where each group contains multiple observations.\nConsider data from patients within hospitals. Each patient is being treated in a hospital, with multiple patients belonging to each hospital.\nIn this case, the observation for a patient is indexed by two variables:\n\n\\(i\\): hospital index.\n\\(j\\): patient index, nested within hospital.\n\nSo, for patient \\(j\\) within hospital \\(i\\), we write the response as \\(Y_{ij}\\)."
  },
  {
    "objectID": "slides/14-hierarchical.html#observations-with-two-indices-y_ij",
    "href": "slides/14-hierarchical.html#observations-with-two-indices-y_ij",
    "title": "Hierarchical Models",
    "section": "Observations with Two Indices: \\(Y_{ij}\\)",
    "text": "Observations with Two Indices: \\(Y_{ij}\\)\n\n\\(Y_{ij}\\) represents the response for patient \\(j\\) in hospital \\(i\\).\nThe first index \\(i\\) represents group-level effects (e.g., hospital-level).\nThe second index \\(j\\) represents individual-level observations (e.g., patient).\nWe typically say that \\(i = 1,\\ldots,n.\\) and \\(j = 1,\\ldots,n_i\\).\nThe total number of observations is \\(N = \\sum_{i = 1}^{n}n_i\\)."
  },
  {
    "objectID": "slides/14-hierarchical.html#why-two-indices",
    "href": "slides/14-hierarchical.html#why-two-indices",
    "title": "Hierarchical Models",
    "section": "Why Two Indices?",
    "text": "Why Two Indices?\nHaving two indices allows us to model both:\n\nWithin-group variation (differences between patients within the same hospital).\nBetween-group variation (differences between hospitals).\n\nThe hierarchical structure captures both types of variation."
  },
  {
    "objectID": "slides/14-hierarchical.html#conceptualizing-hierarchical-data",
    "href": "slides/14-hierarchical.html#conceptualizing-hierarchical-data",
    "title": "Hierarchical Models",
    "section": "Conceptualizing Hierarchical Data",
    "text": "Conceptualizing Hierarchical Data\nConsider the example of patients within hospitals:\n\nEach data point (i.e., observed data \\(Y_{ij}\\)) represents an outcome measured on a patient.\nThe data points are grouped by hospital, indicating that patients from the same hospital are likely to have similar outcomes due to shared hospital-level factors.\n\nThis is a typical example of hierarchical data."
  },
  {
    "objectID": "slides/14-hierarchical.html#hierarchical-model",
    "href": "slides/14-hierarchical.html#hierarchical-model",
    "title": "Hierarchical Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nNow, we can see how hierarchical data appears in the model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\nSubject-specific objects:\n\n\\(Y_{ij}\\): response for patient \\(j\\) in hospital \\(i\\).\n\\(\\mathbf{x}_{ij}\\) are the predictors for patient \\(j\\) in hospital \\(i\\).\n\\(\\epsilon_{ij}\\): residual error for patient \\(j\\) in hospital \\(i\\)."
  },
  {
    "objectID": "slides/14-hierarchical.html#hierarchical-model-1",
    "href": "slides/14-hierarchical.html#hierarchical-model-1",
    "title": "Hierarchical Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nNow, we can see how hierarchical data appears in the model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\nGroup-specific objects:\n\n\\(\\theta_i\\): group-specific parameter for hospital \\(i\\), accounting for hospital-level variation (group-specific, random effect).\n\nThe group-specific parameters are responsible for inducing correlation into the model."
  },
  {
    "objectID": "slides/14-hierarchical.html#hierarchical-model-2",
    "href": "slides/14-hierarchical.html#hierarchical-model-2",
    "title": "Hierarchical Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nNow, we can see how hierarchical data appears in the model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\nPopulation parameters:\n\n\\(\\alpha\\): intercept for the entire population.\n\\(\\boldsymbol{\\beta}\\): regression parameters for the entire population.\n\\(\\sigma\\): residual error parameter for the entire population."
  },
  {
    "objectID": "slides/14-hierarchical.html#random-intercept-model",
    "href": "slides/14-hierarchical.html#random-intercept-model",
    "title": "Hierarchical Models",
    "section": "Random Intercept Model",
    "text": "Random Intercept Model\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\n\nFrom a frequentist perspective, this model may be called a random intercept model, but in the Bayesian framework all parameters are random variables, so the terms fixed and random effects don‚Äôt apply.\n\n\\(\\theta_i\\): group-specific parameters (random effect).\n\\(\\alpha, \\boldsymbol{\\beta}, \\sigma\\): population parameters (common across all groups, \\(\\boldsymbol{\\beta}\\) are the fixed effects)."
  },
  {
    "objectID": "slides/14-hierarchical.html#prior-for-theta_i",
    "href": "slides/14-hierarchical.html#prior-for-theta_i",
    "title": "Hierarchical Models",
    "section": "Prior for \\(\\theta_i\\)",
    "text": "Prior for \\(\\theta_i\\)\nWe model \\(\\theta_i\\) as a parameter drawn from a normal distribution centered at zero, with some variance \\(\\tau^2\\):\n\\[\\theta_i \\stackrel{iid}{\\sim} N(0, \\tau^2).\\]\n\nMean at zero: This assumption reflects that, on average, hospitals don‚Äôt deviate from the population mean (helps with identifiability).\nVariance \\(\\tau^2\\): This represents the variability in hospital-level intercepts. A larger \\(\\tau^2\\) implies greater variability between hospitals.\n\nEach hospital \\(i\\) has a hospital-specific parameter \\(\\theta_i\\), which represents how that hospital‚Äôs baseline (e.g., health outcomes) deviates from the population average."
  },
  {
    "objectID": "slides/14-hierarchical.html#group-specific-intercept-model-conditional-specification",
    "href": "slides/14-hierarchical.html#group-specific-intercept-model-conditional-specification",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\nFor \\(i = 1,\\ldots,n\\) and \\(j = 1,\\ldots,n_i\\), \\[\\begin{aligned}\nY_{ij} | \\boldsymbol{\\Omega},\\theta_i &\\stackrel{ind}{\\sim} N(\\alpha + \\mathbf{x}_{ij}\\boldsymbol{\\beta} + \\theta_i,\\sigma^2)\\\\\n\\theta_i | \\tau^2 &\\stackrel{iid}{\\sim} N(0,\\tau^2)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta},\\sigma,\\tau)\\) are the population parameters."
  },
  {
    "objectID": "slides/14-hierarchical.html#group-specific-intercept-model-conditional-specification-1",
    "href": "slides/14-hierarchical.html#group-specific-intercept-model-conditional-specification-1",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\n\nMoments for the Conditional Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{ij} | \\boldsymbol{\\Omega},\\theta_i] &= \\alpha + \\mathbf{x}_{ij}\\boldsymbol{\\beta} + \\theta_i\\\\\n\\mathbb{V}(Y_{ij} | \\boldsymbol{\\Omega},\\theta_i) &= \\sigma^2\\\\\n\\mathbb{C}(Y_{ij}, Y_{lk} | \\boldsymbol{\\Omega},\\theta_i,\\theta_l) &= 0,\\quad \\forall i,j,l,k.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14-hierarchical.html#understanding-the-random-intercept",
    "href": "slides/14-hierarchical.html#understanding-the-random-intercept",
    "title": "Hierarchical Models",
    "section": "Understanding the Random Intercept",
    "text": "Understanding the Random Intercept\n\n\\(\\theta_i\\): group-specific parameter captures group-level differences (e.g., hospital level).\nThe intercept \\(\\theta_i\\) allows for each group to have its own baseline value.\nThis model introduces dependence within groups because observations from the same group share the same intercept \\(\\theta_i\\).\n\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{ij} | \\boldsymbol{\\Omega},\\theta_i] &= \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i\\\\\n&= (\\alpha + \\theta_i) + \\mathbf{x}_{ij} \\boldsymbol{\\beta}\\\\\n&= \\alpha_i + \\mathbf{x}_{ij} \\boldsymbol{\\beta}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14-hierarchical.html#identifiability-issues",
    "href": "slides/14-hierarchical.html#identifiability-issues",
    "title": "Hierarchical Models",
    "section": "Identifiability Issues",
    "text": "Identifiability Issues\n\nPopulation Intercept (\\(\\alpha\\)): This is the average intercept for the entire population, i.e., the baseline outcome across all hospitals.\nGroup-Specific Intercept (\\(\\alpha + \\theta_i\\)): The group-specific intercept, where \\(\\theta_i\\) represents the deviation from the population intercept for group \\(i\\).\n\nWe face an identifiability issue when estimating the population intercept and group-specific intercepts. We could add the same constant to all \\(\\theta_i\\)‚Äôs and subtract that constant from \\(\\alpha\\).\n\nThis is solved by setting \\(\\theta_i\\) to be mean zero apriori."
  },
  {
    "objectID": "slides/14-hierarchical.html#group-specific-intercept-model-conditional-specification-2",
    "href": "slides/14-hierarchical.html#group-specific-intercept-model-conditional-specification-2",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\n\nDefine \\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) and \\(\\mathbf{Y} = (\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n)\\).\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n \\prod_{j = 1}^{n_i} f(Y_{ij} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})  \\prod_{i=1}^n f(\\theta_i | \\tau^2) f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_n)\\)."
  },
  {
    "objectID": "slides/14-hierarchical.html#group-specific-intercept-model-marginal-specification",
    "href": "slides/14-hierarchical.html#group-specific-intercept-model-marginal-specification",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\nTo derive a marginal model it is useful to write the model at the level of the independent observations, \\(\\mathbf{Y}_i\\).\n\\[\\mathbf{Y}_i = \\begin{bmatrix}\n    Y_{i1}\\\\\n    Y_{i2}\\\\\n    \\vdots\\\\\n    Y_{in_i}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    \\alpha + \\mathbf{x}_{i1} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{i1}\\\\\n    \\alpha + \\mathbf{x}_{i2} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{i2}\\\\\n    \\vdots \\\\\n    \\alpha + \\mathbf{x}_{in_i} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{in_i}\n  \\end{bmatrix} = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i \\boldsymbol{\\beta} + \\theta_i \\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}_i,\\] where \\(\\mathbf{1}_{n_i}\\) is an \\(n_i \\times 1\\) dimensional vector of ones, \\(\\mathbf{X}_i\\) is an \\(n_i \\times p\\) dimensional matrix with rows \\(\\mathbf{x}_{ij}\\).\n\n\\(\\boldsymbol{\\epsilon}_i = (\\epsilon_{i1},\\ldots,\\epsilon_{in_i}) \\stackrel{ind}{\\sim} N(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i})\\), with \\(\\mathbf{0}_{n_i}\\) an \\(n_i \\times 1\\) dimensional vector of zeros."
  },
  {
    "objectID": "slides/14-hierarchical.html#group-specific-intercept-model-marginal-specification-1",
    "href": "slides/14-hierarchical.html#group-specific-intercept-model-marginal-specification-1",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\n\nMoments for the Marginal Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}] &= \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i\\boldsymbol{\\beta}\\\\\n\\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) &= \\tau^2 \\mathbf{1}_{n_i} \\mathbf{1}_{n_i}^\\top + \\sigma^2 \\mathbf{I}_{n_i} = \\boldsymbol{\\Upsilon}_i\\\\\n\\mathbb{C}(\\mathbf{Y}_{i}, \\mathbf{Y}_{i'} | \\boldsymbol{\\Omega}) &= \\mathbf{0}_{n_i \\times n_i},\\quad i \\neq i'.\n\\end{aligned}\\]\n\\[\\implies \\boldsymbol{\\Upsilon}_i = \\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) = \\begin{bmatrix}\n    \\tau^2 + \\sigma^2 & \\tau^2 & \\cdots & \\tau^2\\\\\n    \\tau^2 & \\tau^2 + \\sigma^2 & \\cdots & \\tau^2\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    \\tau^2 & \\tau^2 & \\cdots &\\tau^2 + \\sigma^2\n  \\end{bmatrix}.\\]"
  },
  {
    "objectID": "slides/14-hierarchical.html#covariance-structure",
    "href": "slides/14-hierarchical.html#covariance-structure",
    "title": "Hierarchical Models",
    "section": "Covariance Structure",
    "text": "Covariance Structure\n\nThe variance \\(\\tau^2\\) for \\(\\theta_i\\) can be interpreted as the covariance between two observations from the same hospital.\nThis reflects how much two observations from the same group are expected to be similar in terms of their outcomes.\n\n\\[\\begin{aligned}\n\\mathbb{C}(Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega}) &= \\mathbb{V}(\\theta_i)\\\\\n&= \\tau^2.\n\\end{aligned}\\]\n\nThus, \\(\\tau^2\\) dictates the within-group correlation in our model.\nNote: \\(\\mathbb{C}(Y_{ij}, Y_{i'k} | \\boldsymbol{\\Omega}) = 0\\) for \\(i \\neq i'\\)."
  },
  {
    "objectID": "slides/14-hierarchical.html#induced-within-correlation",
    "href": "slides/14-hierarchical.html#induced-within-correlation",
    "title": "Hierarchical Models",
    "section": "Induced Within Correlation",
    "text": "Induced Within Correlation\n\\[\\begin{aligned}\n\\rho (Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega}) &= \\frac{\\mathbb{C}(Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega})}{\\sqrt{\\mathbb{V}(Y_{ij} |  \\boldsymbol{\\Omega}) \\mathbb{V}(Y_{ik} |  \\boldsymbol{\\Omega})}}\\\\\n&=\\frac{\\tau^2}{\\tau^2 + \\sigma^2}\\\\\n&= \\frac{1}{1 + \\frac{\\sigma^2}{\\tau^2}}.\n\\end{aligned}\\]\nThis model induces positive correlation within group observations."
  },
  {
    "objectID": "slides/14-hierarchical.html#induced-within-correlation-1",
    "href": "slides/14-hierarchical.html#induced-within-correlation-1",
    "title": "Hierarchical Models",
    "section": "Induced Within Correlation",
    "text": "Induced Within Correlation\n\\[\\rho (Y_{ij}, Y_{ik} | \\alpha,\\boldsymbol{\\beta},\\sigma) = \\frac{1}{1 + \\frac{\\sigma^2}{\\tau^2}}\\]"
  },
  {
    "objectID": "slides/14-hierarchical.html#group-specific-intercept-model-marginal-specification-2",
    "href": "slides/14-hierarchical.html#group-specific-intercept-model-marginal-specification-2",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\nFor \\(i = 1,\\ldots,n\\), \\[\\begin{aligned}\n\\mathbf{Y}_{i} | \\boldsymbol{\\Omega} &\\stackrel{ind}{\\sim} N(\\alpha \\mathbf{1}_{n_i}+ \\mathbf{X}_i\\boldsymbol{\\beta},\\boldsymbol{\\Upsilon}_i)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta},\\sigma,\\tau)\\) are the population parameters."
  },
  {
    "objectID": "slides/14-hierarchical.html#group-specific-intercept-model-marginal-specification-3",
    "href": "slides/14-hierarchical.html#group-specific-intercept-model-marginal-specification-3",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\n\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n f(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega}).\n\\end{aligned}\\]\nWhy might we be interested in fitting the marginal model?"
  },
  {
    "objectID": "slides/14-hierarchical.html#recovering-the-group-specific-parameters",
    "href": "slides/14-hierarchical.html#recovering-the-group-specific-parameters",
    "title": "Hierarchical Models",
    "section": "Recovering the Group-Specific Parameters",
    "text": "Recovering the Group-Specific Parameters\n\nWe can still recover the \\(\\theta_i\\) when we fit the marginal model, we only need to compute \\(f(\\theta_i | \\mathbf{Y}_i,\\boldsymbol{\\Omega})\\) for all \\(i\\).\nWe can obtain this full conditional by specifying the joint distribution,\n\n\\[f\\left(\\begin{bmatrix}\n    \\mathbf{Y}_i\\\\\n    \\theta_i\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega}\\right) = N\\left(\\begin{bmatrix}\n    \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i \\boldsymbol{\\beta} + \\theta_i \\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}_i\\\\\n    0\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol{\\Upsilon}_i & \\tau^2 \\mathbf{1}_{n_i}\\\\\n    \\tau^2 \\mathbf{1}_{n_i}^\\top & \\tau^2\n  \\end{bmatrix}\\right).\\]\nWe can then use the conditional specification of a multivariate normal to find, \\(f(\\theta_i | \\mathbf{Y}_i, \\boldsymbol{\\Omega}) = N(\\mathbb{E}_{\\theta_i},\\mathbb{V}_{\\theta_i})\\), where\n\\[\\begin{aligned}\n\\mathbb{E}_{\\theta_i} &= \\mathbf{0}_{n_i} + \\tau^2 \\mathbf{1}_{n_i}^\\top \\boldsymbol{\\Upsilon}_i^{-1} (\\mathbf{Y}_i - \\alpha \\mathbf{1}_{n_i} - \\mathbf{X}_i \\boldsymbol{\\beta})\\\\\n\\mathbb{V}_{\\theta_i} &= \\tau^2 - \\tau^4 \\mathbf{1}_{n_i}^\\top \\boldsymbol{\\Upsilon}_i^{-1} \\mathbf{1}_{n_i}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14-hierarchical.html#example-data-glucose-measurement-in-4-primary-care-clinics",
    "href": "slides/14-hierarchical.html#example-data-glucose-measurement-in-4-primary-care-clinics",
    "title": "Hierarchical Models",
    "section": "Example data: Glucose Measurement in 4 Primary Care Clinics",
    "text": "Example data: Glucose Measurement in 4 Primary Care Clinics\n\nWe will study glucose values for patients being seen at 4 primary care clinics across the city. The clinics each represent a geographical region: east, west, north, and south.\nThe dataset consists of glucose measurements (mg/dl) from patients, and also risk factors:\n\nAge (years).\nBMI (\\(kg/m^2\\)).\nSex (0 = male, 1 = female).\nSmoking status (0 = non-smoker, 1 = smoker).\nPhysical activity level (0 = low, 1 = moderate, 2 = high).\nGlucose lowering medication (0 = none, 1 = yes)."
  },
  {
    "objectID": "slides/14-hierarchical.html#preview-the-data",
    "href": "slides/14-hierarchical.html#preview-the-data",
    "title": "Hierarchical Models",
    "section": "Preview the Data",
    "text": "Preview the Data"
  },
  {
    "objectID": "slides/14-hierarchical.html#writing-down-a-model",
    "href": "slides/14-hierarchical.html#writing-down-a-model",
    "title": "Hierarchical Models",
    "section": "Writing down a model",
    "text": "Writing down a model\nWe would like to fit the following model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\n\n\\(Y_{ij}\\) is the glucose value for patient \\(i\\) in clinic \\(j\\)\n\\(\\theta_i\\) for \\(i = 1,\\ldots,4\\) is the clinic-specific intercept deviation. \\[\\begin{aligned}\n\\mathbf{x}_{ij} &= (Age_{ij}, BMI_{ij}, Female_{ij},Smoker_{ij}, \\\\\n&\\quad Moderate\\_Activity_{ij}, High\\_Activity_{ij}, On\\_Meds_{ij}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14-hierarchical.html#fitting-the-conditional-model-in-stan",
    "href": "slides/14-hierarchical.html#fitting-the-conditional-model-in-stan",
    "title": "Hierarchical Models",
    "section": "Fitting the Conditional Model in Stan",
    "text": "Fitting the Conditional Model in Stan\n\n// conditional-model.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; N;\n  int&lt;lower = 1&gt; p;\n  matrix[N, p] X;\n  vector[N] Y;\n  int&lt;lower = 1, upper = n&gt; Ids[N];\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; tau;\n  vector[n] theta;\n}\nmodel {\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta + theta[Ids[i]];\n  }\n  target += normal_lpdf(Y | mu, sigma);\n  target += normal_lpdf(theta | 0, tau);\n  target += normal_lpdf(alpha | 0, 3);\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n}\ngenerated quantities {\n  real Intercept_East = alpha + theta[1];\n  real Intercept_North = alpha + theta[2];\n  real Intercept_South = alpha + theta[3];\n  real Intercept_West = alpha + theta[4];\n  real rho = 1 / (1 + ((sigma * sigma) / (tau * tau)));\n  vector[N] Y_pred;\n  vector[N] log_lik;\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta + theta[Ids[i]];\n    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);\n    Y_pred[i] = normal_rng(mu[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/14-hierarchical.html#fitting-the-model-in-stan",
    "href": "slides/14-hierarchical.html#fitting-the-model-in-stan",
    "title": "Hierarchical Models",
    "section": "Fitting the Model in Stan",
    "text": "Fitting the Model in Stan\n\nX &lt;- model.matrix(~ age + bmi + gender + smoking + as.factor(activity) + medication, data = data)[, -1]\nstan_data &lt;- list(\n  N = nrow(data),\n  n = length(unique(data$region)),\n  p = ncol(X),\n  X = X,\n  Y = data$glucose,\n  Ids = as.numeric(as.factor(data$region))\n)\nconditional_model &lt;- stan_model(model_code = \"conditional-model.stan\")\nfit_conditional &lt;- sampling(conditional_model, stan_data)"
  },
  {
    "objectID": "slides/14-hierarchical.html#assessing-convergence",
    "href": "slides/14-hierarchical.html#assessing-convergence",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(fit_conditional, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/14-hierarchical.html#assessing-convergence-1",
    "href": "slides/14-hierarchical.html#assessing-convergence-1",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_conditional, regex_pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/14-hierarchical.html#posterior-summaries",
    "href": "slides/14-hierarchical.html#posterior-summaries",
    "title": "Hierarchical Models",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nprint(fit_conditional, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\", \"rho\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nalpha    99.03    0.10 3.77  91.46  96.59  99.06 101.61 106.29  1533    1\nbeta[1]   0.26    0.00 0.03   0.20   0.23   0.26   0.28   0.32  4245    1\nbeta[2]   0.48    0.00 0.07   0.35   0.44   0.48   0.53   0.62  4989    1\nbeta[3]  10.50    0.01 0.65   9.18  10.06  10.51  10.93  11.75  5966    1\nbeta[4]  -4.75    0.01 0.66  -6.03  -5.20  -4.76  -4.30  -3.46  7324    1\nbeta[5]   0.28    0.01 0.78  -1.21  -0.26   0.29   0.80   1.82  4746    1\nbeta[6]  -4.53    0.01 0.77  -6.03  -5.06  -4.53  -4.00  -3.02  4869    1\nbeta[7] -17.89    0.01 0.64 -19.15 -18.31 -17.89 -17.47 -16.64  5387    1\nsigma     7.43    0.00 0.23   6.99   7.28   7.43   7.59   7.90  5900    1\ntau       4.58    0.02 1.33   2.59   3.60   4.35   5.34   7.65  2941    1\nrho       0.27    0.00 0.11   0.11   0.19   0.26   0.34   0.51  3136    1\n\nSamples were drawn using NUTS(diag_e) at Wed Feb 26 10:48:54 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/14-hierarchical.html#comparison-to-linear-regression-boldsymbolbeta",
    "href": "slides/14-hierarchical.html#comparison-to-linear-regression-boldsymbolbeta",
    "title": "Hierarchical Models",
    "section": "Comparison to Linear Regression: \\(\\boldsymbol{\\beta}\\)",
    "text": "Comparison to Linear Regression: \\(\\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "slides/14-hierarchical.html#model-comparison",
    "href": "slides/14-hierarchical.html#model-comparison",
    "title": "Hierarchical Models",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nlibrary(loo)\nwaic_conditional &lt;- waic(extract_log_lik(fit_conditional))\nwaic_lin_reg &lt;- waic(extract_log_lik(fit_lin_reg))\ncomparison &lt;- loo_compare(list(\"Hierarchical Model\" = waic_conditional, \"Linear Regression\" = waic_lin_reg))\nprint(comparison, simplify = FALSE)\n\n                   elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic\nHierarchical Model     0.0       0.0 -1720.6      15.2         11.5     0.7  \nLinear Regression    -71.6      10.7 -1792.2      15.6          8.6     0.6  \n                   waic    se_waic\nHierarchical Model  3441.2    30.4\nLinear Regression   3584.3    31.1"
  },
  {
    "objectID": "slides/14-hierarchical.html#explore-the-clinic-specific-variation",
    "href": "slides/14-hierarchical.html#explore-the-clinic-specific-variation",
    "title": "Hierarchical Models",
    "section": "Explore the Clinic-Specific Variation",
    "text": "Explore the Clinic-Specific Variation\n\nprint(fit_conditional, pars = c(\"Intercept_East\", \"Intercept_South\", \"Intercept_North\", \"Intercept_West\"), probs = c(0.025, 0.975))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n                  mean se_mean   sd  2.5%  97.5% n_eff Rhat\nIntercept_East  102.08    0.05 2.88 96.55 107.62  3461    1\nIntercept_South  98.46    0.05 2.88 92.89 103.99  3498    1\nIntercept_North  92.35    0.05 2.87 86.78  97.90  3398    1\nIntercept_West  103.55    0.05 2.90 97.85 109.10  3502    1\n\nSamples were drawn using NUTS(diag_e) at Wed Feb 26 10:48:54 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/14-hierarchical.html#explore-the-clinic-specific-variation-1",
    "href": "slides/14-hierarchical.html#explore-the-clinic-specific-variation-1",
    "title": "Hierarchical Models",
    "section": "Explore the Clinic-Specific Variation",
    "text": "Explore the Clinic-Specific Variation"
  },
  {
    "objectID": "slides/14-hierarchical.html#fitting-the-marginal-model-in-stan",
    "href": "slides/14-hierarchical.html#fitting-the-marginal-model-in-stan",
    "title": "Hierarchical Models",
    "section": "Fitting the Marginal Model in Stan",
    "text": "Fitting the Marginal Model in Stan\nNeed ragged data structure.\n\n// marginal-model.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; N;\n  int&lt;lower = 1&gt; p;\n  matrix[N, p] X;\n  vector[N] Y;\n  int n_is[n];\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; tau;\n}\ntransformed parameters {\n  real sigma2 = sigma * sigma;\n  real tau2 = tau * tau;\n}\nmodel {\n  // compute the mean process for the marginal model\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta;\n  }\n  // evaluate the likelihood for the marginal model using ragged data structure\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    matrix[n_i, n_i] Upsilon_i = sigma2 * diag_matrix(rep_vector(1.0, n_i)) + tau2 * rep_matrix(1, n_i, n_i);\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    vector[n_i] mu_i = segment(mu, pos, n_i);\n    target += multi_normal_lpdf(Y_i | mu_i, Upsilon_i);\n    pos = pos + n_i;\n  }\n  // priors\n  target += normal_lpdf(alpha | 0, 3);\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n}\ngenerated quantities {\n  // compute the mean process for the marginal model\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta;\n  }\n  // compute theta using the ragged data structure\n  int pos;\n  pos = 1;\n  vector[n] theta;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    vector[n_i] ones_i = rep_vector(1.0, n_i);\n    matrix[n_i, n_i] Upsilon_i = sigma2 * diag_matrix(ones_i) + tau2 * rep_matrix(1, n_i, n_i);\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    vector[n_i] mu_i = segment(mu, pos, n_i);\n    real mean_theta_i = tau2 * ones_i' * inverse_spd(Upsilon_i) * (Y_i - mu_i);\n    real var_theta_i = tau2 - tau2 * tau2 * ones_i' * inverse_spd(Upsilon_i) * ones_i;\n    theta[i] = normal_rng(mean_theta_i, sqrt(var_theta_i));\n    pos = pos + n_i;\n  }\n}"
  },
  {
    "objectID": "slides/14-hierarchical.html#fitting-the-model-in-stan-1",
    "href": "slides/14-hierarchical.html#fitting-the-model-in-stan-1",
    "title": "Hierarchical Models",
    "section": "Fitting the Model in Stan",
    "text": "Fitting the Model in Stan\n\nstan_data &lt;- list(\n  N = nrow(data),\n  n = length(unique(data$region)),\n  p = ncol(X),\n  X = X,\n  Y = data$glucose,\n  n_is = as.numeric(table(data$region))\n)\nmarginal_model &lt;- stan_model(model_code = \"marginal-model.stan\")\nfit_marginal &lt;- sampling(marginal_model, stan_data)"
  },
  {
    "objectID": "slides/14-hierarchical.html#assessing-convergence-2",
    "href": "slides/14-hierarchical.html#assessing-convergence-2",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(fit_marginal, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\"))"
  },
  {
    "objectID": "slides/14-hierarchical.html#assessing-convergence-3",
    "href": "slides/14-hierarchical.html#assessing-convergence-3",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_marginal, regex_pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\"))"
  },
  {
    "objectID": "slides/14-hierarchical.html#posterior-summaries-1",
    "href": "slides/14-hierarchical.html#posterior-summaries-1",
    "title": "Hierarchical Models",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nprint(fit_marginal, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nalpha    99.07    0.06 3.66  91.97  96.66  99.05 101.50 106.36  3285    1\nbeta[1]   0.26    0.00 0.03   0.20   0.24   0.26   0.28   0.32  3957    1\nbeta[2]   0.48    0.00 0.07   0.35   0.44   0.48   0.53   0.61  3389    1\nbeta[3]  10.48    0.01 0.67   9.18  10.01  10.49  10.95  11.76  4532    1\nbeta[4]  -4.75    0.01 0.65  -6.01  -5.18  -4.75  -4.34  -3.45  4611    1\nbeta[5]   0.29    0.01 0.77  -1.26  -0.23   0.28   0.79   1.77  3834    1\nbeta[6]  -4.51    0.01 0.79  -6.04  -5.04  -4.50  -4.00  -3.01  3855    1\nbeta[7] -17.88    0.01 0.66 -19.14 -18.33 -17.88 -17.44 -16.56  4370    1\nsigma     7.43    0.00 0.24   6.99   7.27   7.43   7.60   7.92  4223    1\ntau       4.55    0.02 1.27   2.61   3.62   4.38   5.27   7.48  3714    1\n\nSamples were drawn using NUTS(diag_e) at Wed Feb 26 11:25:39 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/14-hierarchical.html#conclusion",
    "href": "slides/14-hierarchical.html#conclusion",
    "title": "Hierarchical Models",
    "section": "Conclusion",
    "text": "Conclusion\n\nBy introducing a group-specific intercept, we allow for dependencies between observations within groups, making the model more realistic for real-world clustered or repeated measures data.\nFor the remainder of the class, we will expand upon this hierarchical modeling framework to account for complext data types that are frequently encountered in research, including longitudinal and spatial data."
  },
  {
    "objectID": "slides/14-hierarchical.html#prepare-for-next-class",
    "href": "slides/14-hierarchical.html#prepare-for-next-class",
    "title": "Hierarchical Models",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nComplete HW 03, which is due before Thursday‚Äôs class.\nNext Thursday‚Äôs lecture: Longitudinal Data\nExam 1 will be assigned after class on Thursday!"
  },
  {
    "objectID": "prepare/prepare-feb26.html",
    "href": "prepare/prepare-feb26.html",
    "title": "Prepare for February 26 lecture",
    "section": "",
    "text": "üìñ Review the linear mixed model.\nüìñ Read pages 183-192 from Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists.\n‚úÖ Turn in HW 03 before class on Thursday. Prepare for Exam 1, which will be assigned after class on Thursday."
  },
  {
    "objectID": "slides/15-longitudinal.html",
    "href": "slides/15-longitudinal.html",
    "title": "Longitudinal Data",
    "section": "",
    "text": "‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nLoaded lars 1.3\n\n\n\nAttaching package: 'LaplacesDemon'\n\n\nThe following objects are masked from 'package:mvtnorm':\n\n    dmvt, logdet, rmvt\n\n\nThe following objects are masked from 'package:lubridate':\n\n    dst, interval\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\n\nnimble version 1.2.1 is loaded.\nFor more information on NIMBLE and a User Manual,\nplease visit https://R-nimble.org.\n\nNote for advanced users who have written their own MCMC samplers:\n  As of version 0.13.0, NIMBLE's protocol for handling posterior\n  predictive nodes has changed in a way that could affect user-defined\n  samplers in some situations. Please see Section 15.5.1 of the User Manual.\n\n\nAttaching package: 'nimble'\n\n\nThe following objects are masked from 'package:LaplacesDemon':\n\n    cloglog, dcat, dinvgamma, is.model, logdet, logit, rcat, rinvgamma\n\n\nThe following object is masked from 'package:mvtnorm':\n\n    logdet\n\n\nThe following object is masked from 'package:stats':\n\n    simulate\n\n\nThe following object is masked from 'package:base':\n\n    declare\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n\nThis is bayesplot version 1.11.1\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n- bayesplot theme set to bayesplot::theme_default()\n\n   * Does _not_ affect other ggplot2 plots\n\n   * See ?bayesplot_theme_set for details on theme setting"
  },
  {
    "objectID": "slides/15-longitudinal.html#review-of-last-lecture",
    "href": "slides/15-longitudinal.html#review-of-last-lecture",
    "title": "Longitudinal Data",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nDuring our last lecture, we introduced correlated (or dependent) data sources.\nWe discussed the idea of accounting for dependencies within a group using group-specific parameters.\nWe introduced the random intercept model and studied the induced correlation (forced to be positive) in the marginal model.\nToday we will look at longitudinal data and introduce a simple model that accounts for group-level changes."
  },
  {
    "objectID": "slides/15-longitudinal.html#longitudinal-data",
    "href": "slides/15-longitudinal.html#longitudinal-data",
    "title": "Longitudinal Data",
    "section": "Longitudinal Data",
    "text": "Longitudinal Data\nRepeated measurements taken over time from the same subjects. Examples include:\n\nMonitor Disease Progression: Track how diseases evolve, such as diabetes or glaucoma.\nEvaluate Treatments: Understand how interventions work over time.\nPersonalized Health Insights: Capture individual health trajectories for personalized care.\nStudy Long-Term Effects: Evaluate the long-term outcomes of medical treatments or behaviors."
  },
  {
    "objectID": "slides/15-longitudinal.html#example-glaucoma-disease-progression",
    "href": "slides/15-longitudinal.html#example-glaucoma-disease-progression",
    "title": "Longitudinal Data",
    "section": "Example: Glaucoma Disease Progression",
    "text": "Example: Glaucoma Disease Progression\nImagine we are tracking mean deviation (MD, dB), a key measure of visual field loss in glaucoma patients, over time.\n\nMultiple measurements of MD for each patient across several years.\nWe‚Äôre interested in glaucoma progression, which is defined as the rate of change in MD over time (dB/year).\nDefine \\(Y_{it}\\) as the MD value for eye \\(i\\) (\\(i = 1,\\ldots,n\\)) at time \\(t\\) (\\(t = 1,\\ldots,n_i\\)) and the time of each observation as \\(X_{it}\\) with \\(X_{i0} = 0\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#rotterdam-data",
    "href": "slides/15-longitudinal.html#rotterdam-data",
    "title": "Longitudinal Data",
    "section": "Rotterdam data",
    "text": "Rotterdam data"
  },
  {
    "objectID": "slides/15-longitudinal.html#treating-eyes-separately",
    "href": "slides/15-longitudinal.html#treating-eyes-separately",
    "title": "Longitudinal Data",
    "section": "Treating Eyes Separately",
    "text": "Treating Eyes Separately\nWe can model each eye separately using OLS (this is a form of longitudinal analysis!). For \\(t = 1,\\ldots,n_i\\), the model is:\n\\[Y_{it} = \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma_i^2).\\]\nWhere:\n\n\\(\\beta_{0i}\\) is the intercept for eye \\(i\\).\n\\(\\beta_{1i}\\) is the slope for eye \\(i\\) (i.e., disease progression).\n\\(\\sigma_i^2\\) is the residual error for eye \\(i\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#ols-regression",
    "href": "slides/15-longitudinal.html#ols-regression",
    "title": "Longitudinal Data",
    "section": "OLS regression",
    "text": "OLS regression"
  },
  {
    "objectID": "slides/15-longitudinal.html#treating-eyes-separately-1",
    "href": "slides/15-longitudinal.html#treating-eyes-separately-1",
    "title": "Longitudinal Data",
    "section": "Treating Eyes Separately",
    "text": "Treating Eyes Separately\n\nFitting OLS separately allows each eye to have a unique intercept and slope, which of course is consistent with the data generating process.\nHowever, this can lead to eye-specific intercepts and slopes that are not realistic (consider OLS regression with very few data points).\nEstimating eye-specific intercepts and slopes within the context of the whole study sample should shrink extreme values toward the population average."
  },
  {
    "objectID": "slides/15-longitudinal.html#subject-specific-intercepts-and-slopes",
    "href": "slides/15-longitudinal.html#subject-specific-intercepts-and-slopes",
    "title": "Longitudinal Data",
    "section": "Subject-specific intercepts and slopes",
    "text": "Subject-specific intercepts and slopes\nFor \\(i = 1,\\ldots,n\\) and \\(t=1,\\ldots,n_i\\), we can write the model:\n\\[\\begin{aligned}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{aligned}\\]\nPopulation Parameters:\n\n\\(\\beta_0\\) is the population intercept (i.e., average MD value in the population at time zero).\n\\(\\beta_1\\) is the population slope (i.e., average disease progression).\n\\(\\sigma^2\\) is the population residual error."
  },
  {
    "objectID": "slides/15-longitudinal.html#subject-specific-intercepts-and-slopes-1",
    "href": "slides/15-longitudinal.html#subject-specific-intercepts-and-slopes-1",
    "title": "Longitudinal Data",
    "section": "Subject-specific intercepts and slopes",
    "text": "Subject-specific intercepts and slopes\nFor \\(i = 1,\\ldots,n\\) and \\(t=1,\\ldots,n_i\\), we can write the model:\n\\[\\begin{aligned}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{aligned}\\]\nSubject-Specific Parameters:\n\n\\(\\theta_{0i}\\) is the subject-specific deviation from the intercept for eye \\(i\\).\n\\(\\theta_{1i}\\) is the subject-specific deviation from the slope for eye \\(i\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#subject-specific-intercepts-and-slopes-2",
    "href": "slides/15-longitudinal.html#subject-specific-intercepts-and-slopes-2",
    "title": "Longitudinal Data",
    "section": "Subject-specific intercepts and slopes",
    "text": "Subject-specific intercepts and slopes\nFor \\(i = 1,\\ldots,n\\) and \\(t=1,\\ldots,n_i\\), we can write the model:\n\\[\\begin{aligned}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{aligned}\\]\nKey Advantage:\n\nThis model defines subject-specific estimates of \\(\\beta_{0i}\\) and \\(\\beta_{1i}\\) relative to the population average, preventing overfitting and making the estimates more stable.\nShrinks subject-specific parameters to the population average."
  },
  {
    "objectID": "slides/15-longitudinal.html#linear-mixed-model",
    "href": "slides/15-longitudinal.html#linear-mixed-model",
    "title": "Longitudinal Data",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nThe subject-specific intercepts and slope model can be seen as a special case of the linear mixed model (LMM). For \\(i = 1,\\ldots,n\\), LMM is defined as:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\n\\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) are subject-level observations.\n\\(Y_{it}\\) is the \\(t\\)th observation in subject \\(i\\).\n\\(\\boldsymbol{\\epsilon}_i = (\\epsilon_{i1},\\ldots,\\epsilon_{in_i})\\), such that \\(\\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#linear-mixed-model-1",
    "href": "slides/15-longitudinal.html#linear-mixed-model-1",
    "title": "Longitudinal Data",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\n\\(\\mathbf{X}_i\\) is an \\((n_i \\times p)\\)-dimensional matrix with row \\(\\mathbf{x}_{it}\\) (intercept is incorporated).\n\\(\\mathbf{x}_{it}\\) contains variables that are assumed to relate to the outcome only at a population-level.\n\\(p\\) is the number of population-level variables."
  },
  {
    "objectID": "slides/15-longitudinal.html#linear-mixed-model-2",
    "href": "slides/15-longitudinal.html#linear-mixed-model-2",
    "title": "Longitudinal Data",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\n\\(\\mathbf{Z}_i\\) is an \\((n_i \\times q)\\)-dimensional matrix with row \\(\\mathbf{z}_{it}\\) (intercept is incorporated).\n\\(\\mathbf{z}_{it}\\) contains variables that are assumed to relate to the outcome with varying effects at a subject-level.\n\\(q\\) is the number of subject-level variables."
  },
  {
    "objectID": "slides/15-longitudinal.html#linear-mixed-model-3",
    "href": "slides/15-longitudinal.html#linear-mixed-model-3",
    "title": "Longitudinal Data",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\n\\(\\boldsymbol{\\beta}\\) is a \\(p\\)-dimensional vector of population-level parameters (or fixed effects).\n\\(\\boldsymbol{\\theta}_i\\) is a \\(q\\)-dimensional vector of group-level parameters (or random effects).\n\\(\\sigma^2\\) is a population-level parameter that measures residual error."
  },
  {
    "objectID": "slides/15-longitudinal.html#recover-the-random-intercept-model",
    "href": "slides/15-longitudinal.html#recover-the-random-intercept-model",
    "title": "Longitudinal Data",
    "section": "Recover the Random Intercept Model",
    "text": "Recover the Random Intercept Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\nSuppose that \\(\\mathbf{z}_{it} = 1 \\forall i,t\\). Then we get\n\\[\\begin{aligned}\nY_{it} &= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\mathbf{z}_{it}\\boldsymbol{\\theta}_{i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n&= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\theta_{i} + \\epsilon_{it}.\n\\end{aligned}\\]\n\nLMM is a general form of the random intercept model."
  },
  {
    "objectID": "slides/15-longitudinal.html#random-slope-and-intercept-model",
    "href": "slides/15-longitudinal.html#random-slope-and-intercept-model",
    "title": "Longitudinal Data",
    "section": "Random Slope and Intercept Model",
    "text": "Random Slope and Intercept Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\nSuppose that \\(\\mathbf{x}_{it} = \\mathbf{z}_{it} = (1, X_{it})\\), such that \\(p = q = 2\\). Then,\n\\[\\begin{aligned}\nY_{it} &= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\mathbf{z}_{it}\\boldsymbol{\\theta}_{i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n&= \\beta_0 + \\beta_1 X_{it} + \\theta_{0i} + \\theta_{1i} X_{it} + \\epsilon_{it}\\\\\n&= (\\beta_0 + \\theta_{0i}) + (\\beta_1 + \\theta_{1i}) X_{it} + \\epsilon_{it}.\n\\end{aligned}\\]\nwhere \\(\\boldsymbol{\\beta} = (\\beta_0,\\beta_1)\\) and \\(\\boldsymbol{\\theta}_i = (\\theta_{0i},\\theta_{1i})\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#prior-specification",
    "href": "slides/15-longitudinal.html#prior-specification",
    "title": "Longitudinal Data",
    "section": "Prior Specification",
    "text": "Prior Specification\nOne choice could be to specify independent priors for the subject-specific intercepts and slopes:\n\\[\\begin{aligned}\n\\theta_{0i} &\\stackrel{iid}{\\sim} N(0, \\tau_0^2)\\\\\n\\theta_{1i} &\\stackrel{iid}{\\sim} N(0, \\tau_1^2).\n\\end{aligned}\\]\n\nThis is the same assumption we made last lecture, where we assume a normal distribution centered at zero with some variance that reflects variability across subjects.\nOften times this assumption is oversimplified. For example in glaucoma progression, we often assume that if someone has a higher baseline MD they will a more negative slope (i.e., negative correlation)."
  },
  {
    "objectID": "slides/15-longitudinal.html#prior-specification-1",
    "href": "slides/15-longitudinal.html#prior-specification-1",
    "title": "Longitudinal Data",
    "section": "Prior Specification",
    "text": "Prior Specification\nWe can instead model the subject-specific parameters as correlated themselves using a bi-variate normal distribution. Define \\(\\boldsymbol{\\theta}_i = (\\theta_{0i},\\theta_{1i})^\\top\\) and then \\(\\boldsymbol{\\theta}_i \\stackrel{iid}{\\sim} N_2(\\mathbf{0}_2,\\boldsymbol{\\Sigma})\\).\n\\[\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n    \\tau_{0}^2 & \\tau_{01}\\\\\n    \\tau_{01} & \\tau_1^2\\\\\n  \\end{bmatrix}.\\]\n\n\\(\\tau_{01} = \\rho \\tau_0 \\tau_1\\).\n\\(\\rho\\) is the correlation between the subject-specific intercepts and slopes.\n\nLet‚Äôs talk about efficient ways to generate multivariate random variables!"
  },
  {
    "objectID": "slides/15-longitudinal.html#generating-multivariate-normal-rngs",
    "href": "slides/15-longitudinal.html#generating-multivariate-normal-rngs",
    "title": "Longitudinal Data",
    "section": "Generating Multivariate Normal RNGs",
    "text": "Generating Multivariate Normal RNGs\nSuppose we would like to generate samples of a random variable \\(\\mathbf{x}_i \\stackrel{iid}{\\sim} N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\).\nTo sample efficiently, we can decompose the covariance structure:\n\\[\\begin{aligned}\n\\boldsymbol{\\Sigma} &= \\begin{bmatrix}\n    \\tau_{0}^2 & \\rho \\tau_0 \\tau_1\\\\\n    \\rho \\tau_0 \\tau_1 & \\tau_1^2\\\\\n  \\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n    \\tau_{0} & 0\\\\\n    0 & \\tau_1\\\\\n  \\end{bmatrix}  \\begin{bmatrix}\n    1 & \\rho\\\\\n    \\rho & 1\\\\\n  \\end{bmatrix}  \\begin{bmatrix}\n    \\tau_{0} & 0\\\\\n    0 & \\tau_1\\\\\n  \\end{bmatrix}\\\\\n&=  \\mathbf{D} \\boldsymbol{\\Phi} \\mathbf{D}.\n\\end{aligned}\\]\n\n\\(\\mathbf{D}\\) is a \\(p\\)-dimensional matrix with the standard deviations on the diagonal.\n\\(\\boldsymbol{\\Phi}\\) is the correlation matrix."
  },
  {
    "objectID": "slides/15-longitudinal.html#generating-multivariate-normal-rngs-1",
    "href": "slides/15-longitudinal.html#generating-multivariate-normal-rngs-1",
    "title": "Longitudinal Data",
    "section": "Generating Multivariate Normal RNGs",
    "text": "Generating Multivariate Normal RNGs\nWe can further decompose the covariance by computing the cholesky decomposition of the correlation matrix:\n\\[\\begin{aligned}\n\\boldsymbol{\\Sigma} &=  \\mathbf{D} \\boldsymbol{\\Phi} \\mathbf{D}\\\\\n&= \\mathbf{D} \\mathbf{L} \\mathbf{L}^\\top \\mathbf{D},\n\\end{aligned}\\] where \\(\\mathbf{L}\\) is the lower triangular Cholesky decomposition for \\(\\boldsymbol{\\Phi}\\), such that \\(\\boldsymbol{\\Phi} = \\mathbf{L} \\mathbf{L}^\\top\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#generating-multivariate-normal-rngs-2",
    "href": "slides/15-longitudinal.html#generating-multivariate-normal-rngs-2",
    "title": "Longitudinal Data",
    "section": "Generating Multivariate Normal RNGs",
    "text": "Generating Multivariate Normal RNGs\nWe can generate samples \\(\\mathbf{x}_i \\stackrel{iid}{\\sim} N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) using the following approach:\n\\[\\mathbf{x}_i = \\boldsymbol{\\mu} + \\mathbf{D} \\mathbf{L} \\mathbf{z}_i,\\]\nwhere \\(\\mathbf{z}_i = (z_{0i},z_{1i})\\) and \\(z_{ij} \\stackrel{iid}{\\sim} N(0,1)\\), so that \\(\\mathbb{E}[\\mathbf{z}_i] = \\mathbf{0}_2\\) and \\(\\mathbb{C}(\\mathbf{z}_i) = \\mathbf{I}_2\\).\n\\[\\begin{aligned}\n\\mathbb{E}[\\boldsymbol{\\mu} + \\mathbf{D}\\mathbf{L}\\mathbf{z}_i] &= \\boldsymbol{\\mu} +  \\mathbf{D}\\mathbf{L}\\mathbb{E}[\\mathbf{z}_i] = \\boldsymbol{\\mu}\\\\\n\\mathbb{C}(\\boldsymbol{\\mu} + \\mathbf{D}\\mathbf{L}\\mathbf{z}_i) &= \\mathbf{D}\\mathbf{L}\\mathbb{C}(\\mathbf{z}_i)\\left(\\mathbf{D}\\mathbf{L}\\right)^\\top \\\\\n&= \\mathbf{D}\\mathbf{L}\\mathbf{L}^\\top\\mathbf{D}\\\\\n&=\\boldsymbol{\\Sigma}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-longitudinal.html#generating-multivariate-normal-rngs-3",
    "href": "slides/15-longitudinal.html#generating-multivariate-normal-rngs-3",
    "title": "Longitudinal Data",
    "section": "Generating Multivariate Normal RNGs",
    "text": "Generating Multivariate Normal RNGs\n\nSigma &lt;- matrix(c(3, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\nmu &lt;- matrix(c(2, 5), ncol = 1)\nD &lt;- matrix(0, nrow = 2, ncol = 2)\ndiag(D) &lt;- diag(sqrt(Sigma))\nPhi &lt;- cov2cor(Sigma)\nL &lt;- t(chol(Phi))\nn_samples &lt;- 1000\nz &lt;- matrix(rnorm(n_samples * 2), nrow = 2, ncol = n_samples)\nmu_mat &lt;- matrix(rep(mu, n_samples), nrow = 2, ncol = n_samples) \nX &lt;- mu_mat + D %*% L %*% z\napply(X, 1, mean)\n\n[1] 1.946052 4.950226\n\ncov(t(X))\n\n          [,1]      [,2]\n[1,] 3.0612065 0.9283299\n[2,] 0.9283299 2.9567120"
  },
  {
    "objectID": "slides/15-longitudinal.html#conditional-specification",
    "href": "slides/15-longitudinal.html#conditional-specification",
    "title": "Longitudinal Data",
    "section": "Conditional specification",
    "text": "Conditional specification\nFor the conditional specification, we can write the model at the observational level, \\(Y_{it}\\). This is because conditionally on the \\(\\boldsymbol{\\theta}_i\\), \\(Y_{it}\\) and \\(Y_{it'}\\) are independent.\nFor \\(i\\) (\\(i = 1,\\ldots,n\\)) and \\(t\\) (\\(t = 1,\\ldots, n_i\\)), the model is:\n\\[\\begin{aligned}\nY_{it} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}_i &\\stackrel{iid}{\\sim} N\\left((\\beta_{0} + \\theta_{0i}) + (\\beta_1 + \\theta_{0i}) X_{it}, \\sigma^2\\right),\\\\\n\\boldsymbol{\\theta}_i | \\boldsymbol{\\Sigma} &\\stackrel{iid}{\\sim} N_2(\\mathbf{0}_2,\\boldsymbol{\\Sigma})\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\]\nwhere \\(\\boldsymbol{\\Omega} = (\\beta_0, \\beta_1, \\sigma^2, \\boldsymbol{\\Sigma})\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#conditional-specification-1",
    "href": "slides/15-longitudinal.html#conditional-specification-1",
    "title": "Longitudinal Data",
    "section": "Conditional Specification",
    "text": "Conditional Specification\n\nMoments for the Conditional Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{it} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i] &= (\\beta_0 + \\theta_{0i}) + (\\beta_1 + \\theta_{1i}) X_{it}\\\\\n\\mathbb{V}(Y_{it} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i) &= \\sigma^2\\\\\n\\mathbb{C}(Y_{it}, Y_{jt'} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i,\\boldsymbol{\\theta}_{t'}) &= 0,\\quad \\forall i,j,t,t'\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-longitudinal.html#conditional-specification-2",
    "href": "slides/15-longitudinal.html#conditional-specification-2",
    "title": "Longitudinal Data",
    "section": "Conditional Specification",
    "text": "Conditional Specification\n\nDefine \\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) and \\(\\mathbf{Y} = (\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n)\\).\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n \\prod_{t = 1}^{n_i} f(Y_{it} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})  \\prod_{i=1}^n f(\\boldsymbol{\\theta}_i | \\boldsymbol{\\Sigma}) f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\theta}_1,\\ldots,\\boldsymbol{\\theta}_n)\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#marginal-specification",
    "href": "slides/15-longitudinal.html#marginal-specification",
    "title": "Longitudinal Data",
    "section": "Marginal Specification",
    "text": "Marginal Specification\nThe LMM model is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\nMoments for the Marginal Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}] &= \\mathbf{X}_i\\boldsymbol{\\beta}\\\\\n\\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) &= \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top + \\sigma^2 \\mathbf{I}_{n_i} = \\boldsymbol{\\Upsilon}_i\\\\\n\\mathbb{C}(\\mathbf{Y}_{i}, \\mathbf{Y}_{i'} | \\boldsymbol{\\Omega}) &= \\mathbf{0}_{n_i \\times n_i},\\quad i \\neq i'.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-longitudinal.html#marginal-specification-1",
    "href": "slides/15-longitudinal.html#marginal-specification-1",
    "title": "Longitudinal Data",
    "section": "Marginal Specification",
    "text": "Marginal Specification\nFor \\(i = 1,\\ldots,n\\), \\[\\begin{aligned}\n\\mathbf{Y}_{i} | \\boldsymbol{\\Omega} &\\stackrel{ind}{\\sim} N(\\mathbf{X}_i\\boldsymbol{\\beta},\\boldsymbol{\\Upsilon}_i)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\Omega} = (\\boldsymbol{\\beta},\\sigma,\\boldsymbol{\\Sigma})\\) are the population parameters."
  },
  {
    "objectID": "slides/15-longitudinal.html#recovering-the-subject-specific-parameters",
    "href": "slides/15-longitudinal.html#recovering-the-subject-specific-parameters",
    "title": "Longitudinal Data",
    "section": "Recovering the Subject-Specific Parameters",
    "text": "Recovering the Subject-Specific Parameters\n\nWe can still recover the \\(\\boldsymbol{\\theta}_i\\) when we fit the marginal model, we only need to compute \\(f(\\boldsymbol{\\theta}_i | \\mathbf{Y}_i,\\boldsymbol{\\Omega})\\) for all \\(i\\).\nWe can obtain this full conditional by specifying the joint distribution,\n\n\\[f\\left(\\begin{bmatrix}\n    \\mathbf{Y}_i\\\\\n    \\boldsymbol{\\theta}_i\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega}\\right) = N\\left(\\begin{bmatrix}\n    \\mathbf{X}_i \\boldsymbol{\\beta} \\\\\n    \\mathbf{0}_{n_1}\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol{\\Upsilon}_i & \\mathbf{Z}_i\\boldsymbol{\\Sigma}\\\\\n    \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top & \\boldsymbol{\\Sigma}\n  \\end{bmatrix}\\right).\\]\nWe can then use the conditional specification of a multivariate normal to find, \\(f(\\boldsymbol{\\theta}_i | \\mathbf{Y}_i, \\boldsymbol{\\Omega}) = N(\\mathbb{E}_{\\boldsymbol{\\theta}_i},\\mathbb{V}_{\\boldsymbol{\\theta}_i})\\), where\n\\[\\begin{aligned}\n\\mathbb{E}_{\\boldsymbol{\\theta}_i} &= \\mathbf{0}_{n_i} + \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top \\boldsymbol{\\Upsilon}_i^{-1} (\\mathbf{Y}_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\\\\n\\mathbb{V}_{\\boldsymbol{\\theta}_i} &= \\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top \\boldsymbol{\\Upsilon}_i^{-1} \\mathbf{Z}_i\\boldsymbol{\\Sigma}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-longitudinal.html#marginal-specification-2",
    "href": "slides/15-longitudinal.html#marginal-specification-2",
    "title": "Longitudinal Data",
    "section": "Marginal Specification",
    "text": "Marginal Specification\nIt is not as straightforward to gain intuition behind the induced correlation structure, but we can shed some light by studying the scalar form of the covariance:\n\\[\\begin{aligned}\n\\mathbb{V}(Y_{it}| \\boldsymbol{\\Omega}) &= \\tau_0^2 + 2 \\tau_{01} X_{it}^2 + \\tau_1^2 X_{it}^2 + \\sigma^2,\\\\\n\\mathbb{C}(Y_{it}, Y_{it'} | \\boldsymbol{\\Omega}) &= \\tau_0^2 - \\rho \\tau_0 \\tau_1 (X_{it} - X_{it'}) + \\tau_1^2 X_{it} X_{it'}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-longitudinal.html#visualizing-the-dependency",
    "href": "slides/15-longitudinal.html#visualizing-the-dependency",
    "title": "Longitudinal Data",
    "section": "Visualizing the dependency",
    "text": "Visualizing the dependency\n\\(\\tau_0 = 1,\\tau_1 = 1,\\sigma^2 =2, \\rho = 0.5\\)"
  },
  {
    "objectID": "slides/15-longitudinal.html#visualizing-the-dependency-1",
    "href": "slides/15-longitudinal.html#visualizing-the-dependency-1",
    "title": "Longitudinal Data",
    "section": "Visualizing the dependency",
    "text": "Visualizing the dependency\n\\(\\tau_0 = 1,\\tau_1 = 1,\\sigma^2 =2, \\rho = -0.5\\)"
  },
  {
    "objectID": "slides/15-longitudinal.html#marginal-specification-3",
    "href": "slides/15-longitudinal.html#marginal-specification-3",
    "title": "Longitudinal Data",
    "section": "Marginal Specification",
    "text": "Marginal Specification\n\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n f(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-longitudinal.html#specifying-a-prior-distribution-for-boldsymbolomega",
    "href": "slides/15-longitudinal.html#specifying-a-prior-distribution-for-boldsymbolomega",
    "title": "Longitudinal Data",
    "section": "Specifying a Prior Distribution for \\(\\boldsymbol{\\Omega}\\)",
    "text": "Specifying a Prior Distribution for \\(\\boldsymbol{\\Omega}\\)\n\nWe must set a prior for \\(f(\\boldsymbol{\\Omega}) = f(\\boldsymbol{\\beta}) f(\\sigma) f(\\boldsymbol{\\Sigma})\\).\nWe can place standard priors on \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\).\n\\(\\boldsymbol{\\Sigma}\\) is a covariance (i.e., positive definite matrix), so we must be careful here.\nIt is natural to place a prior on the decomposition, \\(\\boldsymbol{\\Sigma} = \\mathbf{D} \\mathbf{L} \\mathbf{L}^\\top \\mathbf{D}\\).\n\nFor each of the standard deviations \\((\\tau_0,\\tau_1)\\) we can place standard priors for scales (e.g., half-normal).\nFor \\(\\mathbf{L}\\) we can place a Lewandowski-Kurowicka-Joe (LKJ) distribution, \\(\\mathbf{L} \\sim LKJ(\\eta)\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#what-does-the-lkj-prior-do",
    "href": "slides/15-longitudinal.html#what-does-the-lkj-prior-do",
    "title": "Longitudinal Data",
    "section": "What Does the LKJ Prior Do?",
    "text": "What Does the LKJ Prior Do?\n\nThe LKJ prior allows you to model the correlation structure in a flexible and non-informative way.\nIt is defined by a single parameter, \\(\\eta &gt; 0\\), which controls the concentration of the prior.\n\nWhen \\((\\eta = 1)\\), it is an uninformative prior (i.e., uniform on the correlations).\nWhen \\((\\eta &gt; 1)\\), the prior favors more highly correlated random effects.\nWhen \\((\\eta &lt; 1)\\), the prior favors weaker correlations.\n\nWhen \\(q=2\\), \\(\\eta = 1\\) is equivalent to \\(\\rho \\sim \\text{Uniform}(-1,1)\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#lkj-prior-formula",
    "href": "slides/15-longitudinal.html#lkj-prior-formula",
    "title": "Longitudinal Data",
    "section": "LKJ Prior Formula",
    "text": "LKJ Prior Formula\n\nThe LKJ prior on a correlation matrix \\(\\mathbf{L}\\) is given by:\n\n\\[f(\\mathbf{L} | \\eta) \\propto \\prod_{j = 2}^q L_{jj}^{q-j+2\\eta-2}.\\]\nWhere:\n\n\\(\\eta\\) is the concentration parameter.\n\\(q\\) is the size of the correlation matrix.\n\\(L_{jk}\\) is the observation in the \\(j\\)th row and \\(k\\)th column of \\(\\mathbf{L}\\)."
  },
  {
    "objectID": "slides/15-longitudinal.html#lkj-prior-in-stan",
    "href": "slides/15-longitudinal.html#lkj-prior-in-stan",
    "title": "Longitudinal Data",
    "section": "LKJ Prior in Stan",
    "text": "LKJ Prior in Stan\n\nparameters {\n  cholesky_factor_corr[2] L;  // Cholesky factor of correlation matrix\n}\nmodel {\n  L ~ lkj_corr_cholesky(eta);\n}"
  },
  {
    "objectID": "slides/15-longitudinal.html#stan-code-for-independent-intercept-and-slope",
    "href": "slides/15-longitudinal.html#stan-code-for-independent-intercept-and-slope",
    "title": "Longitudinal Data",
    "section": "Stan code for independent intercept and slope",
    "text": "Stan code for independent intercept and slope\n\n// lmm-independent.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; N;\n  vector[N] Time;\n  vector[N] MD;\n  int&lt;lower = 1, upper = n&gt; Ids[N];\n}\nparameters {\n  real beta0;\n  real beta1;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] z0;\n  vector[n] z1;\n  real&lt;lower = 0&gt; tau0;\n  real&lt;lower = 0&gt; tau1;\n}\ntransformed parameters {\n  vector[n] theta0;\n  vector[n] theta1;\n  theta0 = tau0 * z0;\n  theta1 = tau1 * z1;\n}\nmodel {\n  // likelihood\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = (beta0 + theta0[Ids[i]]) + (beta1 + theta1[Ids[i]]) * Time[i];\n  }\n  target += normal_lpdf(MD | mu, sigma);\n  // subject-specific parameters\n  target += std_normal_lpdf(z0);\n  target += std_normal_lpdf(z1);\n  // population parameters\n  target += normal_lpdf(beta0 | 0, 3);\n  target += normal_lpdf(beta1 | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau0 | 0, 3);\n  target += normal_lpdf(tau1 | 0, 3);\n}"
  },
  {
    "objectID": "slides/15-longitudinal.html#stan-code-for-conditional-lmm",
    "href": "slides/15-longitudinal.html#stan-code-for-conditional-lmm",
    "title": "Longitudinal Data",
    "section": "Stan code for conditional LMM",
    "text": "Stan code for conditional LMM\n\n// lmm-conditional.stan\ndata {\n  int&lt;lower = 1&gt; N;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1&gt; q;\n  matrix[N, p] X;\n  matrix[N, q] Z;\n  vector[N] Y;\n  int&lt;lower = 1, upper = n&gt; Ids[N];\n  real&lt;lower = 0&gt; eta;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  matrix[q, n] z;\n  vector&lt;lower = 0&gt;[q] tau;\n  cholesky_factor_corr[q] L;\n}\ntransformed parameters {\n  matrix[q, n] theta;\n  theta = diag_pre_multiply(tau, L) * z;\n}\nmodel {\n  // likelihood\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = X[i, ] * beta + Z[i, ] * theta[, Ids[i]];\n  }\n  target += normal_lpdf(Y | mu, sigma);\n  // subject-specific parameter\n  target += std_normal_lpdf(to_vector(z));\n  // population parameters\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n  target += lkj_corr_cholesky_lpdf(L | eta);\n}\ngenerated quantities {\n  corr_matrix[q] Phi = L * transpose(L);\n  real rho = Phi[1, 2];\n  vector[n] subject_intercepts = beta[1] + to_vector(theta[1, ]);\n  vector[n] subject_slopes = beta[2] + to_vector(theta[2, ]);\n  vector[N] Y_pred;\n  vector[N] log_lik;\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = X[i, ] * beta + Z[i, ] * theta[, Ids[i]];\n    Y_pred[i] = normal_rng(mu[i], sigma);\n    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/15-longitudinal.html#stan-code-for-marginal-lmm",
    "href": "slides/15-longitudinal.html#stan-code-for-marginal-lmm",
    "title": "Longitudinal Data",
    "section": "Stan code for marginal LMM",
    "text": "Stan code for marginal LMM\nNeed ragged data structure.\n\n// lmm-marginal.stan\ndata {\n  int&lt;lower = 1&gt; N;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1&gt; q;\n  matrix[N, p] X;\n  matrix[N, q] Z;\n  vector[N] Y;\n  int&lt;lower = 1&gt; n_is[n];\n  real&lt;lower = 0&gt; eta;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector&lt;lower = 0&gt;[q] tau;\n  cholesky_factor_corr[q] L;\n}\ntransformed parameters {\n  cov_matrix[q] Sigma;\n  Sigma = diag_pre_multiply(tau, L) * transpose(diag_pre_multiply(tau, L));\n}\nmodel {\n  // evaluate the likelihood for the marginal model using ragged data structure\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    matrix[n_i, p] X_i;\n    matrix[n_i, q] Z_i;\n    for (j in 1:p) X_i[, j] = segment(X[, j], pos, n_i);\n    for (j in 1:q) Z_i[, j] = segment(Z[, j], pos, n_i);\n    vector[n_i] mu_i = X_i * beta;\n    matrix[n_i, n_i] Upsilon_i = (sigma * sigma) * diag_matrix(rep_vector(1.0, n_i)) + Z_i * Sigma * transpose(Z_i);\n    target += multi_normal_lpdf(Y_i | mu_i, Upsilon_i);\n    pos = pos + n_i;\n  }\n  // population parameters\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n  target += lkj_corr_cholesky_lpdf(L | eta);\n}\ngenerated quantities {\n  corr_matrix[q] Phi = L * transpose(L);\n  real rho = Phi[1, 2];\n  matrix[q, n] theta;\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    matrix[n_i, p] X_i;\n    matrix[n_i, q] Z_i;\n    for (j in 1:p) X_i[, j] = segment(X[, j], pos, n_i);\n    for (j in 1:q) Z_i[, j] = segment(Z[, j], pos, n_i);\n    vector[n_i] mu_i = X_i * beta;\n    matrix[q, n_i] M = Sigma * transpose(Z_i) * inverse_spd(Z_i * Sigma * transpose(Z_i) + (sigma * sigma) * diag_matrix(rep_vector(1.0, n_i)));\n    vector[q] mean_theta_i = M * (Y_i - mu_i);\n    matrix[q, q] cov_theta_i = Sigma - M * Z_i * Sigma;\n    theta[, i] = multi_normal_rng(mean_theta_i, cov_theta_i);\n    pos = pos + n_i;\n  }\n  vector[n] subject_intercepts = beta[1] + to_vector(theta[1, ]);\n  vector[n] subject_slopes = beta[2] + to_vector(theta[2, ]);\n}"
  },
  {
    "objectID": "slides/15-longitudinal.html#glaucoma-disease-progression-data",
    "href": "slides/15-longitudinal.html#glaucoma-disease-progression-data",
    "title": "Longitudinal Data",
    "section": "Glaucoma Disease Progression Data",
    "text": "Glaucoma Disease Progression Data\n\nglaucoma_longitudinal &lt;- readRDS(\"glaucoma_longitudinal.rds\")\nhead(glaucoma_longitudinal)\n\n\n\n  pat_id eye_id mean_deviation      time      age      iop\n1      1      1          -7.69 0.0000000 51.55616 10.87303\n2      1      1          -9.95 0.5753425 51.55616 10.87303\n3      1      1          -9.58 1.0547945 51.55616 10.87303\n4      1      1          -9.53 1.5726027 51.55616 10.87303\n5      1      1          -9.18 2.0136986 51.55616 10.87303\n6      1      1          -9.63 2.5671233 51.55616 10.87303\n\n\n\nlength(unique(glaucoma_longitudinal$eye_id))\n\n[1] 278\n\nnrow(glaucoma_longitudinal)\n\n[1] 4863"
  },
  {
    "objectID": "slides/15-longitudinal.html#fitting-the-conditional-model-in-stan",
    "href": "slides/15-longitudinal.html#fitting-the-conditional-model-in-stan",
    "title": "Longitudinal Data",
    "section": "Fitting the Conditional Model in Stan",
    "text": "Fitting the Conditional Model in Stan\n\nX &lt;- model.matrix(~ time, data = glaucoma_longitudinal)\nstan_data &lt;- list(\n  N = nrow(glaucoma_longitudinal),\n  n = n_eyes,\n  p = ncol(X),\n  q = ncol(X),\n  X = X,\n  Z = X,\n  Y = glaucoma_longitudinal$mean_deviation,\n  Ids = glaucoma_longitudinal$eye_id,\n  eta = 1\n)\nlmm_conditional &lt;- stan_model(model_code = \"lmm-conditional.stan\")\nfit_lmm_conditional &lt;- sampling(lmm_conditional, stan_data, iter = 5000, pars = c(\"z\", \"theta\"), include = FALSE)"
  },
  {
    "objectID": "slides/15-longitudinal.html#assessing-convergence",
    "href": "slides/15-longitudinal.html#assessing-convergence",
    "title": "Longitudinal Data",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(fit_lmm_conditional, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/15-longitudinal.html#assessing-convergence-1",
    "href": "slides/15-longitudinal.html#assessing-convergence-1",
    "title": "Longitudinal Data",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_lmm_conditional, regex_pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/15-longitudinal.html#posterior-summaries",
    "href": "slides/15-longitudinal.html#posterior-summaries",
    "title": "Longitudinal Data",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nprint(fit_lmm_conditional, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -8.19    0.04 0.47 -9.15 -8.50 -8.19 -7.86 -7.34   178 1.03\nbeta[2] -0.10    0.00 0.03 -0.16 -0.12 -0.10 -0.08 -0.05   778 1.00\nsigma    1.27    0.00 0.01  1.24  1.26  1.26  1.27  1.29 15569 1.00\ntau[1]   8.07    0.02 0.34  7.46  7.83  8.05  8.29  8.79   364 1.01\ntau[2]   0.44    0.00 0.02  0.40  0.42  0.44  0.45  0.48  2064 1.00\nrho     -0.27    0.00 0.06 -0.38 -0.31 -0.27 -0.23 -0.16   949 1.00\n\nSamples were drawn using NUTS(diag_e) at Tue Mar  4 15:18:54 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/15-longitudinal.html#fitting-the-marginal-model-in-stan",
    "href": "slides/15-longitudinal.html#fitting-the-marginal-model-in-stan",
    "title": "Longitudinal Data",
    "section": "Fitting the Marginal Model in Stan",
    "text": "Fitting the Marginal Model in Stan\n\nX &lt;- model.matrix(~ time, data = glaucoma_longitudinal)\nstan_data &lt;- list(\n  N = nrow(glaucoma_longitudinal),\n  n = n_eyes,\n  p = ncol(X),\n  q = ncol(X),\n  X = X,\n  Z = X,\n  Y = glaucoma_longitudinal$mean_deviation,\n  n_is = as.numeric(table(glaucoma_longitudinal$eye_id)),\n  eta = 1\n)\nlmm_marginal &lt;- stan_model(model_code = \"lmm-marginal.stan\")\nfit_lmm_marginal &lt;- sampling(lmm_marginal, stan_data, iter = 5000)"
  },
  {
    "objectID": "slides/15-longitudinal.html#assessing-convergence-2",
    "href": "slides/15-longitudinal.html#assessing-convergence-2",
    "title": "Longitudinal Data",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(fit_lmm_marginal, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/15-longitudinal.html#assessing-convergence-3",
    "href": "slides/15-longitudinal.html#assessing-convergence-3",
    "title": "Longitudinal Data",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_lmm_marginal, regex_pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/15-longitudinal.html#posterior-summaries-1",
    "href": "slides/15-longitudinal.html#posterior-summaries-1",
    "title": "Longitudinal Data",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nprint(fit_lmm_marginal, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -8.23       0 0.47 -9.16 -8.55 -8.23 -7.92 -7.31 12896    1\nbeta[2] -0.10       0 0.03 -0.16 -0.12 -0.10 -0.08 -0.05 13230    1\nsigma    1.27       0 0.01  1.24  1.26  1.26  1.27  1.29 13937    1\ntau[1]   8.05       0 0.34  7.42  7.82  8.04  8.27  8.75 14264    1\ntau[2]   0.44       0 0.02  0.40  0.42  0.44  0.45  0.48 13997    1\nrho     -0.27       0 0.06 -0.38 -0.31 -0.28 -0.24 -0.16 12487    1\n\nSamples were drawn using NUTS(diag_e) at Tue Mar  4 11:31:23 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/15-longitudinal.html#comparing-lmm-versus-ols",
    "href": "slides/15-longitudinal.html#comparing-lmm-versus-ols",
    "title": "Longitudinal Data",
    "section": "Comparing LMM versus OLS",
    "text": "Comparing LMM versus OLS"
  },
  {
    "objectID": "slides/15-longitudinal.html#comparing-lmm-versus-ols-1",
    "href": "slides/15-longitudinal.html#comparing-lmm-versus-ols-1",
    "title": "Longitudinal Data",
    "section": "Comparing LMM versus OLS",
    "text": "Comparing LMM versus OLS"
  },
  {
    "objectID": "slides/15-longitudinal.html#prepare-for-next-class",
    "href": "slides/15-longitudinal.html#prepare-for-next-class",
    "title": "Longitudinal Data",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nExam 1 will be released after class!"
  },
  {
    "objectID": "slides/16-gp.html#recalling-what-went-before-the-break",
    "href": "slides/16-gp.html#recalling-what-went-before-the-break",
    "title": "Gaussian Processes",
    "section": "Recalling what went before the break",
    "text": "Recalling what went before the break"
  },
  {
    "objectID": "slides/16-gp.html#is-linear-fit-the-right-way",
    "href": "slides/16-gp.html#is-linear-fit-the-right-way",
    "title": "Gaussian Processes",
    "section": "Is Linear Fit the Right Way?",
    "text": "Is Linear Fit the Right Way?\nThere are various ways to fit a model to time-varying behavior of the curves."
  },
  {
    "objectID": "slides/16-gp.html#a-brief-agenda",
    "href": "slides/16-gp.html#a-brief-agenda",
    "title": "Gaussian Processes",
    "section": "A brief agenda",
    "text": "A brief agenda\nBy the end of this lecture you should:\n\nUnderstand the basic concepts of Gaussian processes (GP) and their usefulness in statistical modeling.\nCompute posterior and predictive inference for a simple GP model.\nUse GP as a building block in modeling time-correlated data."
  },
  {
    "objectID": "slides/16-gp.html#motivation",
    "href": "slides/16-gp.html#motivation",
    "title": "Gaussian Processes",
    "section": "Motivation",
    "text": "Motivation\nLet \\(i\\) index distinct eyes (\\(i=1,2,\\ldots,n\\)) and \\(t\\) index within-eye data in time (\\(t=1,2,\\ldots,n_i\\)).\nSuppose we are interested in effects of time-invariant predictors (age and iop):\n\\[Y_{it} = \\underbrace{\\mathbf{x}_i\\boldsymbol{\\beta}}_{\\text{Fixed in time}} + \\underbrace{\\eta_{it}}_{\\text{Varying in time}} + \\epsilon_{it}\\]\nIn a previous lecture, we included only time (\\(X_{it}\\)) and fit a random slope regression.\n\\[\\eta_{it} = \\theta_{0i} + \\underbrace{X_{it}}_{=Time}\\theta_{1i}.\\]\nWhat if we want a nonlinear model for time effects?"
  },
  {
    "objectID": "slides/16-gp.html#specifying-basis-functions",
    "href": "slides/16-gp.html#specifying-basis-functions",
    "title": "Gaussian Processes",
    "section": "Specifying basis functions",
    "text": "Specifying basis functions\nLinear increase/decrease in time is too limited! What if their visual loss recovers, or fluctuates?\n\\[\\eta_{it} = \\theta_{0i} + \\sum_{k=1}^K X_{it}^{\\color{red} k}\\theta_{1i}^{(k)}.\\]"
  },
  {
    "objectID": "slides/16-gp.html#bypassing-the-choice-of-bases",
    "href": "slides/16-gp.html#bypassing-the-choice-of-bases",
    "title": "Gaussian Processes",
    "section": "Bypassing the choice of bases",
    "text": "Bypassing the choice of bases\nChoosing the ‚Äúright‚Äù type/number of bases is difficult (polynomials are usually not great choices).\nIn a continuous time, \\(\\eta_{it} = \\underbrace{\\eta_i}_{i-\\text{th function}}(t)\\) is useful:\n\nDifferent people/groups have observations spanning different grids.\nMakes easier the formulation of a prediction task: new time \\(t^{pred}\\) not in your dataset?\n\nCan we introduce a probabilistic model for a whole collection of random functions \\((\\eta_i)_{i=1}^{n}\\)?"
  },
  {
    "objectID": "slides/16-gp.html#what-do-i-want-the-function-to-look-like",
    "href": "slides/16-gp.html#what-do-i-want-the-function-to-look-like",
    "title": "Gaussian Processes",
    "section": "What do I want the function to look like?",
    "text": "What do I want the function to look like?\nDenote by \\(f\\) a random function with mean zero. A distribution of a random function is determined by knowing every possible finite-dimensional marginals:\n\\[\\mathbf{f} = \\begin{pmatrix} f(t_{1})\\\\ \\vdots \\\\ f(t_{m}) \\end{pmatrix} {\\sim} N_{m}(\\mathbf{0},\\boldsymbol{\\Sigma}(t_1,\\ldots,t_m)),\\]\nwhere the \\(st\\)-th entry of the matrix is \\(= \\mathbb{C}[f(s), f(t)]\\) for a given covariance function \\(\\mathbb{C}\\).\nWe call such random function a Gaussian process."
  },
  {
    "objectID": "slides/16-gp.html#what-are-my-prior-assumptions-about-the-covariance",
    "href": "slides/16-gp.html#what-are-my-prior-assumptions-about-the-covariance",
    "title": "Gaussian Processes",
    "section": "What are my prior assumptions about the covariance?",
    "text": "What are my prior assumptions about the covariance?\nIf I write that covariance as \\(C(t,s)\\), then we must decide what the function \\(C\\) looks like.\n\nHypothesis 1 : The marginal variability should stay the same.\nHypothesis 2 : Time correlation should decay in, and only depend on, the amount of time elapsed.\nHypothesis 3 : We have expectations about the span of correlation and smoothness of the process.\n\nThese are all a priori hypotheses. The data may come from something very different!"
  },
  {
    "objectID": "slides/16-gp.html#stationary-gaussian-processes",
    "href": "slides/16-gp.html#stationary-gaussian-processes",
    "title": "Gaussian Processes",
    "section": "Stationary Gaussian processes",
    "text": "Stationary Gaussian processes\nBased on our considerations, let‚Äôs choose \\(C\\) that only depends on the lag between two time points.\n\\[\n\\mathbb{C}(f_s, f_t) = \\sigma^2 C(|s-t|).\n\\]\nWe want \\(C(0) = 1\\) and \\(C(h)\\to 0\\) as \\(h = |t-s|\\to\\infty\\).\nDifferent choices of \\(C\\) yields sampled functions with varying smoothness.\n\nExponential kernel: \\(C(h) = \\sigma^2\\exp(-h/\\rho)\\)\nSquare exponential kernel: \\(C(h) = \\sigma^2\\exp\\{-h^2/(2\\rho^2)\\}\\)\nMat√©rn kernels"
  },
  {
    "objectID": "slides/16-gp.html#gaussian-process-covariance-functions-in-stan",
    "href": "slides/16-gp.html#gaussian-process-covariance-functions-in-stan",
    "title": "Gaussian Processes",
    "section": "Gaussian Process Covariance Functions in Stan",
    "text": "Gaussian Process Covariance Functions in Stan\nA list of available kernels is available here.\nThe squared exponential kernel (exponentiated quadratic kernel) is given by:\n\\[C(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma^2 \\exp\\left(-\\frac{|\\mathbf{x}_i - \\mathbf{x}_j|^2}{2\\rho^2}\\right)\\]\n\nmatrix gp_exp_quad_cov(array[] real x, real sigma, real length_scale)\n\nFor us, \\(\\mathbf{x}\\) is 1D (time)‚Ä¶but it does not have to be!"
  },
  {
    "objectID": "slides/16-gp.html#prior-sampling-in-stan",
    "href": "slides/16-gp.html#prior-sampling-in-stan",
    "title": "Gaussian Processes",
    "section": "Prior Sampling in Stan",
    "text": "Prior Sampling in Stan\nOnce we have a GP model, sampling from the prior is equivalent to sampling from a multivariate normal.\n\ndata {\n  int&lt;lower=1&gt; N;\n  array[N] real x;\n  real&lt;lower=0&gt; sigma;\n  real&lt;lower=0&gt; l;\n}\ntransformed data {\n  // 0. A \"small nugget\" to stabilize matrix root computation\n  // This is for.better numerical stability in taking large matrix roots\n  real delta = 1e-9;\n\n  // 1. Compute the squared exponential kernel matrix\n  // x is the time variable\n  vector[N] mu = rep_vector(0, N);\n  matrix[N, N] R_C;\n  matrix[N, N] C = gp_exp_quad_cov(x, sigma, l);\n  for (i in 1:N) {\n    C[i, i] = C[i, i] + delta;\n  }\n\n  // 2. Compute the root of C by Cholesky decomposition\n  R_C = cholesky_decompose(C);\n}\ngenerated quantities {\n  // 3. Sample from the prior: multivariate_normal(0, C)\n  f ~ multi_normal_cholesky(mu, R_C)\n}"
  },
  {
    "objectID": "slides/16-gp.html#visualizing-the-process",
    "href": "slides/16-gp.html#visualizing-the-process",
    "title": "Gaussian Processes",
    "section": "Visualizing the process",
    "text": "Visualizing the process"
  },
  {
    "objectID": "slides/16-gp.html#default-prior-choices",
    "href": "slides/16-gp.html#default-prior-choices",
    "title": "Gaussian Processes",
    "section": "Default Prior Choices",
    "text": "Default Prior Choices\n\nWhen you have longitudinal data, there are theoretical justifications for using \\(\\rho^{-1} \\sim Gamma(5,5)\\).\nA priori the prior is concentrated around 1. For a prior mean, distance increase of 1 corrresponds to a multiplicative decay of correlation by \\(e^{-1}\\sim 37\\%\\).\nBeware of the units! A year correlation is very different from that over a day."
  },
  {
    "objectID": "slides/16-gp.html#combining-gp-with-random-effects-regression",
    "href": "slides/16-gp.html#combining-gp-with-random-effects-regression",
    "title": "Gaussian Processes",
    "section": "Combining GP with Random Effects Regression",
    "text": "Combining GP with Random Effects Regression\nFor each \\(i\\)-th eye: time effect vector is given an independent prior based on the GP model.\n\\[\\boldsymbol{\\eta}_i = (\\eta_{i1},\\ldots,\\eta_{in_i})^\\top \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0},\\mathbf{C}_i)\\]\nEach \\(\\eta_{it}\\) is a pointwise value of the process \\(\\eta_i(t) = \\eta_{it}\\).\n\\[\\mathbf{C}_i = \\begin{bmatrix}\nC(0) & C(|t_{i1}-t_{i2}|) & \\cdots & C(|t_{i1} - t_{in_i}|)\\\\\nC(|t_{i1} - t_{i2}|) & C(0) & \\cdots & C(|t_{i,2} - t_{in_i}|)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\n\\end{bmatrix}\\]\nWe can use a squared-exponential kernel \\(C(|s-t|) = \\alpha^2e^{-|s-t|^2/2\\rho^2}\\) and also draw posterior samples of \\((\\alpha,\\rho)\\)."
  },
  {
    "objectID": "slides/16-gp.html#the-full-model",
    "href": "slides/16-gp.html#the-full-model",
    "title": "Gaussian Processes",
    "section": "The full model",
    "text": "The full model\nFor \\(i\\) (\\(i = 1,\\ldots,n\\)) and \\(t\\) (\\(t = 1,\\ldots,n_i\\)),\n\\[\\begin{align*}\nY_{it} &= \\mathbf{x}_{i}\\boldsymbol{\\beta} + \\eta_{i}(t) + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2),\\\\\n\\boldsymbol{\\eta}_i &\\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0},\\mathbf{C}_i),\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}).\n\\end{align*}\\]\n\n\\(\\boldsymbol{\\Omega}\\) represents the population-level unknown parameters: \\((\\boldsymbol{\\beta},\\sigma^2,\\alpha,\\rho)\\)."
  },
  {
    "objectID": "slides/16-gp.html#first-look-at-the-data",
    "href": "slides/16-gp.html#first-look-at-the-data",
    "title": "Gaussian Processes",
    "section": "First Look at the Data",
    "text": "First Look at the Data\n\nhead(dataset)\n\n  pat_id eye_id mean_deviation      time      age      iop\n1      1      1          -7.69 0.0000000 51.55616 10.87303\n2      1      1          -9.95 0.5753425 51.55616 10.87303\n3      1      1          -9.58 1.0547945 51.55616 10.87303\n4      1      1          -9.53 1.5726027 51.55616 10.87303\n5      1      1          -9.18 2.0136986 51.55616 10.87303\n6      1      1          -9.63 2.5671233 51.55616 10.87303"
  },
  {
    "objectID": "slides/16-gp.html#model-specification",
    "href": "slides/16-gp.html#model-specification",
    "title": "Gaussian Processes",
    "section": "Model Specification",
    "text": "Model Specification\nWe want to fit a model estimating the effects of age and iop on mean deviation, adjusting for nonlinear time effects:\n\\[\\begin{align*}\nY_{it} &= \\underbrace{\\beta_0 + \\text{age}_i\\beta_1 + \\text{iop}_i\\beta_2}_{=\\mathbf{x}_{i}\\boldsymbol{\\beta}} + \\eta_{it} + \\epsilon_{it}\\\\\n\\boldsymbol{\\eta}_i &= \\begin{bmatrix} \\eta_{i1}\\\\ \\vdots\\\\ \\eta_{in_i} \\end{bmatrix} \\stackrel{iid}{\\sim} N(0,\\mathbf{C}_i)\\\\\n\\epsilon_{it} &\\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\\]\nOur model for \\(\\eta_{it}\\) is nonlinear in time: time enters the covariance structure of \\(\\mathbf{C}_i\\) producing a function \\(\\eta_i(t) = \\eta_{it}\\)."
  },
  {
    "objectID": "slides/16-gp.html#preparing-the-data",
    "href": "slides/16-gp.html#preparing-the-data",
    "title": "Gaussian Processes",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nA few data processing steps are needed to handle the data structure with more ease.\n\n\\(N\\) will denote the number of all observations: \\(N = \\sum_{i=1}^{n} n_i\\).\n\\(n\\) will denote the total number of eyes.\n\\(p\\) will denote the number of predictors fixed across time (\\(p=3\\): intercept, slopes for age and iop).\nA separate vector of the number of observations (\\(n_i\\)) for each eye will be stored as s."
  },
  {
    "objectID": "slides/16-gp.html#preparing-the-data-1",
    "href": "slides/16-gp.html#preparing-the-data-1",
    "title": "Gaussian Processes",
    "section": "Preparing the Data",
    "text": "Preparing the Data\n\nlibrary(dplyr)\n\n# Group predictors fixed across time\nfixed_df &lt;- dataset %&gt;% \n  group_by(eye_id) %&gt;% \n  reframe(age = unique(age), iop = unique(iop))\nXmat &lt;- model.matrix(~age + iop, data = fixed_df)\n\n# Number of measurements for each eye\ngroupsizes &lt;- dataset %&gt;% \n  group_by(eye_id) %&gt;% \n  summarise(n = n()) %&gt;% \n  pull(n)\n\nstan_data &lt;- list(\n  N = dim(dataset)[1],\n  n = max(dataset$eye_id),\n  p = dim(Xmat)[2],\n  t = dataset$time,\n  Y = dataset$mean_deviation,\n  s = groupsizes,\n  X = Xmat\n)"
  },
  {
    "objectID": "slides/16-gp.html#conditional-gp-model",
    "href": "slides/16-gp.html#conditional-gp-model",
    "title": "Gaussian Processes",
    "section": "Conditional GP Model",
    "text": "Conditional GP Model\n\ndata {\n  int&lt;lower=0&gt; N;       // total number of observations\n  int&lt;lower=1&gt; n;       // number of eyes\n  int&lt;lower=1&gt; p;       // fixed effects dimension\n  vector[N] Y;          // observation\n  matrix[n, p] X;        // fixed effects predictors\n  array[N] real t;      // obs time points\n  array[n] int s;       // sizes of within-pt obs\n}\ntransformed data {\n  real delta = 1e-9;\n}\nparameters {\n  // Fixed effects model\n  vector[p] beta;\n  real&lt;lower=0&gt; sigma;\n  // GP parameters\n  vector[N] eta;\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; rho;\n}\ntransformed parameters {\n  vector[n] mu = X * beta;\n}\nmodel {\n  beta ~ normal(0,3);\n  z ~ std_normal();\n  alpha ~ std_normal();\n  sigma ~ std_normal();\n  rho ~ inv_gamma(5,5);\n\n  vector[N] mu_rep;\n  vector[N] eta;\n  \n  int pos;\n  pos = 1;\n  // Ragged loop computing the mean for each time obs\n  for (i in 1:n) {\n    // GP covariance for the k-th eye\n    int n_i = s[i];\n    int pos_end = pos + n_i - 1;\n    matrix[n_i, n_i] R_C;\n    matrix[n_i, n_i] C = gp_exp_quad_cov(segment(t, pos, n_i), alpha, rho);\n    for (j in 1:n_i) {\n      // adding a small term to the diagonal entries\n      C[j, j] = C[j, j] + delta;\n    }\n    R_C = cholesky_decompose(C);\n    \n    // Mean of data at each time\n    mu_rep[pos:pos_end] = rep_vector(mu[i], n_i);\n    // GP for the i-th eye\n    eta[pos:pos_end] = R_C * segment(z, pos, n_i);\n    pos = pos_end + 1;\n  }\n  // Normal observation model centered at mu_rep + eta\n  Y ~ normal(mu_rep + eta, sigma);\n}"
  },
  {
    "objectID": "slides/16-gp.html#assessing-convergence",
    "href": "slides/16-gp.html#assessing-convergence",
    "title": "Gaussian Processes",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(gp_condl_fit, pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))"
  },
  {
    "objectID": "slides/16-gp.html#assessing-convergence-1",
    "href": "slides/16-gp.html#assessing-convergence-1",
    "title": "Gaussian Processes",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nbayesplot::mcmc_acf(gp_condl_fit, regex_pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))"
  },
  {
    "objectID": "slides/16-gp.html#marginal-model-specification",
    "href": "slides/16-gp.html#marginal-model-specification",
    "title": "Gaussian Processes",
    "section": "Marginal Model Specification",
    "text": "Marginal Model Specification\nThe conditional model is pretty slow (can be improved) and results in poor mixing (can be run longer). Marginalizing the random effect can stabilize the computation.\nIn vector notation, our observation model for the \\(i\\)-th eye is\n\\[\n\\mathbf{Y}_i = \\begin{bmatrix} Y_{i1}\\\\ \\vdots \\\\ Y_{in_i} \\end{bmatrix}\n= \\mathbf{x}_i\\boldsymbol{\\beta} + \\boldsymbol{\\eta}_i + \\boldsymbol{\\epsilon}_i\n\\] By marginalizing out \\(\\boldsymbol{\\eta}_i\\), we obtain\n\\[\n\\mathbf{Y}_i \\sim N_{n_i}(\\mathbf{x}_i\\boldsymbol{\\beta},\\mathbf{C}_i + \\sigma^2\\mathbf{I}_{n_i})\n\\]"
  },
  {
    "objectID": "slides/16-gp.html#fitting-a-marginal-model",
    "href": "slides/16-gp.html#fitting-a-marginal-model",
    "title": "Gaussian Processes",
    "section": "Fitting a Marginal Model",
    "text": "Fitting a Marginal Model\nOnly the model segment has to be changed.\n\nmodel {\n  beta ~ normal(0,3);\n  alpha ~ std_normal();\n  sigma ~ std_normal();\n  rho ~ inv_gamma(5,5);\n\n  int pos;\n  pos = 1;\n  // Ragged loop computing joint likelihood for each eye\n  for (i in 1:n) {\n    // GP covariance for the k-th eye\n    int n_i = s[i];\n    vector[n_i] mu_rep;\n    \n    matrix[n_i, n_i] R_C;\n    matrix[n_i, n_i] C = gp_exp_quad_cov(segment(t, pos, n_i), alpha, rho);\n    for (j in 1:n_i) {\n      // Add noise variance to the diagonal entries\n      C[j, j] = C[j, j] + square(sigma);\n    }\n    R_C = cholesky_decompose(C);\n    // Marginal model for the i-th eye\n    mu_rep = rep_vector(mu[i], n_i);\n    target += multi_normal_cholesky_lpdf(to_vector(segment(Y, pos, n_i)) | mu_rep, R_C);\n    pos = pos + n_i;\n  }\n}"
  },
  {
    "objectID": "slides/16-gp.html#assessing-convergence-2",
    "href": "slides/16-gp.html#assessing-convergence-2",
    "title": "Gaussian Processes",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(gp_marginal_fit, pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))"
  },
  {
    "objectID": "slides/16-gp.html#assessing-convergence-3",
    "href": "slides/16-gp.html#assessing-convergence-3",
    "title": "Gaussian Processes",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nbayesplot::mcmc_acf(gp_marginal_fit, regex_pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))"
  },
  {
    "objectID": "slides/16-gp.html#estimates-of-eye-level-predictors",
    "href": "slides/16-gp.html#estimates-of-eye-level-predictors",
    "title": "Gaussian Processes",
    "section": "Estimates of eye-level Predictors",
    "text": "Estimates of eye-level Predictors"
  },
  {
    "objectID": "slides/16-gp.html#learned-gp-hyperparameter",
    "href": "slides/16-gp.html#learned-gp-hyperparameter",
    "title": "Gaussian Processes",
    "section": "Learned GP hyperparameter",
    "text": "Learned GP hyperparameter\n\nThe posterior mean of \\(\\rho\\) (in year) is very large: the pattern of mean deviation change is very gradual over time.\nA posteriori we believe mean deviations of an eye, 10 years apart and adjusted for age and iop, are still strongly correlated (\\(e^{-5/6}\\sim 43\\%\\))."
  },
  {
    "objectID": "slides/16-gp.html#random-effects-covariance-structure-for-the-first-5-years",
    "href": "slides/16-gp.html#random-effects-covariance-structure-for-the-first-5-years",
    "title": "Gaussian Processes",
    "section": "Random effects covariance structure for the first 5 years",
    "text": "Random effects covariance structure for the first 5 years"
  },
  {
    "objectID": "slides/16-gp.html#predictive-inference",
    "href": "slides/16-gp.html#predictive-inference",
    "title": "Gaussian Processes",
    "section": "Predictive inference",
    "text": "Predictive inference\nThe model is continuous in nature and defined, in principle, for all times. This is very convenient for visualiztion, interpolation, and forecasting.\n\\[\n\\mathbf{Y}_i = \\underbrace{\\mathbf{f}_{i}}_{ = \\mathbf{x}_{i}\\boldsymbol{\\beta} + \\boldsymbol{\\eta}_{i} } + \\boldsymbol{\\epsilon}_i\n\\]\nThe vector \\(\\mathbf{f}_{i}\\) may be interpreted as the denoised latent process of an eye-specific mean deviation over time: \\((f_{i1},\\ldots,f_{in_i})^\\top\\).\n\\[\n\\mathbb{E}[f_{it}] = \\mathbf{x}_i\\boldsymbol{\\beta},\\; \\mathbb{C}(f_{it}, f_{it'}) = \\mathbb{C}(\\eta_{it},\\eta_{it'})\n\\]"
  },
  {
    "objectID": "slides/16-gp.html#predictive-inference-using-normal-conditioning",
    "href": "slides/16-gp.html#predictive-inference-using-normal-conditioning",
    "title": "Gaussian Processes",
    "section": "Predictive Inference using Normal Conditioning",
    "text": "Predictive Inference using Normal Conditioning\nSay \\(\\mathbf{f}^{pred}_i\\) is the values of \\(f_i\\) at \\(m_i\\) new time points which we want to predict, based on our GP model.\nKey is that the random effects at observed times and new prediction time points are not independent.\n\\[\\begin{align*}\n\\begin{pmatrix}\n\\mathbf{Y}_i \\\\ \\mathbf{f}^{pred}_i\n\\end{pmatrix}\n\\sim N_{n_i + m_i}\\left(\n\\begin{pmatrix}\n\\mathbf{x}_i\\boldsymbol{\\beta}\\\\\n\\mathbf{x}_i\\boldsymbol{\\beta}\n\\end{pmatrix},\n\\begin{pmatrix} \\mathbf{C}_i + \\sigma^2\\mathbf{I}_{n_i} & \\mathbf{k}_i^\\top \\\\ \\mathbf{k}_i & \\mathbf{C}^{pred}_i\\end{pmatrix}\n\\right),\n\\end{align*}\\] where \\(\\mathbf{k}_i = \\mathbb{C}(\\mathbf{f}^{pred}_i, \\mathbf{f}_i)\\) (‚Äúcross-covariances‚Äù).\nAn analytical formula exists for the conditional distribution of \\(\\mathbf{f}^{pred}\\) given observed \\(Y_{it}\\) at \\(n_i\\) time points.\n\\[\n\\mathbf{f}^{pred}_i | \\mathbf{Y}_i \\sim N\\left(\\mathbf{x}_i\\boldsymbol{\\beta} + \\mathbf{k}_i(\\mathbf{C}_i + \\sigma^2\\mathbf{I})^{-1}(\\mathbf{Y}_i - \\mathbf{x}_i\\boldsymbol{\\beta}), \\mathbf{C}_i^{pred}-\\mathbf{k}_i(\\mathbf{C}_i + \\sigma^2\\mathbf{I})^{-1}\\mathbf{k}_i^\\top\\right)\n\\]"
  },
  {
    "objectID": "slides/16-gp.html#new-data-for-predictive-inference",
    "href": "slides/16-gp.html#new-data-for-predictive-inference",
    "title": "Gaussian Processes",
    "section": "New Data for Predictive Inference",
    "text": "New Data for Predictive Inference\nSuppose we are interested in understanding the mean deviation trend for the first 5 years from baseline (\\(t^{pred}\\in [0,5]\\)).\n\nstan_data &lt;- list(\n  N = dim(dataset)[1],\n  n = max(dataset$eye_id),\n  p = dim(Xmat)[2],\n  Y = dataset$mean_deviation,\n  t = dataset$time,\n  s = groupsizes,\n  X = Xmat,\n  # Time window for which we want predictive\n  t_pred = seq(0, 5, by = 0.25),\n  # Total length of this window\n  N_pred = 21\n)"
  },
  {
    "objectID": "slides/16-gp.html#stan-implementation",
    "href": "slides/16-gp.html#stan-implementation",
    "title": "Gaussian Processes",
    "section": "Stan Implementation",
    "text": "Stan Implementation\nSee the Stan Help page for details.\n\nfunctions {\n  // Analytical formula for latent GP conditional on Gaussian observations\n  vector gp_pred_rng(array[] real x_pred,\n                     vector Y,\n                     array[] real x,\n                     real mu,\n                     real alpha,\n                     real rho,\n                     real sigma,\n                     real delta) {\n    int N1 = rows(Y);\n    int N2 = size(x_pred);\n    vector[N2] f_pred;\n    {\n      matrix[N1, N1] L_Sigma;\n      vector[N1] Sigma_div_y;\n      matrix[N1, N2] C_x_xpred;\n      matrix[N1, N2] v_pred;\n      vector[N2] fpred_mu;\n      matrix[N2, N2] cov_fpred;\n      matrix[N2, N2] diag_delta;\n      matrix[N1, N1] Sigma;\n      Sigma = gp_exp_quad_cov(x, alpha, rho);\n      for (n in 1:N1) {\n        Sigma[n, n] = Sigma[n, n] + square(sigma);\n      }\n      L_Sigma = cholesky_decompose(Sigma);\n      Sigma_div_y = mdivide_left_tri_low(L_Sigma, Y - mu);\n      Sigma_div_y = mdivide_right_tri_low(Sigma_div_y', L_Sigma)';\n      C_x_xpred = gp_exp_quad_cov(x, x_pred, alpha, rho);\n      fpred_mu = (C_x_xpred' * Sigma_div_y);\n      v_pred = mdivide_left_tri_low(L_Sigma, C_x_xpred);\n      cov_fpred = gp_exp_quad_cov(x_pred, alpha, rho) - v_pred' * v_pred;\n      diag_delta = diag_matrix(rep_vector(delta, N2));\n\n      f_pred = multi_normal_rng(fpred_mu, cov_fpred + diag_delta);\n    }\n    return f_pred;\n  }\n}\n//...\ngenerated quantities {\n  matrix[n,Np] f_pred;\n  matrix[Np,Np] Cp;\n  Cp = gp_exp_quad_cov(t_pred, alpha, rho);\n  // Posterior predictive on fixed time grid for all eyes\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = s[i];\n    f_pred[i,] = mu[i] + \n      gp_pred_rng(\n          t_pred,\n          segment(Y, pos, n_i),\n          segment(t, pos, n_i),\n          mu[i], \n          alpha,\n          rho,\n          sigma,\n          delta\n        )';\n    pos = pos + n_i;\n  }\n}"
  },
  {
    "objectID": "slides/16-gp.html#visualizing-latent-effects-using-predictive-formulae",
    "href": "slides/16-gp.html#visualizing-latent-effects-using-predictive-formulae",
    "title": "Gaussian Processes",
    "section": "Visualizing latent effects using predictive formulae",
    "text": "Visualizing latent effects using predictive formulae"
  },
  {
    "objectID": "slides/16-gp.html#stan-speed-concerns",
    "href": "slides/16-gp.html#stan-speed-concerns",
    "title": "Gaussian Processes",
    "section": "Stan speed concerns",
    "text": "Stan speed concerns\n\nGP is notorious for not being scalable. Tl;dr is the need for matrix root / inverse computation.\nFor datasets covered in this class, Stan works well. For research purposes, worth exploring other specialized toolkits.\n\n\nDo a bit of software research on Wikipedia\nCode MCMC yourself to make it faster (e.g., using Rcpp)\nScalable Approximations (Activee research area)"
  },
  {
    "objectID": "slides/16-gp.html#a-summary",
    "href": "slides/16-gp.html#a-summary",
    "title": "Gaussian Processes",
    "section": "A summary",
    "text": "A summary\n\nGP is a flexible, high-dimensional model for handling correlated measurements over time.\nWith some basic knowledge about conditioning Gaussian random variables, we can implement posterior computation and prediction / interpolation.\nProgramming in Stan is straightforward but can be expensive with large number of observations."
  },
  {
    "objectID": "slides/16-gp.html#prepare-for-next-class",
    "href": "slides/16-gp.html#prepare-for-next-class",
    "title": "Gaussian Processes",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 04\nHave a nice spring break!"
  },
  {
    "objectID": "prepare/prepare-mar05.html",
    "href": "prepare/prepare-mar05.html",
    "title": "Prepare for March 5 lecture",
    "section": "",
    "text": "üìñ Review the Longitudinal Data lecture slides.\nüìñ Review BDA3 Chapter 21 to learn more about Gaussian process models."
  }
]