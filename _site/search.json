[
  {
    "objectID": "project-old.html",
    "href": "project-old.html",
    "title": "Final project",
    "section": "",
    "text": "Project proposal\n\ndue Friday, October 27 (Tuesday labs)\ndue Sunday, October 29 (Thursday labs)\n\nDraft report + peer review\n\ndue Tuesday, November 14 (Tuesday labs)\ndue Thursday, November 16 (Thursday labs)\n\nRound 1 submission (optional) due Friday, December 1\nPresentation + Presentation comments\n\nTuesday, December 5 (Tuesday labs)\nThursday, December 7 (Thursday labs)\n\nWritten report due Wednesday, December 13\nReproducibility + organization due Wednesday, December 13"
  },
  {
    "objectID": "project-old.html#timeline",
    "href": "project-old.html#timeline",
    "title": "Final project",
    "section": "",
    "text": "Project proposal\n\ndue Friday, October 27 (Tuesday labs)\ndue Sunday, October 29 (Thursday labs)\n\nDraft report + peer review\n\ndue Tuesday, November 14 (Tuesday labs)\ndue Thursday, November 16 (Thursday labs)\n\nRound 1 submission (optional) due Friday, December 1\nPresentation + Presentation comments\n\nTuesday, December 5 (Tuesday labs)\nThursday, December 7 (Thursday labs)\n\nWritten report due Wednesday, December 13\nReproducibility + organization due Wednesday, December 13"
  },
  {
    "objectID": "project-old.html#introduction",
    "href": "project-old.html#introduction",
    "title": "Final project",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group’s interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio using Quarto and GitHub, and your analysis and written report must be reproducible.\n\nLogistics\nYou will work on the project with your lab groups. The four primary deliverables for the final project are\n\na written, reproducible report detailing your analysis\na GitHub repository corresponding to your report\nslides and an in-person presentation\nformal peer review on another team’s work and presentation feedback"
  },
  {
    "objectID": "project-old.html#project-proposal",
    "href": "project-old.html#project-proposal",
    "title": "Final project",
    "section": "Project proposal",
    "text": "Project proposal\n\n\n\n\n\n\nDue dates\n\n\n\n\nFriday, October 27 (Tuesday labs)\nSunday, October 29 (Thursday labs)\n\n\n\nThe purpose of the project proposal is for your team to identify the data set you’re interested in analyzing for the project, do some preliminary exploratory data analysis, and begin to think about a modeling strategy . If you’re unsure where to find data, you can use the list of potential data sources on the Tips + resources page as a starting point. It may also help to think of topics you’re interested in investigating and find data sets on those topics.\nThe data set must meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns, such that at least 6 of the columns are useful and unique predictor variables.\n\ne.g., identifier variables such as “name”, “ID number”, etc. are not useful predictor variables.\ne.g., if you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nMay not be data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\n\n\n\n\n\n\nTypes of data sets to avoid\n\n\n\n\nData that are likely violate the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nData sets in which there is no information about how the data were originally collected\nData sets in which there are missing or unclear definitions about the observations and/or variables\n\n\n\nAsk a member of the teaching team if you’re unsure whether your data set meets the criteria.\nThe proposal will include the following sections:\n\nSection 1: Introduction\nThe introduction section includes\n\nan introduction to the subject matter you’re investigating (citing any relevant literature)\nthe motivation for your research question (citing any relevant literature)\nthe primary research question you are interested in exploring\nyour team’s hypotheses regarding the research question\n\nThis is a narrative about what you think regarding the research question, not formal statistical hypotheses.\n\n\n\n\nSection 2: Data description\nThe data description section includes\n\nthe source of the data set\na description of when and how the data were originally collected (by the original data curator, not necessarily how you found the data)\na description of the observations and general characteristics being measured\n\n\n\nSection 3: Initial exploratory data analysis\nIn this section, you will begin to explore the data. This includes using narrative, visualizations and summary statistics to describe the following:\n\ndistribution of the response variable\ndistributions of one potential quantitative predictor variable and one potential categorical predictor variable\nthe relationships between the response variable and each of the predictors from the previous step\na potential interaction effect you’re interested in exploring (it doesn’t have to be an interaction with the two predictors from above)\n\nThese steps are to help get you started on exploratory data analysis and will not be the complete EDA for the final report. The requirements above are minimum requirements, but your group is welcome to include more at this stage.\nIn this section, you will also describe any data cleaning you need to do to prepare for modeling, such as imputing missing values, collapsing levels for categorical predictors, creating new variables, summarizing data, etc.\n\n\nSection 4: Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes\n\na description of the response variable and list of all potential predictors\nregression model technique (multiple linear regression or logistic regression)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of the data folder. You do not need to include the data dictionary in the PDF document.\n\n\nSubmission\n\n\n\n\n\n\nImportant\n\n\n\nWrite your narrative and analysis for Sections 1 - 4 in the proposal.qmd file. Put the data set and the data dictionary in the data folder.\nSubmit the PDF of the proposal to Gradescope. Mark all pages of the document.\n\n\n\n\nGrading\nThe anticipated length, including all graphs, tables, narrative, etc., is 2 -4 pages; it may not exceed 5 pages.\nThe proposal is worth 15 points and will be graded based on accurately and comprehensively addressing the criteria stated above. Points will be assigned based on a holistic review of the project proposal.\n\nExcellent (14 - 15 points) : All required elements are completed and are accurate. There is a thorough exploration of the data as descrbied above, and the team has demonstrated a careful and thoughtful approach exploring the data and preparing it for analysis. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nStrong: (11 - 13 points): Requirements are mostly met, but there are some elements that are incomplete or inaccurate. Some minor revision of the work required before team is ready for modeling.\nSatisfactory (8 - 10 points): Requirements partially met, but there are some elements that are incomplete and/or inaccurate. Major revision of the work required before team is ready for modeling.\nNeeds Improvement (7 or fewer points points): Requirements are largely unmet, and there are large elements that are incomplete and/or inaccurate. Substantial revisions of the work required before team is ready for modeling."
  },
  {
    "objectID": "project-old.html#draft-report-peer-review",
    "href": "project-old.html#draft-report-peer-review",
    "title": "Final project",
    "section": "Draft report + peer review",
    "text": "Draft report + peer review\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\n\nDraft report\n\n\n\n\n\n\nDue dates\n\n\n\nDraft is due in your project GitHub repo at 9am on\n\nTuesday, November 14 (Tuesday labs)\nThursday, November 16 (Thursday labs)\n\n\n\nWrite the draft in the written-report.qmd file in your project repo. You do not need to submit the draft on Gradescope.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the body of the report, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.\nThis section also includes initial interpretations and conclusions drawn from the model.\n\n\nGrading\nThe draft will be graded based on whether there is demonstration of a reasonable attempt at each of the sections described below in the written-report.qmd file in our GitHub repo by the deadline.\n\n\n\nPeer review\n\n\n\n\n\n\nImportant\n\n\n\nPeer review comments are due in GitHub at 11:59pm on\n\nWednesday, November 15 (Tuesday labs)\nFriday, November 17 (Thursday labs)\n\n\n\nCritically reviewing others’ work is a crucial part of the scientific process, and STA 210 is no exception. Each lab team will be assigned two other teams’s projects to review. Each team should push their draft to their GitHub repo by the 9am on the day their lab’s draft is due. The lab that week will be dedicate to the peer review, so your team will have time to review and provide quality feedback to two other teams.\nDuring the peer review process, you will be provided read-only access to your partner teams’ GitHub repos. Provide your review in the form of GitHub issues to your partner team’s GitHub repo using the issue template provided in the repo.\n\nSteps for peer review\n\n\n\n\n\n\nPeer review assignments\n\n\n\nClick here to see which project your team is reviewing. You’ll spend about 30 minutes reviewing each project.\n\n\nWhen you get to lab, you should have access to the GitHub repos for the teams you’re reviewing. In GitHub, search the repositories for project, and you should see the repos for the projects you’re reviewing. You will be able to read the files in the repo and post issues, but you cannot push changes to the repo. You will have access to the repo until the deadline for the peer review.\nFor each team you’re reviewing:\n\nOpen that team’s repo, read the project draft, and browse the rest of the repo.\nGo to the Issues tab in that repo, click on New issue, and click on Get started for the Peer Review issue. Fill out this issue. You will answer the the following questions:\n\nDescribe the goal of the project.\nDescribe the data set used in the project. What are the observations in the data? What is the source of the data? How were the data originally collected?\nConsider the exploratory data analysis (EDA). Describe one aspect of the EDA that is effective in helping you understand the data. Provide constructive feedback on how the team might improve the EDA.\nDescribe the statistical methods and analysis approach used.\nProvide constructive feedback on how the team might improve their analysis. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but also feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation?\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?\n\n\n\n\nGrading\nThe peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team’s report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions."
  },
  {
    "objectID": "project-old.html#written-report",
    "href": "project-old.html#written-report",
    "title": "Final project",
    "section": "Written report",
    "text": "Written report\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\n\n\n\n\n\n\nNote\n\n\n\nBefore you finalize your write up, make sure the code chunks are not visible and all messages and warnings are suppressed.\n\n\nYou will submit the PDF of your final report on GitHub.\nThe PDF you submit must match the .qmd in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including tables and visualizations, must be no more than 10 pages long. There is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will overwhelmingly be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the variables in the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics, conditions, and diagnostics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. The model conditions and diagnostics are thoroughly and accurately assessed for their model. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the model’s predictive power is thoroughly assessed.\n\n\n\nDiscussion + Conclusion\nIn this section you’ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report.\n\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted and labeled. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages.\n\n\nSubmission\n\n\n\n\n\n\nImportant\n\n\n\nThe written report is due on Wednesday, December 13 at 11:59pm.\nTo submit your report, written-report.qmd and the rendered written-report.pdf to your team’s GitHub repo by the deadline. You will not submit the report on Gradescope.\nThe version of the report in the repo by Wednesday, December 13 will be the one that is graded."
  },
  {
    "objectID": "project-old.html#round1-submission",
    "href": "project-old.html#round1-submission",
    "title": "Final project",
    "section": "Round 1 submission (optional)",
    "text": "Round 1 submission (optional)\n\n\n\n\n\n\nDue date\n\n\n\nFriday, December 1 at 11:59pm on GitHub (all teams)\nReports submitted after this date will not receive preliminary feedback.\n\n\nThe Round 1 submission is an opportunity to receive detailed feedback on your analysis and written report before the final submission. Therefore, to make the feedback most useful, you must submit a complete written report to receive feedback. You will also be notified of the grade you would receive at that point. You will have the option to keep the grade (and thus you don’t need to turn in an updated report) or resubmit the written report by the final submission deadline to receive a new grade.\n\nTo submit the Round 1 submission:\n\nPush the updated written-report.qmd and written-report.pdf to your GitHub repo.\nOpen an issue with the title “Round 1 Submission”. You can use the template issue in the GitHub repo. Make sure I am tagged in the issue (@matackett), so I receive an email notification of your Round 1 submission. See Creating an issue from a repository for instructions on opening an issue. Please ask a member of the teaching team for assistance if you need help opening the issue.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that this is optional, so there is nograde penalty for not turning in a Round 1 submission. Due to time constraints at the end of the semester, only high-level feedback will be given for the reports submitted at the final written report deadline on December 13."
  },
  {
    "objectID": "project-old.html#presentation",
    "href": "project-old.html#presentation",
    "title": "Final project",
    "section": "Presentation",
    "text": "Presentation\n\n\n\n\n\n\nImportant\n\n\n\nPresentations will take place in class during labs December 5 & 7.\n\nClick here for the presentation order.\n\n\nIn addition to the written report, your team will also do an in-person presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. The presentation should be supported by slides that serve as a brief visual addition to the presentation. The presentation and slides will be graded for content and clarity.\nYou can create your slides with any software you like (Keynote, PowerPoint, Google Slides, etc.). We recommend choosing an option that’s easy to collaborate with, e.g., Google Slides.\n\n\n\n\n\n\nNote\n\n\n\nYou can also use Quarto to make your slides! While we won’t be covering making slides with Quarto in the class, we would be happy to help you with it in office hours. It’s no different than writing other documents with Quarto, so the learning curve will not be steep!\n\n\nThe presentation must be no longer than 6 minutes. It is fine if the presentation is shorter than 6 minutes, but it cannot exceed 6 minutes due to the limited time during lab.\nEvery team member is expected to speak in the presentation. Part of the grade will be whether every team member had a meaningful speaking role in the presentation.\n\nSlides\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\nGrading criteria\nThe presentation grade will be based on the following criteira:\n\nContent: The group told a unified story using the appropriate regression analysis.\nSlides: The presentation slides were organized, included clear and informative visualizations, and were easily readable.\nProfessionalism: The group’s communication style was clear and professional.\nTime Management: Team divided the time well and stayed within the 6 minute time limit, with each team member making a meaning contribution to the presentation. (assessed by the teaching team only).\n\n80% of the presentation grade will be the average of the teaching scores and 20% will be the average of the peer scores.\n\n\n\n\n\n\nImportant\n\n\n\nYou can submit the presentation slides in two ways:\n\nPut a PDF of the slides in the presentation folder in your team’s GitHub repo.\nPut the URL to your slides in the README of the presentation folder. If you share the URL, please make sure permissions are set so Prof. Tackett can view the slides.\n\nSlides must be submitted by the start of your lab on December 5 or 7. You will not submit the slides on Gradescope."
  },
  {
    "objectID": "project-old.html#presentation-comments",
    "href": "project-old.html#presentation-comments",
    "title": "Final project",
    "section": "Presentation comments",
    "text": "Presentation comments\n\n\n\n\n\n\nImportant\n\n\n\nClick here to find the teams you’re scoring and a link to the feedback form.\nThis portion of the project will be assessed individually.\n\n\n\nYou will provide feedback on two teams’ presentations. You can find your assigned teams and the link to the feedback from here. Please provide all scores and comments by the end of the lab session. There will be a few minutes between each presentation to submit scores.\nThe grade will be based on submitting the scores and comments for both of your assigned teams by the end of the presentation day (December 5 for Tuesday labs, December 7 for Thursday labs)."
  },
  {
    "objectID": "project-old.html#reproducibility-organization",
    "href": "project-old.html#reproducibility-organization",
    "title": "Final project",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\nproposal.qmd & proposal.pdf: Project proposal\n/data: Folder that contains the data set for the final project.\nproject.Rproj: File specifying the RStudio project\n/presentation: Folder with the presentation slides or link to slides.\n.gitignore: File that lists all files that are in the local RStudio project but not the GitHub repo\n/.github: Folder for peer review issue template\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable.\n\n\n\n\n\n\nImportant\n\n\n\nThe repo must be ready for grading by Wednesday, December 13 at 11:59pm."
  },
  {
    "objectID": "project-old.html#peer-teamwork-evaluation",
    "href": "project-old.html#peer-teamwork-evaluation",
    "title": "Final project",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly."
  },
  {
    "objectID": "project-old.html#overall-grading",
    "href": "project-old.html#overall-grading",
    "title": "Final project",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nProject proposal\n15 pts\n\n\nDraft report + peer review\n15 pts\n\n\nPresentation\n20 pts\n\n\nPresentation comments\n5 pts\n\n\nWritten report\n40 pts\n\n\nReproducibility + organization\n5 pts\n\n\n\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort."
  },
  {
    "objectID": "project-old.html#late-work-policy",
    "href": "project-old.html#late-work-policy",
    "title": "Final project",
    "section": "Late work policy",
    "text": "Late work policy\nThere is no late work accepted on the draft report or presentation. Other components of the project may be accepted up to 48 hours late. A 10% late deduction will apply for each 24-hour period late.\nBe sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final project",
    "section": "",
    "text": "Research questions due Thursday, September 26\nProject proposal due Thursday, October 3\nExploratory data analysis due Thursday, October 31\nPresentation + Presentation comments Monday, November 11 (in lab)\nAnalysis draft + peer review Monday, November 25 (peer review in lab)\nRound 1 submission (optional) due Friday, December 6\nWritten report due Thursday, December 12 at 9pm\nReproducibility + organization due Thursday, December 12 at 9pm",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#project-milestones",
    "href": "project.html#project-milestones",
    "title": "Final project",
    "section": "",
    "text": "Research questions due Thursday, September 26\nProject proposal due Thursday, October 3\nExploratory data analysis due Thursday, October 31\nPresentation + Presentation comments Monday, November 11 (in lab)\nAnalysis draft + peer review Monday, November 25 (peer review in lab)\nRound 1 submission (optional) due Friday, December 6\nWritten report due Thursday, December 12 at 9pm\nReproducibility + organization due Thursday, December 12 at 9pm",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#introduction",
    "href": "project.html#introduction",
    "title": "Final project",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group’s interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio using Quarto and GitHub, and your analysis and written report must be reproducible.\n\nLogistics\nYou will work on the project with your lab groups. The primary deliverables for the project are\n\nan in-person presentation about the exploratory data analysis and initial modeling\na written, reproducible final report detailing your analysis\na GitHub repository containing all work from the project\n\nThere are intermediate milestones and peer review assignments throughout the semester to help you work towards the final deliverables.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#research-questions",
    "href": "project.html#research-questions",
    "title": "Final project",
    "section": "Research questions",
    "text": "Research questions\nThe goal of this milestone is to discuss topics and develop potential research questions your team is interested in investigating for the project. You are only developing potential research questions; you do not need to have a data set identified at this point.\nDevelop three potential research questions. Include the following for each question:\n\nA statement of the research question.\nThe target population of interest for this question.\nA statement about your motivation for investigating this research question and why this question is important.\nIdeas about the type of data you might use to answer this question. Note: These are your ideas about the type of data you could use. You do not need to have a data set at this point.\n\n\nSubmission\nWrite your responses in research-questions.qmd in your team’s project GitHub repo. Push the qmd and rendered pdf documents to GitHub by the deadline, Thursday, September 26 at 11:59pm.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#project-proposal",
    "href": "project.html#project-proposal",
    "title": "Final project",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is for your team to identify the data set you’re interested in analyzing to investigate one of your potential research questions. You will also do some preliminary exploration of the response variable and begin thinking about the modeling strategy. If you’re unsure where to find data, you can use the list of potential data sources on the Tips + resources page as a starting point.\n\n\n\n\n\n\nImportant\n\n\n\nYou must the data set(s) in the proposal for the final project, unless instructed otherwise when given feedback.\n\n\nThe data set must meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns, such that at least 6 of the columns are useful and unique predictor variables.\n\ne.g., identifier variables such as “name”, “ID number”, etc. are not useful predictor variables.\ne.g., if you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nMay not be data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\n\n\n\n\n\n\nTypes of data sets to avoid\n\n\n\n\nData that are likely violate the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nData sets in which there is no information about how the data were originally collected\nData sets in which there are missing or unclear definitions about the observations and/or variables\n\n\n\nAsk a member of the teaching team if you’re unsure whether your data set meets the criteria.\nThe proposal will include the following sections:\n\nSection 1: Introduction\n\n\n\n\n\n\nTip\n\n\n\nReuse and iterate on the work from the Research Questions milestone.\n\n\n\nAn introduction to the subject matter you’re investigating (citing any relevant literature)\nStatement of a well-developed research question.\nThe motivation for your research question and why it is important\nYour team’s hypotheses regarding the research question\n\nThis is a narrative about what you think regarding the research question, not formal statistical hypotheses.\n\n\n\n\nSection 2: Data description\n\nThe source of the data set\nA description of when and how the data were originally collected (by the original data curator, not necessarily how you found the data)\nA description of the observations and general characteristics being measured\n\n\n\nSection 3: Initial exploratory data analysis\n\nDescription of data cleaning you need to do to prepare for analysis (can focus on the response variable for now), such as joining data sets, imputing missing values, variable transformation, creating a new variable, etc.\nVisualizations, summary statistics, and narrative to describe the distribution of the response variable.\n\n\n\nSection 4: Analysis approach\n\na description of the potential predictor variables of interest\nregression model technique (multiple linear regression for quantitative response variable or logistic regression for a categorical response variable)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of the data folder. You do not need to include the data dictionary in the PDF document.\n\n\nSubmission\nWrite your narrative and analysis for Sections 1 - 4 in the proposal.qmd file in your team’s GitHub repo. Put the data set and the data dictionary in the data folder in the repo. Push the qmd and rendered pdf documents to GitHub by the deadline, Thursday, October 3 at 11:59pm.\n\n\nGrading\nThe anticipated length, including all graphs, tables, narrative, etc., is 2 -4 pages.\nThe proposal is worth 10 points and will be graded based on accurately and comprehensively addressing the criteria stated above. Points will be assigned based on a holistic review of the project proposal.\n\nExcellent (9 - 10 points) : All required elements are completed and are accurate. The data set meets the requirements (or the team has otherwise discussed the data with Professor Tackett) and the data do not pose obvious violations to the modeling assumptions. There is a thoughtful and comprehensive description of the data and exploration of the response variable as descrbied above. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nStrong (7 - 8 points): Requirements are mostly met, but there are some elements that are incomplete or inaccurate. Some minor revision of the work required before team is ready for modeling.\nSatisfactory (5 - 6 points): Requirements partially met, but there are some elements that are incomplete and/or inaccurate. Major revision of the work required before team is ready for modeling.\nNeeds Improvement (4 or fewer points points): Requirements are largely unmet, and there are large elements that are incomplete and/or inaccurate. Substantial revisions of the work required before team is ready for modeling.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#eda",
    "href": "project.html#eda",
    "title": "Final project",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\n\n\n\n\nTip\n\n\n\nReuse and iterate on the work from the previous milestones.\n\n\nThe purpose of this milestone is begin exploring the data and get early feedback on your data and analysis. You will submit a draft of the beginning of your report that includes the introduction and exploratory data analysis, with an emphasis on the EDA. It will also help you prepare for the presentation of the exploratory data analysis results.\nBelow is a brief description of the sections to include in this step:\n\nIntroduction\nThis section includes an introduction to the project motivation, background, data, and research question.\n\n\nExploratory data analysis\nThis section includes the following:\n\nDescription of the data set and key variables.\nExploratory data analysis of the response variable and key predictor variables.\n\nUnivariate EDA of the response and key predictor variables.\nBivariate EDA of the response and key predictor variables\nPotential interaction effects.\n\n\n\n\nSubmission\nWrite your draft introduction and exploratory data analysis in the written-report.qmd file in your team’s GitHub repo. Push the qmd and rendered pdf documents to GitHub by the deadline, Thursday, October 31 at 11:59pm.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#presentation",
    "href": "project.html#presentation",
    "title": "Final project",
    "section": "Presentation",
    "text": "Presentation",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#draft-report-peer-review",
    "href": "project.html#draft-report-peer-review",
    "title": "Final project",
    "section": "Analysis + peer review",
    "text": "Analysis + peer review",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#round1-submission",
    "href": "project.html#round1-submission",
    "title": "Final project",
    "section": "Round 1 submission (optional)",
    "text": "Round 1 submission (optional)",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#written-report",
    "href": "project.html#written-report",
    "title": "Final project",
    "section": "Written report",
    "text": "Written report",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#reproducibility-organization",
    "href": "project.html#reproducibility-organization",
    "title": "Final project",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#peer-teamwork-evaluation",
    "href": "project.html#peer-teamwork-evaluation",
    "title": "Final project",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#overall-grading",
    "href": "project.html#overall-grading",
    "title": "Final project",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nResearch question\n3 pts\n\n\nProject proposal\n10 pts\n\n\nExploratory data analysis\n15 pts\n\n\nPresentation\n10 pts\n\n\nPresentation comments\n2 pts\n\n\nDraft report + peer review\n15 pts\n\n\nWritten report\n40 pts\n\n\nReproducibility + organization\n5 pts\n\n\n\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#late-work-policy",
    "href": "project.html#late-work-policy",
    "title": "Final project",
    "section": "Late work policy",
    "text": "Late work policy\nThere is no late work accepted on the draft report or presentation. Other components of the project may be accepted up to 48 hours late. A 10% late deduction will apply for each 24-hour period late.\nBe sure to turn in your work early to avoid any technological mishaps.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-tips.html",
    "href": "project-tips.html",
    "title": "Final project tips + resources",
    "section": "",
    "text": "Data sources\n\nSome resources that may be helpful as you find data:\n\nFiveThirtyEight data\nTidyTuesday\nData Is Plural\nR Data Sources for Regression Analysis\n\n\n\nOther data repositories\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies\n\n\n\n\nTips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your Qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works.\n\n\n\nFormatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\nAn alternative approach is to add the following code to the YAML:\n\nexecute:\n  echo: false\n  warning: false\n  message: false\n\n\n\n\nHeaders\n\nUse headers to clearly label each section. Make sure there is a space between the last # and the title, so the header renders correctly. For example, ###Section Title will not render as header, but ### Section Title will.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\n\nResize plots and figures, so you have more space for the narrative.\n\nResize individual figures: Use the code chunk header {r plot1, fig.height = 3, fig.width = 5}, replacing plot1 with a meaningful label and the height and width with values appropriate for your write up.\nResize all figures: Include the fig_width and fig_height options in your YAML header as shown below:\n\n\n\n---\ntitle: \"Your title\"\nauthor: \"Your names\"\nformat:\n  pdf:\n    fig-width: 7\n    fig-height: 5\n---\nReplace the height and width values with values appropriate for your write up.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\n\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\nIf you’re using base R function, i.e. when using the emplogit functions, put the code par(mfrow = c(rows,columns)) before the code to make the plots. For example, par(mfrow = c(2,3)) will arrange plots in a grid with 2 rows and 3 columns.\n\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\nUse coord_flip() to flip the x and y axes on the plot. This is useful if you a bar plot with an x-axis that is difficult to read due to overlapping text.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg |&gt;\n  count(manufacturer) |&gt;\n  mutate(manufacturer = str_to_title(manufacturer)) |&gt;\n  ggplot(aes(x = fct_reorder(manufacturer,n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document.\n\n\n\n\nAdditional resources\n\nExploring RStudio’s Visual Markdown Editor\nR for Data Science\nQuarto documentation:\n\nQuarto PDF Basics\nPresentations in Quarto\n\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package",
    "crumbs": [
      "Project",
      "Tips + resources"
    ]
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#announcements",
    "href": "slides/10-prop-of-estimators.html#announcements",
    "title": "Properties of estimators",
    "section": "Announcements",
    "text": "Announcements\n\nProject\n\nResearch questions due TODAY\nProposal due Thursday, October 3 at 11:59pm\n\nLab 03 due Thursday, October 3 at 11:59pm\nHW 02 due Thursday, October 3 at 11:59pm (released after class)\nStatistics experience due Tue, Nov 26 at 11:59pm"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#topics",
    "href": "slides/10-prop-of-estimators.html#topics",
    "title": "Properties of estimators",
    "section": "Topics",
    "text": "Topics\n\nCompute and interpret confidence interval for a single coefficient\nProperties of \\(\\hat{\\boldsymbol{\\beta}}\\)\nDefine “linear” model"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#computing-setup",
    "href": "slides/10-prop-of-estimators.html#computing-setup",
    "title": "Properties of estimators",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(kableExtra)  \nlibrary(patchwork)   \n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#data-ncaa-football-expenditures",
    "href": "slides/10-prop-of-estimators.html#data-ncaa-football-expenditures",
    "title": "Properties of estimators",
    "section": "Data: NCAA Football expenditures",
    "text": "Data: NCAA Football expenditures\nToday’s data come from Equity in Athletics Data Analysis and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a March 2022 Tidy Tuesday.\nWe will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :\n\ntotal_exp_m: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\nenrollment_th: Total student enrollment in the 2019 - 2020 academic year (in thousands)\ntype: institution type (Public or Private)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#regression-model",
    "href": "slides/10-prop-of-estimators.html#regression-model",
    "title": "Properties of estimators",
    "section": "Regression model",
    "text": "Regression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type, data = football)\ntidy(exp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#inference-for-beta_j",
    "href": "slides/10-prop-of-estimators.html#inference-for-beta_j",
    "title": "Properties of estimators",
    "section": "Inference for \\(\\beta_j\\)",
    "text": "Inference for \\(\\beta_j\\)\nWe often want to conduct inference on individual model coefficients\n\nHypothesis test: Is there a linear relationship between the response and \\(x_j\\)?\nConfidence interval: What is a plausible range of values \\(\\beta_j\\) can take?"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#confidence-interval-for-beta_j-1",
    "href": "slides/10-prop-of-estimators.html#confidence-interval-for-beta_j-1",
    "title": "Properties of estimators",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\n\nA plausible range of values for a population parameter is called a confidence interval\nUsing only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nSimilarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#what-confidence-means",
    "href": "slides/10-prop-of-estimators.html#what-confidence-means",
    "title": "Properties of estimators",
    "section": "What “confidence” means",
    "text": "What “confidence” means\n\n\nWe will construct \\(C\\%\\) confidence intervals\n\nThe confidence level impacts the width of the interval\n\n“Confidence” means if we were to take repeated samples of the same size as our data, fit regression lines using the same predictors, and calculate \\(C\\%\\) CIs for the coefficient of \\(x_j\\), then \\(C\\%\\) of those intervals will contain the true value of the coefficient \\(\\beta_j\\)\nNeed to balance precision and accuracy when selecting a confidence level"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#confidence-interval-for-beta_j-2",
    "href": "slides/10-prop-of-estimators.html#confidence-interval-for-beta_j-2",
    "title": "Properties of estimators",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE({\\hat{\\beta}_j})\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-p-1\\) degrees of freedom"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#computing-t-in-r",
    "href": "slides/10-prop-of-estimators.html#computing-t-in-r",
    "title": "Properties of estimators",
    "section": "Computing \\(t^*\\) in R",
    "text": "Computing \\(t^*\\) in R\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(football) - 2 - 1)\n\n[1] 1.97928\n\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(football) - 2 - 1)\n\n[1] 1.657235\n\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(football) - 2 - 1)\n\n[1] 2.61606"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#ci-for-coefficient-of-enrollment",
    "href": "slides/10-prop-of-estimators.html#ci-for-coefficient-of-enrollment",
    "title": "Properties of estimators",
    "section": "95% CI for coefficient of enrollment",
    "text": "95% CI for coefficient of enrollment\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n\n\n\n\n\n\n\n\\[\n\\hat{\\beta}_j \\pm t^* \\times SE(\\hat{\\beta}_j)\n\\]\n\n\n\\[\n0.7804 \\pm 1.9793 \\times 0.1103\n\\]\n\n\n\\[\n[0.562, 0.999]\n\\]"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#interpreting-the-ci",
    "href": "slides/10-prop-of-estimators.html#interpreting-the-ci",
    "title": "Properties of estimators",
    "section": "Interpreting the CI",
    "text": "Interpreting the CI\n🔗 edstem.org/us/courses/62513/discussion/648045"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#computing-ci-in-r",
    "href": "slides/10-prop-of-estimators.html#computing-ci-in-r",
    "title": "Properties of estimators",
    "section": "Computing CI in R",
    "text": "Computing CI in R\n\ntidy(exp_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n13.426\n25.239\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n0.562\n0.999\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n-19.466\n-6.986"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#motivation",
    "href": "slides/10-prop-of-estimators.html#motivation",
    "title": "Properties of estimators",
    "section": "Motivation",
    "text": "Motivation\n\n\nWe have discussed how to use least squares to find an estimator of \\(\\hat{\\boldsymbol{\\beta}}\\)\nHow do we know whether our least squares estimator is a “good” estimator?\nWhen we consider what makes an estimator “good”, we’ll look at three criteria:\n\nBias\nVariance\nMean squared error\n\nWe’ll take a look at these over the course of a few lectures and motivate why we might prefer using least squares to compute \\(\\hat{\\boldsymbol{\\beta}}\\) versus other methods"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#bias-and-variance",
    "href": "slides/10-prop-of-estimators.html#bias-and-variance",
    "title": "Properties of estimators",
    "section": "Bias and variance",
    "text": "Bias and variance\nSuppose you are throwing darts at a target\n\n\n\n\n\n\nImage source: Analytics Vidhya\n\n\n\n\nUnbiased: Darts distributed around the target\nBiased: Darts systematically away from the target\nVariance: Darts could be widely spread (high variance) or generally clustered together (low variance)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#bias-and-variance-1",
    "href": "slides/10-prop-of-estimators.html#bias-and-variance-1",
    "title": "Properties of estimators",
    "section": "Bias and variance",
    "text": "Bias and variance\n\nIdeal scenario: Darts are clustered around the target (unbiased and low variance)\nWorst case scenario: Darts are widely spread out and systematically far from the target (high bias and high variance)\nAcceptable scenario: There’s some trade-off between the bias and variance. For example, it may be acceptable for the darts to be clustered around a point that is close to the target (low bias and low variance)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#bias-and-variance-2",
    "href": "slides/10-prop-of-estimators.html#bias-and-variance-2",
    "title": "Properties of estimators",
    "section": "Bias and variance",
    "text": "Bias and variance\n\n\nEach time we take a sample of size \\(n\\), we can find the least squares estimator (throw dart at target)\nSuppose we take many independent samples of size \\(n\\) and find the least squares estimator for each sample (throw many darts at the target). Ideally,\n\nThe estimators are centered at the true parameter (unbiased)\nThe estimators are clustered around the true parameter (unbiased with low variance)\n\n\n\n\nLet’s take a look at the mean and variance of the least squares estimator"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta",
    "href": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta",
    "title": "Properties of estimators",
    "section": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)\nThe bias of an estimator is the difference between the estimator’s expected value and the true value of the parameter\n\nLet \\(\\hat{\\theta}\\) be an estimator of the parameter \\(\\theta\\). Then\n\\[\nBias(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\n\\]\n\n\nAn estimator is unbiased if the bias is 0 and thus \\(E(\\hat{\\theta}) = \\theta\\)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#finding-expected-value-and-variance",
    "href": "slides/10-prop-of-estimators.html#finding-expected-value-and-variance",
    "title": "Properties of estimators",
    "section": "Finding expected value and variance",
    "text": "Finding expected value and variance\nLet \\(\\mathbf{A}\\) be a \\(n \\times p\\) matrix of constants and \\(\\mathbf{b}\\) a \\(p \\times 1\\) vector of random variables. Then\n\\[\nE(\\mathbf{Ab}) = \\mathbf{A}E(\\mathbf{b})\n\\]\n\\[\nVar(\\mathbf{Ab}) = \\mathbf{A}Var(\\mathbf{b})\\mathbf{A}^T\n\\]"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta-1",
    "href": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta-1",
    "title": "Properties of estimators",
    "section": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)\nLet’s take a look at the expected value of the least squares estimator. Given \\(E(\\boldsymbol{\\epsilon}) = \\mathbf{0}\\),\n\\[\n\\begin{aligned}\nE(\\hat{\\boldsymbol{\\beta}}) &= E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}] \\\\[8pt]\n& = \\class{fragment}{E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon})]} \\\\[8pt]\n& = \\class{fragment}{E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}] + E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}]}\\\\[8pt]\n& = \\class{fragment}{\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^TE(\\boldsymbol{\\epsilon})} \\\\[8pt]\n& = \\class{fragment}{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta-2",
    "href": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta-2",
    "title": "Properties of estimators",
    "section": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)\nThe least squares estimator \\(\\hat{\\boldsymbol{\\beta}}\\) is an unbiased estimator of \\(\\boldsymbol{\\beta}\\)\n\\[\nE(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}\n\\]\n\n\nNow let’s take a look at the variance"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#variance-of-hatboldsymbolbeta",
    "href": "slides/10-prop-of-estimators.html#variance-of-hatboldsymbolbeta",
    "title": "Properties of estimators",
    "section": "Variance of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Variance of \\(\\hat{\\boldsymbol{\\beta}}\\)\n\\[\n\\begin{aligned}\nVar(\\hat{\\boldsymbol{\\beta}}) &= Var((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}) \\\\[8pt]\n& = \\class{fragment}{[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]Var(\\mathbf{y})[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]^T }\\\\[8pt]\n& = \\class{fragment}{[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]\\sigma^2_{\\epsilon}\\mathbf{I}[\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}]} \\\\[8pt]\n& = \\class{fragment}{\\sigma^2_{\\epsilon}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}]} \\\\[8pt]\n& = \\class{fragment}{\\sigma^2_{\\epsilon}(\\mathbf{X}^T\\mathbf{X})^{-1}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#variance-of-hatboldsymbolbeta-1",
    "href": "slides/10-prop-of-estimators.html#variance-of-hatboldsymbolbeta-1",
    "title": "Properties of estimators",
    "section": "Variance of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Variance of \\(\\hat{\\boldsymbol{\\beta}}\\)\n\\[\nVar(\\hat{\\boldsymbol{\\beta}}) =  \\sigma^2_{\\epsilon}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\\]\nWe will show that \\(\\hat{\\boldsymbol{\\beta}}\\) is the “best” estimator (has the lowest variance) among the class of linear unbiased estimators"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#linear-regression-model",
    "href": "slides/10-prop-of-estimators.html#linear-regression-model",
    "title": "Properties of estimators",
    "section": "“Linear” regression model",
    "text": "“Linear” regression model\nWhat does it mean for a model to be a “linear” regression model?\n\n\nLinear regression models are linear in the parameters, i.e. given an observation \\(y_i\\)\n\\[\ny_i = \\beta_0 + \\beta_1f_1(x_{i1}) +  \\dots + \\beta_pf_p(x_{ip}) + \\epsilon_i\n\\]\nThe functions \\(f_1, \\ldots, f_p\\) can be non-linear as long as \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are linear in \\(Y\\)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#identify-the-linear-regression-model",
    "href": "slides/10-prop-of-estimators.html#identify-the-linear-regression-model",
    "title": "Properties of estimators",
    "section": "Identify the linear regression model",
    "text": "Identify the linear regression model\n🔗 edstem.org/us/courses/62513/discussion/648051"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#identify-the-linear-regression-model-1",
    "href": "slides/10-prop-of-estimators.html#identify-the-linear-regression-model-1",
    "title": "Properties of estimators",
    "section": "Identify the linear regression model",
    "text": "Identify the linear regression model\n\n\n\\(y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i1}^2 + \\beta_3x_{i2}  + \\epsilon_i\\)\n\\(y_i = \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i1}x_{i2} + \\epsilon_i\\)\n\\(y_i = \\beta_0  + \\beta_1\\sin(x_{i1} + \\beta_2x_{i2}) + \\beta_3x_{i3} + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1e^{x_{i1}} + \\beta_2e^{x_{i2}} + \\epsilon_i\\)\n\\(y_i = \\exp(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3}) + \\epsilon_i\\)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#recap",
    "href": "slides/10-prop-of-estimators.html#recap",
    "title": "Properties of estimators",
    "section": "Recap",
    "text": "Recap\n\nComputed and interpreted confidence interval for a single coefficient\nShowed some properties of \\(\\hat{\\boldsymbol{\\beta}}\\)\nDefined “linear” model\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#announcements",
    "href": "slides/06-mlr-pt2.html#announcements",
    "title": "Multiple linear regression (MLR)",
    "section": "Announcements",
    "text": "Announcements\n\nLab 01 due on TODAY at 11:59pm\n\nPush work to GitHub repo\nSubmit final PDF on Gradescope + mark pages for each question\n\nHW 01 due Thursday, September 19 at 11:59pm\n\nWill be released after class\n\nTeam labs start on Monday"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#homework",
    "href": "slides/06-mlr-pt2.html#homework",
    "title": "Multiple linear regression (MLR)",
    "section": "Homework",
    "text": "Homework\nHomework will generally be split into two sections:\n\n1️⃣ Conceptual exercises\nThe conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\n\nYou may write the answers and associated work for conceptual exercises by hand or type them in your Quarto document."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#homework-1",
    "href": "slides/06-mlr-pt2.html#homework-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Homework",
    "text": "Homework\n2️⃣ Applied exercises\nThe applied exercises are focused on applying the concepts to analyze data.\nAll work for the applied exercises must be typed in your Quarto document following a reproducible workflow.\nWrite all narrative using complete sentences and include informative axis labels / titles on visualizations."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#topics",
    "href": "slides/06-mlr-pt2.html#topics",
    "title": "Multiple linear regression (MLR)",
    "section": "Topics",
    "text": "Topics\n\nCategorical predictors and interaction terms\nAssess model fit using RSME and \\(R^2\\)\nCompare models using \\(Adj. R^2\\)\nIntroduce LaTex"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#computing-setup",
    "href": "slides/06-mlr-pt2.html#computing-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(viridis) #adjust color palette\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#data-peer-to-peer-lender",
    "href": "slides/06-mlr-pt2.html#data-peer-to-peer-lender",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 4\n   annual_income_th debt_to_income verified_income interest_rate\n              &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1             59           0.558  Not Verified            10.9 \n 2             60           1.31   Not Verified             9.92\n 3             75           1.06   Verified                26.3 \n 4             75           0.574  Not Verified             9.92\n 5            254           0.238  Not Verified             9.43\n 6             67           1.08   Source Verified          9.92\n 7             28.8         0.0997 Source Verified         17.1 \n 8             80           0.351  Not Verified             6.08\n 9             34           0.698  Not Verified             7.97\n10             80           0.167  Source Verified         12.6 \n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#variables",
    "href": "slides/06-mlr-pt2.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income_th: Annual income (in $1000s)\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nResponse: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#response-vs.-predictors",
    "href": "slides/06-mlr-pt2.html#response-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Response vs. predictors",
    "text": "Response vs. predictors\n\nGoal: Use these predictors in a single model to understand variability in interest rate."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#model-fit-in-r",
    "href": "slides/06-mlr-pt2.html#model-fit-in-r",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit in R",
    "text": "Model fit in R\n\nint_fit &lt;- lm(interest_rate ~ debt_to_income + verified_income  + annual_income_th,\n              data = loan50)\n\ntidy(int_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#matrix-form-of-multiple-linear-regression",
    "href": "slides/06-mlr-pt2.html#matrix-form-of-multiple-linear-regression",
    "title": "Multiple linear regression (MLR)",
    "section": "Matrix form of multiple linear regression",
    "text": "Matrix form of multiple linear regression\n\\[\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n=\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_{11} & \\dots & x_{1p}\\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n1 &  x_{n1} & \\dots &x_{np}\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n\\]\n\nHow might we include a categorical predictor with \\(k\\) levels in the design matrix, \\(\\mathbf{X}\\) ?"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#indicator-variables",
    "href": "slides/06-mlr-pt2.html#indicator-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables",
    "text": "Indicator variables\nSuppose we want to predict the amount of sleep a Duke student gets based on whether they are in Pratt (Pratt Yes/ No are the only two options). Consider the model\n\\[\nSleep_i = \\beta_0 + \\beta_1\\mathbf{1}(Pratt_i = \\texttt{Yes}) + \\beta_2\\mathbf{1}(Pratt_i = \\texttt{No})\n\\]\n\n\nWrite out the design matrix for this hypothesized linear model.\nDemonstrate that the design matrix is not of full column rank (that is, affirmatively provide one of the columns in terms of the others).\nUse this intuition to explain why when we include categorical predictors, we cannot include both indicators for every level of the variable and an intercept."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#indicator-variables-1",
    "href": "slides/06-mlr-pt2.html#indicator-variables-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(k\\) levels\nWe can make \\(k\\) indicator variables from the data - one indicator for each level\nAn indicator (dummy) variable takes values 1 or 0\n\n1 if the observation belongs to that level\n0 if the observation does not belong to that level"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#indicator-variables-for-verified_income",
    "href": "slides/06-mlr-pt2.html#indicator-variables-for-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables for verified_income",
    "text": "Indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\n\n\n\n# A tibble: 3 × 4\n  verified_income not_verified source_verified verified\n  &lt;fct&gt;                  &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified               1               0        0\n2 Verified                   0               0        1\n3 Source Verified            0               1        0"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#indicator-variables-in-the-model",
    "href": "slides/06-mlr-pt2.html#indicator-variables-in-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables in the model",
    "text": "Indicator variables in the model\n\nWe will use \\(k-1\\) of the indicator variables in the model.\nThe baseline is the category that doesn’t have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\n\n\n\nloan50 |&gt;\n  select(verified_income, source_verified, verified) |&gt;\n  slice(1, 3, 6)\n\n# A tibble: 3 × 3\n  verified_income source_verified verified\n  &lt;fct&gt;                     &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified                  0        0\n2 Verified                      0        1\n3 Source Verified               1        0\n\n\n\nTake a look at the design matrix in AE 02"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#interpreting-verified_income",
    "href": "slides/06-mlr-pt2.html#interpreting-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\n\n\nThe baseline level is Not verified.\nPeople with source verified income are expected to take a loan with an interest rate that is 2.211% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant.\n\n\n\n\n\nWhat is the expected interest rate for someone whose income is Verified, who has a debt-to-income ratio of 0 and annual income of $0?"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#interaction-terms-1",
    "href": "slides/06-mlr-pt2.html#interaction-terms-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#interest-rate-vs.-annual-income",
    "href": "slides/06-mlr-pt2.html#interest-rate-vs.-annual-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is a potential interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#interaction-term-in-model",
    "href": "slides/06-mlr-pt2.html#interaction-term-in-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nint_fit_2 &lt;- lm(interest_rate ~ debt_to_income + verified_income + annual_income_th + verified_income * annual_income_th,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.560\n2.034\n4.700\n0.000\n\n\ndebt_to_income\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n3.577\n2.539\n1.409\n0.166\n\n\nverified_incomeVerified\n9.923\n3.654\n2.716\n0.009\n\n\nannual_income_th\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_th\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_th\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#interpreting-interaction-terms",
    "href": "slides/06-mlr-pt2.html#interpreting-interaction-terms",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#rmse-r2",
    "href": "slides/06-mlr-pt2.html#rmse-r2",
    "title": "Multiple linear regression (MLR)",
    "section": "RMSE & \\(R^2\\)",
    "text": "RMSE & \\(R^2\\)\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#comparing-models",
    "href": "slides/06-mlr-pt2.html#comparing-models",
    "title": "Multiple linear regression (MLR)",
    "section": "Comparing models",
    "text": "Comparing models\n\n\nWhen comparing models, do we prefer the model with the lower or higher RMSE?\nThough we use \\(R^2\\) to assess the model fit, it is generally unreliable for comparing models with different number of predictors. Why?\n\n\\(R^2\\) will stay the same or increase as we add more variables to the model . Let’s show why this is true.\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#adjusted-r2",
    "href": "slides/06-mlr-pt2.html#adjusted-r2",
    "title": "Multiple linear regression (MLR)",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#r2-and-adjusted-r2",
    "href": "slides/06-mlr-pt2.html#r2-and-adjusted-r2",
    "title": "Multiple linear regression (MLR)",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}\\]\n\n\n\\[R^2_{adj} = 1 - \\frac{SSR/(n-p-1)}{SST/(n-1)}\\]\nwhere\n\n\\(n\\) is the number of observations used to fit the model\n\\(p\\) is the number of terms (not including the intercept) in the model"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#using-r2-and-adjusted-r2",
    "href": "slides/06-mlr-pt2.html#using-r2-and-adjusted-r2",
    "title": "Multiple linear regression (MLR)",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables\n\n\n\n📋 https://sta221-fa24.netlify.app/ae/ae-02-mlr"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#latex-in-this-class",
    "href": "slides/06-mlr-pt2.html#latex-in-this-class",
    "title": "Multiple linear regression (MLR)",
    "section": "Latex in this class",
    "text": "Latex in this class\nFor this class you will need to be able to…\n\nProperly write mathematical symbols, e.g., \\(\\beta_1\\) not B1, \\(R^2\\) not R2\nWrite basic regression equations, e.g., \\(\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\nWrite matrix equations: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\)\nWrite hypotheses (we’ll start this next week), e.g., \\(H_0: \\beta = 0\\)\n\nYou are welcome to but not required to write math proofs using LaTex."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#recap",
    "href": "slides/06-mlr-pt2.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nInterpreted categorical predictors and interaction terms\nAssessed model fit using RSME and \\(R^2\\)\nCompared models using \\(Adj. R^2\\)\nIntroduced LaTex"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#next-class",
    "href": "slides/06-mlr-pt2.html#next-class",
    "title": "Multiple linear regression (MLR)",
    "section": "Next class",
    "text": "Next class\n\nGeometric interpretation\nInference for regression\nSee Sep 17 prepare\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#announcements",
    "href": "slides/03-slr-model-assessment.html#announcements",
    "title": "SLR: Model Assessment",
    "section": "Announcements",
    "text": "Announcements\n\nOffice hours start this week. See schedule on Overview page of the course website or on Canvas."
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#topics",
    "href": "slides/03-slr-model-assessment.html#topics",
    "title": "SLR: Model Assessment",
    "section": "Topics",
    "text": "Topics\n\nUse R to conduct exploratory data analysis and fit a model\nEvaluate models using RMSE and \\(R^2\\)\nUse analysis of variance to partition variability in the response variable"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#computing-set-up",
    "href": "slides/03-slr-model-assessment.html#computing-set-up",
    "title": "SLR: Model Assessment",
    "section": "Computing set up",
    "text": "Computing set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling (includes broom, yardstick, and other packages)\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme for ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#data-houses-in-duke-forest",
    "href": "slides/03-slr-model-assessment.html#data-houses-in-duke-forest",
    "title": "SLR: Model Assessment",
    "section": "Data: Houses in Duke Forest",
    "text": "Data: Houses in Duke Forest\n\n\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\nSource: openintro::duke_forest\n\n\n\n\nGoal: Use the area (in square feet) to understand variability in the price of houses in Duke Forest."
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#clone-repo-start-new-rstudio-project",
    "href": "slides/03-slr-model-assessment.html#clone-repo-start-new-rstudio-project",
    "title": "SLR: Model Assessment",
    "section": "Clone repo + Start new RStudio project",
    "text": "Clone repo + Start new RStudio project\n\nGo to the course organization. Click on the repo with the prefix ae-01. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File → New Project → Version Control → Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#two-statistics",
    "href": "slides/03-slr-model-assessment.html#two-statistics",
    "title": "SLR: Model Assessment",
    "section": "Two statistics",
    "text": "Two statistics\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\n\n\nWhat indicates a good model fit? Higher or lower RMSE? Higher or lower \\(R^2\\)?"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#rmse",
    "href": "slides/03-slr-model-assessment.html#rmse",
    "title": "SLR: Model Assessment",
    "section": "RMSE",
    "text": "RMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}}\n\\]\n\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nThe value of RMSE is more useful for comparing across models than evaluating a single model (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#analysis-of-variance-anova",
    "href": "slides/03-slr-model-assessment.html#analysis-of-variance-anova",
    "title": "SLR: Model Assessment",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of Variance (ANOVA): Technique to partition variability in \\(Y\\) by the sources of variability"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#total-variability-response",
    "href": "slides/03-slr-model-assessment.html#total-variability-response",
    "title": "SLR: Model Assessment",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\n\n\n\n\nMin\nMedian\nMax\nMean\nStd.Dev\n\n\n\n\n95000\n540000\n1520000\n559898.7\n225448.1"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#partition-sources-of-variability-in-price",
    "href": "slides/03-slr-model-assessment.html#partition-sources-of-variability-in-price",
    "title": "SLR: Model Assessment",
    "section": "Partition sources of variability in price",
    "text": "Partition sources of variability in price"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#total-variability-response-1",
    "href": "slides/03-slr-model-assessment.html#total-variability-response-1",
    "title": "SLR: Model Assessment",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\\[\\text{Sum of Squares Total (SST)} = \\sum_{i=1}^n(y_i - \\bar{y})^2 = (n-1)s_y^2\\]"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#explained-variability-model",
    "href": "slides/03-slr-model-assessment.html#explained-variability-model",
    "title": "SLR: Model Assessment",
    "section": "Explained variability (Model)",
    "text": "Explained variability (Model)\n\n\\[\\text{Sum of Squares Model (SSM)} = \\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#unexplained-variability-residuals",
    "href": "slides/03-slr-model-assessment.html#unexplained-variability-residuals",
    "title": "SLR: Model Assessment",
    "section": "Unexplained variability (Residuals)",
    "text": "Unexplained variability (Residuals)\n\n\\[\\text{Sum of Squares Residuals (SSR)} = \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2\\]"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#sum-of-squares",
    "href": "slides/03-slr-model-assessment.html#sum-of-squares",
    "title": "SLR: Model Assessment",
    "section": "Sum of Squares",
    "text": "Sum of Squares\n\n\\[\n\\begin{aligned}\n\\color{#407E99}{SST} \\hspace{5mm}&= &\\color{#993399}{SSM} &\\hspace{5mm} +  &\\color{#8BB174}{SSR} \\\\[10pt]\n\\color{#407E99}{\\sum_{i=1}^n(y_i - \\bar{y})^2} \\hspace{5mm}&= &\\color{#993399}{\\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2} &\\hspace{5mm}+ &\\color{#8BB174}{\\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#r2",
    "href": "slides/03-slr-model-assessment.html#r2",
    "title": "SLR: Model Assessment",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nThe coefficient of determination \\(R^2\\) is the proportion of variation in the response, \\(Y\\), that is explained by the regression model\n\n\\[\\large{R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}}\\]\n\n\nWhat is the range of \\(R^2\\)? Does \\(R^2\\) have units?"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#interpreting-r2",
    "href": "slides/03-slr-model-assessment.html#interpreting-r2",
    "title": "SLR: Model Assessment",
    "section": "Interpreting $R^2$",
    "text": "Interpreting $R^2$\n\nQuestionSubmit\n\n\n\nSubmit your response to the following question on Ed Discussion.\n\nThe \\(R^2\\) of the model for price from area of houses in Duke Forest is 44.5%. Which of the following is the correct interpretation of this value?\n\nArea correctly predicts 44.5% of price for houses in Duke Forest.\n44.5% of the variability in price for houses in Duke Forest can be explained by area.\n44.5% of the variability in area for houses in Duke Forest can be explained by price.\n44.5% of the time price for houses in Duke Forest can be predicted by area.\n\nDo you think this model is useful for explaining variability in the price of Duke Forest houses?\n\n\n\n\n\n\n\n\n🔗 https://edstem.org/us/courses/62513/discussion/629888"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#augmented-data-frame",
    "href": "slides/03-slr-model-assessment.html#augmented-data-frame",
    "title": "SLR: Model Assessment",
    "section": "Augmented data frame",
    "text": "Augmented data frame\nUse the augment() function from the broom package to add columns for predicted values, residuals, and other observation-level model statistics\n\n\nduke_forest_aug &lt;- augment(duke_forest_fit)\nduke_forest_aug\n\n# A tibble: 98 × 8\n     price  area  .fitted  .resid   .hat  .sigma   .cooksd .std.resid\n     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 1520000  6040 1079931. 440069. 0.133  162605. 0.604         2.80  \n 2 1030000  4475  830340. 199660. 0.0435 168386. 0.0333        1.21  \n 3  420000  1745  394951.  25049. 0.0226 169664. 0.000260      0.150 \n 4  680000  2091  450132. 229868. 0.0157 168011. 0.0150        1.37  \n 5  428500  1772  399257.  29243. 0.0220 169657. 0.000345      0.175 \n 6  456000  1950  427645.  28355. 0.0182 169659. 0.000266      0.170 \n 7 1270000  3909  740072. 529928. 0.0250 160502. 0.130         3.18  \n 8  557450  2841  569744. -12294. 0.0102 169679. 0.0000277    -0.0732\n 9  697500  3924  742465. -44965. 0.0254 169620. 0.000948     -0.270 \n10  650000  2173  463209. 186791. 0.0145 168582. 0.00912       1.11  \n# ℹ 88 more rows"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#finding-rmse-in-r",
    "href": "slides/03-slr-model-assessment.html#finding-rmse-in-r",
    "title": "SLR: Model Assessment",
    "section": "Finding RMSE in R",
    "text": "Finding RMSE in R\nUse the rmse() function from the yardstick package (part of tidymodels)\n\nrmse(duke_forest_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     167067.\n\n\n\n\nDo you think this model is useful for predicting the price of Duke Forest houses?"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#finding-r2-in-r",
    "href": "slides/03-slr-model-assessment.html#finding-r2-in-r",
    "title": "SLR: Model Assessment",
    "section": "Finding \\(R^2\\) in R",
    "text": "Finding \\(R^2\\) in R\nUse the rsq() function from the yardstick package (part of tidymodels)\n\nrsq(duke_forest_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.445\n\n\n\n\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(duke_forest_fit)$r.squared\n\n[1] 0.4451945"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#recap",
    "href": "slides/03-slr-model-assessment.html#recap",
    "title": "SLR: Model Assessment",
    "section": "Recap",
    "text": "Recap\n\nUsed R to conduct exploratory data analysis and fit a model\nEvaluated models using RMSE and \\(R^2\\)\nUsed analysis of variance to partition variability in the response variable"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#next-class",
    "href": "slides/03-slr-model-assessment.html#next-class",
    "title": "SLR: Model Assessment",
    "section": "Next class",
    "text": "Next class\n\nMatrix representation of simple linear regression\n\nSee Sep 5 prepare\n\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#announcements",
    "href": "slides/07-mlr-pt3.html#announcements",
    "title": "ANOVA + Geometric interpretation",
    "section": "Announcements",
    "text": "Announcements\n\nLab 02 due on Thursday at 11:59pm\n\nPush work to GitHub repo\nSubmit final PDF on Gradescope + select all team members + mark pages for each question\n\nHW 01 due Thursday at 11:59pm\n\nNote submission instructions"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#homework-submission",
    "href": "slides/07-mlr-pt3.html#homework-submission",
    "title": "ANOVA + Geometric interpretation",
    "section": "Homework submission",
    "text": "Homework submission\nIf you write your responses to Exercises 1 - 4 by hand, you will need to combine your written work to the completed PDF for Exercises 5 - 10 before submitting on Gradescope.\nInstructions to combine PDFs:\n\nPreview (Mac): support.apple.com/guide/preview/combine-pdfs-prvw43696/mac\nAdobe (Mac or PC): helpx.adobe.com/acrobat/using/merging-files-single-pdf.html\n\nGet free access to Adobe Acrobat as a Duke student: oit.duke.edu/help/articles/kb0030141/"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#latex-in-this-class",
    "href": "slides/07-mlr-pt3.html#latex-in-this-class",
    "title": "ANOVA + Geometric interpretation",
    "section": "Latex in this class",
    "text": "Latex in this class\nFor this class you will need to be able to…\n\nProperly write mathematical symbols, e.g., \\(\\beta_1\\) not B1, \\(R^2\\) not R2\nWrite basic regression equations, e.g., \\(\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\nWrite matrix equations: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\)\nWrite hypotheses (we’ll start this next week), e.g., \\(H_0: \\beta = 0\\)\n\nYou are welcome to but not required to write math proofs using LaTex."
  },
  {
    "objectID": "slides/07-mlr-pt3.html#topics",
    "href": "slides/07-mlr-pt3.html#topics",
    "title": "ANOVA + Geometric interpretation",
    "section": "Topics",
    "text": "Topics\n\nCompare models using Adjusted \\(R^2\\)\nIntroduce the ANOVA table\nUse a geometric interpretation to find the least squares estimates"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#computing-setup",
    "href": "slides/07-mlr-pt3.html#computing-setup",
    "title": "ANOVA + Geometric interpretation",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(viridis) #adjust color palette\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#data-peer-to-peer-lender",
    "href": "slides/07-mlr-pt3.html#data-peer-to-peer-lender",
    "title": "ANOVA + Geometric interpretation",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 4\n   annual_income_th debt_to_income verified_income interest_rate\n              &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1             59           0.558  Not Verified            10.9 \n 2             60           1.31   Not Verified             9.92\n 3             75           1.06   Verified                26.3 \n 4             75           0.574  Not Verified             9.92\n 5            254           0.238  Not Verified             9.43\n 6             67           1.08   Source Verified          9.92\n 7             28.8         0.0997 Source Verified         17.1 \n 8             80           0.351  Not Verified             6.08\n 9             34           0.698  Not Verified             7.97\n10             80           0.167  Source Verified         12.6 \n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#variables",
    "href": "slides/07-mlr-pt3.html#variables",
    "title": "ANOVA + Geometric interpretation",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income_th: Annual income (in $1000s)\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nResponse: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#model-fit-in-r",
    "href": "slides/07-mlr-pt3.html#model-fit-in-r",
    "title": "ANOVA + Geometric interpretation",
    "section": "Model fit in R",
    "text": "Model fit in R\n\nint_fit &lt;- lm(interest_rate ~ debt_to_income + verified_income  + annual_income_th, data = loan50)\n\nint_fit2 &lt;- lm(interest_rate ~ debt_to_income + verified_income  + annual_income_th + verified_income * annual_income_th, data = loan50)"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#rmse-r2",
    "href": "slides/07-mlr-pt3.html#rmse-r2",
    "title": "ANOVA + Geometric interpretation",
    "section": "RMSE & \\(R^2\\)",
    "text": "RMSE & \\(R^2\\)\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#comparing-models",
    "href": "slides/07-mlr-pt3.html#comparing-models",
    "title": "ANOVA + Geometric interpretation",
    "section": "Comparing models",
    "text": "Comparing models\n\n\nThough we use \\(R^2\\) to assess the model fit, it is generally unreliable for comparing models with different number of predictors. Why?\n\n\\(R^2\\) will stay the same or increase as we add more variables to the model . Let’s show why this is true.\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables."
  },
  {
    "objectID": "slides/07-mlr-pt3.html#adjusted-r2",
    "href": "slides/07-mlr-pt3.html#adjusted-r2",
    "title": "ANOVA + Geometric interpretation",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#r2-and-adjusted-r2",
    "href": "slides/07-mlr-pt3.html#r2-and-adjusted-r2",
    "title": "ANOVA + Geometric interpretation",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}\\]\n\n\n\\[R^2_{adj} = 1 - \\frac{SSR/(n-p-1)}{SST/(n-1)}\\]\nwhere\n\n\\(n\\) is the number of observations used to fit the model\n\\(p\\) is the number of terms (not including the intercept) in the model"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#compare-models",
    "href": "slides/07-mlr-pt3.html#compare-models",
    "title": "ANOVA + Geometric interpretation",
    "section": "Compare models",
    "text": "Compare models\nWhich model would you select int_fit (main effects only) or int_fit2 (main effects + interaction) based on…\n\\(R^2\\)\n\nglance(int_fit)$r.squared\n\n[1] 0.279854\n\nglance(int_fit2)$r.squared\n\n[1] 0.2963437\n\n\n\n\\(Adj. R^2\\)\n\nglance(int_fit)$adj.r.squared\n\n[1] 0.215841\n\nglance(int_fit2)$adj.r.squared\n\n[1] 0.1981591"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#anova-table",
    "href": "slides/07-mlr-pt3.html#anova-table",
    "title": "ANOVA + Geometric interpretation",
    "section": "ANOVA table",
    "text": "ANOVA table\n\n\n\nSource\nSum of squares\nDF\nMean square\nF\n\n\n\n\nModel\n\\(\\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2\\)\n\\(p\\)\n\\(SSM / p\\)\n\\(MSM / MSR\\)\n\n\nResidual\n\\(\\sum_{i=1}^n(y_i- \\hat{y}_i)^2\\)\n\\(n - p - 1\\)\n\\(SSR / (n - p - 1)\\)\n\n\n\nTotal\n\\(\\sum_{i = 1}^n(y_i - \\bar{y})^2\\)\n\\(n - 1\\)\n\n\n\n\n\n\n\n\nThe degrees of freedom (df) are the number of independent pieces of information used to calculate a statistic.\nMean square (MS) is the sum of squares divided by the associated degrees of freedom."
  },
  {
    "objectID": "slides/07-mlr-pt3.html#using-r2-and-adjusted-r2",
    "href": "slides/07-mlr-pt3.html#using-r2-and-adjusted-r2",
    "title": "ANOVA + Geometric interpretation",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression",
    "href": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression",
    "title": "ANOVA + Geometric interpretation",
    "section": "Geometry of least squares regression",
    "text": "Geometry of least squares regression\n\n\nLet \\(\\text{Col}(\\mathbf{X})\\) be the column space of \\(\\mathbf{X}\\): the set all possible linear combinations (span) of the columns of \\(\\mathbf{X}\\)\nThe vector of responses \\(\\mathbf{y}\\) is not in \\(\\text{Col}(\\mathbf{X})\\).\nGoal: Find another vector \\(\\mathbf{z} = \\mathbf{Xb}\\) that is in \\(\\text{Col}(\\mathbf{X})\\) and is as close as possible to \\(\\mathbf{y}\\).\n\n\\(\\mathbf{z}\\) is called a projection of \\(\\mathbf{y}\\) onto \\(\\text{Col}(\\mathbf{X})\\) ."
  },
  {
    "objectID": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression-1",
    "href": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression-1",
    "title": "ANOVA + Geometric interpretation",
    "section": "Geometry of least squares regression",
    "text": "Geometry of least squares regression\n\n\nFor any \\(\\mathbf{z} = \\mathbf{Xb}\\) in \\(\\text{Col}(\\mathbf{X})\\), the vector \\(\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}\\) is the difference between \\(\\mathbf{y}\\) and \\(\\mathbf{Xb}\\).\n\nIn other words, we want to minimize \\(||\\mathbf{e}||^2 = ||\\mathbf{y} - \\mathbf{Xb}||^2\\)\n\nThis is minimized for the \\(\\mathbf{b}\\) ( we’ll call it \\(\\hat{\\boldsymbol{\\beta}}\\) ) that makes \\(\\mathbf{e}\\) orthogonal to \\(\\text{Col}(\\mathbf{X})\\)\nRecall: If \\(\\mathbf{e}\\) is orthogonal to \\(\\text{Col}(\\mathbf{X})\\), then the inner product of any vector in \\(\\text{Col}(\\mathbf{X})\\) and \\(\\mathbf{e}\\) is 0 \\(\\Rightarrow \\mathbf{X}^T\\mathbf{e} = \\mathbf{0}\\)"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression-2",
    "href": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression-2",
    "title": "ANOVA + Geometric interpretation",
    "section": "Geometry of least squares regression",
    "text": "Geometry of least squares regression\n\nTherefore, we have\n\n\\[\n\\mathbf{X}^T(\\mathbf{y} - \\mathbf{Xb}) = \\mathbf{0}\n\\]\nLet’s solve for \\(\\mathbf{b}\\) to get the least squares estimate."
  },
  {
    "objectID": "slides/07-mlr-pt3.html#recap",
    "href": "slides/07-mlr-pt3.html#recap",
    "title": "ANOVA + Geometric interpretation",
    "section": "Recap",
    "text": "Recap\n\nCompared models using Adjusted \\(R^2\\)\nIntroduced the ANOVA table\nUsed a geometric interpretation to find the least squares estimates"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#next-class",
    "href": "slides/07-mlr-pt3.html#next-class",
    "title": "ANOVA + Geometric interpretation",
    "section": "Next class",
    "text": "Next class\n\nInference for regression\nSee Sep 19 prepare\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/08-inference.html#announcements",
    "href": "slides/08-inference.html#announcements",
    "title": "Inference for regression",
    "section": "Announcements",
    "text": "Announcements\n\nLab 02 due on TODAY at 11:59pm\nHW 01 due TODAY at 11:59pm\nStatistics experience due Tue, Nov 26 at 11:59pm"
  },
  {
    "objectID": "slides/08-inference.html#statistics-experience",
    "href": "slides/08-inference.html#statistics-experience",
    "title": "Inference for regression",
    "section": "Statistics experience",
    "text": "Statistics experience\nGoal: Engage with statistics / data science outside the classroom and connect your experience with what you’re learning in the course.\nWhat: Have a statistics experience + create a slide reflecting on the experience. Counts as a homework grade.\nWhen: Must do the activity this semester. Reflection due Tuesday, November 26 at 11:59pm\nFor more info: sta221-fa24.netlify.app/hw/stats-experience"
  },
  {
    "objectID": "slides/08-inference.html#reminder-course-policies-about-assignments",
    "href": "slides/08-inference.html#reminder-course-policies-about-assignments",
    "title": "Inference for regression",
    "section": "Reminder: course policies about assignments",
    "text": "Reminder: course policies about assignments\n\nLate work\n\nHW and labs accepted up to 2 days late.\n5% deduction for each 24-hour period the assignment is late.\n\nOne time late waiver\n\nCan use on HW and individual labs\n\nLowest HW and lowest lab grade dropped at the end of the semester."
  },
  {
    "objectID": "slides/08-inference.html#reminder-course-policies-about-assignments-1",
    "href": "slides/08-inference.html#reminder-course-policies-about-assignments-1",
    "title": "Inference for regression",
    "section": "Reminder: course policies about assignments",
    "text": "Reminder: course policies about assignments\n\nRead the feedback on Gradescope carefully! If you have questions about the comments, ask a member of the teaching team during office hours or before/after class.\nRegrade requests\n\nOpened 1 day after assignment is returned and due within 1 week\nOnly submit regrade request if there is an error in the grading not to dispute points or ask questions about grading.\nProf. Tackett or Kat (Head TA) will regrade the entire exercise being disputed, which could potentially result in a lower grade."
  },
  {
    "objectID": "slides/08-inference.html#poll-office-hours-availability",
    "href": "slides/08-inference.html#poll-office-hours-availability",
    "title": "Inference for regression",
    "section": "Poll: Office hours availability",
    "text": "Poll: Office hours availability"
  },
  {
    "objectID": "slides/08-inference.html#topics",
    "href": "slides/08-inference.html#topics",
    "title": "Inference for regression",
    "section": "Topics",
    "text": "Topics\n\nUnderstand statistical inference in the context of regression\nDescribe the assumptions for regression\nUnderstand connection between distribution of residuals and inferential procedures\nConduct inference on a single coefficient"
  },
  {
    "objectID": "slides/08-inference.html#computing-setup",
    "href": "slides/08-inference.html#computing-setup",
    "title": "Inference for regression",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(kableExtra)  \nlibrary(patchwork)   \n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "slides/08-inference.html#data-ncaa-football-expenditures",
    "href": "slides/08-inference.html#data-ncaa-football-expenditures",
    "title": "Inference for regression",
    "section": "Data: NCAA Football expenditures",
    "text": "Data: NCAA Football expenditures\nToday’s data come from Equity in Athletics Data Analysis and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a March 2022 Tidy Tuesday.\nWe will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :\n\ntotal_exp_m: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\nenrollment_th: Total student enrollment in the 2019 - 2020 academic year (in thousands)\ntype: institution type (Public or Private)\n\n\nfootball &lt;- read_csv(\"data/ncaa-football-exp.csv\")"
  },
  {
    "objectID": "slides/08-inference.html#univariate-eda",
    "href": "slides/08-inference.html#univariate-eda",
    "title": "Inference for regression",
    "section": "Univariate EDA",
    "text": "Univariate EDA"
  },
  {
    "objectID": "slides/08-inference.html#bivariate-eda",
    "href": "slides/08-inference.html#bivariate-eda",
    "title": "Inference for regression",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA"
  },
  {
    "objectID": "slides/08-inference.html#regression-model",
    "href": "slides/08-inference.html#regression-model",
    "title": "Inference for regression",
    "section": "Regression model",
    "text": "Regression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type, data = football)\ntidy(exp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n\n\n\n\n\n\nFor every additional 1,000 students, we expect the institution’s total expenditures on football to increase by $780,000, on average, holding institution type constant."
  },
  {
    "objectID": "slides/08-inference.html#from-sample-to-population",
    "href": "slides/08-inference.html#from-sample-to-population",
    "title": "Inference for regression",
    "section": "From sample to population",
    "text": "From sample to population\n\nFor every additional 1,000 students, we expect the institution’s total expenditures on football to increase by $780,000, on average, holding institution type constant.\n\n\n\n\nThis estimate is valid for the single sample of 127 higher education institutions in the 2019 - 2020 academic year.\nBut what if we’re not interested quantifying the relationship between student enrollment, institution type, and football expenditures for this single sample?\nWhat if we want to say something about the relationship between these variables for all colleges and universities with football programs and across different years?"
  },
  {
    "objectID": "slides/08-inference.html#statistical-inference",
    "href": "slides/08-inference.html#statistical-inference",
    "title": "Inference for regression",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\n\nStatistical inference provides methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be representative (ideally random) of the population we’re interested in\n\n\n\n\n\nImage source: Eugene Morgan © Penn State"
  },
  {
    "objectID": "slides/08-inference.html#inference-for-linear-regression",
    "href": "slides/08-inference.html#inference-for-linear-regression",
    "title": "Inference for regression",
    "section": "Inference for linear regression",
    "text": "Inference for linear regression\n\nInference based on ANOVA\n\nHypothesis test for the statistical significance of the overall regression model\nHypothesis test for a subset of coefficients\n\nInference for a single coefficient \\(\\beta_j\\)\n\nHypothesis test for a coefficient \\(\\beta_j\\)\nConfidence interval for a coefficient \\(\\beta_j\\)"
  },
  {
    "objectID": "slides/08-inference.html#linear-regression-model",
    "href": "slides/08-inference.html#linear-regression-model",
    "title": "Inference for regression",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\n\\begin{aligned}\n\\mathbf{y} &= Model + Error \\\\[5pt]\n&= f(\\mathbf{X}) + \\boldsymbol{\\epsilon} \\\\[5pt]\n&= E(\\mathbf{y}|\\mathbf{X}) + \\mathbf{\\epsilon} \\\\[5pt]\n&= \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\n\\end{aligned}\n\\]\n\n\n\nWe have discussed multiple ways to find the least squares estimates of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\\\beta_1\\end{bmatrix}\\)\n\nNone of these approaches depend on the distribution of \\(\\boldsymbol{\\epsilon}\\)\n\nNow we will use statistical inference to draw conclusions about \\(\\boldsymbol{\\beta}\\) that depend on particular assumptions about the distribution of \\(\\boldsymbol{\\epsilon}\\)"
  },
  {
    "objectID": "slides/08-inference.html#linear-regression-model-1",
    "href": "slides/08-inference.html#linear-regression-model-1",
    "title": "Inference for regression",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\\begin{aligned}\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\hspace{8mm} \\boldsymbol{\\epsilon} \\sim N(0, \\sigma^2_{\\epsilon}\\mathbf{I})\n\\end{aligned}\n\\]\nsuch that the errors are independent and normally distributed.\n\n\nIndependent: Knowing the error term for one observation doesn’t tell you anything about the error term for another observation\nNormally distributed: Tell us the shape of the distribution of residuals\n\n\nWhat else do we know about the distribution of the residuals based on this equation?"
  },
  {
    "objectID": "slides/08-inference.html#describing-random-phenomena",
    "href": "slides/08-inference.html#describing-random-phenomena",
    "title": "Inference for regression",
    "section": "Describing random phenomena",
    "text": "Describing random phenomena\n\n\nThere is some uncertainty in the residuals (and the predicted responses), so we use mathematical models to describe that uncertainty.\nSome terminology:\n\nSample space: Set of all possible outcomes\nRandom variable: Function (mapping) from the sample space onto real numbers\nEvent: Subset of the sample space, i.e., a set of possible outcomes (possible values the random variable can take)\nProbability distribution function: Mathematical function that produces probability of occurrences for events in the sample space"
  },
  {
    "objectID": "slides/08-inference.html#example",
    "href": "slides/08-inference.html#example",
    "title": "Inference for regression",
    "section": "Example",
    "text": "Example\nSuppose we are tossing 2 fair coins with sides heads (H) and tails (T)\n\n\nSample space: {HH, HT, TH, TT}\nRandom variable: \\(X\\) : The number of heads in two coin tosses\nEvent: We flip two coins and get 1 head\nProbability distribution function: \\[P(X = x_i) = {2 \\choose x_i}0.5^{x_i}{0.5}^{2-x_i}\\]\nNow we can find \\[P(X = 1) = {2 \\choose 1}0.5^1{0.5}^{2-1} = 0.5\\]"
  },
  {
    "objectID": "slides/08-inference.html#mathematical-representation",
    "href": "slides/08-inference.html#mathematical-representation",
    "title": "Inference for regression",
    "section": "Mathematical representation",
    "text": "Mathematical representation\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/08-inference.html#expected-value-of-mathbfy",
    "href": "slides/08-inference.html#expected-value-of-mathbfy",
    "title": "Inference for regression",
    "section": "Expected value of \\(\\mathbf{y}\\)",
    "text": "Expected value of \\(\\mathbf{y}\\)\nLet \\(\\mathbf{b} = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\b_p\\end{bmatrix}\\) be a \\(p \\times 1\\) vector of random variables.\n\n\nThen \\(E(\\mathbf{b}) = E\\begin{bmatrix}b_1 \\\\ \\vdots \\\\ b_p\\end{bmatrix} = \\begin{bmatrix}E(b_1) \\\\ \\vdots \\\\ E(b_p)\\end{bmatrix}\\)\n\n\n\n\nUse this to find \\(E(\\mathbf{y}|\\mathbf{X})\\)."
  },
  {
    "objectID": "slides/08-inference.html#variance",
    "href": "slides/08-inference.html#variance",
    "title": "Inference for regression",
    "section": "Variance",
    "text": "Variance\nLet \\(\\mathbf{b} = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\b_p\\end{bmatrix}\\) be a \\(p \\times 1\\) vector of independent random variables.\n\n\nThen \\(Var(\\mathbf{b}) = \\begin{bmatrix}Var(b_1) & 0 & \\dots & 0 \\\\ 0 & Var(b_2) & \\dots & 0 \\\\ \\vdots & \\vdots & \\dots & \\cdot \\\\ 0 & 0 & \\dots & Var(b_p)\\end{bmatrix}\\)\n\n\n\n\nUse this to find \\(Var(\\mathbf{y}|\\mathbf{X})\\)."
  },
  {
    "objectID": "slides/08-inference.html#assumptions-of-regression",
    "href": "slides/08-inference.html#assumptions-of-regression",
    "title": "Inference for regression",
    "section": "Assumptions of regression",
    "text": "Assumptions of regression\n\n\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n\\]\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\n\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from one another."
  },
  {
    "objectID": "slides/08-inference.html#estimating-sigma2_epsilon",
    "href": "slides/08-inference.html#estimating-sigma2_epsilon",
    "title": "Inference for regression",
    "section": "Estimating \\(\\sigma^2_{\\epsilon}\\)",
    "text": "Estimating \\(\\sigma^2_{\\epsilon}\\)\n\nOnce we fit the model, we can use the residuals to estimate \\(\\sigma_{\\epsilon}^2\\)\n\\(\\hat{\\sigma}^2_{\\epsilon}\\) is needed for hypothesis testing and constructing confidence intervals for regression\n\n\\[\n\\hat{\\sigma}^2_\\epsilon = \\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-p-1} = \\frac{\\sum_\\limits{i=1}^ne_i^2}{n - p - 1} = \\frac{SSR}{n - p - 1}\n\\]\n\n\nThe regression standard error \\(\\hat{\\sigma}_{\\epsilon}\\) is a measure of the average distance between the observations and regression line\n\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{SSR}{n - p - 1}}\n\\]"
  },
  {
    "objectID": "slides/08-inference.html#inference-for-beta_j",
    "href": "slides/08-inference.html#inference-for-beta_j",
    "title": "Inference for regression",
    "section": "Inference for \\(\\beta_j\\)",
    "text": "Inference for \\(\\beta_j\\)\nWe often want to conduct inference on individual model coefficients\n\nHypothesis test: Is there a linear relationship between the response and \\(x_j\\)?\nConfidence interval: What is a plausible range of values \\(\\beta_j\\) can take?\n\n\nBut first we need to understand the distribution of \\(\\hat{\\beta}_j\\)"
  },
  {
    "objectID": "slides/08-inference.html#sampling-distribution-of-hatbeta",
    "href": "slides/08-inference.html#sampling-distribution-of-hatbeta",
    "title": "Inference for regression",
    "section": "Sampling distribution of \\(\\hat{\\beta}\\)",
    "text": "Sampling distribution of \\(\\hat{\\beta}\\)\n\nA sampling distribution is the probability distribution of a statistic based on a large number of random samples of size \\(n\\) from a population\nThe sampling distribution of \\(\\hat{\\boldsymbol{\\beta}}\\) is the probability distribution of the estimated coefficients if we repeatedly took samples of size \\(n\\) and fit the regression model\n\n\\[\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon(\\mathbf{X}^T\\mathbf{X})^{-1})\n\\]\n\nThe estimated coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) are normally distributed with\n\\[\nE(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta} \\hspace{10mm} Var(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2_{\\epsilon}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\n\\]"
  },
  {
    "objectID": "slides/08-inference.html#sampling-distribution-of-hatbeta_j",
    "href": "slides/08-inference.html#sampling-distribution-of-hatbeta_j",
    "title": "Inference for regression",
    "section": "Sampling distribution of \\(\\hat{\\beta}_j\\)",
    "text": "Sampling distribution of \\(\\hat{\\beta}_j\\)\n\\[\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon(\\mathbf{X}^T\\mathbf{X})^{-1})\n\\]\nLet \\(\\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\). Then, for each coefficient \\(\\hat{\\beta}_j\\),\n\n\n\\(E(\\hat{\\beta}_j) = \\boldsymbol{\\beta}_j\\), the \\(j^{th}\\) element of \\(\\boldsymbol{\\beta}\\)\n\\(Var(\\hat{\\beta}_j) = \\sigma^2_{\\epsilon}C_{jj}\\)\n\\(Cov(\\hat{\\beta}_i, \\hat{\\beta}_j) = \\sigma^2_{\\epsilon}C_{ij}\\)"
  },
  {
    "objectID": "slides/08-inference.html#steps-for-a-hypothesis-test",
    "href": "slides/08-inference.html#steps-for-a-hypothesis-test",
    "title": "Inference for regression",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState the null and alternative hypotheses.\nCalculate a test statistic.\nCalculate the p-value.\nState the conclusion."
  },
  {
    "objectID": "slides/08-inference.html#hypothesis-test-for-beta_j-hypotheses",
    "href": "slides/08-inference.html#hypothesis-test-for-beta_j-hypotheses",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Hypotheses",
    "text": "Hypothesis test for \\(\\beta_j\\): Hypotheses\nWe will generally test the hypotheses:\n\\[\n\\begin{aligned}\n&H_0: \\beta_j = 0 \\\\\n&H_a: \\beta_j \\neq 0\n\\end{aligned}\n\\]\n\nState these hypotheses in words."
  },
  {
    "objectID": "slides/08-inference.html#hypothesis-test-for-beta_j-test-statistic",
    "href": "slides/08-inference.html#hypothesis-test-for-beta_j-test-statistic",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Test statistic",
    "text": "Hypothesis test for \\(\\beta_j\\): Test statistic\nTest statistic: Number of standard errors the estimate is away from the null\n\\[\n\\text{Test Statstic} = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n\\]\n\nIf \\(\\sigma^2_{\\epsilon}\\) was known, the test statistic would be\n\\[Z = \\frac{\\hat{\\beta}_j - 0}{SE(\\hat{\\beta}_j)} ~ = ~\\frac{\\hat{\\beta}_j - 0}{\\sqrt{\\sigma^2_\\epsilon C_{jj}}} ~\\sim ~ N(0, 1)\n\\]\n\n\nIn general, \\(\\sigma^2_{\\epsilon}\\) is not known, so we use \\(\\hat{\\sigma}_{\\epsilon}^2\\) to calculate \\(SE(\\hat{\\beta}_j)\\)\n\\[T = \\frac{\\hat{\\beta}_j - 0}{SE(\\hat{\\beta}_j)} ~ = ~\\frac{\\hat{\\beta}_j - 0}{\\sqrt{\\hat{\\sigma}^2_\\epsilon C_{jj}}} ~\\sim ~ t_{n-p-1}\n\\]"
  },
  {
    "objectID": "slides/08-inference.html#hypothesis-test-for-beta_j-test-statistic-1",
    "href": "slides/08-inference.html#hypothesis-test-for-beta_j-test-statistic-1",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Test statistic",
    "text": "Hypothesis test for \\(\\beta_j\\): Test statistic\n\nThe test statistic \\(T\\) follows a \\(t\\) distribution with \\(n - p -1\\) degrees of freedom.\nWe need to account for the additional variability introduced by calculating \\(SE(\\hat{\\beta}_j)\\) using an estimated value instead of a constant"
  },
  {
    "objectID": "slides/08-inference.html#t-vs.-n01",
    "href": "slides/08-inference.html#t-vs.-n01",
    "title": "Inference for regression",
    "section": "t vs. N(0,1)",
    "text": "t vs. N(0,1)\n\n\nFigure 1: Standard normal vs. t distributions"
  },
  {
    "objectID": "slides/08-inference.html#hypothesis-test-for-beta_j-p-value",
    "href": "slides/08-inference.html#hypothesis-test-for-beta_j-p-value",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): P-value",
    "text": "Hypothesis test for \\(\\beta_j\\): P-value\nThe p-value is the probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\np-value = P(|t| &gt; |\\text{test statistic}|),\n\\]\ncalculated from a \\(t\\) distribution with \\(n- p - 1\\) degrees of freedom\n\n\nWhy do we take into account “extreme” on both the high and low ends?"
  },
  {
    "objectID": "slides/08-inference.html#understanding-the-p-value",
    "href": "slides/08-inference.html#understanding-the-p-value",
    "title": "Inference for regression",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/08-inference.html#hypothesis-test-for-beta_j-conclusion",
    "href": "slides/08-inference.html#hypothesis-test-for-beta_j-conclusion",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Conclusion",
    "text": "Hypothesis test for \\(\\beta_j\\): Conclusion\nThere are two parts to the conclusion\n\nMake a conclusion by comparing the p-value to a predetermined decision-making threshold called the significance level ( \\(\\alpha\\) level)\n\nIf \\(\\text{P-value} &lt; \\alpha\\): Reject \\(H_0\\)\nIf \\(\\text{P-value} \\geq \\alpha\\): Fail to reject \\(H_0\\)\n\nState the conclusion in the context of the data"
  },
  {
    "objectID": "slides/08-inference.html#recap",
    "href": "slides/08-inference.html#recap",
    "title": "Inference for regression",
    "section": "Recap",
    "text": "Recap\n\nIntroduced statistical inference in the context of regression\nDescribed the assumptions for regression\nConnected the distribution of residuals and inferential procedures\nConducted inference on a single coefficient\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/02-slr.html#announcements",
    "href": "slides/02-slr.html#announcements",
    "title": "Simple linear regression",
    "section": "",
    "text": "No labs on Mon, Sep 2 (Labor Day)\nApplication exercises start Tue, Sep 3\n\nBring fully-charged laptop or device with keyboard\nMake sure you have accepted invite to GitHub course organization\n\nSee website for resources to learn / review R\nOffice hours start Tue, Sep 3"
  },
  {
    "objectID": "slides/02-slr.html#topics",
    "href": "slides/02-slr.html#topics",
    "title": "Simple linear regression",
    "section": "Topics",
    "text": "Topics\n\nHow regression is used to understand the relationship between multiple variables\nLeast squares estimation for the slope and intercept\nInterpret the slope and intercept\nPredict the response given a value of the predictor"
  },
  {
    "objectID": "slides/02-slr.html#computing-set-up",
    "href": "slides/02-slr.html#computing-set-up",
    "title": "Simple linear regression",
    "section": "Computing set up",
    "text": "Computing set up\n\n# load packages\nlibrary(tidyverse)        # for data wrangling\nlibrary(broom)            # for formatting regression output\nlibrary(fivethirtyeight)  # for the fandango dataset\nlibrary(knitr)            # for formatting tables\nlibrary(patchwork)        # for arranging graphs\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)\n\n\n\n\n\nSource: R for Data Science with additions from The Art of Statistics: How to Learn from Data.\n\n\n\n\n\n\nSource:R for Data Science"
  },
  {
    "objectID": "slides/02-slr.html#movie-scores",
    "href": "slides/02-slr.html#movie-scores",
    "title": "Simple linear regression",
    "section": "Movie scores",
    "text": "Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/02-slr.html#data-prep",
    "href": "slides/02-slr.html#data-prep",
    "title": "Simple linear regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/02-slr.html#data-overview",
    "href": "slides/02-slr.html#data-overview",
    "title": "Simple linear regression",
    "section": "Data overview",
    "text": "Data overview\n\nglimpse(movie_scores)\n\nRows: 146\nColumns: 23\n$ film                       &lt;chr&gt; \"Avengers: Age of Ultron\", \"Cinderella\", \"A…\n$ year                       &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ critics                    &lt;int&gt; 74, 85, 80, 18, 14, 63, 42, 86, 99, 89, 84,…\n$ audience                   &lt;int&gt; 86, 80, 90, 84, 28, 62, 53, 64, 82, 87, 77,…\n$ metacritic                 &lt;int&gt; 66, 67, 64, 22, 29, 50, 53, 81, 81, 80, 71,…\n$ metacritic_user            &lt;dbl&gt; 7.1, 7.5, 8.1, 4.7, 3.4, 6.8, 7.6, 6.8, 8.8…\n$ imdb                       &lt;dbl&gt; 7.8, 7.1, 7.8, 5.4, 5.1, 7.2, 6.9, 6.5, 7.4…\n$ fandango_stars             &lt;dbl&gt; 5.0, 5.0, 5.0, 5.0, 3.5, 4.5, 4.0, 4.0, 4.5…\n$ fandango_ratingvalue       &lt;dbl&gt; 4.5, 4.5, 4.5, 4.5, 3.0, 4.0, 3.5, 3.5, 4.0…\n$ rt_norm                    &lt;dbl&gt; 3.70, 4.25, 4.00, 0.90, 0.70, 3.15, 2.10, 4…\n$ rt_user_norm               &lt;dbl&gt; 4.30, 4.00, 4.50, 4.20, 1.40, 3.10, 2.65, 3…\n$ metacritic_norm            &lt;dbl&gt; 3.30, 3.35, 3.20, 1.10, 1.45, 2.50, 2.65, 4…\n$ metacritic_user_nom        &lt;dbl&gt; 3.55, 3.75, 4.05, 2.35, 1.70, 3.40, 3.80, 3…\n$ imdb_norm                  &lt;dbl&gt; 3.90, 3.55, 3.90, 2.70, 2.55, 3.60, 3.45, 3…\n$ rt_norm_round              &lt;dbl&gt; 3.5, 4.5, 4.0, 1.0, 0.5, 3.0, 2.0, 4.5, 5.0…\n$ rt_user_norm_round         &lt;dbl&gt; 4.5, 4.0, 4.5, 4.0, 1.5, 3.0, 2.5, 3.0, 4.0…\n$ metacritic_norm_round      &lt;dbl&gt; 3.5, 3.5, 3.0, 1.0, 1.5, 2.5, 2.5, 4.0, 4.0…\n$ metacritic_user_norm_round &lt;dbl&gt; 3.5, 4.0, 4.0, 2.5, 1.5, 3.5, 4.0, 3.5, 4.5…\n$ imdb_norm_round            &lt;dbl&gt; 4.0, 3.5, 4.0, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5…\n$ metacritic_user_vote_count &lt;int&gt; 1330, 249, 627, 31, 88, 34, 17, 124, 62, 54…\n$ imdb_user_vote_count       &lt;int&gt; 271107, 65709, 103660, 3136, 19560, 39373, …\n$ fandango_votes             &lt;int&gt; 14846, 12640, 12055, 1793, 1021, 397, 252, …\n$ fandango_difference        &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…"
  },
  {
    "objectID": "slides/02-slr.html#univariate-exploratory-data-analysis-eda",
    "href": "slides/02-slr.html#univariate-exploratory-data-analysis-eda",
    "title": "Simple linear regression",
    "section": "Univariate exploratory data analysis (EDA)",
    "text": "Univariate exploratory data analysis (EDA)\nThe data set contains the “Tomatometer” score (critics) and audience score (audience) for 146 movies rated on rottentomatoes.com."
  },
  {
    "objectID": "slides/02-slr.html#bivariate-eda",
    "href": "slides/02-slr.html#bivariate-eda",
    "title": "Simple linear regression",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA"
  },
  {
    "objectID": "slides/02-slr.html#bivariate-eda-1",
    "href": "slides/02-slr.html#bivariate-eda-1",
    "title": "Simple linear regression",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA\nGoal: Fit a line to describe the relationship between the critics score and audience score."
  },
  {
    "objectID": "slides/02-slr.html#why-fit-a-line",
    "href": "slides/02-slr.html#why-fit-a-line",
    "title": "Simple linear regression",
    "section": "Why fit a line?",
    "text": "Why fit a line?\nWe fit a line to accomplish one or both of the following:\n\n. . .\n\nPrediction\n\n\nWhat is an example of a prediction question for this data set?\n\n\n. . .\n\nInference\n\n\nWhat is an example of an inference question for this data set?"
  },
  {
    "objectID": "slides/02-slr.html#terminology",
    "href": "slides/02-slr.html#terminology",
    "title": "Simple linear regression",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nResponse, \\(Y\\): variable describing the outcome of interest\nPredictor, \\(X\\): variable we use to help understand the variability in the response"
  },
  {
    "objectID": "slides/02-slr.html#regression-model",
    "href": "slides/02-slr.html#regression-model",
    "title": "Simple linear regression",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the response, \\(Y\\), and the predictor, \\(X\\).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{f(X)} + \\epsilon \\\\[8pt]\n& = \\color{black}{E(Y|X)} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\mu_{Y|X}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-slr.html#regression-model-1",
    "href": "slides/02-slr.html#regression-model-1",
    "title": "Simple linear regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{purple}{f(X)} + \\epsilon \\\\[8pt]\n&= \\color{purple}{E(Y|X)} + \\epsilon \\\\[8pt]\n&= \\color{purple}{\\mu_{Y|X}} + \\epsilon \\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\(E(Y|X) = \\mu_{Y|X}\\), the mean value of \\(Y\\) given a particular value of \\(X\\)."
  },
  {
    "objectID": "slides/02-slr.html#regression-model-2",
    "href": "slides/02-slr.html#regression-model-2",
    "title": "Simple linear regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\color{blue}{\\textbf{Error}} \\\\[8pt]\n&= \\color{purple}{f(X)} + \\color{blue}{\\epsilon}\\\\[8pt]\n&= \\color{purple}{E(Y|X)} + \\color{blue}{\\epsilon}\\\\[8pt]\n&= \\color{purple}{\\mu_{Y|X}} + \\color{blue}{\\epsilon} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-slr.html#determine-fx",
    "href": "slides/02-slr.html#determine-fx",
    "title": "Simple linear regression",
    "section": "Determine \\(f(X)\\)",
    "text": "Determine \\(f(X)\\)\n\nGoal: Determine \\(f(X)\\)\nHow do we determine \\(f(X)\\)\n\nMake an assumption about the functional form \\(f(X)\\) (parametric model)\nUse the data to fit a model based on that form"
  },
  {
    "objectID": "slides/02-slr.html#slr-statistical-model-population",
    "href": "slides/02-slr.html#slr-statistical-model-population",
    "title": "Simple linear regression",
    "section": "SLR: Statistical model (population)",
    "text": "SLR: Statistical model (population)\nWhen we have a quantitative response, \\(Y\\), and a single quantitative predictor, \\(X\\), we can use a simple linear regression model to describe the relationship between \\(Y\\) and \\(X\\). \\[\\large{Y = \\mathbf{\\beta_0 + \\beta_1 X} + \\epsilon}, \\hspace{8mm} \\epsilon \\sim N(0, \\sigma_{\\epsilon}^2)\\]\n. . .\n\n\\(\\beta_1\\): Population (true) slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): Population (true) intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "slides/02-slr.html#slr-regression-equation-sample",
    "href": "slides/02-slr.html#slr-regression-equation-sample",
    "title": "Simple linear regression",
    "section": "SLR: Regression equation (sample)",
    "text": "SLR: Regression equation (sample)\n\\[\\Large{\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X}\\]\n\n\\(\\hat{\\beta}_1\\): Estimated (sample) slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{\\beta}_0\\): Estimated (sample) intercept of the relationship between \\(X\\) and \\(Y\\)\nNo error term!"
  },
  {
    "objectID": "slides/02-slr.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "href": "slides/02-slr.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "title": "Simple linear regression",
    "section": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)",
    "text": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)"
  },
  {
    "objectID": "slides/02-slr.html#residuals",
    "href": "slides/02-slr.html#residuals",
    "title": "Simple linear regression",
    "section": "Residuals",
    "text": "Residuals\n\n\n\n\n\n\n\n\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]"
  },
  {
    "objectID": "slides/02-slr.html#least-squares-line",
    "href": "slides/02-slr.html#least-squares-line",
    "title": "Simple linear regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[e_i = \\text{observed} - \\text{predicted}\n= y_i - \\hat{y}_i\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals\n\nClick here for full calculations."
  },
  {
    "objectID": "slides/02-slr.html#properties-of-least-squares-regression",
    "href": "slides/02-slr.html#properties-of-least-squares-regression",
    "title": "Simple linear regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\)\nThe slope has the same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}\\)\nThe sum of the residuals is approximately zero: \\(\\sum_{i = 1}^n e_i \\approx 0\\)\nThe residuals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/02-slr.html#estimating-the-slope",
    "href": "slides/02-slr.html#estimating-the-slope",
    "title": "Simple linear regression",
    "section": "Estimating the slope",
    "text": "Estimating the slope\n\\[\\large{\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}}\\]\n\n. . .\n\\[\n\\begin{aligned} s_X = 30.1688  \\hspace{15mm} &s_Y =  20.0244 \\hspace{15mm} r  = 0.7814 \\\\[10pt]\\hat{\\beta}_1  &= 0.7814 \\times \\frac{20.0244}{30.1688} \\\\&= \\mathbf{0.5187}\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-slr.html#estimating-the-intercept",
    "href": "slides/02-slr.html#estimating-the-intercept",
    "title": "Simple linear regression",
    "section": "Estimating the intercept",
    "text": "Estimating the intercept\n\\[\\large{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}}\\]\n\n. . .\n\\[\n\\begin{aligned}\\bar{x} = 60.8493 & \\hspace{15mm} \\bar{y} = 63.8767 \\hspace{15mm} \\hat{\\beta}_1 = 0.5187 \\\\[10pt]\n\\hat{\\beta}_0 &= 63.8767 - 0.5187 \\times 60.8493 \\\\\n&= \\mathbf{32.3142}\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-slr.html#interpretation",
    "href": "slides/02-slr.html#interpretation",
    "title": "Simple linear regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nQuestionSubmit\n\n\n\n\nSubmit your answers to the following questions on Ed Discussion:\n\nThe slope of the model for predicting audience score from critics score is 0.5187 . Which of the following is the best interpretation of this value?\n32.3142 is the predicted mean audience score for what type of movies?\n\n\n\n\n\n\n\n\n\n🔗 https://edstem.org/us/courses/62513/discussion/5181157"
  },
  {
    "objectID": "slides/02-slr.html#does-it-make-sense-to-interpret-the-intercept",
    "href": "slides/02-slr.html#does-it-make-sense-to-interpret-the-intercept",
    "title": "Simple linear regression",
    "section": "Does it make sense to interpret the intercept?",
    "text": "Does it make sense to interpret the intercept?\n. . .\n✅ The intercept is meaningful in the context of the data if\n\nthe predictor can feasibly take values equal to or near zero, or\nthere are values near zero in the observed data.\n\n. . .\n🛑 Otherwise, the intercept may not be meaningful!"
  },
  {
    "objectID": "slides/02-slr.html#making-a-prediction",
    "href": "slides/02-slr.html#making-a-prediction",
    "title": "Simple linear regression",
    "section": "Making a prediction",
    "text": "Making a prediction\nSuppose that a movie has a critics score of 70. According to this model, what is the movie’s predicted audience score?\n\\[\\begin{aligned}\n\\widehat{\\text{audience}} &= 32.3142 + 0.5187 \\times \\text{critics} \\\\\n&= 32.3142 + 0.5187 \\times 70 \\\\\n&= \\mathbf{68.6232}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-slr.html#fit-the-model",
    "href": "slides/02-slr.html#fit-the-model",
    "title": "Simple linear regression",
    "section": "Fit the model",
    "text": "Fit the model\nUse the lm() function to fit a linear regression model\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\nmovie_fit\n\n\nCall:\nlm(formula = audience ~ critics, data = movie_scores)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187"
  },
  {
    "objectID": "slides/02-slr.html#tidy-results",
    "href": "slides/02-slr.html#tidy-results",
    "title": "Simple linear regression",
    "section": "Tidy results",
    "text": "Tidy results\nUse the tidy() function from the broom R package to “tidy” the data\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\ntidy(movie_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/02-slr.html#format-results",
    "href": "slides/02-slr.html#format-results",
    "title": "Simple linear regression",
    "section": "Format results",
    "text": "Format results\nUse the kable() function from the knitr package to neatly format the results\n\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\ntidy(movie_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n32.316\n2.343\n13.795\n0\n\n\ncritics\n0.519\n0.035\n15.028\n0"
  },
  {
    "objectID": "slides/02-slr.html#prediction-1",
    "href": "slides/02-slr.html#prediction-1",
    "title": "Simple linear regression",
    "section": "Prediction",
    "text": "Prediction\nUse the predict() function to calculate predictions for new observations\n\nSingle observation\n\nnew_movie &lt;- tibble(critics = 70)\npredict(movie_fit, new_movie)\n\n       1 \n68.62297 \n\n\n\n. . .\nMultiple observations\n\nmore_new_movies &lt;- tibble(critics = c(24,70, 85))\npredict(movie_fit, more_new_movies)\n\n       1        2        3 \n44.76379 68.62297 76.40313"
  },
  {
    "objectID": "slides/02-slr.html#recap",
    "href": "slides/02-slr.html#recap",
    "title": "Simple linear regression",
    "section": "Recap",
    "text": "Recap\n\nDescribed how regression is used to understand the relationship between multiple variables\nUsed least squares to estimate the slope and intercept\nInterpreted the slope and intercept for simple linear regression\nPredicted the response given a value of the predictor"
  },
  {
    "objectID": "slides/02-slr.html#next-time",
    "href": "slides/02-slr.html#next-time",
    "title": "Simple linear regression",
    "section": "Next time",
    "text": "Next time\n\nModel assessment for simple linear regression\n\nSee Sep 3 prepare\n\nBring fully-charged laptop or device with keyboard for in-class application exercise (AE)"
  },
  {
    "objectID": "slides/lab-01.html#getting-started",
    "href": "slides/lab-01.html#getting-started",
    "title": "Lab 01",
    "section": "Getting started",
    "text": "Getting started\nAsk your TA if\n\nYou do not have a lab-01 repo in the GitHub course organization: github.com/sta221-fa24\nYou need help cloning the repo and starting a new RStudio project"
  },
  {
    "objectID": "slides/lab-01.html#tips-for-working-on-lab",
    "href": "slides/lab-01.html#tips-for-working-on-lab",
    "title": "Lab 01",
    "section": "Tips for working on lab",
    "text": "Tips for working on lab\n\nYou do not have to finish the lab in class, they will always be due Thursdays at 11:59pm. One work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when a TA can help you on the spot and leave the narrative writing until later.\nDo not pressure each other to finish early (particularly once you start working on teams); use the time wisely to really learn the material and produce a quality report."
  },
  {
    "objectID": "slides/lab-01.html#workflow-and-formatting",
    "href": "slides/lab-01.html#workflow-and-formatting",
    "title": "Lab 01",
    "section": "Workflow and formatting",
    "text": "Workflow and formatting\nPart of the lab grade is for “workflow and formatting” assessing the reproducible workflow and document format. This includes\n\nHaving at least 3 informative commit messages (practicing version control)\n\nThere are markers in Lab 01 to help you incorporate version control in your workflow\n\nThe PDF is neatly organized document with clear exercise headings and readable code and narrative\nThe name (first and last) and date are updated at the top of the document."
  },
  {
    "objectID": "slides/lab-01.html#when-youre-done-with-lab",
    "href": "slides/lab-01.html#when-youre-done-with-lab",
    "title": "Lab 01",
    "section": "When you’re done with lab",
    "text": "When you’re done with lab\n\nMake sure all your final changes have been pushed to your GitHub repo\nSubmit your final PDF to Gradescope\n\nAccess Gradescope through the course Canvas site\nMark the pages associated with each exercise."
  },
  {
    "objectID": "slides/lab-01.html#lab-01-park-access",
    "href": "slides/lab-01.html#lab-01-park-access",
    "title": "Lab 01",
    "section": "Lab 01: Park access",
    "text": "Lab 01: Park access\nToday’s lab focuses on exploratory data analysis and simple linear regression, content from Weeks 01 and 02 in the course.\n🔗 sta221-fa24.netlify.app/labs/lab-01.html\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/lab-03.html#goals",
    "href": "slides/lab-03.html#goals",
    "title": "Lab 03",
    "section": "Goals",
    "text": "Goals\n\nTeam project\nLab 03: Palmer penguins"
  },
  {
    "objectID": "slides/lab-03.html#lab-03-palmer-penguins",
    "href": "slides/lab-03.html#lab-03-palmer-penguins",
    "title": "Lab 03",
    "section": "Lab 03: Palmer penguins",
    "text": "Lab 03: Palmer penguins\nLab 03 focuses on\n\nusing linear regression and statistical inference to draw conclusions about penguins living in Palmer Archipelago in Antarctica.\nuse the data to check conditions about the distribution of the model residuals.\n\nUse this week to get started on the lab. We will continue discussing statistical inference in this week’s lectures, so this lab will be due on Thursday, October 3, 2024.\n🔗 https://sta221-fa24.netlify.app/labs/lab-03"
  },
  {
    "objectID": "slides/lab-03.html#final-team-project",
    "href": "slides/lab-03.html#final-team-project",
    "title": "Lab 03",
    "section": "Final Team Project",
    "text": "Final Team Project\nGoal: Use the methods from STA 221 to analyze data and answer a research question developed by your team\nPrimary deliverables:\n\nan in-person presentation about the exploratory data analysis and initial modeling\na written, reproducible final report detailing your analysis\na GitHub repository containing all work from the project\n\nSubmission: All work for the project will be submitted in your team’s GitHub repo. You will receive feedback via an Issue on GitHub to model a workflow often used in practice."
  },
  {
    "objectID": "slides/lab-03.html#final-team-project-1",
    "href": "slides/lab-03.html#final-team-project-1",
    "title": "Lab 03",
    "section": "Final team project",
    "text": "Final team project\nMilestones: There are periodic project milestones throughout the semester to help you work towards the final deliverables:\n\nResearch questions (today’s lab)\nProject proposal (next week’s lab)\nExploratory data analysis draft\nPresentation + Presentation comments\nAnalysis draft + peer review\nRound 1 submission (optional)\nWritten report\nReproducibility + organization\n\nSee the Final Project Instructions for a timeline and details for each milestone."
  },
  {
    "objectID": "slides/lab-03.html#today-research-questions",
    "href": "slides/lab-03.html#today-research-questions",
    "title": "Lab 03",
    "section": "Today: Research questions",
    "text": "Today: Research questions\nGoal: Develop three potential research questions your team may be interested in investigating.\nYou do not need to have a data set at this point\nFull instructions here: sta221-fa24.netlify.app/project#research-questions"
  },
  {
    "objectID": "slides/lab-03.html#reminder-team-workflow",
    "href": "slides/lab-03.html#reminder-team-workflow",
    "title": "Lab 03",
    "section": "Reminder: Team workflow",
    "text": "Reminder: Team workflow\n\nOnly one team member should type at a time. There are markers in today’s lab to help you determine whose turn it is to type.\n\nEvery team member should still be engaged in discussion for all questions, even if it’s not your turn type.\n\nDon’t forget to pull to get your teammates’ updates before making changes to the .qmd file.\n\n\n\n\n\n\nImportant\n\n\nOnly one submission per team on Gradescope. Read the submission instructions carefully!"
  },
  {
    "objectID": "slides/lab-03.html#reminder-tips-for-working-on-a-team",
    "href": "slides/lab-03.html#reminder-tips-for-working-on-a-team",
    "title": "Lab 03",
    "section": "Reminder: Tips for working on a team",
    "text": "Reminder: Tips for working on a team\n\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report.\nThe labs are structured to help you learn the steps of a data analysis. Do not split up the lab among the team members; work on it together in its entirety.\nEveryone has something to contribute! Use the lab groups as an opportunity to share ideas and learn from each other.\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\ntopic\nprepare\nslides\nae\nhw\nnotes\n\n\n\n\n1\nTh\nJan 8\nWelcome + What Is Bayesian Health Data Science?\n\n\n\n\n\n\n\n\n\n\n\n\nHW00 assigned\n\n\n2\nTu\nJan 13\nMonte Carlo Sampling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 15\nMarkov Chain Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\nHW 01 assigned, HW00 due\n\n\n3\nTu\nJan 20\nProbabilistic Programming (Intro to Stan!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 22\nPriors, Posteriors, and PPDs!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\nTu\nJan 27\nModel Checking\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 29\nModel Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nHW 02 assigned, HW 01 due\n\n\n5\nTu\nFeb 3\nBayesian Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 5\nNonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\nTu\nFeb 10\nRobust Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 12\nRegularization\n\n\n\n\n\n\n\n\n\n\n\n\nHW 03 assigned, HW 02 due\n\n\n7\nTu\nFeb 17\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 19\nMulticlass Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\nTu\nFeb 24\nMissing Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 26\nHierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\nExam 01 assigned, HW03 due\n\n\n9\nTu\nMar 3\nExam 1 Office Hours\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 5\nLongitudinal Data\n\n\n\n\n\n\n\n\n\n\n\n\nHW04 assigned, Exam 01 due\n\n\n10\nTu\nMar 10\nNO CLASS: Spring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 12\nNO CLASS: Spring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\nTu\nMar 17\nGaussian Processes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 19\nGeospatial Modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\nTu\nMar 24\nDisease Mapping\n\n\n\n\n\n\n\n\n\n\n\n\nHW 05 assigned, HW04 due\n\n\n\nTh\nMar 26\nBayesian Meta-Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\nTu\nMar 31\nScalable Gaussian Processes #1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nApr 2\nScalable Gaussian Processes #2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\nTu\nApr 7\nBayesian Clustering\n\n\n\n\n\n\n\n\n\n\n\n\nHW 05 due, Exam 02 assigned\n\n\n\nTh\nApr 9\nLive-Coding Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\nTu\nApr 14\nExam 02 Office Hours\n\n\n\n\n\n\n\n\n\n\n\n\nExam 02 due for feedback\n\n\nExam period\nTu\nApr 29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExam 02 due by 12:00pm",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "Below are freely available resources to learn or review the following in R: data wrangling, data visualization, Quarto basics.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#in-depth-introduction",
    "href": "computing-r-resources.html#in-depth-introduction",
    "title": "Resources for learning R",
    "section": "In-depth introduction",
    "text": "In-depth introduction\nCoursera: Data Visualization and Transformation with R by Mine Çetinkaya-Rundel and Elijah Meyer\n\nIncludes videos, readings, practice exercise, quizzes, and other resources\nYou can select content within the modules you want to complete.\nFocus on Modules 2 and 3. Review the content in Module 1 as needed.\nClick here for instructions to register for Coursera for free as a Duke student",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#in-depth-review",
    "href": "computing-r-resources.html#in-depth-review",
    "title": "Resources for learning R",
    "section": "In-depth review",
    "text": "In-depth review\nData Science with R videos by Mine Çetinkaya-Rundel and Elijah Meyer\n\nVideos from the data science Coursera course\nFocus on videos on visualizing and summarizing data\nYou need to join the Coursera course to access the files from the code along videos.\n\nLearn R: An interactive introduction to data analysis with R\n\nHands-on tutorial that can be completed within the site (no RStudio required)\nFocus on Chapters 4 - 6",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#shorter-review",
    "href": "computing-r-resources.html#shorter-review",
    "title": "Resources for learning R",
    "section": "Shorter review",
    "text": "Shorter review\nR for Data Science (2nd ed) by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund\n\nFocus on Chapters 1 - 3, 10",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nTidy Modeling with R by Max Kuhn & Julia Silge\nPosit Cheatsheets\nR workshops by Duke Center for Data and Visualization Sciences",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "ae/ae-02-mlr.html",
    "href": "ae/ae-02-mlr.html",
    "title": "AE 02: Multiple linear regression",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-02 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class to submit your AE.\n\n\n\nPackages\n\nlibrary(tidyverse)   \nlibrary(tidymodels)   \nlibrary(openintro)    \nlibrary(knitr)       \n\n\n\nData\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\nWe will focus on the following variables:\n\nannual_income_th: Annual income (in $1000s)\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\ninterest_rate: Interest rate for the loan\n\nThe goal of this analysis is to use the annual income, debt-to-income ratio, and income verification to understand variability in the interest rate on the loan.\nWe’ll start with data prep to rescale annual income to $1000’s and recode verified_income to fix an issue with the underlying data.\n\nloan50 &lt;- loan50 |&gt;\n   mutate(annual_income_th = annual_income / 1000, \n          verified_income = \n            case_when(verified_income == \"Not Verified\" ~ \"Not Verified\",\n                      verified_income == \"Source Verified\" ~ \"Source Verified\",\n                      verified_income == \"Verified\" ~ \"Verified\"),\n          verified_income = as_factor(verified_income)\n   )                    \n\n\nglimpse(loan50)\n\nRows: 50\nColumns: 19\n$ state                   &lt;fct&gt; NJ, CA, SC, CA, OH, IN, NY, MO, FL, FL, MD, HI…\n$ emp_length              &lt;dbl&gt; 3, 10, NA, 0, 4, 6, 2, 10, 6, 3, 8, 10, 10, 2,…\n$ term                    &lt;dbl&gt; 60, 36, 36, 36, 60, 36, 36, 36, 60, 60, 36, 36…\n$ homeownership           &lt;fct&gt; rent, rent, mortgage, rent, mortgage, mortgage…\n$ annual_income           &lt;dbl&gt; 59000, 60000, 75000, 75000, 254000, 67000, 288…\n$ verified_income         &lt;fct&gt; Not Verified, Not Verified, Verified, Not Veri…\n$ debt_to_income          &lt;dbl&gt; 0.55752542, 1.30568333, 1.05628000, 0.57434667…\n$ total_credit_limit      &lt;int&gt; 95131, 51929, 301373, 59890, 422619, 349825, 1…\n$ total_credit_utilized   &lt;int&gt; 32894, 78341, 79221, 43076, 60490, 72162, 2872…\n$ num_cc_carrying_balance &lt;int&gt; 8, 2, 14, 10, 2, 4, 1, 3, 10, 4, 3, 4, 3, 2, 3…\n$ loan_purpose            &lt;fct&gt; debt_consolidation, credit_card, debt_consolid…\n$ loan_amount             &lt;int&gt; 22000, 6000, 25000, 6000, 25000, 6400, 3000, 1…\n$ grade                   &lt;fct&gt; B, B, E, B, B, B, D, A, A, C, D, A, A, A, A, E…\n$ interest_rate           &lt;dbl&gt; 10.90, 9.92, 26.30, 9.92, 9.43, 9.92, 17.09, 6…\n$ public_record_bankrupt  &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ loan_status             &lt;fct&gt; Current, Current, Current, Current, Current, C…\n$ has_second_income       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ total_income            &lt;dbl&gt; 59000, 60000, 75000, 75000, 254000, 67000, 288…\n$ annual_income_th        &lt;dbl&gt; 59.0, 60.0, 75.0, 75.0, 254.0, 67.0, 28.8, 80.…\n\n\n\n\nCategorical predictors\n\n\n\n\n\n\nExercise 1\n\n\n\nLet’s take a look at the design matrix for the model with predictors debt_to_income, annual_income_th, and verified_income.\nHow does R choose the baseline level by default?\n\n\n\n## add code here\n\n[Add response here]\n\n\n\n\n\n\nExercise 2\n\n\n\nFit the model with the predictors debt_to_income, annual_income_th, verified_income , and the interaction between annual_income_th and verified_income.\nNeatly display the model results using 3 digits.\n\n\n\n# add code here\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nWrite the estimated regression equation for the people with Not Verified income.\nWrite the estimated regression equation for people with Verified income.\n\n\n\n[add response here]\n\n\n\n\n\n\nExercise 4\n\n\n\nIn general, how do\n\nindicators for categorical predictors impact the model equation?\ninteraction terms impact the model equation?\n\n\n\n[Add response here]\n\n\nModel assessment\n\n\n\n\n\n\nExercise 5\n\n\n\nLet’s compare the original model without interaction effects to the model you fit in Exercise 2.\nCalculate \\(R^2\\) and \\(Adj. R^2\\) for each model. You can find \\(Adj. R^2\\) from the glance function:\nglance(model_name)$adj.r.squared\n\n\n\nint_fit &lt;- lm(interest_rate ~ debt_to_income + annual_income_th +\n                verified_income, data = loan50)\n\n\n# add code here\n\n\n\n\n\n\n\nExercise 6\n\n\n\nWhich model would you choose based on\n\n\\(R^2\\)?\n\\(Adj. R^2\\)?\n\n\n\n[add response here]\n\n\nLaTex\nSometimes, you will need to include mathematical notation in your document. There are two ways you can display mathematics in your document:\nInline: Your mathematics will display within the line of text.\n\nUse $ to start and end your LaTex syntax. You can also use the menu: Insert -&gt; LaTex Math -&gt; Inline Math.\nExample: The text The simple linear regression model is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ produces\n\nThe simple linear regression model is \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\)\n\n\nDisplayed: Your mathematics will display outside the line of text\n\nUse a $$ to start and end your LaTex syntax. You can also use the menu: Insert -&gt; LaTex Math -&gt; Display Math.\nExample: The text The estimated regression equation is $$\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$$produces\n\nThe estimated regression equation is\n\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nClick here for a quick reference of LaTex code.\n\n\n\n\nSubmission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "ae/ae-03-inference.html",
    "href": "ae/ae-03-inference.html",
    "title": "AE 03: Inference",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-03 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class to submit your AE.\n\n\n\nSet up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\nfootball &lt;- read_csv(\"data/ncaa-football-exp.csv\")\n\n\n\nData\n\n\nRegression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type, data = football)\n\ntidy(exp_fit)|&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n\n\n\n\n\n\n\nHypothesis test\nWe want to conduct a hypothesis test to determine if there is a linear relationship between enrollment and football expenditures after accounting for institution type.\nWe’ll start by getting estimates for statistics we’ll need for inference.\n\n\n\n\n\n\nExercise 1\n\n\n\nWe will use the vector of responses \\(\\mathbf{y}\\) and the design matrix \\(\\mathbf{X}\\) to calculate the values needed for inference.\nGet \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) from the football data frame. What are their dimensions?\n\n\n\n# add code here\n\n\n\n\n\n\n\nExercise 2\n\n\n\nNext, let’s calculate \\(\\hat{\\sigma}_\\epsilon^2\\) the estimate. Use \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) from the previous exercise to calculate this value.\n\n\n\n## add code here\n\n\n\n\n\n\n\nExercise 3\n\n\n\nNow we’re ready to conduct the hypothesis test. State the null and alternative hypotheses in words and using mathematical notation.\n\n\n. . .\n\n\n\n\n\n\nExercise 4\n\n\n\nCalculate \\(SE(\\beta_j)\\), then use this value to calculate the test statistic for the hypothesis test.\n\n\n\n## add code here\n\n\n\n\n\n\n\nExercise 5\n\n\n\nNow we need to calculate p-value to help make our final conclusion.\n\nState the distribution used to calculate the p-value.\nFill in the code below to calculate the p-value. Remove #| eval: false once you’ve filled in the code.\n\n\n\n\npt([test-statistic], [df], lower.tail = FALSE)\n\n\n\n\n\n\n\nExercise 6\n\n\n\nState your conclusion in the context of the data. Use a threshold of \\(\\alpha = 0.05\\).\n\n\n. . .\n\n\n\n\n\n\nSubmission\n\n\n\nTo submit the AE:\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "prepare/prepare-sep5.html",
    "href": "prepare/prepare-sep5.html",
    "title": "Prepare for September 5 lecture",
    "section": "",
    "text": "Review linear algebra concepts (as needed)\n\nMatrices and vectors: [slides][video]\nMatrix-Vector products: [slides][video]\nVector geometry: [slides][video]\nMatrix multiplication: [slides][video]\n\n\n\n\n\n\n\nNote\n\n\n\nAll linear algebra review materials from Math 218: Matrices and Vectors (Summer 2024) taught by Dr. Brian Fitzpatrick at Duke University"
  },
  {
    "objectID": "prepare/prepare-sep12.html",
    "href": "prepare/prepare-sep12.html",
    "title": "Prepare for September 12 lecture",
    "section": "",
    "text": "📖 Read Multiple Linear Regression\n✅ Review Vector Geometry [slides][video]1\n🎥: Watch Geometric interpretation of least squares"
  },
  {
    "objectID": "prepare/prepare-sep12.html#footnotes",
    "href": "prepare/prepare-sep12.html#footnotes",
    "title": "Prepare for September 12 lecture",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom Math 218: Matrices and Vectors (Summer 2024) taught by Dr. Brian Fitzpatrick at Duke University↩︎"
  },
  {
    "objectID": "prepare/prepare-sep10.html",
    "href": "prepare/prepare-sep10.html",
    "title": "Prepare for September 10 lecture",
    "section": "",
    "text": "Review class notes and readings on simple linear regression.\nWe will extend what we’ve done thus far to multiple linear regression, with 2 or more predictors."
  },
  {
    "objectID": "hw/hw-02.html",
    "href": "hw/hw-02.html",
    "title": "HW 02: Bayesian linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis assignment is due on Thursday, February 13 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#instructions",
    "href": "hw/hw-02.html#instructions",
    "title": "HW 02: Multiple linear regression",
    "section": "Instructions",
    "text": "Instructions\nThe conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\n\nYou may write the answers and associated work for conceptual exercises by hand or type them in your Quarto document.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-1",
    "href": "hw/hw-02.html#exercise-1",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nSetup a multivariable linear regression to estimate the association between access to recreational facilities and exercise, making sure to allow for this relationship to change based on crime. Be sure to control for the following confounders: age, marital status, and race. Define the random variable \\(Y_i\\) as the exercise in MET minutes per week for women \\(i\\) and assume that \\(Y_i \\stackrel{ind}{\\sim} N(\\mu_i, \\sigma^2)\\) for \\(i = 1,\\ldots,n\\) where \\[\\begin{align*}\n\\mu_i &= \\alpha + recreation_i \\beta_1 + crime_i \\beta_2+(recreation_i \\times crime_i) \\beta_3\\\\\n&\\quad+ age_i \\beta_4 + black_i \\beta_5 + asian_i \\beta_6 + married_i \\beta_7\\\\\n&= \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta}.\n\\end{align*}\\]\nFit this regression using a Bayesian framework in Stan to estimate \\((\\alpha, \\boldsymbol{\\beta},\\sigma)\\). For all model parameters, choose weakly-informative priors, \\(\\alpha \\sim N(0,100)\\), \\(\\beta_j \\sim N(0,100)\\) for \\(j=1,\\ldots,p\\), and \\(\\sigma \\sim \\text{Half-Normal}(0,100)\\).\nEvaluate model convergence and present posterior predictive checks. Provide an argument for whether the model fits the data well.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-2",
    "href": "hw/hw-02.html#exercise-2",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nRefit the model in Exercise 1, this time using centered outcome and predictor variables, \\(Y_i^* \\stackrel{ind}{\\sim} N(\\mu_i, \\sigma^2)\\), where \\(\\mu_i^* = \\alpha + \\mathbf{x}_i^*\\boldsymbol{\\beta}\\). The centered data are defined as, \\(Y_i^* = Y_i - \\overline{Y}\\), where \\(\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\) and \\(\\mathbf{x}_i^* = \\mathbf{x}_i-\\bar{\\mathbf{x}}\\), where \\(\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\\). Use the same priors as before. Once again, evaluate model convergence and present posterior predictive checks. Provide an argument for whether the model fits the data well and make a comparison to the model from Exercise 1. Make an argument for why this model may have improved model fit.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#instructions-1",
    "href": "hw/hw-02.html#instructions-1",
    "title": "HW 02: Multiple linear regression",
    "section": "Instructions",
    "text": "Instructions\nThe applied exercises are focused on applying the concepts to analyze data.\nAll work for the applied exercises must be typed in your Quarto document following a reproducible workflow.\nWrite all narrative using complete sentences and include informative axis labels / titles on visualizations.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#data-lego-sets",
    "href": "hw/hw-02.html#data-lego-sets",
    "title": "HW 02: Multiple linear regression",
    "section": "Data: LEGO® sets",
    "text": "Data: LEGO® sets\nThe data for Exercises 3 - 5 includes information about LEGO® sets from themes produced January 1, 2018 and September 11, 2020. The data were originally scraped from Brickset.com, an online LEGO set guide and were obtained for this assignment from Peterson and Ziegler (2021).\nYou will work with data on about 400 randomly selected LEGO sets produced during this time period. The primary variables are interest in this analysis are\n\nPieces: Number of pieces in the set from brickset.com.\nMinifigures: Number of minifigures (LEGO® people) in the set scraped from brickset.com.\nAmazon_Price: Price of the set on Amazon.com (in U.S. dollars)\nSize: General size of the interlocking bricks (Large = LEGO Duplo® sets - which include large brick pieces safe for children ages 1 to 5, Small = LEGO® sets which- include the traditional smaller brick pieces created for age groups 5 and - older, e.g., City, Friends)\n\nThe data are contained in lego-sample.csv. Use the code below to read in the data and remove any observations that have missing values for the relevant variables.\n\nlegos &lt;- read_csv(\"data/lego-sample.csv\")|&gt;\n  drop_na(Pieces, Amazon_Price, Size, Minifigures)\n\n\n\n\n\n\n\nAnalysis goal\n\n\n\nWe want to fit a multiple linear regression model to predict the price of LEGO® sets on Amazon.com based on Pieces, Size, and Minifigures.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-3",
    "href": "hw/hw-02.html#exercise-3",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nFor the model from Exercise 2, present the posterior means, standard deviations, and 95% credible intervals for all model parameters. What is interpretation of the slope main effect corresponding to recreation in your model? Within the context of the association between access to recreational facilities and exercise, is this main effect parameter useful to interpret?",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-4",
    "href": "hw/hw-02.html#exercise-4",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the association between access to recreational facilities and exercise, for a pregnant women living in an area with 5 annual crimes/1,000 people? What about for 15 annual crimes/1,000 people? Provide posterior mean and standard deviations for both quantities.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-5",
    "href": "hw/hw-02.html#exercise-5",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret the posterior mean and standard deviations from Exercise 4 and compare and contrast them. What do these posterior slopes say about the impact of crime on the relationship between access to recreational facilities and exercise.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#data-world-bank",
    "href": "hw/hw-02.html#data-world-bank",
    "title": "HW 02: Multiple linear regression",
    "section": "Data: World Bank",
    "text": "Data: World Bank\nThe World Bank collects “world development indicators” about the past and current development of countries. These data are made available on the World Bank’s website. It can be used to understand the relationships between these various factors and trends over time.\n\nThis analysis focuses on indicators from 2011 on 165 countries. The variables of interest are:\n\ngdp.per.capita: gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.\nsanit.access.factor: Population access to sanitation facilities (Low, High)\nedu.expend: Government expenditure on education, total (% of government expenditure)\nlife.expect: Life expectancy at birth (in years)",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-6",
    "href": "hw/hw-02.html#exercise-6",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nResearchers are interested in the level of crime where the association between recreational facilities and exercise disappears. Present the posterior median and interquartile range (i.e., 25% and 75% percentiles) for this quantity.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-7",
    "href": "hw/hw-02.html#exercise-7",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nCompute the posterior predictive distribution for a patient with 10 recreational facilities within a one-mile radius, 5 crimes within a one-mile buffer per 1,000 people, is 40 years old, married, and white race. Report the posterior mean and 95% credible intervals.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-8",
    "href": "hw/hw-02.html#exercise-8",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nResearchers are interested in comparing their original model (i.e., the one from Exercise 2) with a model that does not contain an interaction term between recreational facility access and crime. Fit the model without the interaction term and perform a model comparison between the two models using an information criteria. Which model would you suggest as more scientifically plausible?",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#bonus-optional",
    "href": "hw/hw-02.html#bonus-optional",
    "title": "HW 02: Multiple linear regression",
    "section": "Bonus (optional)",
    "text": "Bonus (optional)\nUse the model from Exercise 6 to interpret the coefficients of edu.expend and sanit.access.factorhigh in term of GDP (not log(GDP)). Write your interpretations in the context of the data.\nEach interpretation is worth 1 point out of 50. The interpretation must be exactly correct to receive credit.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-01.html",
    "href": "hw/hw-01.html",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "",
    "text": "Due date\n\n\n\nThis assignment is due on Thursday, January 30 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#instructions",
    "href": "hw/hw-01.html#instructions",
    "title": "HW 01: Simple linear regression",
    "section": "Instructions",
    "text": "Instructions\nThe conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\n\nYou may write the answers and associated work for conceptual exercises by hand or type them in your Quarto document.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-1",
    "href": "hw/hw-01.html#exercise-1",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\na. Show that the hat matrix \\(\\mathbf{H}\\) is symmetric \\((\\mathbf{H}^T = \\mathbf{H})\\) and idempotent \\((\\mathbf{H}^2 = \\mathbf{H})\\).\nb. Show that \\((\\mathbf{I} - \\mathbf{H})\\) is symmetric and idempotent.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-2",
    "href": "hw/hw-01.html#exercise-2",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet \\(\\mathbf{x}\\) be a \\(k \\times 1\\) vector and \\(\\mathbf{A}\\) be a symmetric \\(k \\times k\\) matrix, such that \\(\\mathbf{A}\\) is not a function of \\(\\mathbf{x}\\).\nShow that the gradient of \\(\\boldsymbol{x}^T\\mathbf{A}\\mathbf{x}\\) with respect to \\(\\mathbf{x}\\) is\n\\[\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^T\\mathbf{A}\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}\n\\]\n(Proposition 2 from class)",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-3",
    "href": "hw/hw-01.html#exercise-3",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nIn class we used the sum of squared residuals (SSR) to estimate the regression coefficients, \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) . To show this is the least squares estimate, we now need to show that we have, in fact, found the estimate of \\(\\boldsymbol{\\beta}\\) that minimizes the SSR (rather than maximize).\nIf the Hessian matrix \\(\\nabla_{\\boldsymbol{\\beta}}^2 SSR\\) is positive definite, then we know we have found the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes SSR, i.e., the least squares estimator. Additionally, we have the following proposition:",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#proposition",
    "href": "hw/hw-01.html#proposition",
    "title": "HW 01: Simple linear regression",
    "section": "Proposition",
    "text": "Proposition\nA matrix \\(\\mathbf{A}\\) is positive definite if \\(\\mathbf{z}^T\\mathbf{A}\\mathbf{z} &gt; 0\\) , given \\(\\mathbf{z}\\) is a non-zero vector.\nShow that \\(\\nabla_{\\boldsymbol{\\beta}}^2 SSR\\) is positive definite.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-4",
    "href": "hw/hw-01.html#exercise-4",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nProve that the maximum value of \\(R^2\\) must be less than 1 if the data set contains observations such that there are different observed values of the response for the same value of the predictor (e.g., the dataset contains observations \\((x_i, y_i)\\) and \\((x_j, y_j)\\) such that \\(x_i = x_j\\) and \\(y_i \\neq y_j\\) ).",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#instructions-1",
    "href": "hw/hw-01.html#instructions-1",
    "title": "HW 01: Simple linear regression",
    "section": "Instructions",
    "text": "Instructions\nThe applied exercises are focused on applying the concepts to analyze data.\nAll work for the applied exercises must be typed in your Quarto document following a reproducible workflow.\nWrite all narrative using complete sentences and include informative axis labels / titles on visualizations.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#data",
    "href": "hw/hw-01.html#data",
    "title": "HW 01: Simple linear regression",
    "section": "Data",
    "text": "Data\nThe datasets wi-icecover.csv and wi-air-temperature.csv contain information about ice cover and air temperature, respectively, at Lake Monona and Lake Mendota (both in Madison, Wisonsin) for days in 1886 through 2019. The data were obtained from the ntl_icecover and ntl_airtemp data frames in the lterdatasampler R package. They were originally collected by the US Long Term Ecological Research program (LTER) Network.\n\nicecover &lt;- read_csv(\"data/wi-icecover.csv\")\nairtemp &lt;- read_csv(\"data/wi-air-temperature.csv\")\n\nThe analysis will focus on the following variables:\n\nyear: year of observation\nlakeid: lake name\nice_duration: number of days between the freeze and ice breakup dates of each lake\nair_temp_avg: yearly average air temperature in Madison, WI (degrees Celsius)",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#analysis-goal",
    "href": "hw/hw-01.html#analysis-goal",
    "title": "HW 01: Simple linear regression",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goal of this analysis is to use linear regression explain variability in ice duration for lakes in Madison, WI based on air temperature. Because ice cover is impacted by various environmental factors, researchers are interested in examining the association between these two factors to better understand the changing climate.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-5",
    "href": "hw/hw-01.html#exercise-5",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nLet’s start by looking at the response variable ice_duration.\n\nCreate side-by-side boxplots to visualize the distribution of ice_duration for each lake.\nVisualize the distribution of ice duration over time for each lake.\nThere are separate measurements for each lake in the icecover data frame. In this analysis, we will combine the data from both lakes and use the average ice duration each year.\nEvaluate the analysis choice to use the average per year rather than the individual lake measurements. Some things to consider in your evaluation: Does the average accurately reflects the ice duration for lakes in Madison, WI for that year? Will there be information loss? How might that impact (or not) the analysis conclusions? Etc.\n\n\n\n\n\n\n\nTip\n\n\n\nSee the ggplot2 reference for example code and plots.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-6",
    "href": "hw/hw-01.html#exercise-6",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nNext, let’s combine the ice duration and air temperature data into a single analysis data frame.\n\nFill in the code below to create a new data frame, icecover_avg, of the average ice duration by year.\nThen join icecover_avg and airtemp to create a new data frame. The new data frame should have 134 observations.\n\nicecover_avg &lt;- icecover |&gt;\n  group_by(_____) |&gt;\n  summarise(_____) |&gt;\n  ungroup()\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the new data frame with average ice duration and average air temperature for the remainder of the assignment.\n\n\n\nVisualize the relationship between the air temperature and average ice duration. Do you think a linear model would be a good fit to capture the relationship between the two variables? \n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-7",
    "href": "hw/hw-01.html#exercise-7",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nWe will fit a model using the average air temperature to explain variability in ice duration that takes the form\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]\n\nState the dimensions of \\(\\mathbf{y}\\), \\(\\mathbf{X}\\), \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\epsilon}\\) for this analysis. Your answer should have exact values given this data set.\nFind the estimated regression coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) using the matrix representation of the model. Show the code used to get the answer.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-8",
    "href": "hw/hw-01.html#exercise-8",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\n\nFit the model from the previous exercise using the lm function. Neatly display the results using 3 digits.\nInterpret the slope in the context of the data.\n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-9",
    "href": "hw/hw-01.html#exercise-9",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCalculate \\(R^2\\) for the model in the previous exercise and interpret it in the context of the data.\nBriefly comment on the model fit based on \\(R^2\\).",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-10",
    "href": "hw/hw-01.html#exercise-10",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nYou are asked to use a reproducible workflow for all of your work in the class, and the goal of this question to is better understand potential real-world implications of doing (or not) so. Below are some real-life examples in which having a non-reproducible workflow resulted in errors that impacted research or public records.\n\nSource: Ostblom and Timbers (2022)\n\n\nReproducibility error\nConsequence\nSource(s)\n\n\n\n\nLimitations in Excel data formats\nLoss of 16,000 COVID case records in the UK\n(Kelion 2020)\n\n\nAutomatic formatting in Excel\nImportant genes disregarded in scientific studies\n(Ziemann, Eren, and El-Osta 2016)\n\n\nDeletion of a cell caused rows to shift\nMix-up of which patient group received the treatment\n(Wallensteen et al. 2018)\n\n\nUsing binary instead of explanatory labels\nMix-up of the intervention with the control group\n(Aboumatar and Wise 2019)\n\n\nUsing the same notation for missing data and zero values\nPaper retraction\n(Whitehouse et al. 2021)\n\n\nIncorrectly copying data in a spreadsheet\nDelay in the opening of a hospital\n(Picken 2020)\n\n\n\nChoose one of the scenarios from the table and read the linked article discussing what went wrong. Then,\n\nBriefly describe what went wrong, i.e., what part of the process of was not reproducible and what error or impact that had.\nDescribe one way the researchers could have made the process reproducible.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "This course will teach students how to analyze biomedical data from a Bayesian inference perspective with a strong emphasis on using real-world data, including electronic health records, wearables, and imaging data. The course will begin by introducing the machinery of Bayesian statistics through the lens of linear regression, giving enough context for students with no prior experience with Bayesian statistics. A history of computational approaches used in Bayesian statistics will be given before ultimately landing on Stan, a state-of-the-art probabilistic programming language that makes Bayesian inference accessible as a viable data science tool. The course will then branch out from regression and introduce Bayesian versions of machine learning tools, including regularization and classification. The course will then emphasize Bayesian hierarchical models, including Gaussian process models for temporal and spatial data; and clustering. Additional topics may be discussed from the Bayesian perspective, including causal inference, and meta-analysis. While an applied course, the methods will be introduced from a mathematical perspective, allowing students to obtain a fundamental understanding of the introduced models. Students will learn computational skills for implementing Bayesian models using R and Stan. By the end of this course, students will be well-equipped to tackle complex problems in biomedical research using Bayesian inference.\n\n\nBIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#pre-requisites",
    "href": "overview.html#pre-requisites",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "BIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#teaching-assistants",
    "href": "overview.html#teaching-assistants",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\n\n\n\n\n\n\nName\nRole\nOffice Hours\nLocation\n\n\nDr. Youngsoo Baek\nTA\nWed 2 - 4pm\nHock 10090\n\n\nBraden Scherting\nTA\nMon 9 – 10am\nWed 1 – 2pm\nOld Chemistry 203A/B",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "labs/lab-07.html",
    "href": "labs/lab-07.html",
    "title": "Lab 07",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "labs/lab-04.html",
    "href": "labs/lab-04.html",
    "title": "Lab 04",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "labs/lab-01.html",
    "href": "labs/lab-01.html",
    "title": "Lab 01: Simple linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Thursday, September 12 at 11:59pm. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "labs/lab-01.html#learning-goals",
    "href": "labs/lab-01.html#learning-goals",
    "title": "Lab 01: Simple linear regression",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to produce visualizations and summary statistics to describe distributions\nBe able to fit, interpret, and evaluate simple linear regression models"
  },
  {
    "objectID": "labs/lab-01.html#clone-the-repo-start-new-rstudio-project",
    "href": "labs/lab-01.html#clone-the-repo-start-new-rstudio-project",
    "title": "Lab 01: Simple linear regression",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta221-fa24 organization on GitHub.\nClick on the repo with the prefix lab-01-. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the Lab 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab-01.html#r-and-r-studio",
    "href": "labs/lab-01.html#r-and-r-studio",
    "title": "Lab 01: Simple linear regression",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of an Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "labs/lab-01.html#exercise-1",
    "href": "labs/lab-01.html#exercise-1",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWe begin with some exploratory data analysis (EDA). As a first step, let’s get a quick summary look at the data using the glimpse function.\nViewing a summary of the data is a useful starting point for analysis, especially if there are a large number of observations or variables.\n\nglimpse(parks)\n\n\nHow many observations are in the parks data frame?\nWhat information is provided in the data about the time and location of the measurements?"
  },
  {
    "objectID": "labs/lab-01.html#exercise-2",
    "href": "labs/lab-01.html#exercise-2",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe predictor variable for this analysis, spend_per_resident_data, is quantitative; however, from the glimpse of the data in Exercise 1, we see its data type is chr (character) in R. We would expect it to be dbl (double), the data type for numeric data.\nWhy did spend_per_resident_data get read by R as a character data type instead of a double? Be specific."
  },
  {
    "objectID": "labs/lab-01.html#exercise-3",
    "href": "labs/lab-01.html#exercise-3",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nUse the code below to transform spend_per_resident_data , so that it is correctly treated as quantitative data in R. Write a brief explanation of what each numbered line of code does.\n\n1parks &lt;-\n  parks |&gt;  \n  mutate(spend_per_resident_data = \n2           str_replace(spend_per_resident_data,\"\\\\$\", \"\")) |&gt;\n  mutate(spend_per_resident_data = \n3           as.numeric(spend_per_resident_data))\n\n\n1\n\n______\n\n2\n\n______\n\n3\n\n______\n\n\n\n\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., “Completed exercises 1 - 3”), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab-01.html#exercise-4",
    "href": "labs/lab-01.html#exercise-4",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nNow we’ll examine the distributions of the variables of interest.\n\nMake a histogram of spend_per_resident_data and calculate summary statistics for this variable.\nComment on the features of the distribution of this variable by describing the shape, center, spread, and presence of potential outliers.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen performing data visualization, make sure that all your plots have clear and informative titles and axis labels. When investigating more complex relationships with many variables, this simple tip will save you and your readers a lot of time and confusion.\n\nSee AE 01 for example code."
  },
  {
    "objectID": "labs/lab-01.html#exercise-5",
    "href": "labs/lab-01.html#exercise-5",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNext let’s explore the response variable, pct_near_park_points. Visualize the distribution of the variable and calculate summary statistics. Describe the distribution pct_near_park_points."
  },
  {
    "objectID": "labs/lab-01.html#exercise-6",
    "href": "labs/lab-01.html#exercise-6",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nNow let’s use bivariate exploratory data analysis to look at the relationship between spend_per_resident_data and pct_near_park_points.\n\nMake a scatterplot to visualize the relationship between the two variables.\nDoes there seem to be a relationship between spending and park access? If so, what is the shape and direction of the relationship?\n\n\nThis is a another good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g. “Completed exercises 4 - 6”), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab-01.html#exercise-7",
    "href": "labs/lab-01.html#exercise-7",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nWe have seen the mathematical formulation for simple linear regression. In particular, given a response variable \\(Y\\) and predictor variable \\(X\\), the simple linear regression model is \\[Y = \\beta_0 + \\beta_1 X + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\]\nfor some unknown regression coefficients for slope and intercept\\((\\beta_0, \\beta_1)\\). This means that the expected value of each observation lies on the regression line\n\\[ E(Y|X) = \\beta_0 + \\beta_1 X\\]\nAnswer the following questions about simple linear regression. Your response should be in general terms about simple linear, not be specific to the parks data.\n\nWhat does \\(E(Y|X) = \\beta_0 + \\beta_1X\\) mean in terms of a given value of \\(X\\)?\nWhat is the interpretation of the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in terms of the expected value of \\(Y\\)?"
  },
  {
    "objectID": "labs/lab-01.html#exercise-8",
    "href": "labs/lab-01.html#exercise-8",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nIn class we’ve seen how matrices can be used to represent the simple linear model from the previous exercise. In particular\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\n\\]\nRecall that the goal is to fit a model that uses spend_per_resident_data to explain variability in park_near_pct_points.Estimate regression coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) for the model using the matrix representation. Show any work and/or code used to get the answer."
  },
  {
    "objectID": "labs/lab-01.html#exercise-9",
    "href": "labs/lab-01.html#exercise-9",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nNow let’s fit the model using the lm() function in R.\n\nFit the model and neatly display the output using 4 digits.\nInterpret the slope in the context of the data.\nDoes it make sense of the interpret the intercept? If so, interpret the intercept in the context of the data. Otherwise, explain why not."
  },
  {
    "objectID": "labs/lab-01.html#exercise-10",
    "href": "labs/lab-01.html#exercise-10",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nDo you think that city expenditure on residents is a useful predictor of park access? Briefly explain your response, reporting any statistics used to make your assessment.\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Lab 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "labs/lab-02.html",
    "href": "labs/lab-02.html",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Thursday, September 19 at 11:59pm. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your team’s GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "labs/lab-02.html#exercise-1",
    "href": "labs/lab-02.html#exercise-1",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet’s start with some exploratory data analysis. Visualize the distribution of the response variable mcsa and calculate summary statistics. Describe the distribution of this variable, including the shape, center, spread, and presence of potential outliers.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the childcare_train for all analysis in Exercises 1 - 7."
  },
  {
    "objectID": "labs/lab-02.html#exercise-2",
    "href": "labs/lab-02.html#exercise-2",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nAs you can see from the data dictionary in the README of the data folder, there are many interesting potential variables that could be included in the model to predict median childcare cost for school-age children. Therefore, we will do some feature selection and feature design to choose potential predictors and construct new ones.\nAs a team, select four variables you want to use as predictors for the model. For each variable, state the variable name, definition, and a brief explanation about why your team hypothesizes this will be a relevant predictor of median childcare costs. The explanation may (but is not required to) include some short exploratory analysis.\n\nTeam Member 1: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 1- 2.\nTeam Member 2: It’s your turn! Type the team’s response to exercises 3 - 4."
  },
  {
    "objectID": "labs/lab-02.html#exercise-3",
    "href": "labs/lab-02.html#exercise-3",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nOnce we’ve identified potential predictor variables, we often need to transform some variables (e.g., change raw counts into proportions) or create new ones (e.g., create a categorical variable out of quantitative data) before fitting the regression model. This process is particularly useful when putting a variable in the model “as-is” may result in interpretation issues.\nChoose one of the variables selected in the previous exercise. For this variable,\n\nTransform the variable or use it to create a new variable. Be sure to save the variable to the childcare_train data frame.\nBriefly explain your reasoning for the transformation or new variable.\nUse visualizations and/or summary statistics to display the distribution of the original variable and the transformed / newly created variable. Note: This is to help ensure the transformation / new variable is what you expect.\n\nAn example using h_6to17_both_work is below. Note you cannot use this variable for your transformation / new variable.\n\n\n\n\n\n\nExample\n\n\n\nSay we believe that the amount of households with two working parents increases demand for childcare services, and hence their price. We have the column h_6to17_both_work encoding the number of such households per county. The raw population count differs across county, so having a larger value of h_6to17_both_work may reflect population size and not necessarily imply the type of such household is more prevalent in the county.\nA reasonable thing would be creating a variable encoding the proportion of households with both parents working. We could do this by creating a variable p_6to17_both_work:\n\np_6to17_both_work = h_6to17_both_work / households\n\nIn this case, we would then use p_6to17_both_work not h_6to17_both_work as a predictor in the model.\n\n\nYou may decide to transform and/or create multiple new variables; however, you will only be graded on the one of them.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the transformed / new variable (not the original variable) in the model!"
  },
  {
    "objectID": "labs/lab-02.html#exercise-4",
    "href": "labs/lab-02.html#exercise-4",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nNow let’s conduct some bivariate exploratory data analysis. Visualize the relationship between the response variable and one of your predictor variables.\nWrite two distinct observations from the visualization.\n\nTeam Member 2: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 3 - 4.\nTeam Member 3: It’s your turn! Type the team’s response to exercises 5 - 6."
  },
  {
    "objectID": "labs/lab-02.html#exercise-5",
    "href": "labs/lab-02.html#exercise-5",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse the matrix form of the model to represent the regression model with the variables you selected and transformed/created in exercises 2 and 3 as the predictors. For each symbol in the model\n\ndescribe what it represents, and\nstate the dimensions.\n\nThe description and dimensions should be in the context of these data, not in general."
  },
  {
    "objectID": "labs/lab-02.html#exercise-6",
    "href": "labs/lab-02.html#exercise-6",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse lm to fit the regression model you described in the previous exercise.\n\nNeatly display the model using a reasonable number of digits.\n\n\n\nInterpret the coefficient for one predictor in the model.\n\n\nTeam Member 3: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 5 - 6.\nTeam Member 4: It’s your turn! Type the team’s response to exercises 7 - 9."
  },
  {
    "objectID": "labs/lab-02.html#exercise-7",
    "href": "labs/lab-02.html#exercise-7",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow let’s assess the fit of the model.\n\nHow much of the variability in the childcare costs is explained by your chosen predictor variables?\n\n\n\nBased on this, do you think the model explains a significant portion of the variability in childcare costs for school-age children in North Carolina? Briefly explain."
  },
  {
    "objectID": "labs/lab-02.html#exercise-8",
    "href": "labs/lab-02.html#exercise-8",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let’s use the testing data to explore the predictive power of the model.\n\nAdd the variable you created in Exercise 3 to the testing data.\nThen, use the code below to compute the predicted childcare costs for the observations in the testing data using the predict function.\n\n\n# compute predictions\npred &lt;- predict(childcare_fit, childcare_test)\n\n# add predictions to testing data set\nchildcare_test &lt;- childcare_test |&gt;\n  mutate(pred = pred)"
  },
  {
    "objectID": "labs/lab-02.html#exercise-9",
    "href": "labs/lab-02.html#exercise-9",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCompute the RMSE for the test set, and compare it to the standard deviation of the response variable mcsa.\n\n\n\nHow do these values compare?\nBased on this, how would assess the predictive power of the model?\n\n\nTeam Member 4: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and the rest of the team can see the completed lab.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the team’s completed lab!"
  },
  {
    "objectID": "labs/lab-02.html#exercise-10",
    "href": "labs/lab-02.html#exercise-10",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nIf you haven’t already, make sure you have completed the team agreement (see the instructions in [Meet your team!])."
  },
  {
    "objectID": "labs/lab-02.html#footnotes",
    "href": "labs/lab-02.html#footnotes",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDon’t trust yourself to keep your hands off the keyboard? Put them in your pocket or cross your arms. No matter how silly it might feel, resist the urge to touch your keyboard until otherwise instructed!↩︎"
  },
  {
    "objectID": "labs/lab-03.html",
    "href": "labs/lab-03.html",
    "title": "Lab 03: Inference for regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Thursday, October 3 at 11:59pm. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your team’s GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "labs/lab-03.html#exercise-0",
    "href": "labs/lab-03.html#exercise-0",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 0",
    "text": "Exercise 0\nThere are two penguins in the data frame that do not have reported values for flipper length or body mass and thus will not be included in any analysis. Remove these observations from the data frame, so that we have an accurate count of the number of observations used for the analysis.\n\n\n\n\n\n\nNote\n\n\n\nExericse 0 is not graded."
  },
  {
    "objectID": "labs/lab-03.html#exercise-1",
    "href": "labs/lab-03.html#exercise-1",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet’s begin by exploring the relationship between between flipper length and body mass, while accounting for species.\n\nVisualize the relationship between flipper length and body mass. Then describe the relationship.\nFit the main effects linear regression model (no interaction terms) between these three variables. Neatly display the results using three digits.\nInterpret the coefficient of flipper length in the context of the data."
  },
  {
    "objectID": "labs/lab-03.html#exercise-2",
    "href": "labs/lab-03.html#exercise-2",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nNow let’s look at the assumptions underlying the regression model. Consider the linear regression model\n\\[ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I}_n)  \\tag{1}\\]\nand let \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\) be the least squares estimator. This model relies on four assumptions:\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from one another.\n\nFor each condition, state the components of Equation 1 that are used to represent it."
  },
  {
    "objectID": "labs/lab-03.html#exercise-3",
    "href": "labs/lab-03.html#exercise-3",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe can visually assess the linearity and constant variance assumptions by examining a scatterplot of the residuals versus fitted (predicted) values.\n\nCreate a scatterplot of the residuals (y-axis) versus fitted values (x-axis) for the model fit in Exercise 1.\nIf there is a linear relationship between the response and predictor variables, no discernible pattern should be present between fitted values and residuals. Does the linearity assumption appear to be satisfied?\nBriefly explain why no discernible pattern in the plot of residuals versus fitted values would indicate the linearity condition is satisfied."
  },
  {
    "objectID": "labs/lab-03.html#exercise-4",
    "href": "labs/lab-03.html#exercise-4",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nIf errors have constant variance, we would expect the variability the of residuals about their mean to be approximately equal as the fitted value increases. Does the constant variance assumption appear to be satisfied? Briefly explain."
  },
  {
    "objectID": "labs/lab-03.html#exercise-5",
    "href": "labs/lab-03.html#exercise-5",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNext, let’s assess the assumptions about the distribution fo the residuals. Under the normality assumption, the residuals are expected to be normally distributed. Visualize the distribution of the residuals. Does the normality assumption appear to be satisfied? Briefly explain."
  },
  {
    "objectID": "labs/lab-03.html#exercise-6",
    "href": "labs/lab-03.html#exercise-6",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nThe last assumption is that the residuals are independent of one another. Do you think it’s reasonable to assume the independence of residuals in this analysis? Briefly explain."
  },
  {
    "objectID": "labs/lab-03.html#exercise-7",
    "href": "labs/lab-03.html#exercise-7",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow let’s set up the test of whether the flipper length has a statistically significant effect on body mass in this model.\n\nWrite the null and alternative hypotheses in words and in mathematical notation.\nShow how the test statistic is computed specifically for this problem. In your response, show the code to obtain each relevant quantity in the formula for the test statistic using the matrix form of the model. Do not merely refer to the values in the lm output. You must show how each value is computed using the matrix / vector calculations.\nState the distribution of the test statistic under the null hypothesis for this problem."
  },
  {
    "objectID": "labs/lab-03.html#exercise-8",
    "href": "labs/lab-03.html#exercise-8",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nIn the regression output from Exercise 1, you are provided the \\(p\\)-value for the test of significance of each individual coefficient.\n\nInterpret the p-value for the coefficient of flipper length in the context of the data.\nThen, use the p-value and a decision-making threshold of \\(\\alpha = 0.05\\) to draw a conclusion about the relationship between flipper length and body mass in this model. State your conclusion in the context of the data."
  },
  {
    "objectID": "labs/lab-03.html#exercise-9",
    "href": "labs/lab-03.html#exercise-9",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nNow let’s construct the 95% confidence interval for the coefficient of flipper length.\n\nWrite the general formula for the 95% confidence interval.\nUse R functions to compute all the quantities you need for the interval, then compute the interval."
  },
  {
    "objectID": "labs/lab-03.html#exercise-10",
    "href": "labs/lab-03.html#exercise-10",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nInterpret the interval from the previous exercise in the context of the data."
  },
  {
    "objectID": "labs/lab-00.html",
    "href": "labs/lab-00.html",
    "title": "Lab 00: Welcome + Getting Started",
    "section": "",
    "text": "Important\n\n\n\nPlease complete all today’s lab tasks before leaving lab today."
  },
  {
    "objectID": "labs/lab-00.html#rstudio",
    "href": "labs/lab-00.html#rstudio",
    "title": "Lab 00: Welcome + Getting Started",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\n\nReserve RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers. You will log in using your NetID credentials.\nClick “Reserve STA 210” to reserve an RStudio container. Be sure you reserve the container labeled STA 210 to ensure you have the computing set up you need for the class.\n\nYou only need to reserve a container once per semester.\n\n\nOpen RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA210 to log into the Docker container. You should now see the RStudio environment."
  },
  {
    "objectID": "labs/lab-00.html#git-and-github",
    "href": "labs/lab-00.html#git-and-github",
    "title": "Lab 00: Welcome + Getting Started",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nIn addition to R and RStudio, we will use git and GitHub for version control and collaboration.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, project, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username.\n\n\n\nIf you already have a GitHub account, you can move on to the next step."
  },
  {
    "objectID": "labs/lab-00.html#connect-rstudio-and-github",
    "href": "labs/lab-00.html#connect-rstudio-and-github",
    "title": "Lab 00: Welcome + Getting Started",
    "section": "Connect RStudio and GitHub",
    "text": "Connect RStudio and GitHub\nNow that you have RStudio and a GitHub account, we will configure git so that RStudio and GitHub communicate with one another.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps; you are encouraged to follow along as your TA demonstrates the steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system.\n\n\n\nStep 0: Open your STA 210 RStudio container.\nStep 1: Type credentials::ssh_setup_github() into the console on the bottom left of the RStudio environment.\nStep 2: R will ask “No SSH key found. Generate one now?” Click 1 for yes.\nStep 3: You will generate a key. It will begin with “ssh-rsa….” R will then ask “Would you like to open a browser now?” Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used, e.g., sta221)\n\n\n\nConfigure git\nThe last thing we need to do is configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"Maria Tackett\",\n  user.email = \"maria.tackett@duke.edu\")\n\nIt may look like nothing happened but you are now ready interact between GitHub and RStudio! We will begin working with RStudio and GitHub in lecture this week."
  },
  {
    "objectID": "labs/lab-05.html",
    "href": "labs/lab-05.html",
    "title": "Lab 05",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "labs/lab-06.html",
    "href": "labs/lab-06.html",
    "title": "Lab 06",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "hw/hw-03.html",
    "href": "hw/hw-03.html",
    "title": "HW 03: Going beyond linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis assignment is due on Thursday, February 27 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/stats-experience.html",
    "href": "hw/stats-experience.html",
    "title": "Statistics Experience",
    "section": "",
    "text": "Important\n\n\n\nThis assignment is due on Tuesday, November 26 at 11:59pm on Gradescope.\nThe world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1️⃣ Have a statistics experience.\n2️⃣ Make a slide reflecting on your experience.\nYou must complete both parts to receive credit. The statistics experience will count as a homework grade.",
    "crumbs": [
      "Homework",
      "Statistics experience"
    ]
  },
  {
    "objectID": "hw/stats-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/stats-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you’d like to do but you’re not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::conf talks\n\n2022 conference\n2021 conference\n2020 conference\n\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask Professor Tackett if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn’t on this list, ask Professor Tackett to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nWeapons of Math Destruction by Cathy O’Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don’t by Nate Silver\nList of books about data science ethics\n\nThis list is not exhaustive.\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n✅ Create a GitHub repo for your TidyTuesday submission. Your repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The visualization should include features or customization that are beyond what we’ve done in class .\n✅ Include the link to your GitHub repo in the slide summarizing your experience.\n\n\nCategory 7: CURV - connecting, uplifting, and recognizing voices\nCURV is a project by Dr. Jo Hardin at Pomona College to highlight statisticians and data scientists from groups who have been historically marginalized in the discipline. \n\n\n\n\n\n\nFor this statistics experience, you can contribute to the CURV data base. If there is a scholar you would like to suggest for the data base, submit your suggestion as an issue or pull request on the CURV GitHub repo and create a sample CURV page.\nA few guidelines:\n✅ Create a draft of the CURV page for your suggested scholar. For reference, click here for the CURV page for W.E.B. Du Bois. The page must be created in a Quarto document.\n\n\n\n\n\n\nTip\n\n\n\nYou can find the Quarto documents for current scholars in the data base in the CURV GitHub repo. You can use one of these as a template to format your page.\n\n\n✅ Make a pull request to the CURV GitHub repo to add the .qmd file for your suggested scholar, OR open an issue with a link to the .qmd file for your suggested scholar. You can ask a member of the teaching team if you have questions about how to do this.\n✅ Include the URL to your pull request or issue in your one-slide reflection.",
    "crumbs": [
      "Homework",
      "Statistics experience"
    ]
  },
  {
    "objectID": "hw/stats-experience.html#part-2-reflect-on-your-experience",
    "href": "hw/stats-experience.html#part-2-reflect-on-your-experience",
    "title": "Statistics Experience",
    "section": "Part 2: Reflect on your experience",
    "text": "Part 2: Reflect on your experience\nMake one slide summarizing and reflecting on your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nDescription of the experience\n\nName and brief description of the event/podcast/competition/etc.\n\nSomething you learned\n\nWrite 2 - 4 sentences about something you learned or found particularly interesting or unexpected.\n\nConnection to STA 221\n\nWrite 2 - 4 sentences about how the experience connects to what we’ve done in the course.\n\nCitation or link to web page for event/competition/etc.\n\nNo citation needed if you do an interview.\n\n\nMake sure the slide includes the information mentioned above and is easily readable (i.e. use a reasonable font size!). Creativity on the experience and slide design is encouraged!",
    "crumbs": [
      "Homework",
      "Statistics experience"
    ]
  },
  {
    "objectID": "hw/stats-experience.html#submission",
    "href": "hw/stats-experience.html#submission",
    "title": "Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the Statistics Experience assignment on Gradescope by Tuesday, November 26 at 11:59pm. Standard homework late policy applies.",
    "crumbs": [
      "Homework",
      "Statistics experience"
    ]
  },
  {
    "objectID": "prepare/prepare-sep3.html",
    "href": "prepare/prepare-sep3.html",
    "title": "Prepare for September 3 lecture",
    "section": "",
    "text": "📖 Read Section 4.7: Model Assessment\n✅ Go through R resource (optional)"
  },
  {
    "objectID": "prepare/prepare-aug29.html",
    "href": "prepare/prepare-aug29.html",
    "title": "Prepare for August 29 lecture",
    "section": "",
    "text": "📖 Read Simple Linear Regression\n✅ Complete Lab 00 tasks"
  },
  {
    "objectID": "prepare/prepare-sep19.html",
    "href": "prepare/prepare-sep19.html",
    "title": "Prepare for September 19 lecture",
    "section": "",
    "text": "📖 Read Inference for Simple Linear Regression:\n\nSections 5.1 - 5.3\nSection 5.6\nSection 5.8\nSection 5.9"
  },
  {
    "objectID": "ae/ae-01-slr.html",
    "href": "ae/ae-01-slr.html",
    "title": "AE 01: Simple linear regression",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-01 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class to submit your AE.\n\nThis AE will not count towards your participation grade.\nlibrary(tidyverse)    # data wrangling and visualization\nlibrary(tidymodels)   # broom and yardstick package\nlibrary(openintro)    # duke_forest dataset\nlibrary(knitr)        # format output\nlibrary(scales)       # format plot axes\nlibrary(skimr)        # quickly calculate summary statistics"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-1",
    "href": "ae/ae-01-slr.html#exercise-1",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWhat are 1 - 2 observations about the distribution of price?"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-2",
    "href": "ae/ae-01-slr.html#exercise-2",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nVisualize the distribution of area and calculate summary statistics.\n\n# add code here\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-3",
    "href": "ae/ae-01-slr.html#exercise-3",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat are 1 - 2 observations about the distribution of area?"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-4",
    "href": "ae/ae-01-slr.html#exercise-4",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nFill in the code to visualize the relationship between price and area. What are 1 - 2 observations about the relationship between these two variables?\n\n\n\n\n\n\nImportant\n\n\n\nRemove #|eval: false after you have filled in the code!\n\n\n\nggplot(duke_forest, aes(x = ____, y = ____)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    x = \"_______\",\n    y = \"_________\",\n    title = \"Price and area of houses in Duke Forest\"\n  ) +\n  scale_y_continuous(labels = label_dollar())"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-5",
    "href": "ae/ae-01-slr.html#exercise-5",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nYou want to fit a model of the form\n\\[\nprice = \\beta_0 + \\beta_1 ~ area + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2_\\epsilon)\n\\]\nWould a model of this form be a reasonable fit for the data? Why or why not?"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-6",
    "href": "ae/ae-01-slr.html#exercise-6",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nFit the linear model described in the previous exercise and neatly display the output.\nSee notes for example code.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-7",
    "href": "ae/ae-01-slr.html#exercise-7",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nInterpret the slope in the context of the data.\nInterpret the slope in terms of area increasing by 100 sqft.\nWhich interpretation do you think is more meaningful in practice?"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-8",
    "href": "ae/ae-01-slr.html#exercise-8",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nDoes it make sense to interpret the intercept? If so, interpret it in the context of the data. Otherwise, explain why not."
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#lectures-and-labs",
    "href": "support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#office-hours",
    "href": "support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!\nMake a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#ed-discussion",
    "href": "support.html#ed-discussion",
    "title": "Course support",
    "section": "Ed Discussion",
    "text": "Ed Discussion\nOutside of class and office hours, any general questions about course content or assignments should be posted on Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters that are not appropriate for the class discussion forum (e.g. illness, accommodations, etc.), you may email me at sib2@duke.edu. If you email me, please include “BIOSTAT 725” in the subject line. Barring extenuating circumstances, I will respond to BIOSTAT 725 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#academic-support",
    "href": "support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact ARC@duke.edu, 919-684-5917.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#mental-health-and-wellness",
    "href": "support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\n\nDukeReach: Provides comprehensive outreach services to identify and support students in managing all aspects of well being. If you have concerns about a student’s behavior or health visit the website for resources and assistance. Go to studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS): CAPS services include individual, group, and couples counseling services, health coaching, psychiatric services, and workshops and discussions. (919) 660-1000 or students.duke.edu/wellness/caps\nTimelyCare (formerly known as Blue Devils Care): An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#technology-accommodations",
    "href": "support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\n\nWhile we encourage students to use their own laptops for work in this course, we will provide the opportunity to use Duke’s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#course-materials-costs",
    "href": "support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#assistance-with-zoom-or-canvas",
    "href": "support.html#assistance-with-zoom-or-canvas",
    "title": "Course support",
    "section": "Assistance with Zoom or Canvas",
    "text": "Assistance with Zoom or Canvas\nFor technical help with Canvas or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Canvas here.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA725 on the Reservations available menu on the right. You only need to do this once, and when you do, you’ll see this container moved to the My reservations menu on the left.\nNext, click on STA725 under My reservations to access the RStudio instance you’ll use for the course.",
    "crumbs": [
      "Computing",
      "Access"
    ]
  },
  {
    "objectID": "slides/lab-02.html#goals",
    "href": "slides/lab-02.html#goals",
    "title": "Lab 02",
    "section": "Goals",
    "text": "Goals\n\nLaTex in this course\nMeet your team!\nTeam agreement\nLab 02: Childcare costs"
  },
  {
    "objectID": "slides/lab-02.html#latex-in-this-class",
    "href": "slides/lab-02.html#latex-in-this-class",
    "title": "Lab 02",
    "section": "LaTex in this class",
    "text": "LaTex in this class\nFor this class you will need to be able to…\n\nProperly write mathematical symbols, e.g., \\(\\beta_1\\) not B1, \\(R^2\\) not R2\nWrite basic regression equations, e.g., \\(\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\nWrite matrix equations: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\)\nWrite hypotheses (we’ll start this next week), e.g., \\(H_0: \\beta = 0\\)\n\nYou are welcome to but not required to write math proofs using LaTex."
  },
  {
    "objectID": "slides/lab-02.html#writing-latex-from-ae-02",
    "href": "slides/lab-02.html#writing-latex-from-ae-02",
    "title": "Lab 02",
    "section": "Writing LaTex (from AE 02)",
    "text": "Writing LaTex (from AE 02)\nInline: Your mathematics will display within the line of text.\n\nUse $ to start and end your LaTex syntax. You can also use the menu: Insert -&gt; LaTex Math -&gt; Inline Math.\nExample: The text The simple linear regression model is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ produces\nThe simple linear regression model is \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\)"
  },
  {
    "objectID": "slides/lab-02.html#writing-latex-from-ae-02-1",
    "href": "slides/lab-02.html#writing-latex-from-ae-02-1",
    "title": "Lab 02",
    "section": "Writing LaTex (from AE 02)",
    "text": "Writing LaTex (from AE 02)\nDisplay: Your mathematics will display outside the line of text\n\nUse a $$ to start and end your LaTex syntax. You can also use the menu: Insert -&gt; LaTex Math -&gt; Display Math.\nExample: The text The estimated regression equation is $$\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$$ produces\nThe estimated regression equation is\n\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\]\n\n\n\n\n\n\nTip\n\n\nClick here for a quick reference of LaTex code."
  },
  {
    "objectID": "slides/lab-02.html#meet-your-team",
    "href": "slides/lab-02.html#meet-your-team",
    "title": "Lab 02",
    "section": "Meet your team!",
    "text": "Meet your team!\n\nClick here to find your team.\nSit with your team."
  },
  {
    "objectID": "slides/lab-02.html#team-name-agreement",
    "href": "slides/lab-02.html#team-name-agreement",
    "title": "Lab 02",
    "section": "Team name + agreement",
    "text": "Team name + agreement\n\nCome up with a team name. You can’t have the same name as another group in the class, so be creative!\n\nYour TA will get your team name by the end of lab.\n\nFill out the team agreement. The goals of the agreement are to…\n\nGain a common understanding of the team’s goals and expectations for collaboration\nMake a plan for team communication\nMake a plan for working outside of lab"
  },
  {
    "objectID": "slides/lab-02.html#team-workflow",
    "href": "slides/lab-02.html#team-workflow",
    "title": "Lab 02",
    "section": "Team workflow",
    "text": "Team workflow\n\nOnly one team member should type at a time. There are markers in today’s lab to help you determine whose turn it is to type.\n\nEvery team member should still be engaged in discussion for all questions, even if it’s not your turn type.\n\nDon’t forget to pull to get your teammates’ updates before making changes to the .qmd file.\n\n\n\n\n\n\nImportant\n\n\nOnly one submission per team on Gradescope. Read the submission instructions carefully!"
  },
  {
    "objectID": "slides/lab-02.html#team-workflow-in-action",
    "href": "slides/lab-02.html#team-workflow-in-action",
    "title": "Lab 02",
    "section": "Team workflow, in action",
    "text": "Team workflow, in action\n\nComplete the “Workflow: Using Git and GitHub as a team” section of the lab in your teams.\nRaise your hand if you have any questions about the workflow.\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/lab-02.html#tips-for-working-on-a-team",
    "href": "slides/lab-02.html#tips-for-working-on-a-team",
    "title": "Lab 02",
    "section": "Tips for working on a team",
    "text": "Tips for working on a team\n\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report.\nThe labs are structured to help you learn the steps of a data analysis. Do not split up the lab among the team members; work on it together in its entirety.\nEveryone has something to contribute! Use the lab groups as an opportunity to share ideas and learn from each other."
  },
  {
    "objectID": "slides/lab-02.html#lab-02-childcare-costs",
    "href": "slides/lab-02.html#lab-02-childcare-costs",
    "title": "Lab 02",
    "section": "Lab 02: Childcare costs",
    "text": "Lab 02: Childcare costs\nToday’s lab focuses on using multiple linear regression (Week 03 content) to predict childcare costs for school-aged children in North Carolina.\n🔗 sta221-fa24.netlify.app/labs/lab-02.html\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#announcements",
    "href": "slides/04-slr-model-assessment-contd.html#announcements",
    "title": "SLR: Model assessment cont’d",
    "section": "Announcements",
    "text": "Announcements\n\nOffice hours start this week. See schedule on Overview page of the course website or on Canvas.\nLabs resume Monday, September 09"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#topics",
    "href": "slides/04-slr-model-assessment-contd.html#topics",
    "title": "SLR: Model assessment cont’d",
    "section": "Topics",
    "text": "Topics\n\nEvaluate models using RMSE and \\(R^2\\)\nUse analysis of variance to partition variability in the response variable"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#computing-set-up",
    "href": "slides/04-slr-model-assessment-contd.html#computing-set-up",
    "title": "SLR: Model assessment cont’d",
    "section": "Computing set up",
    "text": "Computing set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling (includes broom, yardstick, and other packages)\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme for ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#data-houses-in-duke-forest",
    "href": "slides/04-slr-model-assessment-contd.html#data-houses-in-duke-forest",
    "title": "SLR: Model assessment cont’d",
    "section": "Data: Houses in Duke Forest",
    "text": "Data: Houses in Duke Forest\n\n\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\nSource: openintro::duke_forest\n\n\n\n\nGoal: Use the area (in square feet) to understand variability in the price of houses in Duke Forest."
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#regression-model",
    "href": "slides/04-slr-model-assessment-contd.html#regression-model",
    "title": "SLR: Model assessment cont’d",
    "section": "Regression model",
    "text": "Regression model\n\nduke_forest_fit &lt;- lm(price ~ area, data = duke_forest)\n\ntidy(duke_forest_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000\n\n\n\n\n\n\n\nWe fit a model but is it any good?"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#two-statistics",
    "href": "slides/04-slr-model-assessment-contd.html#two-statistics",
    "title": "SLR: Model assessment cont’d",
    "section": "Two statistics",
    "text": "Two statistics\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#rmse",
    "href": "slides/04-slr-model-assessment-contd.html#rmse",
    "title": "SLR: Model assessment cont’d",
    "section": "RMSE",
    "text": "RMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}}\n\\]\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nThe value of RMSE is more useful for comparing across models than evaluating a single model (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#analysis-of-variance-anova",
    "href": "slides/04-slr-model-assessment-contd.html#analysis-of-variance-anova",
    "title": "SLR: Model assessment cont’d",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of Variance (ANOVA): Technique to partition variability in \\(Y\\) by the sources of variability"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#total-variability-response",
    "href": "slides/04-slr-model-assessment-contd.html#total-variability-response",
    "title": "SLR: Model assessment cont’d",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\nGoal: Quantify how much variability in price is accounted for by the model (area) and how much accounted for by factors not included in the model."
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#partition-variability-in-price",
    "href": "slides/04-slr-model-assessment-contd.html#partition-variability-in-price",
    "title": "SLR: Model assessment cont’d",
    "section": "Partition variability in price",
    "text": "Partition variability in price\nFor now, let’s focus on two observations"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#total-variability-response-1",
    "href": "slides/04-slr-model-assessment-contd.html#total-variability-response-1",
    "title": "SLR: Model assessment cont’d",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\\[\\text{Sum of Squares Total (SST)} = \\sum_{i=1}^n(y_i - \\bar{y})^2 = (n-1)s_y^2\\]"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#explained-variability-model",
    "href": "slides/04-slr-model-assessment-contd.html#explained-variability-model",
    "title": "SLR: Model assessment cont’d",
    "section": "Explained variability (Model)",
    "text": "Explained variability (Model)\n\n\\[\\text{Sum of Squares Model (SSM)} = \\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#unexplained-variability-residuals",
    "href": "slides/04-slr-model-assessment-contd.html#unexplained-variability-residuals",
    "title": "SLR: Model assessment cont’d",
    "section": "Unexplained variability (Residuals)",
    "text": "Unexplained variability (Residuals)\n\n\\[\\text{Sum of Squares Residuals (SSR)} = \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2\\]"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#sum-of-squares",
    "href": "slides/04-slr-model-assessment-contd.html#sum-of-squares",
    "title": "SLR: Model assessment cont’d",
    "section": "Sum of Squares",
    "text": "Sum of Squares\n\n\\[\n\\begin{aligned}\n\\color{#407E99}{SST} \\hspace{5mm}&= &\\color{#993399}{SSM} &\\hspace{5mm} +  &\\color{#8BB174}{SSR} \\\\[10pt]\n\\color{#407E99}{\\sum_{i=1}^n(y_i - \\bar{y})^2} \\hspace{5mm}&= &\\color{#993399}{\\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2} &\\hspace{5mm}+ &\\color{#8BB174}{\\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nNote\n\n\nSee Sum of Squares for mathematical details showing \\(SST = SSM + SSR\\)."
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#r2",
    "href": "slides/04-slr-model-assessment-contd.html#r2",
    "title": "SLR: Model assessment cont’d",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nThe coefficient of determination \\(R^2\\) is the proportion of variation in the response, \\(Y\\), that is explained by the regression model\n\n\\[\\large{R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}}\\]\n\n\nWhat is the range of \\(R^2\\)? Does \\(R^2\\) have units?"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#interpreting-r2",
    "href": "slides/04-slr-model-assessment-contd.html#interpreting-r2",
    "title": "SLR: Model assessment cont’d",
    "section": "Interpreting $R^2$",
    "text": "Interpreting $R^2$\n\nQuestionSubmit\n\n\n\nSubmit your response to the following question on Ed Discussion.\n\nThe \\(R^2\\) of the model for price from area of houses in Duke Forest is 44.5%. Which of the following is the correct interpretation of this value?\n\nArea correctly predicts 44.5% of price for houses in Duke Forest.\n44.5% of the variability in price for houses in Duke Forest can be explained by area.\n44.5% of the variability in area for houses in Duke Forest can be explained by price.\n44.5% of the time price for houses in Duke Forest can be predicted by area.\n\nDo you think this model is useful for explaining variability in the price of Duke Forest houses?\n\n\n\n\n\n\n\n\n🔗 https://edstem.org/us/courses/62513/discussion/629888"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#augmented-data-frame",
    "href": "slides/04-slr-model-assessment-contd.html#augmented-data-frame",
    "title": "SLR: Model assessment cont’d",
    "section": "Augmented data frame",
    "text": "Augmented data frame\nUse the augment() function from the broom package (part of tidymodels) to add columns for predicted values, residuals, and other observation-level model statistics\n\n\nduke_forest_aug &lt;- augment(duke_forest_fit)\nduke_forest_aug\n\n# A tibble: 98 × 8\n     price  area  .fitted  .resid   .hat  .sigma   .cooksd .std.resid\n     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 1520000  6040 1079931. 440069. 0.133  162605. 0.604         2.80  \n 2 1030000  4475  830340. 199660. 0.0435 168386. 0.0333        1.21  \n 3  420000  1745  394951.  25049. 0.0226 169664. 0.000260      0.150 \n 4  680000  2091  450132. 229868. 0.0157 168011. 0.0150        1.37  \n 5  428500  1772  399257.  29243. 0.0220 169657. 0.000345      0.175 \n 6  456000  1950  427645.  28355. 0.0182 169659. 0.000266      0.170 \n 7 1270000  3909  740072. 529928. 0.0250 160502. 0.130         3.18  \n 8  557450  2841  569744. -12294. 0.0102 169679. 0.0000277    -0.0732\n 9  697500  3924  742465. -44965. 0.0254 169620. 0.000948     -0.270 \n10  650000  2173  463209. 186791. 0.0145 168582. 0.00912       1.11  \n# ℹ 88 more rows"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#finding-rmse-in-r",
    "href": "slides/04-slr-model-assessment-contd.html#finding-rmse-in-r",
    "title": "SLR: Model assessment cont’d",
    "section": "Finding RMSE in R",
    "text": "Finding RMSE in R\nUse the rmse() function from the yardstick package (part of tidymodels)\n\nrmse(duke_forest_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     167067.\n\n\n\n\nWhat does this value mean?"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#finding-r2-in-r",
    "href": "slides/04-slr-model-assessment-contd.html#finding-r2-in-r",
    "title": "SLR: Model assessment cont’d",
    "section": "Finding \\(R^2\\) in R",
    "text": "Finding \\(R^2\\) in R\nUse the rsq() function from the yardstick package (part of tidymodels)\n\nrsq(duke_forest_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.445\n\n\n\n\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\)\n\nglance(duke_forest_fit)$r.squared\n\n[1] 0.4451945"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#recap",
    "href": "slides/04-slr-model-assessment-contd.html#recap",
    "title": "SLR: Model assessment cont’d",
    "section": "Recap",
    "text": "Recap\n\nEvaluated models using RMSE and \\(R^2\\)\nUsed analysis of variance to partition variability in the response variable\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/lab-00.html#meet-your-tas",
    "href": "slides/lab-00.html#meet-your-tas",
    "title": "Welcome to STA 221 labs!",
    "section": "Meet your TAs!",
    "text": "Meet your TAs!"
  },
  {
    "objectID": "slides/lab-00.html#meet-each-other",
    "href": "slides/lab-00.html#meet-each-other",
    "title": "Welcome to STA 221 labs!",
    "section": "Meet each other!",
    "text": "Meet each other!\n\n\nGet into groups of 2 or 3\nIntroduce yourself: Name, year, major (or academic interest), a highlight from the summer\nIntroduce your partner to the class\n\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/lab-00.html#what-to-expect-in-lab",
    "href": "slides/lab-00.html#what-to-expect-in-lab",
    "title": "Welcome to STA 221 labs!",
    "section": "What to expect in lab",
    "text": "What to expect in lab\n\nIntroduction to the lab assignment (~ 5 - 10 minutes)\nReview lecture content, as needed (~ 10 minutes)\nWork on the lab assignment (individual for Lab 01 and in teams for the remainder of the semester)\nStarting with Lab 01, you will find the starter materials for lab in your repo in the course GitHub organization."
  },
  {
    "objectID": "slides/lab-00.html#todays-lab",
    "href": "slides/lab-00.html#todays-lab",
    "title": "Welcome to STA 221 labs!",
    "section": "Today’s lab",
    "text": "Today’s lab\nThe rest of the today’s lab is focused on setting up the computing for the course and completing the class survey. Click the link below for the Lab 00 instructions. The instructions are available on the course website.\n\n🔗 sta221-fa24.netlify.app/labs/lab-00.html\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-tackett",
    "href": "slides/01-welcome.html#meet-prof.-tackett",
    "title": "Welcome to BIOSTAT 725",
    "section": "Meet Prof. Tackett!",
    "text": "Meet Prof. Tackett!\n\n\nEducation and career journey\n\nBS in Math and MS in Statistics from University of Tennessee\nStatistician at Capital One\nPhD in Statistics from University of Virginia\nAssistant Professor of the Practice, Department of Statistical Science at Duke\n\nWork focuses on statistics education and sense of belonging in introductory math and statistics classes\nCo-leader of the Bass Connections team Mental Health and the Justice System in Durham County\nMom of 19-month-old twins 🙂"
  },
  {
    "objectID": "slides/01-welcome.html#teaching-assistants-tas",
    "href": "slides/01-welcome.html#teaching-assistants-tas",
    "title": "Welcome to BIOSTAT 725!",
    "section": "",
    "text": "Dr. Youngsoo Baek (PhD)\n\nPhd in Statistical Science from Duke University\nPostdoc in Biostatistics & Bioinformatics\n\nBraden Scherting\n\nPhd candidate in Statistical Science"
  },
  {
    "objectID": "slides/01-welcome.html#check-in-on-ed-discussion",
    "href": "slides/01-welcome.html#check-in-on-ed-discussion",
    "title": "Welcome to BIOSTAT 725!",
    "section": "",
    "text": "Click on the link or scan the QR code to answer the Ed Discussion poll\nhttps://edstem.org/us/courses/68995/discussion/5942168"
  },
  {
    "objectID": "slides/01-welcome.html#topics",
    "href": "slides/01-welcome.html#topics",
    "title": "Welcome to BIOSTAT 725!",
    "section": "",
    "text": "Introduction to the course\nSyllabus activity\nReview of probability"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-regression-analysis",
    "href": "slides/01-welcome.html#what-is-regression-analysis",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\n\nRegression analysis is a statistical method used to examine the relationship between a response variable and one or more predictor variables. It is used for predicting future values, understanding relationships between variables, and identifying key predictors. It also helps in modeling trends, assessing the impact of changes, and detecting outliers in data.\n\nSource: ChatGPT (with modification)"
  },
  {
    "objectID": "slides/01-welcome.html#example-rent-vs.-commute-time",
    "href": "slides/01-welcome.html#example-rent-vs.-commute-time",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Example: Rent vs. commute time",
    "text": "Example: Rent vs. commute time\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\n\n\\[\n\\text{rent} = \\beta_0 + \\beta_1 ~ \\text{commute_time} + \\epsilon\n\\]\n\n\n\n\\[\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\end{bmatrix} +  \\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-sta-221",
    "href": "slides/01-welcome.html#what-is-sta-221",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is STA 221?",
    "text": "What is STA 221?\n\n\n\n\n\n STA 210 \n\nApplication\n\n\n\n\n+\n\n\n\n\n\nSTA 211\n\nTheory\n\n\n\n\nPrerequisites: Introductory statistics or probability course and linear algebra\nRecommended corequisite: Probability course at Duke"
  },
  {
    "objectID": "slides/01-welcome.html#course-learning-objectives",
    "href": "slides/01-welcome.html#course-learning-objectives",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to…\n\nunderstand fundamental concepts of Bayesian statistics, including prior and posterior, and predictive distributions,\nimplement the Bayesian workflow, including model building, checking, and refinement,\nuse probabilistic programming software for Bayesian analysis (e.g., Stan),\napply Bayesian techniques to real-world health data,\ncommunicate Bayesian analysis results effectively to both technical and non-technical audiences, and\nidentify opportunities for using Bayesian statistics in your research and/or job."
  },
  {
    "objectID": "slides/01-welcome.html#course-topics",
    "href": "slides/01-welcome.html#course-topics",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course topics",
    "text": "Course topics\n\n\n\n\nLinear regression\n\nMethods for inference\nPrior elicitation\nPosterior estimation\nUncertainty quantification\nModel assessment\nBayesian workflow\nPrediction\n\n\n\n\n\nHealth Datasets\n\n\n\n\n\nExtensions\n\nRobust regression\nRegularization\nClassification\nMissing data\n\n\n\n\n\nHierarchical Model\n\nGaussian processes\nLongitudinal data\nSpatial data"
  },
  {
    "objectID": "slides/01-welcome.html#course-toolkit",
    "href": "slides/01-welcome.html#course-toolkit",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nWebsite: https://biostat725-sp25.netlify.app/\n\nCentral hub for the course!\nTour of the website\n\nCanvas: https://canvas.duke.edu/courses/53305\n\nGradebook\nAnnouncements\nGradescope\nEd Discussion\n\nGitHub: github.com/biostat725-sp25\n\nDistribute assignments\nPlatform for version control and collaboration"
  },
  {
    "objectID": "slides/01-welcome.html#computing-toolkit",
    "href": "slides/01-welcome.html#computing-toolkit",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nInference using Stan, a probabilistic programming language (rstan)\nWrite reproducible reports in Quarto\nAccess RStudio through STA725 Docker Containers\n\n\n\n\n\n\n\n\n\n\n\n\nAccess assignments\nFacilitates version control and collaboration\nAll work in BIOSTAT 725 course organization"
  },
  {
    "objectID": "slides/01-welcome.html#classroom-community",
    "href": "slides/01-welcome.html#classroom-community",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Classroom community",
    "text": "Classroom community\n\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know.\nPlease let me know your preferred pronouns, if you are comfortable sharing.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said or done in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/01-welcome.html#accessibility",
    "href": "slides/01-welcome.html#accessibility",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nIf you have documented accommodations from SDAO, please send the documentation as soon as possible.\nI am committed to making all course activities and materials accessible. If any course component is not accessible to you in any way, please don’t hesitate to let me know."
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity",
    "href": "slides/01-welcome.html#syllabus-activity",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity",
    "text": "Syllabus activity\n\n\nIntroduce yourself to your group members.\nChoose a reporter. This person will share the group’s summary with the class.\nRead the portion of the syllabus assigned to your group.\nDiscuss the key points and questions you my have.\nThe reporter will share a summary with the class."
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity-assignments",
    "href": "slides/01-welcome.html#syllabus-activity-assignments",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity assignments",
    "text": "Syllabus activity assignments\n\nGroup 1: What to expect in the course\nGroup 2: Homework\nGroup 3: Exams\nGroup 4: Live Coding\nGroup 5: Application Exercises\nGroup 6: Academic honesty (except AI policy)\nGroup 7: Artificial intelligence policy\nGroup 8: Late work policy and waiver for extenuating circumstances\nGroup 9: Regrade requests and attendance policy\nGroup 10: Getting help in the course"
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity-report-out",
    "href": "slides/01-welcome.html#syllabus-activity-report-out",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity report out",
    "text": "Syllabus activity report out\n\n\nGroup 1: What to expect in the course\nGroup 2: Homework\nGroup 3: Exams\nGroup 4: Live Coding\nGroup 5: Application Exercises\nGroup 6: Academic honesty (except AI policy)\nGroup 7: Artificial intelligence policy\nGroup 8: Late work policy and waiver for extenuating circumstances\nGroup 9: Regrade requests and attendance policy\nGroup 10: Getting help in the course"
  },
  {
    "objectID": "slides/01-welcome.html#grading",
    "href": "slides/01-welcome.html#grading",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n40%\n\n\nExam 01\n20%\n\n\nExam 02\n20%\n\n\nLive Coding\n10%\n\n\nApplication Exercises\n10%\n\n\nTotal\n100%"
  },
  {
    "objectID": "slides/01-welcome.html#five-tips-for-success-in-sta-221",
    "href": "slides/01-welcome.html#five-tips-for-success-in-sta-221",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Five tips for success in STA 221",
    "text": "Five tips for success in STA 221\n\nComplete all the preparation work before class.\nAsk questions in class, office hours, and on Ed Discussion.\nDo the homework and labs; get started on homework early when possible.\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStay up-to-date on announcements on Ed Discussion and sent via email."
  },
  {
    "objectID": "slides/01-welcome.html#reproducibility-checklist",
    "href": "slides/01-welcome.html#reproducibility-checklist",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n✔️ Can the tables and figures be exactly reproduced from the code and data?\n✔️ Does the code actually do what you think it does?\n✔️ In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n✔️ Can the code be used for other data?\n✔️ Can you extend the code to do other things?"
  },
  {
    "objectID": "slides/01-welcome.html#why-is-reproducibility-important",
    "href": "slides/01-welcome.html#why-is-reproducibility-important",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)"
  },
  {
    "objectID": "slides/01-welcome.html#toolkit",
    "href": "slides/01-welcome.html#toolkit",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub"
  },
  {
    "objectID": "slides/01-welcome.html#r-and-rstudio",
    "href": "slides/01-welcome.html#r-and-rstudio",
    "title": "Welcome to BIOSTAT 725!",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\nSource: Statistical Inference via Data Science"
  },
  {
    "objectID": "slides/01-welcome.html#rstudio-ide",
    "href": "slides/01-welcome.html#rstudio-ide",
    "title": "Welcome to BIOSTAT 725!",
    "section": "RStudio IDE",
    "text": "RStudio IDE"
  },
  {
    "objectID": "slides/01-welcome.html#quarto",
    "href": "slides/01-welcome.html#quarto",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports – the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)"
  },
  {
    "objectID": "slides/01-welcome.html#quarto-1",
    "href": "slides/01-welcome.html#quarto-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "slides/01-welcome.html#how-will-we-use-quarto",
    "href": "slides/01-welcome.html#how-will-we-use-quarto",
    "title": "Welcome to BIOSTAT 725!",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou’ll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-versioning",
    "href": "slides/01-welcome.html#what-is-versioning",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is versioning?",
    "text": "What is versioning?"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-versioning-1",
    "href": "slides/01-welcome.html#what-is-versioning-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is versioning?",
    "text": "What is versioning?\nwith human readable messages"
  },
  {
    "objectID": "slides/01-welcome.html#why-do-we-need-version-control",
    "href": "slides/01-welcome.html#why-do-we-need-version-control",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\n\n\n\n\n\n\n\nProvides a clear record of how the analysis methods evolved. This makes analysis auditable and thus more trustworthy and reliable. (Ostblom and Timbers 2022)"
  },
  {
    "objectID": "slides/01-welcome.html#git-and-github",
    "href": "slides/01-welcome.html#git-and-github",
    "title": "Welcome to BIOSTAT 725!",
    "section": "git and GitHub",
    "text": "git and GitHub\n\n\ngit is a version control system – like “Track Changes” features from Microsoft Word.\nGitHub is the home for your git-based projects on the internet (like DropBox but much better).\nThere are a lot of git commands and very few people know them all. 99% of the time you will use git to add, commit, push, and pull."
  },
  {
    "objectID": "slides/01-welcome.html#this-week",
    "href": "slides/01-welcome.html#this-week",
    "title": "Welcome to BIOSTAT 725!",
    "section": "This week",
    "text": "This week\n\nComplete Lab 00 tasks\nReview syllabus\nComplete reading to prepare for Thursday’s lecture\nThursday’s lecture: Simple linear regression"
  },
  {
    "objectID": "slides/01-welcome.html#references",
    "href": "slides/01-welcome.html#references",
    "title": "Welcome to BIOSTAT 725!",
    "section": "References",
    "text": "References\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/09-inference-pt2.html#announcements",
    "href": "slides/09-inference-pt2.html#announcements",
    "title": "Inference for regression",
    "section": "Announcements",
    "text": "Announcements\n\nProject\n\nResearch questions due Thursday at 11:59pm\nProposal due Thursday, October 3 at 11:59pm\n\nLab 03 due Thursday, October 3 at 11:59pm\nStatistics experience due Tue, Nov 26 at 11:59pm"
  },
  {
    "objectID": "slides/09-inference-pt2.html#topics",
    "href": "slides/09-inference-pt2.html#topics",
    "title": "Inference for regression",
    "section": "Topics",
    "text": "Topics\n\nUnderstand statistical inference in the context of regression\nDescribe the assumptions for regression\nUnderstand connection between distribution of residuals and inferential procedures\nConduct inference on a single coefficient\nConduct inference on the overall regression model"
  },
  {
    "objectID": "slides/09-inference-pt2.html#computing-setup",
    "href": "slides/09-inference-pt2.html#computing-setup",
    "title": "Inference for regression",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(kableExtra)  \nlibrary(patchwork)   \n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "slides/09-inference-pt2.html#data-ncaa-football-expenditures",
    "href": "slides/09-inference-pt2.html#data-ncaa-football-expenditures",
    "title": "Inference for regression",
    "section": "Data: NCAA Football expenditures",
    "text": "Data: NCAA Football expenditures\nToday’s data come from Equity in Athletics Data Analysis and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a March 2022 Tidy Tuesday.\nWe will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :\n\ntotal_exp_m: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\nenrollment_th: Total student enrollment in the 2019 - 2020 academic year (in thousands)\ntype: institution type (Public or Private)\n\n\nfootball &lt;- read_csv(\"data/ncaa-football-exp.csv\")"
  },
  {
    "objectID": "slides/09-inference-pt2.html#univariate-eda",
    "href": "slides/09-inference-pt2.html#univariate-eda",
    "title": "Inference for regression",
    "section": "Univariate EDA",
    "text": "Univariate EDA"
  },
  {
    "objectID": "slides/09-inference-pt2.html#bivariate-eda",
    "href": "slides/09-inference-pt2.html#bivariate-eda",
    "title": "Inference for regression",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA"
  },
  {
    "objectID": "slides/09-inference-pt2.html#regression-model",
    "href": "slides/09-inference-pt2.html#regression-model",
    "title": "Inference for regression",
    "section": "Regression model",
    "text": "Regression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type, data = football)\ntidy(exp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n\n\n\n\n\n\nFor every additional 1,000 students, we expect the institution’s total expenditures on football to increase by $780,000, on average, holding institution type constant."
  },
  {
    "objectID": "slides/09-inference-pt2.html#statistical-inference",
    "href": "slides/09-inference-pt2.html#statistical-inference",
    "title": "Inference for regression",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\n\nStatistical inference provides methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be representative (ideally random) of the population we’re interested in\n\n\n\n\n\nImage source: Eugene Morgan © Penn State"
  },
  {
    "objectID": "slides/09-inference-pt2.html#inference-for-linear-regression",
    "href": "slides/09-inference-pt2.html#inference-for-linear-regression",
    "title": "Inference for regression",
    "section": "Inference for linear regression",
    "text": "Inference for linear regression\n\nInference based on ANOVA\n\nHypothesis test for the statistical significance of the overall regression model\nHypothesis test for a subset of coefficients\n\nInference for a single coefficient \\(\\beta_j\\)\n\nHypothesis test for a coefficient \\(\\beta_j\\)\nConfidence interval for a coefficient \\(\\beta_j\\)"
  },
  {
    "objectID": "slides/09-inference-pt2.html#linear-regression-model",
    "href": "slides/09-inference-pt2.html#linear-regression-model",
    "title": "Inference for regression",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\n\\begin{aligned}\n\\mathbf{y} &= Model + Error \\\\[5pt]\n&= f(\\mathbf{X}) + \\boldsymbol{\\epsilon} \\\\[5pt]\n&= E(\\mathbf{y}|\\mathbf{X}) + \\mathbf{\\epsilon} \\\\[5pt]\n&= \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\n\\end{aligned}\n\\]\n\n\n\nWe have discussed multiple ways to find the least squares estimates of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\\\beta_1\\end{bmatrix}\\)\n\nNone of these approaches depend on the distribution of \\(\\boldsymbol{\\epsilon}\\)\n\nNow we will use statistical inference to draw conclusions about \\(\\boldsymbol{\\beta}\\) that depend on particular assumptions about the distribution of \\(\\boldsymbol{\\epsilon}\\)"
  },
  {
    "objectID": "slides/09-inference-pt2.html#linear-regression-model-1",
    "href": "slides/09-inference-pt2.html#linear-regression-model-1",
    "title": "Inference for regression",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/09-inference-pt2.html#expected-value-of-mathbfy",
    "href": "slides/09-inference-pt2.html#expected-value-of-mathbfy",
    "title": "Inference for regression",
    "section": "Expected value of \\(\\mathbf{y}\\)",
    "text": "Expected value of \\(\\mathbf{y}\\)\nLet \\(\\mathbf{b} = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\b_p\\end{bmatrix}\\) be a \\(p \\times 1\\) vector of random variables.\n\n\nThen \\(E(\\mathbf{b}) = E\\begin{bmatrix}b_1 \\\\ \\vdots \\\\ b_p\\end{bmatrix} = \\begin{bmatrix}E(b_1) \\\\ \\vdots \\\\ E(b_p)\\end{bmatrix}\\)\n\n\n\n\nUse this to find \\(E(\\mathbf{y}|\\mathbf{X})\\)."
  },
  {
    "objectID": "slides/09-inference-pt2.html#variance",
    "href": "slides/09-inference-pt2.html#variance",
    "title": "Inference for regression",
    "section": "Variance",
    "text": "Variance\nLet \\(\\mathbf{b} = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\b_p\\end{bmatrix}\\) be a \\(p \\times 1\\) vector of independent random variables.\n\n\nThen \\(Var(\\mathbf{b}) = \\begin{bmatrix}Var(b_1) & 0 & \\dots & 0 \\\\ 0 & Var(b_2) & \\dots & 0 \\\\ \\vdots & \\vdots & \\dots & \\cdot \\\\ 0 & 0 & \\dots & Var(b_p)\\end{bmatrix}\\)\n\n\n\n\nUse this to find \\(Var(\\mathbf{y}|\\mathbf{X})\\)."
  },
  {
    "objectID": "slides/09-inference-pt2.html#assumptions-of-regression",
    "href": "slides/09-inference-pt2.html#assumptions-of-regression",
    "title": "Inference for regression",
    "section": "Assumptions of regression",
    "text": "Assumptions of regression\n\n\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n\\]\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\n\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from one another."
  },
  {
    "objectID": "slides/09-inference-pt2.html#estimating-sigma2_epsilon",
    "href": "slides/09-inference-pt2.html#estimating-sigma2_epsilon",
    "title": "Inference for regression",
    "section": "Estimating \\(\\sigma^2_{\\epsilon}\\)",
    "text": "Estimating \\(\\sigma^2_{\\epsilon}\\)\n\nOnce we fit the model, we can use the residuals to estimate \\(\\sigma_{\\epsilon}^2\\)\n\\(\\hat{\\sigma}^2_{\\epsilon}\\) is needed for hypothesis testing and constructing confidence intervals for regression\n\n\\[\n\\hat{\\sigma}^2_\\epsilon = \\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-p-1} = \\frac{\\sum_\\limits{i=1}^ne_i^2}{n - p - 1} = \\frac{SSR}{n - p - 1}\n\\]\n\nThe regression standard error \\(\\hat{\\sigma}_{\\epsilon}\\) is a measure of the average distance between the observations and regression line\n\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{SSR}{n - p - 1}}\n\\]"
  },
  {
    "objectID": "slides/09-inference-pt2.html#inference-for-beta_j",
    "href": "slides/09-inference-pt2.html#inference-for-beta_j",
    "title": "Inference for regression",
    "section": "Inference for \\(\\beta_j\\)",
    "text": "Inference for \\(\\beta_j\\)\nWe often want to conduct inference on individual model coefficients\n\nHypothesis test: Is there a linear relationship between the response and \\(x_j\\)?\nConfidence interval: What is a plausible range of values \\(\\beta_j\\) can take?\n\nBut first we need to understand the distribution of \\(\\hat{\\beta}_j\\)"
  },
  {
    "objectID": "slides/09-inference-pt2.html#sampling-distribution-of-hatbeta_j",
    "href": "slides/09-inference-pt2.html#sampling-distribution-of-hatbeta_j",
    "title": "Inference for regression",
    "section": "Sampling distribution of \\(\\hat{\\beta}_j\\)",
    "text": "Sampling distribution of \\(\\hat{\\beta}_j\\)\n\\[\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon(\\mathbf{X}^T\\mathbf{X})^{-1})\n\\]\nLet \\(\\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\). Then, for each coefficient \\(\\hat{\\beta}_j\\),\n\n\n\\(E(\\hat{\\beta}_j) = \\boldsymbol{\\beta}_j\\), the \\(j^{th}\\) element of \\(\\boldsymbol{\\beta}\\)\n\\(Var(\\hat{\\beta}_j) = \\sigma^2_{\\epsilon}C_{jj}\\)\n\\(Cov(\\hat{\\beta}_i, \\hat{\\beta}_j) = \\sigma^2_{\\epsilon}C_{ij}\\)"
  },
  {
    "objectID": "slides/09-inference-pt2.html#steps-for-a-hypothesis-test",
    "href": "slides/09-inference-pt2.html#steps-for-a-hypothesis-test",
    "title": "Inference for regression",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState the null and alternative hypotheses.\nCalculate a test statistic.\nCalculate the p-value.\nState the conclusion."
  },
  {
    "objectID": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-hypotheses",
    "href": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-hypotheses",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Hypotheses",
    "text": "Hypothesis test for \\(\\beta_j\\): Hypotheses\nWe will generally test the hypotheses:\n\nNull Hypothesis: \\(H_0: \\beta_j = 0\\)\n\nThere is no linear relationship between \\(\\beta_j\\) and \\(y\\) after accounting for the other variables in the model\n\nAlternative hypothesis: \\(H_a: \\beta_j \\neq 0\\)\n\nThere is a linear relationship between \\(\\beta_j\\) and \\(y\\) after accounting for the other variables in the model"
  },
  {
    "objectID": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-test-statistic",
    "href": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-test-statistic",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Test statistic",
    "text": "Hypothesis test for \\(\\beta_j\\): Test statistic\nTest statistic: Number of standard errors the estimate is away from the null hypothesized value\n\\[\n\\text{Test Statstic} = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n\\]\n\n\n\\[T = \\frac{\\hat{\\beta}_j - 0}{SE(\\hat{\\beta}_j)} ~ = ~\\frac{\\hat{\\beta}_j - 0}{\\sqrt{\\hat{\\sigma}^2_\\epsilon C_{jj}}} ~\\sim ~ t_{n-p-1}\n\\]"
  },
  {
    "objectID": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-p-value",
    "href": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-p-value",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): P-value",
    "text": "Hypothesis test for \\(\\beta_j\\): P-value\nThe p-value is the probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\np-value = P(|t| &gt; |\\text{test statistic}|),\n\\]\ncalculated from a \\(t\\) distribution with \\(n- p - 1\\) degrees of freedom"
  },
  {
    "objectID": "slides/09-inference-pt2.html#understanding-the-p-value",
    "href": "slides/09-inference-pt2.html#understanding-the-p-value",
    "title": "Inference for regression",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-conclusion",
    "href": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-conclusion",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Conclusion",
    "text": "Hypothesis test for \\(\\beta_j\\): Conclusion\nThere are two parts to the conclusion\n\nMake a conclusion by comparing the p-value to a predetermined decision-making threshold called the significance level ( \\(\\alpha\\) level)\n\nIf \\(\\text{p-value} &lt; \\alpha\\): Reject \\(H_0\\)\nIf \\(\\text{p-value} \\geq \\alpha\\): Fail to reject \\(H_0\\)\n\nState the conclusion in the context of the data"
  },
  {
    "objectID": "slides/09-inference-pt2.html#confidence-interval-for-beta_j-1",
    "href": "slides/09-inference-pt2.html#confidence-interval-for-beta_j-1",
    "title": "Inference for regression",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\n\nA plausible range of values for a population parameter is called a confidence interval\nUsing only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nSimilarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter"
  },
  {
    "objectID": "slides/09-inference-pt2.html#what-confidence-means",
    "href": "slides/09-inference-pt2.html#what-confidence-means",
    "title": "Inference for regression",
    "section": "What “confidence” means",
    "text": "What “confidence” means\n\n\nWe will construct \\(C\\%\\) confidence intervals.\n\nThe confidence level impacts the width of the interval\n\n\n\n\n“Confident” means if we were to take repeated samples of the same size as our data, fit regression lines using the same predictors, and calculate \\(C\\%\\) CIs for the coefficient of \\(x_j\\), then \\(C\\%\\) of those intervals will contain the true value of the coefficient \\(\\beta_j\\)\n\n\n\nBalance precision and accuracy when selecting a confidence level"
  },
  {
    "objectID": "slides/09-inference-pt2.html#confidence-interval-for-beta_j-2",
    "href": "slides/09-inference-pt2.html#confidence-interval-for-beta_j-2",
    "title": "Inference for regression",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE({\\hat{\\beta}_j})\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-p-1\\) degrees of freedom"
  },
  {
    "objectID": "slides/09-inference-pt2.html#confidence-interval-critical-value",
    "href": "slides/09-inference-pt2.html#confidence-interval-critical-value",
    "title": "Inference for regression",
    "section": "Confidence interval: Critical value",
    "text": "Confidence interval: Critical value\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(football) - 2 - 1)\n\n[1] 1.97928\n\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(football) - 2 - 1)\n\n[1] 1.657235\n\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(football) - 2 - 1)\n\n[1] 2.61606"
  },
  {
    "objectID": "slides/09-inference-pt2.html#ci-for-beta_j-calculation",
    "href": "slides/09-inference-pt2.html#ci-for-beta_j-calculation",
    "title": "Inference for regression",
    "section": "95% CI for \\(\\beta_j\\): Calculation",
    "text": "95% CI for \\(\\beta_j\\): Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0"
  },
  {
    "objectID": "slides/09-inference-pt2.html#ci-for-beta_j-in-r",
    "href": "slides/09-inference-pt2.html#ci-for-beta_j-in-r",
    "title": "Inference for regression",
    "section": "95% CI for \\(\\beta_j\\) in R",
    "text": "95% CI for \\(\\beta_j\\) in R\n\ntidy(exp_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n13.426\n25.239\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n0.562\n0.999\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n-19.466\n-6.986\n\n\n\n\n\n\nInterpretation: We are 95% confident that for each additional 1,000 students enrolled, the institution’s expenditures on football will be greater by $562,000 to $999,000, on average, holding institution type constant."
  },
  {
    "objectID": "slides/09-inference-pt2.html#test-for-overall-significance-hypotheses",
    "href": "slides/09-inference-pt2.html#test-for-overall-significance-hypotheses",
    "title": "Inference for regression",
    "section": "Test for overall significance: Hypotheses",
    "text": "Test for overall significance: Hypotheses\nWe can conduct a hypothesis test using the ANOVA table to determine if there is at least one non-zero coefficient in the model\n\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\dots = \\beta_p = 0\\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one }j\n\\end{aligned}\n\\]\n\nFor the football data\n\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\beta_2 = 0\\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one }j\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-inference-pt2.html#test-for-overall-significance-test-statistic",
    "href": "slides/09-inference-pt2.html#test-for-overall-significance-test-statistic",
    "title": "Inference for regression",
    "section": "Test for overall significance: Test statistic",
    "text": "Test for overall significance: Test statistic\n\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF Stat\nPr(&gt; F)\n\n\n\n\nModel\n2\n7138.591\n3569.296\n26.628\n0\n\n\nResiduals\n124\n16621.344\n134.043\n\n\n\n\nTotal\n126\n23759.935\n\n\n\n\n\n\n\n\n\nTest statistic: Ratio of explained to unexplained variability\n\\[\nF = \\frac{\\text{Mean Square Model}}{\\text{Mean Square Residuals}}\n\\]\nThe test statistic follows an \\(F\\) distribution with \\(p\\) and \\(n -  p - 1\\) degrees of freedom"
  },
  {
    "objectID": "slides/09-inference-pt2.html#test-for-overall-significance-p-value",
    "href": "slides/09-inference-pt2.html#test-for-overall-significance-p-value",
    "title": "Inference for regression",
    "section": "Test for overall significance: P-value",
    "text": "Test for overall significance: P-value\n\n\\[\n\\text{P-value} = \\text{Pr}(F &gt; \\text{F Stat})\n\\]"
  },
  {
    "objectID": "slides/09-inference-pt2.html#test-for-overall-significance-conclusion",
    "href": "slides/09-inference-pt2.html#test-for-overall-significance-conclusion",
    "title": "Inference for regression",
    "section": "Test for overall significance: Conclusion",
    "text": "Test for overall significance: Conclusion\n\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\beta_2 = 0\\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one }j\n\\end{aligned}\n\\]\n\nfootball_anova |&gt;\n  kable(digits = 3)\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF Stat\nPr(&gt; F)\n\n\n\n\nModel\n2\n7138.591\n3569.296\n26.628\n0\n\n\nResiduals\n124\n16621.344\n134.043\n\n\n\n\nTotal\n126\n23759.935\n\n\n\n\n\n\n\n\n\nWhat is the conclusion from this hypothesis test?"
  },
  {
    "objectID": "slides/09-inference-pt2.html#recap",
    "href": "slides/09-inference-pt2.html#recap",
    "title": "Inference for regression",
    "section": "Recap",
    "text": "Recap\n\nIntroduced statistical inference in the context of regression\nDescribed the assumptions for regression\nConnected the distribution of residuals and inferential procedures\nConducted inference on a single coefficient\nConducted inference on the overall regression model\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/04-slr-matrix.html#topics",
    "href": "slides/04-slr-matrix.html#topics",
    "title": "SLR: Matrix representation",
    "section": "Topics",
    "text": "Topics\n\nMatrix representation for simple linear regression\n\nModel form\nLeast square estimate\nPredicted (fitted) values\nResiduals\n\nMatrix representation in R"
  },
  {
    "objectID": "slides/04-slr-matrix.html#slr-statistical-model-population",
    "href": "slides/04-slr-matrix.html#slr-statistical-model-population",
    "title": "SLR: Matrix representation",
    "section": "SLR: Statistical model (population)",
    "text": "SLR: Statistical model (population)\nWhen we have a quantitative response, \\(Y\\), and a single quantitative predictor, \\(X\\), we can use a simple linear regression model to describe the relationship between \\(Y\\) and \\(X\\). \\[\\large{Y = \\mathbf{\\beta_0 + \\beta_1 X} + \\epsilon}, \\hspace{8mm} \\epsilon \\sim N(0, \\sigma_{\\epsilon}^2)\\]\n\n\n\\(\\beta_1\\): Population (true) slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): Population (true) intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "slides/04-slr-matrix.html#slr-in-matrix-form",
    "href": "slides/04-slr-matrix.html#slr-in-matrix-form",
    "title": "SLR: Matrix representation",
    "section": "SLR in matrix form",
    "text": "SLR in matrix form\nSuppose we have \\(n\\) observations.\n\\[\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n=\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_1 \\\\\n\\vdots &  \\vdots \\\\\n1 &  x_n\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n\\]\n\n\nWhat are the dimensions of \\(\\mathbf{y}\\), \\(\\mathbf{X}\\), \\(\\boldsymbol{\\beta}\\), and \\(\\boldsymbol{\\epsilon}\\)?"
  },
  {
    "objectID": "slides/04-slr-matrix.html#sum-of-squared-residuals",
    "href": "slides/04-slr-matrix.html#sum-of-squared-residuals",
    "title": "SLR: Matrix representation",
    "section": "Sum of squared residuals",
    "text": "Sum of squared residuals\nWe use the sum of squared residuals (also called “sum of squared error”) to find the least squares line:\n\\[\nSSR = \\sum_{i=1}^ne_i^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - \\hat{\\mathbf{y}})^T(\\mathbf{y} - \\hat{\\mathbf{y}})\n\\]\n\n\n\nWhat is the dimension of SSR?\nWhat is \\(\\hat{\\mathbf{y}}\\) in terms of \\(\\mathbf{y}\\), \\(\\mathbf{X}\\), and/or \\(\\boldsymbol{\\beta}\\) ?"
  },
  {
    "objectID": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals",
    "href": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nWe want to find values of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimize the sum of squared residuals \\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-1",
    "href": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-1",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nWe want to find values of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimize the sum of squared residuals \\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-2",
    "href": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-2",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nWe want to find values of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimize the sum of squared residuals \\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-3",
    "href": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-3",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nWe want to find values of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimize the sum of squared residuals \\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#least-squares-estimators",
    "href": "slides/04-slr-matrix.html#least-squares-estimators",
    "title": "SLR: Matrix representation",
    "section": "Least squares estimators",
    "text": "Least squares estimators\n\\[\nSSR = \\mathbf{e}^T\\mathbf{e} =\\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\\]\n\n\nThe least squares estimators must satisfy\n\\[\n\\nabla_{\\boldsymbol{\\beta}} SSR = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = 0\n\\]\n\n\n\n\\[\n\\color{#993399}{\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#did-we-find-a-minimum",
    "href": "slides/04-slr-matrix.html#did-we-find-a-minimum",
    "title": "SLR: Matrix representation",
    "section": "Did we find a minimum?",
    "text": "Did we find a minimum?\n\\[\n\\nabla^2_{\\beta} SSR \\propto  2\\mathbf{X}^T\\mathbf{X} = 0\n\\]\n\n\n\\(\\mathbf{X}\\) is full rank \\(\\Rightarrow\\) \\(\\mathbf{X}^T\\mathbf{X}\\) is positive definite\nTherefore we have found the minimizing point"
  },
  {
    "objectID": "slides/04-slr-matrix.html#obtain-mathbfy-vector",
    "href": "slides/04-slr-matrix.html#obtain-mathbfy-vector",
    "title": "SLR: Matrix representation",
    "section": "Obtain \\(\\mathbf{y}\\) vector",
    "text": "Obtain \\(\\mathbf{y}\\) vector\nLet’s go back to the Duke Forest data. We want to use the matrix representation to fit a model of the form:\n\\[\nprice = \\beta_0 + \\beta_1 ~ area + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2_\\epsilon)\n\\]\n\nGet \\(\\mathbf{y}\\), the vector of responses\n\ny &lt;- duke_forest$price\n\n\n\n\nLet’s look at the first 10 observations of \\(y\\)\n\ny[1:10]\n\n [1] 1520000 1030000  420000  680000  428500  456000 1270000  557450  697500\n[10]  650000"
  },
  {
    "objectID": "slides/04-slr-matrix.html#obtain-mathbfx-matrix",
    "href": "slides/04-slr-matrix.html#obtain-mathbfx-matrix",
    "title": "SLR: Matrix representation",
    "section": "Obtain \\(\\mathbf{X}\\) matrix",
    "text": "Obtain \\(\\mathbf{X}\\) matrix\nUse the model.matrix() function to get \\(\\mathbf{X}\\)\n\nX &lt;- model.matrix(price ~ area, data = duke_forest)\n\n\n\nLet’s look at the first 10 rows of \\(\\mathbf{X}\\)\n\nX[1:10,]\n\n   (Intercept) area\n1            1 6040\n2            1 4475\n3            1 1745\n4            1 2091\n5            1 1772\n6            1 1950\n7            1 3909\n8            1 2841\n9            1 3924\n10           1 2173"
  },
  {
    "objectID": "slides/04-slr-matrix.html#calculate-hatboldsymbolbeta",
    "href": "slides/04-slr-matrix.html#calculate-hatboldsymbolbeta",
    "title": "SLR: Matrix representation",
    "section": "Calculate \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Calculate \\(\\hat{\\boldsymbol{\\beta}}\\)\nMatrix functions in R. Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be matrices\n\nt(A): transpose \\(\\mathbf{A}\\)\nsolve(A): inverse of \\(\\mathbf{A}\\)\nA %*% B: multiply \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)\n\n\nNow let’s calculate \\(\\hat{\\boldsymbol{\\beta}}\\)\n\nbeta_hat &lt;- solve(t(X)%*%X)%*%t(X)%*%y\nbeta_hat\n\n                   [,1]\n(Intercept) 116652.3251\narea           159.4833"
  },
  {
    "objectID": "slides/04-slr-matrix.html#compare-to-result-from-lm",
    "href": "slides/04-slr-matrix.html#compare-to-result-from-lm",
    "title": "SLR: Matrix representation",
    "section": "Compare to result from lm",
    "text": "Compare to result from lm\n\nduke_forest_model &lt;- lm(price ~ area, data = duke_forest)\ntidy(duke_forest_model) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000\n\n\n\n\n\n\n\n\nbeta_hat \n\n                   [,1]\n(Intercept) 116652.3251\narea           159.4833"
  },
  {
    "objectID": "slides/04-slr-matrix.html#predicted-fitted-values",
    "href": "slides/04-slr-matrix.html#predicted-fitted-values",
    "title": "SLR: Matrix representation",
    "section": "Predicted (fitted) values",
    "text": "Predicted (fitted) values\nNow that we have \\(\\hat{\\boldsymbol{\\beta}}\\), let’s predict values of \\(\\mathbf{y}\\) using the model\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\underbrace{\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T}_{\\mathbf{H}}\\mathbf{y} = \\mathbf{H}\\mathbf{y}\n\\]\n\nHat matrix: \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\n\n\\(\\mathbf{H}\\) is an \\(n\\times n\\) matrix\nMaps vector of observed values \\(\\mathbf{y}\\) to a vector of fitted values \\(\\hat{\\mathbf{y}}\\)"
  },
  {
    "objectID": "slides/04-slr-matrix.html#residuals",
    "href": "slides/04-slr-matrix.html#residuals",
    "title": "SLR: Matrix representation",
    "section": "Residuals",
    "text": "Residuals\nRecall that the residuals are the difference between the observed and predicted values\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{y} - \\hat{\\mathbf{y}}\\\\[10pt]\n& = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n& = \\mathbf{y} - \\mathbf{H}\\mathbf{y} \\\\[10pt]\n& = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}\n\\end{aligned}\n\\]\n\n\\[\n\\color{#993399}{\\mathbf{e} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#recap",
    "href": "slides/04-slr-matrix.html#recap",
    "title": "SLR: Matrix representation",
    "section": "Recap",
    "text": "Recap\n\nIntroduced matrix representation for simple linear regression\n\nModel from\nLeast square estimate\nPredicted (fitted) values\nResiduals\n\nUsed R for matrix calculations"
  },
  {
    "objectID": "slides/04-slr-matrix.html#next-class",
    "href": "slides/04-slr-matrix.html#next-class",
    "title": "SLR: Matrix representation",
    "section": "Next class",
    "text": "Next class\n\nMultiple linear regression\nSee Sep 10 prepare\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/05-mlr.html#topics",
    "href": "slides/05-mlr.html#topics",
    "title": "Multiple linear regression (MLR)",
    "section": "Topics",
    "text": "Topics\n\nExploratory data analysis for multiple linear regression\nFitting the least squares line\nInterpreting coefficients for quantitative predictors\nPrediction"
  },
  {
    "objectID": "slides/05-mlr.html#computing-setup",
    "href": "slides/05-mlr.html#computing-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/05-mlr.html#data-peer-to-peer-lender",
    "href": "slides/05-mlr.html#data-peer-to-peer-lender",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 4\n   annual_income debt_to_income verified_income interest_rate\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/05-mlr.html#variables",
    "href": "slides/05-mlr.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nOutcome: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/05-mlr.html#outcome-interest_rate",
    "href": "slides/05-mlr.html#outcome-interest_rate",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\n\n\n\n\nMin\nMedian\nMax\nIQR\n\n\n\n\n5.31\n9.93\n26.3\n5.755"
  },
  {
    "objectID": "slides/05-mlr.html#predictors",
    "href": "slides/05-mlr.html#predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/05-mlr.html#data-manipulation-1-rescale-income",
    "href": "slides/05-mlr.html#data-manipulation-1-rescale-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_th = annual_income / 1000)\n\n\n\n\nWhy did we rescale income?"
  },
  {
    "objectID": "slides/05-mlr.html#outcome-vs.-predictors",
    "href": "slides/05-mlr.html#outcome-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors\n\n\nGoal: Use these predictors in a single model to understand variability in interest rate.\n\n\n\nWhy do we want to use a single model versus 3 separate simple linear regression models?"
  },
  {
    "objectID": "slides/05-mlr.html#multiple-linear-regression-mlr",
    "href": "slides/05-mlr.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\text{interest_rate} ~ =\n\\beta_0 & + \\beta_1 ~ \\text{debt_to_income} \\\\ & + \\beta_2 ~ \\text{verified_income} \\\\ &+ \\beta_3~ \\text{annual_income_th} \\\\\n& +\\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-mlr.html#multiple-linear-regression-1",
    "href": "slides/05-mlr.html#multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nRecall: The simple linear regression model\n\\[\nY = \\beta_0 + \\beta_1~ X + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\]\n\nThe form of the multiple linear regression model is\n\\[\nY = \\beta_0 + \\beta_1X_1 +  \\dots + \\beta_pX_p + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\]\n\n\n\nTherefore,\n\\[\nE(Y|X_1, \\ldots, X_p) = \\beta_0 + \\beta_1X_1 +  \\dots + \\beta_pX_p\n\\]"
  },
  {
    "objectID": "slides/05-mlr.html#fitting-the-least-squares-line",
    "href": "slides/05-mlr.html#fitting-the-least-squares-line",
    "title": "Multiple linear regression (MLR)",
    "section": "Fitting the least squares line",
    "text": "Fitting the least squares line\nSimilar to simple linear regression, we want to find estimates for \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) that minimize\n\\[\n\\sum_{i=1}^{n}e_i^2 = \\sum_{i=1}^n[y_i - \\hat{y}_i]^2 = \\sum_{i=1}^n[y_i - (\\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_px_{ip})]^2\n\\]\n\n\nThe calculations can be very tedious, especially if \\(p\\) is large"
  },
  {
    "objectID": "slides/05-mlr.html#matrix-form-of-multiple-linear-regression",
    "href": "slides/05-mlr.html#matrix-form-of-multiple-linear-regression",
    "title": "Multiple linear regression (MLR)",
    "section": "Matrix form of multiple linear regression",
    "text": "Matrix form of multiple linear regression\nSuppose we have \\(n\\) observations, a quantitative response variable, and \\(p\\) &gt; 1 predictors \\[\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n=\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_{11} & \\dots & x_{1p}\\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n1 &  x_{n1} & \\dots &x_{np}\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n\\]\n\nWhat are the dimensions of \\(\\mathbf{y}\\), \\(\\mathbf{X}\\), \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\epsilon}\\)?"
  },
  {
    "objectID": "slides/05-mlr.html#matrix-form-of-multiple-linear-regression-1",
    "href": "slides/05-mlr.html#matrix-form-of-multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Matrix form of multiple linear regression",
    "text": "Matrix form of multiple linear regression\nAs with simple linear regression, we have\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]\n\nGeneralizing the derivations from SLR to \\(p &gt; 2\\), we have\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\\]\nas before."
  },
  {
    "objectID": "slides/05-mlr.html#model-fit-in-r",
    "href": "slides/05-mlr.html#model-fit-in-r",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit in R",
    "text": "Model fit in R\n\nint_fit &lt;- lm(interest_rate ~ debt_to_income + verified_income  + annual_income_th,\n              data = loan50)\n\ntidy(int_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078"
  },
  {
    "objectID": "slides/05-mlr.html#model-equation",
    "href": "slides/05-mlr.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{interest_rate}} =  10.726 &+0.671 \\times \\text{debt_to_income}\\\\\n&+ 2.211 \\times \\text{source_verified}\\\\  \n&+ 6.880 \\times \\text{verified}\\\\\n& -0.021 \\times \\text{annual_income_th}\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\nWe will talk about why there are two terms in the model for verified_income soon!"
  },
  {
    "objectID": "slides/05-mlr.html#interpreting-hatbeta_j",
    "href": "slides/05-mlr.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(Y\\) when \\(X_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n\n\nExample: The estimated coefficient for debt_to_income is 0.671. This means for each point in an borrower’s debt to income ratio, the interest rate on the loan is expected to be greater by 0.671%, holding annual income and income verification constant."
  },
  {
    "objectID": "slides/05-mlr.html#interpreting-hatbeta_j-1",
    "href": "slides/05-mlr.html#interpreting-hatbeta_j-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient for annual_income_th is -0.021. Interpret this coefficient in the context of the data.\n\n\n\n\nWhy do we need to include a statement about holding all other predictors constant?"
  },
  {
    "objectID": "slides/05-mlr.html#interpreting-hatbeta_0",
    "href": "slides/05-mlr.html#interpreting-hatbeta_0",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_0\\)",
    "text": "Interpreting \\(\\hat{\\beta}_0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\nDescribe the subset of borrowers who are expected to get an interest rate of 10.726% based on our model. Is this interpretation meaningful? Why or why not?"
  },
  {
    "objectID": "slides/05-mlr.html#prediction",
    "href": "slides/05-mlr.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted interest rate for an borrower with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000?\n\n\n\n10.726 + 0.671 * 0.558 + 2.211 * 0 + 6.880 * 0 - 0.021 * 59\n\n[1] 9.861418\n\n\n\nThe predicted interest rate for an borrower with with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000 is 9.86%."
  },
  {
    "objectID": "slides/05-mlr.html#prediction-in-r",
    "href": "slides/05-mlr.html#prediction-in-r",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction in R",
    "text": "Prediction in R\nJust like with simple linear regression, we can use the predict() function in R to calculate the appropriate intervals for our predicted values:\n\nnew_borrower &lt;- tibble(\n  debt_to_income  = 0.558, \n  verified_income = \"Not Verified\", \n  annual_income_th = 59\n)\n\npredict(int_fit, new_borrower)\n\n       1 \n9.890888 \n\n\n\n\n\n\n\n\nNote\n\n\nDifference in predicted value due to rounding the coefficients on the previous slide."
  },
  {
    "objectID": "slides/05-mlr.html#cautions",
    "href": "slides/05-mlr.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/05-mlr.html#recap",
    "href": "slides/05-mlr.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nShowed exploratory data analysis for multiple linear regression\nUsed least squares to fit the regression line\nInterpreted the coefficients for quantitative predictors\nPredicted the response for new observations"
  },
  {
    "objectID": "slides/05-mlr.html#next-class",
    "href": "slides/05-mlr.html#next-class",
    "title": "Multiple linear regression (MLR)",
    "section": "Next class",
    "text": "Next class\n\nMore on multiple linear regression\n\nCategorical predictors\nModel assessment\nGeometric interpretation (as time permits)\n\nSee Sep 12 prepare\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#announcements",
    "href": "slides/05-slr-matrix-contd.html#announcements",
    "title": "SLR: Matrix representation",
    "section": "Announcements",
    "text": "Announcements\n\nLab 01 due on Thursday, September 12 at 11:59pm\n\nPush work to GitHub repo\nSubmit final PDF on Gradescope + mark pages for each question\n\nHW 01 will be assigned on Thursday"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#topics",
    "href": "slides/05-slr-matrix-contd.html#topics",
    "title": "SLR: Matrix representation",
    "section": "Topics",
    "text": "Topics\n\nMatrix representation of simple linear regression\n\nModel form\nLeast square estimate\nPredicted (fitted) values\nResiduals"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#slr-in-matrix-form",
    "href": "slides/05-slr-matrix-contd.html#slr-in-matrix-form",
    "title": "SLR: Matrix representation",
    "section": "SLR in matrix form",
    "text": "SLR in matrix form\nSuppose we have \\(n\\) observations, a quantitative response variable, and a single predictor\\[\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n=\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_1 \\\\\n\\vdots &  \\vdots \\\\\n1 &  x_n\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n\\]\n\n\n\\(\\mathbf{y}\\): \\(n\\times 1\\) vector of responses\n\\(\\mathbf{X}\\): \\(n \\times 2\\) design matrix\n\\(\\boldsymbol{\\beta}\\): \\(2 \\times 1\\) vector of coefficients\n\\(\\boldsymbol{\\epsilon}\\): \\(n \\times 1\\) vector of error terms"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals",
    "href": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nGoal: Find \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimizes the sum of squared residuals \\[\n\\begin{aligned}\nSSR = \\sum_{i=1}^n e_i^2 = \\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-1",
    "href": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-1",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\n\\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&\\class{fragment}{= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})}\\\\[10pt]\n&\\class{fragment}{=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}}\\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-2",
    "href": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-2",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\n\\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-3",
    "href": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-3",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nThe estimate of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimizes SSR is the one such that\n\\[\n\\nabla_{\\boldsymbol{\\beta}} SSR = \\nabla_{\\boldsymbol{\\beta}}( \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}) = 0\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-vector-operations",
    "href": "slides/05-slr-matrix-contd.html#side-note-vector-operations",
    "title": "SLR: Matrix representation",
    "section": "Side note: Vector operations",
    "text": "Side note: Vector operations\nLet \\(\\mathbf{x} = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\x_k\\end{bmatrix}\\)be a \\(k \\times 1\\) vector and \\(f(\\mathbf{x})\\) be a function of \\(\\mathbf{x}\\).\n\nThen \\(\\nabla_\\mathbf{x}f\\), the gradient of \\(f\\) with respect to \\(\\mathbf{x}\\) is\n\\[\n\\nabla_\\mathbf{x}f = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_k}\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-vector-operations-1",
    "href": "slides/05-slr-matrix-contd.html#side-note-vector-operations-1",
    "title": "SLR: Matrix representation",
    "section": "Side note: Vector operations",
    "text": "Side note: Vector operations\nTh Hessian matrix, \\(\\nabla_\\mathbf{x}^2f\\) is a \\(k \\times k\\) matrix of partial second derivatives\n\\[\n\\nabla_{\\mathbf{x}}^2f = \\begin{bmatrix} \\frac{\\partial^2f}{\\partial x_1^2} & \\frac{\\partial^2f}{\\partial x_1 \\partial x_2} & \\dots & \\frac{\\partial^2f}{\\partial x_1\\partial x_k} \\\\\n\\frac{\\partial^2f}{\\partial\\ x_2 \\partial x_1} & \\frac{\\partial^2f}{\\partial x_2^2} & \\dots & \\frac{\\partial^2f}{\\partial x_2 \\partial x_k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2f}{\\partial x_k\\partial x_1} & \\frac{\\partial^2f}{\\partial x_k\\partial x_2} & \\dots & \\frac{\\partial^2f}{\\partial x_k^2} \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-vector-operations-2",
    "href": "slides/05-slr-matrix-contd.html#side-note-vector-operations-2",
    "title": "SLR: Matrix representation",
    "section": "Side note: Vector operations",
    "text": "Side note: Vector operations\n\n\n\nProposition 1\n\n\nLet \\(\\mathbf{x}\\) be a \\(k \\times 1\\) vector and \\(\\mathbf{z}\\) be a \\(k \\times 1\\) vector, such that \\(\\mathbf{z}\\) is not a function of \\(\\mathbf{x}\\) .\nThe gradient of \\(\\mathbf{x}^T\\mathbf{z}\\) with respect to \\(\\mathbf{x}\\) is\n\\[\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^T\\mathbf{z} = \\mathbf{z}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-proposition-1",
    "href": "slides/05-slr-matrix-contd.html#side-note-proposition-1",
    "title": "SLR: Matrix representation",
    "section": "Side note: Proposition 1",
    "text": "Side note: Proposition 1\n\\[\n\\begin{aligned}\n\\mathbf{x}^T\\mathbf{z} &= \\class{fragment}{\\begin{bmatrix}x_1 & x_2 & \\dots &x_k\\end{bmatrix}\n\\begin{bmatrix}z_1 \\\\ z_2 \\\\ \\vdots \\\\z_k\\end{bmatrix}} \\\\[10pt]\n&\\class{fragment}{= x_1z_1 + x_2z_2 + \\dots + x_kz_k} \\\\\n&\\class{fragment}{= \\sum_{i=1}^k x_iz_i}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-proposition-1-1",
    "href": "slides/05-slr-matrix-contd.html#side-note-proposition-1-1",
    "title": "SLR: Matrix representation",
    "section": "Side note: Proposition 1",
    "text": "Side note: Proposition 1\n\\[\n\\nabla_\\mathbf{x}\\hspace{1mm}\\mathbf{x}^T\\mathbf{z} = \\class{fragment}{\\begin{bmatrix}\\frac{\\partial \\mathbf{x}^T\\mathbf{z}}{\\partial x_1} \\\\ \\frac{\\partial \\mathbf{x}^T\\mathbf{z}}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathbf{x}^T\\mathbf{z}}{\\partial x_k}\\end{bmatrix}}  \n= \\class{fragment}{\\begin{bmatrix}\\frac{\\partial}{\\partial x_1} (x_1z_1 + x_2z_2 + \\dots + x_kz_k) \\\\ \\frac{\\partial}{\\partial x_2} (x_1z_1 + x_2z_2 + \\dots + x_kz_k)\\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_k} (x_1z_1 + x_2z_2 + \\dots + x_kz_k)\\end{bmatrix}}\n= \\class{fragment}{\\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_k\\end{bmatrix} = \\mathbf{z}}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-vector-matrix-operations",
    "href": "slides/05-slr-matrix-contd.html#side-note-vector-matrix-operations",
    "title": "SLR: Matrix representation",
    "section": "Side note: Vector + matrix operations",
    "text": "Side note: Vector + matrix operations\n\n\n\nProposition 2\n\n\nLet \\(\\mathbf{x}\\) be a \\(k \\times 1\\) vector and \\(\\mathbf{A}\\) be a \\(k \\times k\\) matrix, such that \\(\\mathbf{A}\\) is not a function of \\(\\mathbf{x}\\) .\nThen the gradient of \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\) with respect to \\(\\mathbf{x}\\) is\n\\[\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^T\\mathbf{A}\\mathbf{x} = (\\mathbf{A}\\mathbf{x} + \\mathbf{A}^T \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\n\\]\nIf \\(\\mathbf{A}\\) is symmetric, then\n\\[\n(\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}\n\\]\n\n\n\n\nProof in HW 01"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#find-the-least-squares-estimators",
    "href": "slides/05-slr-matrix-contd.html#find-the-least-squares-estimators",
    "title": "SLR: Matrix representation",
    "section": "Find the least squares estimators",
    "text": "Find the least squares estimators\n\\[\n\\begin{aligned}\n\\nabla_{\\boldsymbol{\\beta}} SSR &= \\nabla_{\\boldsymbol{\\beta}}( \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta})  \\\\[10pt]\n& \\class{fragment}{= \\nabla_\\boldsymbol{\\beta} \\hspace{1mm} \\mathbf{y}^T\\mathbf{y} - 2\\nabla_\\boldsymbol{\\beta} \\hspace{1mm} \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\nabla_\\boldsymbol{\\beta} \\hspace{1mm} \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}} \\\\[10pt]\n&\\class{fragment}{= 0 - 2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}}\\class{fragment}{=0} \\\\[10pt]\n&\\class{fragment}{\\Rightarrow \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}} \\\\[10pt]\n&\\class{fragment}{\\Rightarrow   (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}} \\\\[10pt]\n&\\class{fragment}{\\color{#993399}{\\Rightarrow \\hat{\\boldsymbol{\\beta}} =  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#did-we-find-a-minimum",
    "href": "slides/05-slr-matrix-contd.html#did-we-find-a-minimum",
    "title": "SLR: Matrix representation",
    "section": "Did we find a minimum?",
    "text": "Did we find a minimum?\n\\[\n\\begin{aligned}\n\\nabla^2_{\\boldsymbol{\\beta}} SSR &= \\nabla_{\\boldsymbol{\\beta}} (-2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&\\class{fragment}{=-2\\nabla_{\\boldsymbol{\\beta}}\\mathbf{X}^T\\mathbf{y} + 2\\nabla_{\\boldsymbol{\\beta}}(\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta})} \\\\[10pt]\n&\\class{fragment}{\\propto \\mathbf{X}^T\\mathbf{X}}\\class{fragment}{ &gt; 0}\n\\end{aligned}\n\\]\n\nShow the details in HW 01"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#predicted-fitted-values",
    "href": "slides/05-slr-matrix-contd.html#predicted-fitted-values",
    "title": "SLR: Matrix representation",
    "section": "Predicted (fitted) values",
    "text": "Predicted (fitted) values\nNow that we have \\(\\hat{\\boldsymbol{\\beta}}\\), let’s predict values of \\(\\mathbf{y}\\) using the model\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\underbrace{\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T}_{\\mathbf{H}}\\mathbf{y} = \\mathbf{H}\\mathbf{y}\n\\]\n\nHat matrix: \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\n\n\n\n\\(\\mathbf{H}\\) is an \\(n\\times n\\) matrix\nMaps vector of observed values \\(\\mathbf{y}\\) to a vector of fitted values \\(\\hat{\\mathbf{y}}\\)\nIt is only a function of \\(\\mathbf{X}\\) not \\(\\mathbf{y}\\)"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#residuals",
    "href": "slides/05-slr-matrix-contd.html#residuals",
    "title": "SLR: Matrix representation",
    "section": "Residuals",
    "text": "Residuals\nRecall that the residuals are the difference between the observed and predicted values\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{y} - \\hat{\\mathbf{y}}\\\\[10pt]\n&\\class{fragment}{ = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}} \\\\[10pt]\n&\\class{fragment}{ = \\mathbf{y} - \\mathbf{H}\\mathbf{y}} \\\\[20pt]\n\\class{fragment}{\\color{#993399}{\\mathbf{e}}} &\\class{fragment}{\\color{#993399}{=(\\mathbf{I} - \\mathbf{H})\\mathbf{y}}} \\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#recap",
    "href": "slides/05-slr-matrix-contd.html#recap",
    "title": "SLR: Matrix representation",
    "section": "Recap",
    "text": "Recap\n\nIntroduced matrix representation for simple linear regression\n\nModel from\nLeast square estimate\nPredicted (fitted) values\nResiduals\n\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\n🔗 for Duke Container Manager\n\n\nCourse GitHub organization\n🔗 for GitHub\n\n\nCourse Canvas site\n🔗 for Canvas\n\n\nDiscussion forum\n🔗 to Ed Discussion\n\n\nAssignment submission\n🔗 to Gradescope\n\n\nZoom links\n🔗 on Canvas",
    "crumbs": [
      "Useful links"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "BIOSTAT 725 Syllabus",
    "section": "",
    "text": "Lecture\nTue & Thu 11:45am - 1pm\nHock 10089\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nRole\nOffice Hours\nLocation\n\n\n\n\nProf. Sam Berchuck\nInstructor\nTue 2 - 4pm\nor by appointment\nHock 10092\n\n\nDr. Youngsoo Baek\nTA\n\nWed 2 - 4pm\nHock 10090\n\n\nBraden Scherting\nTA\n\nMon 9 – 10am\nWed 1 – 2pm\nOld Chemistry 203A/B",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "BIOSTAT 725 Syllabus",
    "section": "",
    "text": "Lecture\nTue & Thu 11:45am - 1pm\nHock 10089\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nRole\nOffice Hours\nLocation\n\n\n\n\nProf. Sam Berchuck\nInstructor\nTue 2 - 4pm\nor by appointment\nHock 10092\n\n\nDr. Youngsoo Baek\nTA\n\nWed 2 - 4pm\nHock 10090\n\n\nBraden Scherting\nTA\n\nMon 9 – 10am\nWed 1 – 2pm\nOld Chemistry 203A/B",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course description",
    "text": "Course description\nThis course will teach students how to analyze biomedical data from a Bayesian inference perspective with a strong emphasis on using real-world data, including electronic health records, wearables, and imaging data. The course will begin by introducing the machinery of Bayesian statistics through the lens of linear regression, giving enough context for students with no prior experience with Bayesian statistics. A history of computational approaches used in Bayesian statistics will be given before ultimately landing on Stan, a state-of-the-art probabilistic programming language that makes Bayesian inference accessible as a viable data science tool. The course will then branch out from regression and introduce Bayesian versions of machine learning tools, including regularization and classification. The course will then emphasize Bayesian hierarchical models, including Gaussian process models for temporal and spatial data; and clustering. Additional topics may be discussed from the Bayesian perspective, including causal inference, and meta-analysis. While an applied course, the methods will be introduced from a mathematical perspective, allowing students to obtain a fundamental understanding of the introduced models. Students will learn computational skills for implementing Bayesian models using R and Stan. By the end of this course, students will be well-equipped to tackle complex problems in biomedical research using Bayesian inference.\n\nPrerequisites\nBIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to…\n\nunderstand fundamental concepts of Bayesian statistics, including prior and posterior, and predictive distributions,\nimplement the Bayesian workflow, including model building, checking, and refinement,\nuse probabilistic programming software for Bayesian analysis (e.g., Stan),\napply Bayesian techniques to real-world health data,\ncommunicate Bayesian analysis results effectively to both technical and non-technical audiences, and\nidentify opportunities for using Bayesian statistics in your research and/or job.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course materials",
    "text": "Course materials\nWhile there is no official textbook for the course; readings will primarily be made available as they are assigned. We will use the statistical software R. Students will be encourage to download the required software on their own laptops. As a courtesy, students will also be able to access R and the required software through Docker containers provided by Duke Office of Information Technology. See the computing page for more information.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course community",
    "text": "Course community\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke’s Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean or director of graduate studies (DGS) are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nPronouns\nPronouns are meaningful tools to communicate identities and experiences, and using pronouns supports a campus environment where all community members can thrive. Please update your gender pronouns in Duke Hub. You can find instructions to do so here. You can learn more at the Center for Sexual and Gender Diversity’s website.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, biostat725-sp25.netlify.app.\nLinks to Zoom meetings may be found in Canvas. Periodic announcements will be sent via email and will also be available through Ed Discussion and Canvas Announcements. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nEmail\nIf you have questions about assignment extensions, accommodations, or any other matter not appropriate for the class discussion forum, please email me directly at sib2@duke.edu. If you email me, please include “BIOSTAT 725” in the subject line. Barring extenuating circumstances, I will respond to BIOSTAT 725 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#five-tips-for-success",
    "href": "syllabus.html#five-tips-for-success",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. Your TA(s) and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TA, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you’re not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings and other preparation work.\nDo the homeworks. The earlier you start, the better. It’s not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example).\nDon’t procrastinate. The content builds upon what was taught in previous weeks, so if something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, etc. Don’t let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours and work with a member of the teaching team to help you identify a good (re)starting point.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#getting-help-in-the-course",
    "href": "syllabus.html#getting-help-in-the-course",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Getting help in the course",
    "text": "Getting help in the course\n\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours1 to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the class discussion forum Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts in Ed Discussion before adding a new question. If you know the answer to a question posted in the discussion forum, you are encouraged to respond!\n\nCheck out the Support page for more resources.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#what-to-expect-in-the-course",
    "href": "syllabus.html#what-to-expect-in-the-course",
    "title": "BIOSTAT 725 Syllabus",
    "section": "What to expect in the course",
    "text": "What to expect in the course\n\nLectures\nLectures are designed to be interactive, so you gain experience applying new concepts and learning from each other. My role as instructor is to introduce you to new methods, tools, and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities during the lectures. You are expected to prepare for class by completing assigned readings, attend all lecture sessions, and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded based on completing what we do in class.\nYou are expected to bring a laptop, tablet, or any device with internet and a keyboard to each class so that you can participate in the in-class exercises. Please make sure your device is fully charged before you come to class, as the number of outlets in the classroom will not be sufficient to accommodate everyone.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\nYou will be assessed based on four components: homework, exams, live coding, and application exercises.\n\nHomework\nIn homework, you will apply what you’ve learned during lecture to complete data analysis tasks and explain the underlying mathematics, with a focus on the computation and communication. Homework assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and submitted as a PDF for grading in Gradescope. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. HW0 will not be graded for credit.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams\nThere will be two exams in this course. Each exam will be an open-note take-home assessment. Through these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. The exams will focus on both conceptual understanding of the applied and mathematical content and application through analysis and computational tasks. The exams will be based on content in reading assignments, lectures, application exercises, and homework assignments. More detail about the exams will be given during the semester.\n\n\nLive Coding\nThere will be a live coding evaluation in this course. Students will meet with Prof. Berchuck and/or a TA in a closed session to demonstrate their ability to code in Stan. The session will be 15-30 minutes. During the session, they will be asked to implement a Bayesian model and interpret the results. This exercise aims to assess their understanding of Bayesian methods and their proficiency in using Stan. Students will be required to schedule and complete the session after returning from spring break and before the last day of classes. The evaluation will surround one of the course assignments (e.g., a particular homework) which the student will choose. More information about the project will be provided during the semester.\n\n\nApplication exercises\nYou will get the most out of the course if you actively participate in class. Parts of some lectures will be dedicated to working on Application Exercises (AEs). AEs are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59p ET, and AEs from Thursday lectures are should be submitted by Sunday at 11:59p ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nAEs will be graded based on making a good-faith effort to attempt all questions covered in class. You are welcome to, but not required, to work on AEs beyond lecture.\nSuccessful on-time effort on at least 80% of AEs will result in full credit for AEs in the final course grade.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n40%\n\n\nExam 01\n20%\n\n\nExam 02\n20%\n\n\nLive Coding\n10%\n\n\nApplication exercises\n10%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nDuke Community Standard\nAll students must adhere to the Duke Community Standard (DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard, students agree:\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;and\nI will act if the Standard is compromised.\n\n\n\n\n\n\nAcademic honesty\nTL;DR: Don’t cheat!\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nYou may not discuss or otherwise work with others on the exams. Unauthorized collaboration or using unauthorized materials will be considered a violation for all students involved. More details will be given closer to the exam date.\n\n\n\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g. StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:2 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\nAI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\nNo AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments. In general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask a member of the teaching team.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including academic integrity (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects,and more). Ignoring these requirements is a violation of the Duke Community Standard. Any questions and/or concerns regarding academic integrity can be directed to the Office of Student Conduct and Community Standards at conduct@duke.edu.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback in a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework assignment will be dropped to accommodate such circumstances.\n\nHomework may be submitted up to 2 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThe late work policy for exams will be provided with the exam instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email me at sib2@duke.edu before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your director of graduate studies (DGS) know, as they can be a resource. Please let me know if you need help contacting your DGS.\n\n\nRegrade Requests\nRegrade requests must be submitted on Gradescope within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the last day of classes.\n\n\nAttendance policy\nEvery student is expected to attend and participate in lecture. There may be times, however, when you cannot attend class. Lecture recordings will be made available upon request for students who have an excused absence. If you miss a lecture, make sure to review the material and complete the application exercise, if applicable, before the next lecture.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\n\nAcademic accommodations\nIf you need accommodations for this class, you will need to register with the Student Disability Access Office (SDAO) and provide them with documentation related to your needs. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#academic-and-wellness-support",
    "href": "syllabus.html#academic-and-wellness-support",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Academic and wellness support",
    "text": "Academic and wellness support\n\n\n\nCAPS\nDuke Counseling & Psychological Services (CAPS) helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJanuary 8: Classes begin\nJanuary 20: MLK Jr Day. No classes\nJanuary 22: Drop/Add ends\nMarch 8 - 16: Spring Break. No classes\nApril 16: Graduate classes end.\nApril 17: Reading period begins (no classes or projects are due during the reading period).\nApril 27: Reading period ends.\nApril 28 - May 3: Final exam period\n\nClick here for the full Duke academic calendar.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOffice hours are times the teaching team set aside each week to meet with students. Click here to learn more about how to effectively use office hours.↩︎\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you’re having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you’ll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it’s been resolved. If there’s a deadline coming up soon, post on the course forum to let us know that there’s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don’t anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you’ve tried and the errors you see (including verbatim errors and/or screenshots).",
    "crumbs": [
      "Computing",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "hw/hw-05.html",
    "href": "hw/hw-05.html",
    "title": "HW 05: Geospatial Modeling",
    "section": "",
    "text": "Due date\n\n\n\nThis assignment is due on Tuesday, April 8 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "slides/02-slr.html",
    "href": "slides/02-slr.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "No labs on Mon, Sep 2 (Labor Day)\nApplication exercises start Tue, Sep 3\n\nBring fully-charged laptop or device with keyboard\nMake sure you have accepted invite to GitHub course organization\n\nSee website for resources to learn / review R\nOffice hours start Tue, Sep 3"
  },
  {
    "objectID": "slides/02-prob-bayes.html#announcements",
    "href": "slides/02-prob-bayes.html#announcements",
    "title": "Probability and Bayesian Statistics",
    "section": "Announcements",
    "text": "Announcements\n\nNo labs on Mon, Sep 2 (Labor Day)\nApplication exercises start Tue, Sep 3\n\nBring fully-charged laptop or device with keyboard\nMake sure you have accepted invite to GitHub course organization\n\nSee website for resources to learn / review R\nOffice hours start Tue, Sep 3\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "support.html#lectures",
    "href": "support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "computing-r-resources.html#stan-resources",
    "href": "computing-r-resources.html#stan-resources",
    "title": "Resources for learning R",
    "section": "Stan resources",
    "text": "Stan resources\n\nRStan Getting Started by Stan Development Team\nBrief Introduction to Stan by Mark Lai",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "hw/hw-00.html",
    "href": "hw/hw-00.html",
    "title": "HW 00",
    "section": "",
    "text": "Important\n\n\n\nThis first homework will not be graded, however it will be critical that you complete this on time. All of the computing tools we’ll use in the class will be introduced during this assignemnt.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-berchuck",
    "href": "slides/01-welcome.html#meet-prof.-berchuck",
    "title": "Welcome to BIOSTAT 725!",
    "section": "",
    "text": "Education and career journey\n\nBS in Statistical Science from Duke University\nPhD in Biostatistics from University of North Carolina - Chapel Hill\nPostdoc in Duke Forge: Duke’s Center for Actionable Health Data Science\nNIH/NEI Pathway to Independence Fellow (K99/R00)\nAssistant Professor, Department of Biostatistics & Bioinformatics and Statistical Science at Duke; Faculty Affiliate of Duke AI Health\n\nWork focuses on developing data science tools to improve patient experience using biomedical data (including EHR)\nDad of 4 and 6 year old daughters 🙂"
  },
  {
    "objectID": "slides/01-welcome.html#early-years",
    "href": "slides/01-welcome.html#early-years",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Early Years",
    "text": "Early Years\n\n\n\nThomas Bayes, 1701-1761\n\n\nPierre-Simon Laplace, 1749-1827"
  },
  {
    "objectID": "slides/01-welcome.html#bayes-theorem",
    "href": "slides/01-welcome.html#bayes-theorem",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nFirst introduced in its’ modern form in the 1920’s.\n\nSuppose we have events \\(A\\) and \\(B\\) with probabilities \\(P(A)\\) and \\(P(B)\\).\nThe basic form of Bayes theorem is given by, \\[P(A|B)=\\frac{P(A, B)}{P(B)}=\\frac{P(B|A)P(A)}{P(B)}.\\]\nBayes rule gives the relationship between the marginal probabilities of \\(A\\) and \\(B\\) and the conditional probabilities"
  },
  {
    "objectID": "slides/01-welcome.html#what-did-bayes-say",
    "href": "slides/01-welcome.html#what-did-bayes-say",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What did Bayes say?",
    "text": "What did Bayes say?"
  },
  {
    "objectID": "slides/01-welcome.html#translation-the-table-game",
    "href": "slides/01-welcome.html#translation-the-table-game",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Translation: The Table Game",
    "text": "Translation: The Table Game\nWe will illuminate a version of the example explored by Bayes in his original paper Originally from Eddy 2004.\n\n\nAlice and Bob are playing a game in which the first person to get 6 points wins. The way each point is decided is a little strange. The Casino has a pool table that Alice and Bob can’t see. Before the game begins, the Casino rolls an initial ball onto the table, which comes to rest at a completely random position, which the Casino marks. Then, each point is decided by the Casino rolling another ball onto the table randomly. If it comes to rest to the left of the initial mark, Alice wins the point; to the right of the mark, Bob wins the point. The Casino reveals nothing to Alice and Bob except who won each point."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-biostat-725",
    "href": "slides/01-welcome.html#what-is-biostat-725",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is BIOSTAT 725?",
    "text": "What is BIOSTAT 725?\n\n\n\n\n\n Bayes \n\nModeling\n\n\n\n\n+\n\n\n\n\n\n Stan\n\nProbabilistic Programming\n\n\n\n\nPrerequisites: BIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission."
  },
  {
    "objectID": "slides/01-welcome.html#exploring-the-game",
    "href": "slides/01-welcome.html#exploring-the-game",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Exploring the Game",
    "text": "Exploring the Game\n\nThe probability that Alice wins a point is the fraction of the table to the left of the mark, call this probability \\(\\pi\\), and for Bob, \\(1-\\pi.\\)\nNote: Because the Casino rolled the initial ball to a random position, before any points were decided every value of \\(\\pi\\) was equally probable.\nThe mark is only set once per game, so \\(\\pi\\) is the same for every point.\n\n\nThe Question: Imagine Alice is already winning 5 points to 3, and now she bets Bob that she’s going to win. What are fair betting odds for Alice to offer Bob? That is, what is the expected probability that Alice will win?"
  },
  {
    "objectID": "slides/01-welcome.html#different-ways-to-approach-the-question",
    "href": "slides/01-welcome.html#different-ways-to-approach-the-question",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Different Ways to Approach The Question",
    "text": "Different Ways to Approach The Question\n\n\nIf \\(\\pi\\) were known this would be easy!\nInferring \\(\\pi\\) from the data, classical inference.\nInferring \\(\\pi\\) from the data, Bayesian inference."
  },
  {
    "objectID": "slides/01-welcome.html#if-pi-were-known-this-would-be-easy.",
    "href": "slides/01-welcome.html#if-pi-were-known-this-would-be-easy.",
    "title": "Welcome to BIOSTAT 725!",
    "section": "If \\(\\pi\\) were known this would be easy.",
    "text": "If \\(\\pi\\) were known this would be easy.\n\nBecause Alice just needs one more point to win, Bob only wins the game if he takes the next three points in a row. The probability of this is \\((1-\\pi)^3\\) ; Alice will win on any other outcome, so the probability of her winning is \\([1-(1-\\pi)^3]\\). Example: If we were flipping a coin, \\(\\pi=\\frac{1}{2}\\).\n\n\n\\(\\implies\\)Alice will win with probability \\(\\frac{7}{8}\\) and the odds will be \\(7:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#if-pi-were-known-this-would-be-easy",
    "href": "slides/01-welcome.html#if-pi-were-known-this-would-be-easy",
    "title": "Welcome to BIOSTAT 725!",
    "section": "If \\(\\pi\\) were known this would be easy!",
    "text": "If \\(\\pi\\) were known this would be easy!\n\nBecause Alice just needs one more point to win, Bob only wins the game if he takes the next three points in a row. The probability of this is \\((1-\\pi)^3\\).\nAlice will win on any other outcome, so the probability of her winning is \\(1-(1-\\pi)^3\\).\n\n\nExample: If we were flipping a coin, \\(\\pi=\\frac{1}{2}\\).\n\n\\(\\implies\\)Alice will win with probability \\(\\frac{7}{8}\\) and the odds will be \\(7:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-classical-inference.",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-classical-inference.",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, classical inference.",
    "text": "Inferring \\(\\pi\\) from the data, classical inference.\n\n\n\nDefine the random variable \\(A_i\\) as the event that Alice wins a point on throw \\(i\\). Then, assuming conditional independence, \\[P(data|model)=\\prod_{i=1}^8 p(A_i|\\pi) \\sim Binomial(8,\\pi)\\]\nThe interpretation of \\(\\hat{\\pi}\\) is intuitive: the frequency at which Alice has won so far (\\(\\hat{\\pi} = \\frac{5}{8}\\)).\nWhat comes with this? The usual, asymptotic normality, confidence intervals, p-values, etc…"
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-classical-inference.-1",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-classical-inference.-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, classical inference.",
    "text": "Inferring \\(\\pi\\) from the data, classical inference.\nSuppose our interest is in: \\(H_0: \\pi = 0.5, H_1: \\pi &gt; 0.5\\).\n\n\nAsymptotic Theory: \\(\\pi \\sim N\\left(\\hat{\\pi}, \\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}\\right) = N\\left(\\frac{5}{8}, \\frac{15}{512}\\right)\\). \nConfidence Intervals: \\(\\pi: \\hat{\\pi} \\pm 1.96 \\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}=(0.29,0.96)\\).\np-values: \\(z=\\frac{(\\hat{\\pi}-0.5)}{\\sqrt{\\frac{0.5\\left(1-0.5\\right)}{8}}}=0.71\\sim N(0,1)\\implies p=0.24\\).\n\n\nThe expected value that Alice wins is \\(1-(1-\\hat{\\pi})^3=\\frac{485}{512}\\) and the odds are \\(\\frac{485/512}{27/512}=18:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#bayesian-framework",
    "href": "slides/01-welcome.html#bayesian-framework",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Bayesian Framework",
    "text": "Bayesian Framework\n\n\nFundamental assumption: unknown parameters are random variables. (i.e., no longer interested in assuming that unknown parameters are fixed).\nProbability statements can be assigned to parameters, since each parameters is assumed to have a distribution.\nPrior information is incorporated into our estimates (do not want to ignore large body of prior research).\nThis is a huge step from the Frequentist framework where theory is based on asymptotics and unknown parameters are assumed to have a true value."
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\n\n\nUnder Bayes’ prior, he was able to compute a posterior distribution, \\(\\text{Beta}(6,4)\\).\n\n\n\nAsymptotic Theory: Not needed\nExact inference always exists.\n95% Credible Interval:\n\n\\(\\pi \\in (0.30,0.86)\\)\n\np-values: \\(P(\\pi&gt;0.5|A^8)=0.75\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the uniform prior, the posterior probability that Alice wins is \\(\\frac{10}{11}\\), with odds \\(10:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-1",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\n\nLet’s return to Alice and Bob: \\[\\mathbb{E}[\\text{Alice wins}|A^8]=1-\\int_{0}^1 (1-\\pi)^3 f(\\pi |A^8) d\\pi,\\]\nwhere \\[\\begin{aligned}\n\\int_{0}^1 (1-\\pi)^3 f(\\pi |A^8) d\\pi&=\\int_{0}^1 (1-\\pi)^3  \\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)}\\pi^{(a+5)-1}(1-\\pi)^{(b+3)-1} d\\pi\\\\\n&=\\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)} \\int_{0}^1 \\pi^{(a+5)-1} (1-\\pi)^{(b+6)-1} d\\pi\\\\\n&= \\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)} \\frac{\\Gamma(a+5)\\Gamma(b+6)} {\\Gamma(a+b+11)}.\n\\end{aligned}\\]\nFor the uniform prior, \\(\\mathbb{E}[\\text{Alice wins}|A^8]=\\frac{10}{11}\\), with odds \\(10:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#bringing-it-back-to-thomas-bayes",
    "href": "slides/01-welcome.html#bringing-it-back-to-thomas-bayes",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Bringing it back to Thomas Bayes",
    "text": "Bringing it back to Thomas Bayes\n\n\n 1763: An Essay towards solving a Problem in the Doctrine of Chances\n\n\nHe understood that the underlying probability of a win was random\n\nNot just random, but clearly uniform between 0 and 1\n\\(f(\\pi)\\sim \\text{Uniform}(0,1)\\)\n\\([\\text{Uniform}(0,1)=\\text{Beta}(1,1)]\\)\n\n\n\\(f(\\pi)=\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\pi^{a-1}(1-\\pi)^{b-1}\\)"
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-2",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-2",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\nComputing the posterior distribution: \\[\\begin{aligned}\nf(\\pi|A^8)&=\\frac{\\frac{8!}{5!3!} \\pi^5 (1-\\pi)^3f(\\pi)}{\\int_0^1 \\frac{8!}{5!3!} \\pi^5 (1-\\pi)^3f(\\pi) d\\pi}\\\\\n&=\\frac{\\frac{8!}{5!3!} \\pi^5 (1-\\pi)^3 \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\pi^{a-1}(1-\\pi)^{b-1}}{\\int_0^1 \\frac{8!}{5!3!} \\pi^5 (1-\\pi)^3 \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\pi^{a-1}(1-\\pi)^{b-1} d\\pi}\\\\\n&=\\frac{\\pi^{(a+5)-1} (1-\\pi)^{(3+b)-1}}{\\int_0^1 \\pi^{(a+5)-1} (1-\\pi)^{(3+b)-1} d\\pi}\\\\\n&=\\frac{\\Gamma(a+5+3+b)}{\\Gamma(a+5)\\Gamma(3+b)}\\pi^{(a+5)-1} (1-\\pi)^{(3+b)-1}\\\\\n&\\sim \\text{Beta}(a+5,b+3).\n\\end{aligned}\\] That seemed HARD! Let’s do this another way."
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-3",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-3",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\n\nComputing the posterior distribution: \\[\\begin{aligned}\nf(\\pi|A^8)&=\\frac{f(A^8|\\pi)f(\\pi)}{\\int_0^1 f(A^8|\\pi) f(\\pi) d\\pi}\\\\\n&\\propto f(A^8|\\pi)f(\\pi)\\\\\n&=\\frac{8!}{5!3!} \\pi^5 (1-\\pi)^3 \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\pi^{a-1}(1-\\pi)^{b-1}\\\\\n&\\propto \\pi^{(a+5)-1} (1-\\pi)^{(3+b)-1}\\\\\n&\\sim \\text{Beta}(a+5,b+3).\n\\end{aligned}\\] Under uniform prior, the posterior is \\(\\text{Beta}(6,4)\\).\nKey concepts: Kernel tricks, conjugacy"
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-4",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-4",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\nSuppose our interest is in: \\(H_0: \\pi =0.5, H_1: \\pi &gt; 0.5.\\) We will use our posterior distribution: \\(\\text{Beta}(6,4)\\).\n\n\n\nAsymptotic Theory: Not needed\nExact inference always exists.\n95% Credible Interval:\n\n\\(\\pi \\in (0.30,0.86)\\)\n\np-values: \\(P(\\pi&gt;0.5|A^8)=0.75\\)."
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-5",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-5",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\n\nLet’s return to Alice and Bob: \\[\\mathbb{E}[\\text{Alice wins}|A^8]=1-\\int_{0}^1 (1-\\pi)^3 f(\\pi |A^8) d\\pi,\\]\nwhere \\[\\begin{aligned}\n\\int_{0}^1 (1-\\pi)^3 f(\\pi |A^8) d\\pi&=\\int_{0}^1 (1-\\pi)^3  \\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)}\\pi^{(a+5)-1}(1-\\pi)^{(b+3)-1} d\\pi\\\\\n&=\\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)} \\int_{0}^1 \\pi^{(a+5)-1} (1-\\pi)^{(b+6)-1} d\\pi\\\\\n&= \\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)} \\frac{\\Gamma(a+5)\\Gamma(b+6)} {\\Gamma(a+b+11)}.\n\\end{aligned}\\]\nFor the uniform prior, \\(\\mathbb{E}[\\text{Alice wins}|A^8]=\\frac{10}{11}\\), with odds \\(10:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#why-do-frequentist-and-bayesian-approaches-differ",
    "href": "slides/01-welcome.html#why-do-frequentist-and-bayesian-approaches-differ",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Why do Frequentist and Bayesian Approaches Differ?",
    "text": "Why do Frequentist and Bayesian Approaches Differ?\n\nFrequentist could lose a lot of money!\nFrequentist approach can be improved by estimating \\(\\pi\\) after each throw.\nWhat if we assume the probability of Alice winning each throw are not independent, \\[\\begin{aligned}\n\\mathbb{E}[\\text{Alice wins}]&=1-\\mathbb{E}[A_9=0,A_{10}=0,A_{11}=0]\\\\\n&=1-\\mathbb{E}[A_9=0]\\mathbb{E}[A_{10}=0|A_9=0] \\\\\n&\\times \\mathbb{E}[A_{11}=0|A_9=0,A_{10}=0]\\\\\n&=1-\\left(\\frac{3}{8}\\right)\\left(\\frac{4}{9}\\right)\\left(\\frac{5}{10}\\right)=0.92.\n\\end{aligned}\\] \\(\\implies\\) Odds=11:1."
  },
  {
    "objectID": "slides/01-welcome.html#whats-going-on",
    "href": "slides/01-welcome.html#whats-going-on",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What’s going on?",
    "text": "What’s going on?\n\n\n\nThe Frequentist method can be seen as a subset of the Bayesian method, with a Beta(0,0) prior.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Methods\n\n\n\n\n\nMethod\n\\(\\hat{\\pi}\\)\n95% CI\np-value\n\\(\\mathbb{E}\\)[Alice wins]\nOdds\n\n\n\n\nMLE: Naive\n0.625\n(0.29, 0.96)\n0.24\n0.95\n18:1\n\n\nMLE: Dependent\n0.625\n(0.29, 0.96)\n0.24\n0.92\n11:1\n\n\nBayes: Beta(0,0)\n0.625\n(0.29, 0.90)\n0.77\n0.92\n11:1\n\n\nBayes: Beta(1,1)\n0.600\n(0.30, 0.86)\n0.75\n0.91\n10:1"
  },
  {
    "objectID": "slides/01-welcome.html#pros-and-cons-of-bayesian-inference",
    "href": "slides/01-welcome.html#pros-and-cons-of-bayesian-inference",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Pros and Cons of Bayesian Inference",
    "text": "Pros and Cons of Bayesian Inference\nCons:\n\nComputations can be difficult.\nBayesian methods require specifying prior probability distributions, which are often themselves unknown.\nIt is not clear that parameters or hypotheses should be treated as random variables.\n\n\nPros:\n\nOften not possible to get good estimates in complex problems without taking a Bayesian or approximately Bayesian approach.\nPrior distributions can incorporate prior knowledge.\nProbability statements can be made about parameters."
  },
  {
    "objectID": "slides/01-welcome.html#what-did-the-table-game-teach-us",
    "href": "slides/01-welcome.html#what-did-the-table-game-teach-us",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What did the Table Game teach us?",
    "text": "What did the Table Game teach us?\nThe beauty of Bayes’ Table Game analogy is that it circumvented all three cons in one stroke…\n\nThe resulting integrals have analytic solutions.\nIt provided a physical mechanism for drawing a probability from a uniform prior.\nRepresenting the unknown parameter as random is logical.\n\nIt is easy to verify that the correct answer to the table game problem is 10:1.\nDoes this mean that Frequentist methods can’t be used in this problem?"
  },
  {
    "objectID": "slides/01-welcome.html#when-to-use-bayesian-inference",
    "href": "slides/01-welcome.html#when-to-use-bayesian-inference",
    "title": "Welcome to BIOSTAT 725!",
    "section": "When to use Bayesian inference?",
    "text": "When to use Bayesian inference?\nThe choice is up to the statistician!\n\nBayesian methods can be a tool for statisticians.\nDon’t be stubborn (i.e., only use Frequentist or Bayesian methods)\nThere are times that Frequentist methods are preferred…and times that Bayesian methods are preferred.\nIt is good to know about both!"
  },
  {
    "objectID": "slides/01-welcome.html#data-science",
    "href": "slides/01-welcome.html#data-science",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Data Science",
    "text": "Data Science\nRoses are Data Science, violets are blue."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-bayesian-health-data-science-1",
    "href": "slides/01-welcome.html#what-is-bayesian-health-data-science-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is Bayesian Health Data Science?",
    "text": "What is Bayesian Health Data Science?\n\nPeople seem Distinguishing between Data Science,"
  },
  {
    "objectID": "slides/01-welcome.html#why-data-science",
    "href": "slides/01-welcome.html#why-data-science",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Why Data Science?",
    "text": "Why Data Science?\n\n\nStatistics versus Data Science?\nIntroductory Bayesian statistics courses are often very mathematical and involve intense computation; thus Bayesian methods are not as frequently used in applied settings.\nModern software now exists to lower the mathematical burden and computational intensity of Bayesian statistics, but courses do not reflect this.\nThis course focuses on teaching students Bayesian statistics as a tool for research; or anywhere data science is practiced."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-bayesian-health-data-science",
    "href": "slides/01-welcome.html#what-is-bayesian-health-data-science",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is Bayesian Health Data Science?",
    "text": "What is Bayesian Health Data Science?\n\n\nBayesian Health Data Science involves using Bayesian methods to analyze health data, which can include electronic health records (EHR), clinical trial data, and other health-related datasets. These methods are model-based and can appropriately quantify and propagate uncertainty, making them suitable for tackling challenges in health research.\n\nSource: ChatGPT"
  },
  {
    "objectID": "slides/01-welcome.html#five-tips-for-success-in-biostat-725",
    "href": "slides/01-welcome.html#five-tips-for-success-in-biostat-725",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Five tips for success in BIOSTAT 725",
    "text": "Five tips for success in BIOSTAT 725\n\nComplete all the preparation work before class.\nAsk questions in class, office hours, and on Ed Discussion.\nDo the homework; get started on homework early when possible.\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStay up-to-date on announcements on Ed Discussion and sent via email."
  },
  {
    "objectID": "slides/02-probability.html#defining-the-model",
    "href": "slides/02-probability.html#defining-the-model",
    "title": "Probability and Bayesian Statistics",
    "section": "Defining the model",
    "text": "Defining the model\nSuppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\).\nFor the purpose of today’s lecture, we assume \\(\\sigma\\) is known."
  },
  {
    "objectID": "slides/02-probability.html#defining-the-likelihood",
    "href": "slides/02-probability.html#defining-the-likelihood",
    "title": "Probability and Bayesian Statistics",
    "section": "Defining the likelihood",
    "text": "Defining the likelihood\nThe individual likelihood contribution for subject \\(i\\) is given by, \\[Y_i|\\boldsymbol{\\beta} \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta}) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta}),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/02-probability.html#defining-a-posterior",
    "href": "slides/02-probability.html#defining-a-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Defining a posterior",
    "text": "Defining a posterior\nJust like in a Frequentist approach to linear regression, our goal is inference for the regression parameters, \\(\\boldsymbol{\\beta}\\). In a Bayesian setting we are interested in the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "slides/02-probability.html#axioms-of-probability-in-words",
    "href": "slides/02-probability.html#axioms-of-probability-in-words",
    "title": "Probability and Bayesian Statistics",
    "section": "Axioms of probability (in words)",
    "text": "Axioms of probability (in words)\nP1. Probabilities are between 0 and 1, importantly \\(P(\\neg H | H) = 0\\) and \\(P(H | H) = 1\\).\nP2. If two events \\(A\\) and \\(B\\) are disjoint, then \\(P(A\\text{ or }B) = P(A) + P(B)\\).\nP3. The joint probability of two events may be broken down stepwise: \\(P(A,B) = P(A | B)P(B)\\).\n\nIt follows that,\n\nfor any partition \\(\\{H_i\\}_{i = 1}^n\\), \\(\\sum_{i=1}^n P(H_i) = 1\\) (rule of total probability)\n\nNote: simplest partition \\(P(A) + P(\\neg A) = 1\\)\n\n\\(P(A) = \\sum_{i=1}^n P(A, H_i)\\) (rule of marginal probability)\n\nNote: P3 implies that equivalently, \\(P(A) = \\sum_{i=1}^n P(A | H_i) P(H_i)\\)\n\n\\(P(A | B) = P(A,B) / P(B)\\) when \\(P(B) \\neq 0\\)\n\nNote: these statements can also be made where each term is additionally conditioned on another event \\(C\\)"
  },
  {
    "objectID": "slides/02-probability.html#defining-the-likelihood-matrix-version",
    "href": "slides/02-probability.html#defining-the-likelihood-matrix-version",
    "title": "Probability and Bayesian Statistics",
    "section": "Defining the likelihood (matrix version)",
    "text": "Defining the likelihood (matrix version)\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta} ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "hw/hw-00.html#rstudio",
    "href": "hw/hw-00.html#rstudio",
    "title": "HW 00",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\nIn this class, you have the option to use RStudio on your laptop (i.e., locally) or on through a container hosted by Duke OIT. My suggestion is for everyone to be comfortable using both options, for the following reasons:\n\nFlexibility: If you’re laptop has problems right before a due date, it will be helpful to be setup in the container.\nIndependence: It is important to be able to compute on your laptop, because when you graduate you will no longer have access to the Duke containers.\n\nThe container is offered as a convenience and you should take advantage of it when needed. We will now give instructions for using both.\n\nInstalling RStudio on your laptop\n\nMost of you probably already have RStudio installed on your laptop. In case you do not, please follow these instructions to install both R and RStudio, Installation instruction.\nWhen given the option, choose the most recent stable version of both.\n\n\n\nReserve RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers. You will log in using your NetID credentials.\nClick “Reserve STA725” to reserve an RStudio container. Be sure you reserve the container labeled STA725 to ensure you have the computing set up you need for the class.\n\nYou only need to reserve a container once per semester.\n\n\nOpen RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA725 to log into the Docker container. You should now see the RStudio environment.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#git-and-github",
    "href": "hw/hw-00.html#git-and-github",
    "title": "HW 00",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nIn addition to R and RStudio, we will use git and GitHub for version control and collaboration.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better). Git is important because:\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)\n\n\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, exams, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username.\n\n\n\nIf you already have a GitHub account, you can move on to the next step.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#connect-rstudio-and-github",
    "href": "hw/hw-00.html#connect-rstudio-and-github",
    "title": "HW 00",
    "section": "Connect RStudio and GitHub",
    "text": "Connect RStudio and GitHub\nNow that you have RStudio and a GitHub account, we will configure git so that RStudio and GitHub communicate with one another.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system. So, if you are using both your laptop and the container, you will need to do this process twice.\n\n\n\nStep 0: Open your RStudio (either the STA725 RStudio container or your laptop).\nStep 1: Type credentials::ssh_setup_github() into the console on the bottom left of the RStudio environment.\nStep 2: R will ask “No SSH key found. Generate one now?” Click 1 for yes.\nStep 3: You will generate a key. It will begin with “ssh-rsa….” R will then ask “Would you like to open a browser now?” Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used (e.g., biostat725).\n\n\n\nConfigure git\nThe last thing we need to do is configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"berchuck\",\n  user.email = \"sib2@duke.edu\")\n\nIt may look like nothing happened but you are now ready interact between GitHub and RStudio! We will begin working with RStudio and GitHub in lecture this week.\n\n\n\n\n\n\nNote\n\n\n\nYou should be using the email address you used to create your GitHub account, it’s ok if it isn’t your Duke email.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#stan-hello-world",
    "href": "hw/hw-00.html#stan-hello-world",
    "title": "HW 00",
    "section": "Stan Hello World!",
    "text": "Stan Hello World!\nIn this course, we will use the package rstan as our primary tool for conducting Bayesian inference. The container already has rstan installed, so these steps need to be used for installation on your laptop.\n\nFollow the installation guide here: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n\nPrior to installing RStan, you need to configure your R installation to be able to compile C++ code. Follow\nWe will start by making sure we can load the rstan R package.\n\nlibrary(rstan)\n\nNow we will simulate some data that we can fit with linear regression. We will also define a Stan data object (to be defined in detail in future lectures).\n\n###Set a seed for reproducibility\nset.seed(54)\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- 3 # number of covariates\n\n###True parameter values\nbeta &lt;- matrix(c(rnorm(p + 1)), ncol = 1)\nsigma &lt;- 1.5\n\n###Simulate covariates and outcome\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n###Create a Stan data object\nstan_data &lt;- list(\n  n = n,\n  p = p,\n  Y = Y,  \n  X = X\n)\n\nNow, we can define a Stan model (you do not need to understand this yet, we are just testing!)\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p + 1] X;\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  Y ~ normal(X * beta, sigma);\n}\n\n\n\n\n\nOK, great. We will now obtain posterior samples.\n\nfit &lt;- sampling(compiled_model, data = stan_data)\nprint(fit)",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#stan",
    "href": "hw/hw-00.html#stan",
    "title": "HW 00",
    "section": "Stan",
    "text": "Stan\nIn this course, we will use the package rstan as our primary tool for conducting Bayesian inference. The container already has rstan installed, so these steps need to be used for installation on your laptop.\n\nFollow the installation guide here: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n\nMake sure to follow these instructions closely, since prior to installing rstan, you need to configure your R installation to be able to compile C++ code.\n\nStan Hello World!\nOnce rstan has been installed, test to make sure we can load the R package.\n\nlibrary(rstan)\n\nNow we will simulate some data that we can fit with linear regression. We will also define a Stan data object (no need to understand this now, we will go into this in detail in future lectures).\n\n###Set a seed for reproducibility\nset.seed(54)\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- 3 # number of covariates\n\n###True parameter values\nbeta &lt;- matrix(c(rnorm(p + 1)), ncol = 1)\nsigma &lt;- 1.5\n\n###Simulate covariates and outcome\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n###Create a Stan data object\nstan_data &lt;- list(\n  n = n,\n  p = p,\n  Y = Y,  \n  X = X\n)\n\nNow, we can define a Stan model (you do not need to understand this yet, we are just testing!). In RStudio, create a new .stan file called test.stan and then copy and paste the following Stan code. To create a .stan file from RStudio, File -&gt; New File -&gt; Stan File.\n\n// Saved in test.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p + 1] X;\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  Y ~ normal(X * beta, sigma);\n}\n\nNext, we test to see if the model can compile. Note compilation can sometimes take a bit of time.\n\nstan_model &lt;- stan_model(file = \"test.stan\")\n\nOK, great. We will now obtain posterior samples, using default specifications for inference.\n\nfit &lt;- sampling(stan_model, data = stan_data, refresh = 0)\n\nFinally, print some summary estimates for the model parameters.\n\nprint(fit)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n           mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\nbeta[1]    1.89    0.00 0.18    1.54    1.77    1.89    2.01    2.23  4666    1\nbeta[2]    0.20    0.00 0.18   -0.15    0.08    0.20    0.32    0.55  4391    1\nbeta[3]   -0.30    0.00 0.16   -0.62   -0.41   -0.31   -0.19    0.01  5189    1\nbeta[4]    1.58    0.00 0.19    1.22    1.46    1.58    1.71    1.95  4983    1\nsigma      1.71    0.00 0.13    1.49    1.63    1.71    1.79    1.98  4479    1\nlp__    -102.49    0.04 1.61 -106.30 -103.33 -102.13 -101.31 -100.34  1907    1\n\nSamples were drawn using NUTS(diag_e) at Mon Jan  6 15:15:47 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "slides/02-probability.html#review-set-theory",
    "href": "slides/02-probability.html#review-set-theory",
    "title": "Probability and Bayesian Statistics",
    "section": "Review: Set theory",
    "text": "Review: Set theory\n\n\n\n\n\n\nDefinition\n\n\nset: a collection of elements, denoted by {}\nExamples\n\n\\(\\phi\\) = {} “the empty set”\n\\(A\\) = {1, 2, 3}\n\\(B\\) = {taken BIOSTAT 724, has not taken BIOSTAT 724}\n\\(C\\) = {{1,2,3}, {4, 5}}\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nsubset: denoted by \\(\\subset\\), \\(A \\subset B\\) iff \\(a \\in A \\implies a \\in B\\)\nExamples\nUsing the previously examples of \\(A\\), \\(B\\) and \\(C\\) above,\n\n\\(A \\subset C\\)\n\\(A \\not\\subset B\\)\n\n\n\n\nRecall: \\(\\cup\\) means “union”, “or”; \\(\\cap\\) means “intersection”, “and”"
  },
  {
    "objectID": "slides/02-probability.html#independence",
    "href": "slides/02-probability.html#independence",
    "title": "Probability and Bayesian Statistics",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\nDefinition\n\n\nTwo events \\(F\\) and \\(G\\) are conditionally independent given \\(H\\) if \\(p(F, G | H) = p(F | H) p(G | H)\\)\nThis implies that \\(p(F | H, G) = p(F | H).\\)\n\n\n\nThis means that if we know \\(H\\), then \\(G\\) does not supply any additional information about \\(F\\)."
  },
  {
    "objectID": "slides/02-probability.html#random-variables",
    "href": "slides/02-probability.html#random-variables",
    "title": "Probability and Bayesian Statistics",
    "section": "Random variables",
    "text": "Random variables\n\n\\(X\\) (capital) is a random variable\nWe want to compute the probability that \\(X\\) takes on a specific value \\(x\\) (lowercase)\n\nThis is denoted \\(P(X = x)\\)\n\nWe also might want to compute the probability of \\(X\\) being in a set \\(\\mathcal A\\)\n\nThis is denoted \\(P(X \\in \\mathcal A)\\)\n\nThe set of possible values that \\(X\\) can take on is called its support, \\(\\mathcal S\\)"
  },
  {
    "objectID": "slides/02-probability.html#moments",
    "href": "slides/02-probability.html#moments",
    "title": "Probability and Bayesian Statistics",
    "section": "Moments",
    "text": "Moments\nFor a random variable \\(X\\), the \\(n\\)th moment is defined as \\(\\mathbb{E}[X^n]\\).\nRecall, the expected value is defined for discrete random variable \\(X\\), \\[\n\\mathbb{E}[X] = \\sum_{x \\in \\mathcal{X}} x p(x)\n\\] and for continuous random variable \\(Y\\), \\[\n\\mathbb{E}[Y] = \\int_{-\\infty}^{\\infty} y f(y) dy\n\\] The variance of a random variable, is also known as the second central moment and is defined \\[\n\\mathbb{E}[(X - \\mathbb{E}[X])^2]\n\\] or equivalently, \\(\\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\). More generally, the covariance between two random variables \\(X\\) and \\(Y\\) is defined as, \\[\n\\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])].\n\\]"
  },
  {
    "objectID": "slides/02-probability.html#exchangeability",
    "href": "slides/02-probability.html#exchangeability",
    "title": "Probability and Bayesian Statistics",
    "section": "Exchangeability",
    "text": "Exchangeability\n\noffline notes"
  },
  {
    "objectID": "slides/02-probability.html#axioms-of-probability-in-words-1",
    "href": "slides/02-probability.html#axioms-of-probability-in-words-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Axioms of probability (in words)",
    "text": "Axioms of probability (in words)\n\nProbabilities are between 0 and 1, importantly for an event \\(A\\), \\(P(\\neg A|A) = 0\\) and \\(P(A|A) = 1\\).\nIf two events \\(A\\) and \\(B\\) are disjoint, then \\(P(A\\text{ or }B)\\) = \\(P(A) + P(B)\\).\nThe joint probability of two events may be broken down stepwise: \\(P(A,B) = P(A|B)P(B)\\)."
  },
  {
    "objectID": "slides/02-probability.html#review-set-theory-1",
    "href": "slides/02-probability.html#review-set-theory-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Review: Set theory",
    "text": "Review: Set theory\n\n\n\n\n\n\nDefinition\n\n\npartition: {\\(H_1, H_2, ... H_n\\)} = \\(\\{H_i\\}_{i = 1}^n\\) is a partition of \\(\\mathcal{H}\\) if\n\nthe union of sets is \\(\\mathcal{H}\\) i.e., \\(\\cup_{i = 1}^n H_i = \\mathcal{H}\\)\nthe sets are disjoint i.e., \\(H_i \\cap H_j = \\phi\\) for all \\(i \\neq j\\)\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nsample space: \\(\\mathcal{H}\\), the set of all possible data sets (outcomes)\nevent: a set of one or more outcomes\nNote: p(\\(\\mathcal{H}\\)) = 1\nExamples\n\nRoll a six-sided die once. The sample space \\(\\mathcal{H} = \\{1, 2, 3, 4, 5, 6\\}\\).\nLet \\(A\\) be the event that the die lands on an even number. \\(A = \\{2, 4, 6 \\}\\)"
  },
  {
    "objectID": "slides/02-probability.html#bayes-rule",
    "href": "slides/02-probability.html#bayes-rule",
    "title": "Probability and Bayesian Statistics",
    "section": "Bayes rule",
    "text": "Bayes rule\nOur course will focus on Bayes rule,\n\\[f(\\boldsymbol{\\theta}|\\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{\\int f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}.\\]\nBayes’ rule tells us how to update beliefs about \\(\\boldsymbol{\\theta}\\) given data \\(\\mathbf{Y}\\)."
  },
  {
    "objectID": "slides/02-probability.html#discrete-random-variables",
    "href": "slides/02-probability.html#discrete-random-variables",
    "title": "Probability and Bayesian Statistics",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe joint pmf is \\(f(x, y) = P(X = x, Y = y)\\)\n\n\\(\\sum_x \\sum_y f(x, y) = 1\\)\n\nThe marginal pmf for \\(X\\) is \\(f_X(x) = P(X = x) = \\sum_y f(x, y)\\)\nThe marginal pmf for \\(Y\\) is \\(f_Y(y) = P(Y = y) = \\sum_x\nf(x, y)\\)\nThe marginal distribution is the same as univariate distribution as if we ignored the other variable"
  },
  {
    "objectID": "slides/02-probability.html#discrete-random-variables-1",
    "href": "slides/02-probability.html#discrete-random-variables-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe conditional pmf of \\(Y\\) given \\(X\\) is \\(f(y|x) = P(Y = y|X = x) = \\frac{P(X = x, Y = y)}{P(X = x)} = \\frac{f(x, y)}{f_X (x)}.\\)\n\\(X\\) and \\(Y\\) are independent if \\(f(x, y) = f_X(x)f_Y(y)\\) for all \\(x\\) and \\(y\\)\n\nVariables are dependent if they are not independent\n\nEquivalently, \\(X\\) and \\(Y\\) are independent if \\(f(x|y) = f_X(x)\\) for all \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "slides/02-probability.html#example-discrete-random-variables",
    "href": "slides/02-probability.html#example-discrete-random-variables",
    "title": "Probability and Bayesian Statistics",
    "section": "Example discrete random variables",
    "text": "Example discrete random variables\n\nBinomial pmf: the probability of \\(y\\) successes in \\(n\\) trials, where each trial has an individual probability of success \\(\\theta\\). \\[p(y | \\theta) = {n \\choose y} \\theta ^y (1-\\theta)^{n-y} \\text{ for } y \\in \\{0, 1, \\ldots n \\}\\]\n\nsupport: \\(y \\in \\{0, 1, 2, \\ldots n\\}\\)\nsuccess probability \\(\\theta \\in [0, 1]\\)\ndbinom(y, n, theta) computes this pmf in R\n\nPoisson pmf: probability of \\(y\\) events occurring during a fixed interval at a mean rate \\(\\theta\\) \\[p(y | \\theta) = \\frac{\\theta^y e^{-\\theta}}{y!}\\]\n\nsupport: \\(y \\in \\{0, 1, 2, \\ldots \\}\\)\nrate \\(\\theta \\in \\mathbb{R}^+\\)\ndpois(y, theta) computes this pmf in R"
  },
  {
    "objectID": "slides/02-probability.html#continuous-random-variables",
    "href": "slides/02-probability.html#continuous-random-variables",
    "title": "Probability and Bayesian Statistics",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nManipulating joint pdfs is similar to joint pmfs but sums are replaced by integrals\nThe joint pdf is denoted \\(f(x, y)\\)\nProbabilities are computed as volume under the pdf: \\[P((X, Y) ∈ A) = \\int_A f(x, y)dxdy\\] where \\(A \\subset \\mathbb{R}^2\\)"
  },
  {
    "objectID": "slides/02-probability.html#example-continuous-random-variables",
    "href": "slides/02-probability.html#example-continuous-random-variables",
    "title": "Probability and Bayesian Statistics",
    "section": "Example continuous random variables",
    "text": "Example continuous random variables\n\nNormal pdf: \\[\nf(x | \\mu, \\sigma) = (2\\pi \\sigma^2)^{-\\frac{1}{2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\n\\]\nUniform pdf: \\[f(x|a,b) =\n\\begin{cases}\n\\frac{1}{b - a} \\hspace{.6cm}\\text{ for } x \\in [a, b]\\\\\n0 \\hspace{1cm}\\text{ otherwise }\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/02-probability.html#other-definitions",
    "href": "slides/02-probability.html#other-definitions",
    "title": "Probability and Bayesian Statistics",
    "section": "Other definitions",
    "text": "Other definitions\n\n\n\n\n\n\nDefinition\n\n\nThe part of the density/mass function that depends on the variable is called the kernel.\nExample\n\nthe kernel of the normal pdf is \\(e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\nWhat’s the kernel of a gamma random variable X?\nRecall: the pdf of a gamma distribution:\n\\[\np(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}\n\\]"
  },
  {
    "objectID": "slides/02-probability.html#kernel",
    "href": "slides/02-probability.html#kernel",
    "title": "Probability and Bayesian Statistics",
    "section": "Kernel",
    "text": "Kernel\n\n\n\n\n\n\nDefinition\n\n\nThe part of the density/mass function that depends on the variable is called the kernel.\nExample:\n\nthe kernel of the normal pdf is \\(e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)"
  },
  {
    "objectID": "slides/02-probability.html#prior-definition",
    "href": "slides/02-probability.html#prior-definition",
    "title": "Probability and Bayesian Statistics",
    "section": "Prior definition",
    "text": "Prior definition\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/02-probability.html#computing-the-posterior",
    "href": "slides/02-probability.html#computing-the-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Computing the posterior",
    "text": "Computing the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-probability.html#how-can-we-use-the-posterior",
    "href": "slides/02-probability.html#how-can-we-use-the-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nLet’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/02-probability.html#how-can-we-use-the-posterior-1",
    "href": "slides/02-probability.html#how-can-we-use-the-posterior-1",
    "title": "Probability and Bayesian Statistics",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nFirst, we define hyperparameteters.\n\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\n\nWe then compute the posterior moments and compare to OLS/MLE.\n\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\n\n\n\n\n\nparameter\ntrue\nbayes\nols\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984\n\n\n\n\n\n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/02-probability.html#review-of-probability",
    "href": "slides/02-probability.html#review-of-probability",
    "title": "Probability and Bayesian Statistics",
    "section": "Review of probability",
    "text": "Review of probability\n\nThe goal of Bayesian statistics is to compute the posterior distribution (i.e., the uncertainty distribution of the parameters, \\(\\boldsymbol{\\theta}\\), after observing the data, \\(\\mathbf{Y}\\))\nThis is the conditional distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{Y}\\)\nTherefore, we need to review the probability concepts that lead to the conditional distribution of one variable conditioned on another\n\nProbability mass (pmf) and density (pdf) functions\nJoint distributions\nMarginal and conditional distributions"
  },
  {
    "objectID": "slides/02-probability.html#random-variables-1",
    "href": "slides/02-probability.html#random-variables-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Random variables",
    "text": "Random variables\n\n\n\n\n\n\nDefinition\n\n\nIn Bayesian inference, a random variable is an unknown numerical quantity about which we make probability statements.\nThe support of a random variable is the set of values a random variable can take.\nExamples,\n\nData. E.g., the amount of a wheat a field will yield later this year. Since this data has not yet been generated, the quantity is unknown.\nA population parameter. E.g., the true mean resting heart rate of Duke students. Note: this is a fixed (non-random) quantity, but it is also unknown. We use probability to describe our uncertainty in this quantity."
  },
  {
    "objectID": "slides/02-probability.html#random-variables---example",
    "href": "slides/02-probability.html#random-variables---example",
    "title": "Probability and Bayesian Statistics",
    "section": "Random variables - example",
    "text": "Random variables - example\n\nExample 1: \\(X\\) is the roll of a die\n\nThe support is \\(\\mathcal S = \\{1, 2, 3, 4, 5, 6\\}\\)\n\\(P(X = 1) = 1/6\\)\n\nExample 2: \\(X\\) is a newborn baby’s weight\n\nThe support is \\(\\mathcal S = (0, \\infty)\\)\n\\(P(X \\in [0, \\infty]) = 1\\)"
  },
  {
    "objectID": "slides/02-probability.html#what-is-probability",
    "href": "slides/02-probability.html#what-is-probability",
    "title": "Probability and Bayesian Statistics",
    "section": "What is probability?",
    "text": "What is probability?\nObjective (associated with frequentist)\n\n\\(P(X = x)\\) as a purely mathematical statement\nIf we repeatedly sampled \\(X\\), the the proportion of draws equal to \\(x\\) converges to \\(P(X = x)\\)\n\nSubjective (associated with Bayesian)\n\n\\(P(X = x)\\) represents an individual’s degree of belief\nOften quantified as the amount an individual would be willing to wager that \\(X\\) will be \\(x\\)\n\nA Bayesian analysis makes use of both of these concepts"
  },
  {
    "objectID": "slides/02-probability.html#what-is-uncertainty",
    "href": "slides/02-probability.html#what-is-uncertainty",
    "title": "Probability and Bayesian Statistics",
    "section": "What is uncertainty?",
    "text": "What is uncertainty?\nAleatoric uncertainty (likelihood)\n\nUncontrollable randomness in the experiment\nFor example, the results of a fair coin flip can never be predicted with certainty\n\nEpistemic uncertainty (prior/posterior)\n\nUncertainty about a quantity that could theoretically be known\nFor example, if we flipped a coin infinitely-many times we could know the true probability of a head\n\nA Bayesian analysis makes use of both of these concepts"
  },
  {
    "objectID": "slides/02-probability.html#probability-versus-statistics",
    "href": "slides/02-probability.html#probability-versus-statistics",
    "title": "Probability and Bayesian Statistics",
    "section": "Probability versus statistics",
    "text": "Probability versus statistics\nProbability is the forward problem\n\nWe assume we know how the data are being generated and compute the probability of events\nFor example, what is the probability of flipping 5 straight heads if the coin is fair?\n\nStatistics is the inverse problem\n\nWe use data to learn about the data-generating mechanism\nFor example, if we flipped five straight head, can we conclude the coin is biased?\n\nAny statistical analysis obviously relies on probability"
  },
  {
    "objectID": "slides/02-probability.html#univariate-distributions",
    "href": "slides/02-probability.html#univariate-distributions",
    "title": "Probability and Bayesian Statistics",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables\nThe random variable \\(X\\) is discrete if its support \\(\\mathcal S\\) is countable\nExamples:\n\n\\(X \\in \\{0, 1, 2, 3\\}\\) is the number of successes in 3 trials\n\n\\(X \\in \\{0, 1, 2, \\ldots\\}\\) is the number of patients with COVID in Durham County"
  },
  {
    "objectID": "slides/02-probability.html#univariate-distributions-1",
    "href": "slides/02-probability.html#univariate-distributions-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables\nThe random variable \\(X\\) is continuous if its support \\(\\mathcal S\\) is uncountable\nExamples with \\(\\mathcal S = (0, \\infty)\\):\n\n\\(X &gt; 0\\) is systolic blood pressure\n\n\\(X &gt; 0\\) is a patient’s BMI"
  },
  {
    "objectID": "slides/02-probability.html#discrete-univariate-distributions",
    "href": "slides/02-probability.html#discrete-univariate-distributions",
    "title": "Probability and Bayesian Statistics",
    "section": "Discrete univariate distributions",
    "text": "Discrete univariate distributions\n\nIf \\(X\\) is discrete we describe its distribution with its probability mass function (pmf)\nThe pmf is \\(f(x) = P(X = x)\\)\nThe domain of \\(X\\) is the set of \\(x\\) with \\(f(x) &gt; 0\\)\nWe must have \\(f(x) \\geq 0\\) and \\(\\sum_x f(x) = 1\\)\nThe mean is \\(\\mathbb E[X] = \\sum_x x f(x)\\)\nThe variance is \\(\\mathbb V(X) = \\sum_x(x − \\mathbb E[X])^2f(x)\\)\nThe last three sums are over \\(X\\)’s domain"
  },
  {
    "objectID": "slides/02-probability.html#parametric-families-of-distributions",
    "href": "slides/02-probability.html#parametric-families-of-distributions",
    "title": "Probability and Bayesian Statistics",
    "section": "Parametric families of distributions",
    "text": "Parametric families of distributions\n\nA statistical analysis typically proceeds by selecting a pmf that seems to match the distribution of a sample\nWe rarely know the pmf exactly, but we assume it is from a parametric family of distributions\nFor example, Binomial(10, 0.5) and Binomial(4, 0.1) are different but both from the binomial family\nA family of distributions have the same equation for the pmf but differ by some unknown parameters \\(\\boldsymbol{\\theta}\\)\nWe must estimate these parameters"
  },
  {
    "objectID": "slides/02-probability.html#continuous-univariate-distributions",
    "href": "slides/02-probability.html#continuous-univariate-distributions",
    "title": "Probability and Bayesian Statistics",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nIf \\(X\\) is continuous we describe its distribution with the probability density function (pdf) \\(f(x) \\geq 0\\)\nSince there are uncountably many possible values, \\(P(X = x) = 0\\) for all \\(x\\),\nProbabilities are computed as areas under the pdf curve \\[P(a &lt; X &lt; b) = \\int_a^b f(x)dx\\]\nTherefore, to be valid \\(f(x)\\) must satisfy \\(f(x) \\geq 0\\) and \\[P(−\\infty &lt; X &lt; \\infty) = \\int_{-\\infty}^{\\infty} f(x)dx = 1\\]"
  },
  {
    "objectID": "slides/02-probability.html#continuous-univariate-distributions-1",
    "href": "slides/02-probability.html#continuous-univariate-distributions-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nThe domain is the set of \\(x\\) values with \\(f(x) &gt; 0\\)\nThe mean and the variance are defined similarly to the discrete case but with the sums replaced by integrals\nThe mean is \\(\\mathbb E[X] = \\int x f(x)dx\\)\nThe variance is \\(\\mathbb V(X) = \\int (x − \\mathbb E[X])^2 f(x)dx\\)"
  },
  {
    "objectID": "slides/02-probability.html#joint-distributions",
    "href": "slides/02-probability.html#joint-distributions",
    "title": "Probability and Bayesian Statistics",
    "section": "Joint distributions",
    "text": "Joint distributions\n\n\\(X = (X_1, \\ldots, X_p)\\) is a random vector (vectors and matrices should be in bold).\nFor notational convenience, let’s consider only \\(p = 2\\) random variables \\(X\\) and \\(Y\\).\n\\((X, Y)\\) is discrete if it can take on a countable number of values, such as\n\\(X\\) = number of hearts and \\(Y\\) = number of face cards.\n\\((X, Y)\\) is continuous if it can take on an uncountable number of values, such as\n\\(X\\) = birthweight and \\(Y\\) = gestational age."
  },
  {
    "objectID": "slides/02-probability.html#discrete-random-variables-2",
    "href": "slides/02-probability.html#discrete-random-variables-2",
    "title": "Probability and Bayesian Statistics",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nNotation: \\(X_1, \\dots, X_n \\overset{\\mathrm{iid}}{\\sim} f(x)\\) means that \\(X_1, \\ldots, X_n\\) are independent and identically distributed\nThis implies the joint pmf is \\[P(X_1 = x_1, \\ldots, X_n = x_n) = \\prod_{i=1}^n f(x_i)\\]\nThe same notation and definitions of independence apply to continuous random variables\nIn this class, assume independence unless otherwise noted"
  },
  {
    "objectID": "slides/02-probability.html#discrete-random-variables-3",
    "href": "slides/02-probability.html#discrete-random-variables-3",
    "title": "Probability and Bayesian Statistics",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\n\n\n\n\n\nDefinition\n\n\ndiscrete random variable: a random variable that takes countably many values. \\(Y\\) is discrete if its possible outcomes can be enumerated \\(\\mathcal{Y} = \\{y_1, y_2, \\ldots \\}\\).\nNote: discrete does not mean finite. There may be infinitely many outcomes!\nExamples,\n\n\\(Y\\) = the number of children of a randomly sampled person\n\\(Y\\) = the number of visible stars in the sky on a randomly sampled night of the year\n\nFor each \\(y \\in \\mathcal{Y}\\), let \\(p(Y) = probability(Y = y)\\). Then \\(p\\) is the probability mass function (pmf) of the random variable \\(Y\\). Finally, we have that,\n\\[\n\\begin{aligned}\n0 \\leq p(y) \\leq 1,\\\\\n\\sum_{y \\in \\mathcal{Y}} p(y) = 1.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-probability.html#continuous-random-variables-1",
    "href": "slides/02-probability.html#continuous-random-variables-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nThe marginal pdf of \\(X\\) is \\(f_X(x) = \\int f(x, y)dy\\)\n\\(f_X\\) is the univariate pdf for \\(X\\) as if we never considered \\(Y\\)\nThe conditional pdf of \\(Y\\) given \\(X\\) is \\[f(y|x) = \\frac{f(x, y)}{f_X (x)}\\]\nProper: \\(\\int f(y|x)dy = \\int \\frac{f(x,y)}{f_X(x)}dy = \\int \\frac{f(x,y)dy}{f_X(x)} = 1\\)"
  },
  {
    "objectID": "slides/02-probability.html#continuous-random-variables-2",
    "href": "slides/02-probability.html#continuous-random-variables-2",
    "title": "Probability and Bayesian Statistics",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\n\n\n\n\n\nDefinition\n\n\ncontinuous random variable: a random variable that takes uncountably many values.\nThe probability density function (pdf) of a continuous random variable, \\(X\\) is defined\n\\(f(x) = pdf(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{p(x &lt; X &lt; x + \\Delta x)}{\\Delta x}\\)\nand the probability \\(X\\) is in some interval,\n\\(P(x_1 &lt; X &lt; x_2) = \\int_{x_1}^{x_2} f(x) dx\\).\nFinally, we have that, \\[\n\\begin{aligned}\n0 \\leq p(y) \\ \\text{and}, \\\\\n\\int_{y \\in \\mathcal{Y}} p(y) = 1.\n\\end{aligned}\n\\] Note: For a continuous random variable \\(Y\\), \\(f(y)\\) can be larger than 1 and \\(f(y)\\) is not \\(P(Y = y)\\), which equals 0."
  },
  {
    "objectID": "slides/02-probability.html#defining-joint-distributions-conditionally",
    "href": "slides/02-probability.html#defining-joint-distributions-conditionally",
    "title": "Probability and Bayesian Statistics",
    "section": "Defining joint distributions conditionally",
    "text": "Defining joint distributions conditionally\n\nSpecifying joint distributions is hard\nEvery joint distribution can be written \\(f(x, y) = f(y|x)f(x)\\)\nTherefore, any joint distribution can be defined by\n\n\\(X\\)’s marginal distribution\nThe conditional distribution of \\(Y|X\\)\n\nThe joint problem reduces to two univariate problems\nThis idea forms the basis of hierarchical modeling"
  },
  {
    "objectID": "slides/02-probability.html#axioms-of-probability",
    "href": "slides/02-probability.html#axioms-of-probability",
    "title": "Probability and Bayesian Statistics",
    "section": "Axioms of probability",
    "text": "Axioms of probability\nFor two events \\(A\\) and \\(B\\),\nA1. Probabilities are between 0 and 1, importantly \\(P(A^c | A) = 0\\) and \\(P(A | A) = 1\\).\nA2. If \\(A\\) and \\(B\\) are disjoint, then \\(P(A\\text{ or }B) = P(A) + P(B)\\).\nA3. The joint probability of two events may be broken down stepwise: \\(P(A,B) = P(A | B)P(B)\\)."
  },
  {
    "objectID": "slides/02-probability.html#consequences-of-probability-axioms",
    "href": "slides/02-probability.html#consequences-of-probability-axioms",
    "title": "Probability and Bayesian Statistics",
    "section": "Consequences of probability axioms",
    "text": "Consequences of probability axioms\n\nFor any partition \\(\\{H_i\\}_{i = 1}^n\\), \\(\\sum_{i=1}^n P(H_i) = 1\\) (rule of total probability)\n\nNote: simplest partition \\(P(A) + P(A^c) = 1\\)\n\n\\(P(A) = \\sum_{i=1}^n P(A, H_i)\\) (rule of marginal probability)\n\nNote: A3 implies that equivalently, \\(P(A) = \\sum_{i=1}^n P(A | H_i) P(H_i)\\)\n\n\\(P(A | B) = P(A,B) / P(B)\\) when \\(P(B) \\neq 0\\)\n\nNote: these statements can also be made where each term is additionally conditioned on another event \\(C\\)"
  },
  {
    "objectID": "slides/02-probability.html#visualize-simulated-data",
    "href": "slides/02-probability.html#visualize-simulated-data",
    "title": "Probability and Bayesian Statistics",
    "section": "Visualize simulated data",
    "text": "Visualize simulated data"
  },
  {
    "objectID": "slides/02-probability.html#where-did-this-come-from",
    "href": "slides/02-probability.html#where-did-this-come-from",
    "title": "Probability and Bayesian Statistics",
    "section": "Where did this come from?",
    "text": "Where did this come from?\n\n\n\n\n\n\nKernel of a multivariate normal distribution: \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)\n\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]\n\n\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-probability.html#kernel-of-a-multivariate-normal-distribution",
    "href": "slides/02-probability.html#kernel-of-a-multivariate-normal-distribution",
    "title": "Probability and Bayesian Statistics",
    "section": "Kernel of a multivariate normal distribution:",
    "text": "Kernel of a multivariate normal distribution:\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\beta}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\beta}\\right)\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-probability.html#techniques-to-find-this-posterior",
    "href": "slides/02-probability.html#techniques-to-find-this-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Techniques to find this posterior",
    "text": "Techniques to find this posterior\n\nBrute force: complete the square\nEasy: kernel recognition\n\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the multivariate normal pdf, \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-probability.html#using-the-kernel-to-find-the-posterior",
    "href": "slides/02-probability.html#using-the-kernel-to-find-the-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Using the kernel to find the posterior",
    "text": "Using the kernel to find the posterior\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{aligned}\\]\nThis is the kernel of a multivariate normal for \\(\\boldsymbol{\\beta}\\), with \\(\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}\\) and \\(\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\). It’s easy to see then that, \\(f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\\)\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/02-probability.html#back-to-the-posterior",
    "href": "slides/02-probability.html#back-to-the-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Back to the posterior",
    "text": "Back to the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/02-probability.html#linear-regression-estimation-techniques",
    "href": "slides/02-probability.html#linear-regression-estimation-techniques",
    "title": "Probability and Bayesian Statistics",
    "section": "Linear regression estimation techniques",
    "text": "Linear regression estimation techniques\n\nOrdinary least squares (OLS)\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\nMaximum likelihood estimation (MLE)\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)"
  },
  {
    "objectID": "slides/02-probability.html#linear-regression-estimation",
    "href": "slides/02-probability.html#linear-regression-estimation",
    "title": "Probability and Bayesian Statistics",
    "section": "Linear regression estimation",
    "text": "Linear regression estimation\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)"
  },
  {
    "objectID": "slides/02-probability.html#bayesian-estimation",
    "href": "slides/02-probability.html#bayesian-estimation",
    "title": "Probability and Bayesian Statistics",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "slides/02-probability.html#comparing-to-olsmle",
    "href": "slides/02-probability.html#comparing-to-olsmle",
    "title": "Probability and Bayesian Statistics",
    "section": "Comparing to OLS/MLE",
    "text": "Comparing to OLS/MLE\nFirst, we define hyperparameteters.\n\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10"
  },
  {
    "objectID": "slides/02-probability.html#how-can-we-use-the-posterior-for-inference",
    "href": "slides/02-probability.html#how-can-we-use-the-posterior-for-inference",
    "title": "Probability and Bayesian Statistics",
    "section": "How can we use the posterior for inference?",
    "text": "How can we use the posterior for inference?\n\nThe posterior distribution has been computed in closed form, so we can use it to compute any summary we would like.\n\n\nJoint probabilities: \\(P(\\boldsymbol{\\beta} \\in \\mathbf{A} | \\mathbf{Y})\\)\nConditional probabilities: \\(P(\\beta_j \\in A | \\boldsymbol{\\beta}_{-j} , \\mathbf{Y})\\)\nMarginal probabilities: \\(P(\\beta_j \\in A | \\mathbf{Y})\\)\n\\((100\\alpha)\\%\\) quantiles: \\(\\{q_{\\alpha} : P(\\beta_j &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n…"
  },
  {
    "objectID": "slides/02-probability.html#making-posterior-inference-easy",
    "href": "slides/02-probability.html#making-posterior-inference-easy",
    "title": "Probability and Bayesian Statistics",
    "section": "Making posterior inference easy!",
    "text": "Making posterior inference easy!\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\beta_j &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/mathematical software packages\n\n\nThese methods work well for standard posterior quantities and distributions\n\nSolution: Monte Carlo approximation\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/02-probability.html#summarizing-the-posterior",
    "href": "slides/02-probability.html#summarizing-the-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\beta_j &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pnorm}\\))\n\n\nThese methods work well for standard posterior quantities and distributions"
  },
  {
    "objectID": "slides/02-probability.html#asdf",
    "href": "slides/02-probability.html#asdf",
    "title": "Probability and Bayesian Statistics",
    "section": "asdf",
    "text": "asdf\n\nSummarizing a posterior (with known parametric form) is straightforward for basic quantities of interest\n\n\nPosterior means, medians, modes, and variances\nJoint probabilities: \\(P(\\boldsymbol{\\beta} \\in \\mathbf{A} | \\mathbf{Y})\\)\nConditional probabilities: \\(P(\\beta_j \\in A | \\boldsymbol{\\beta}_{-j} , \\mathbf{Y})\\)\nMarginal probabilities: \\(P(\\beta_j \\in A | \\mathbf{Y})\\)\n\\((100\\alpha)\\%\\) quantiles: \\(\\{q_{\\alpha} : P(\\beta_j &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n…"
  },
  {
    "objectID": "slides/02-probability.html#summarizing-the-posterior-1",
    "href": "slides/02-probability.html#summarizing-the-posterior-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?"
  },
  {
    "objectID": "slides/02-probability.html#summarizing-the-posterior-2",
    "href": "slides/02-probability.html#summarizing-the-posterior-2",
    "title": "Probability and Bayesian Statistics",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?"
  },
  {
    "objectID": "slides/02-probability.html#monte-carlo-approximation",
    "href": "slides/02-probability.html#monte-carlo-approximation",
    "title": "Probability and Bayesian Statistics",
    "section": "Monte Carlo approximation",
    "text": "Monte Carlo approximation\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/02-probability.html#monte-carlo-mc-approximation",
    "href": "slides/02-probability.html#monte-carlo-mc-approximation",
    "title": "Probability and Bayesian Statistics",
    "section": "Monte Carlo (MC) approximation",
    "text": "Monte Carlo (MC) approximation\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/02-probability.html#mc-approximation",
    "href": "slides/02-probability.html#mc-approximation",
    "title": "Probability and Bayesian Statistics",
    "section": "MC approximation",
    "text": "MC approximation\nLet \\(g\\left(\\boldsymbol{\\beta}\\right)\\) be (just about) any function of \\(\\boldsymbol{\\beta}\\). The law of large numbers says that if,\n\\[\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right),\\] then, \\[\\frac{1}{S}\\sum_{s=1}^S g\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\boldsymbol{\\beta}\\right)f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)d\\boldsymbol{\\beta},\\] as \\(S\\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/02-probability.html#mc-approximation-1",
    "href": "slides/02-probability.html#mc-approximation-1",
    "title": "Probability and Bayesian Statistics",
    "section": "MC approximation",
    "text": "MC approximation\n\nImplications (as \\(S\\rightarrow \\infty\\)):\n\n\\(\\overline{\\boldsymbol{\\beta}}=\\frac{1}{S}\\sum_{s=1}^S \\boldsymbol{\\beta}^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\boldsymbol{\\beta}|\\mathbf{Y}\\right]\\)\n\\(\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\boldsymbol{\\beta}^{\\left(s\\right)}-\\overline{\\boldsymbol{\\beta}}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\n\\(\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\leq \\mathbf{c}\\right) \\rightarrow P\\left(\\boldsymbol{\\beta}\\leq \\mathbf{c} | \\mathbf{Y}\\right)\\)\n\\(\\alpha\\)-quantile of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}\\)\n\nJust about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample"
  },
  {
    "objectID": "slides/02-probability.html#posterior-inference-for-arbitrary-functions",
    "href": "slides/02-probability.html#posterior-inference-for-arbitrary-functions",
    "title": "Probability and Bayesian Statistics",
    "section": "Posterior inference for arbitrary functions",
    "text": "Posterior inference for arbitrary functions\nInterest in the posterior distribution of a function of \\(\\boldsymbol{\\beta}\\), \\(g\\left(\\boldsymbol{\\beta}\\right)\\)\n\nMC sampling plan:\n\nDraw \\(\\boldsymbol{\\beta}^{\\left(1\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right)\\)\n\\(\\ldots\\)\nDraw \\(\\boldsymbol{\\beta}^{\\left(S\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\)\n\n\\(\\Rightarrow g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right),\\ldots,g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right)\\)\nTherefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)"
  },
  {
    "objectID": "slides/02-probability.html#comparison-with-olsmle",
    "href": "slides/02-probability.html#comparison-with-olsmle",
    "title": "Probability and Bayesian Statistics",
    "section": "Comparison with OLS/MLE",
    "text": "Comparison with OLS/MLE\nCompute posterior moments\n\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\n\n\n\n\n\nparameter\ntrue\nbayes\nols/mle\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984"
  },
  {
    "objectID": "slides/02-probability.html#inspecting-the-prior",
    "href": "slides/02-probability.html#inspecting-the-prior",
    "title": "Probability and Bayesian Statistics",
    "section": "Inspecting the prior",
    "text": "Inspecting the prior\nFirst, we define hyperparameteters.\n\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html",
    "href": "ae/ae-01-linear-regression.html",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE is due on Sunday, January 19 at 11:59pm. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-1",
    "href": "ae/ae-01-linear-regression.html#exercise-1",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "Exercise 1",
    "text": "Exercise 1\nCompute the posterior mean and standard deviation for the intercept, slope, and measurement error. Provide an interpretation for each of these parameter estimates.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-2",
    "href": "ae/ae-01-linear-regression.html#exercise-2",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "Exercise 2",
    "text": "Exercise 2\nCompute a 95% confidence interval for the regression slope. Provide an interpretation for this confidence interval.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-3",
    "href": "ae/ae-01-linear-regression.html#exercise-3",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute a one-sided Bayesian p-value for the regression slope: \\(P(\\beta_1 &lt; 0)\\). Interpret the results in plain English. Is intraocular pressure associated with disease progression?\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-4",
    "href": "ae/ae-01-linear-regression.html#exercise-4",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "Exercise 4",
    "text": "Exercise 4\nFill in the code to visualize the relationship between price and area. What are 1 - 2 observations about the relationship between these two variables?"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-5",
    "href": "ae/ae-01-linear-regression.html#exercise-5",
    "title": "AE 01: Monte Carlo samping",
    "section": "Exercise 5",
    "text": "Exercise 5\nYou want to fit a model of the form\n\\[\nprice = \\beta_0 + \\beta_1 ~ area + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2_\\epsilon)\n\\]\nWould a model of this form be a reasonable fit for the data? Why or why not?"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-6",
    "href": "ae/ae-01-linear-regression.html#exercise-6",
    "title": "AE 01: Monte Carlo samping",
    "section": "Exercise 6",
    "text": "Exercise 6\nFit the linear model described in the previous exercise and neatly display the output.\nSee notes for example code.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-7",
    "href": "ae/ae-01-linear-regression.html#exercise-7",
    "title": "AE 01: Monte Carlo samping",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nInterpret the slope in the context of the data.\nInterpret the slope in terms of area increasing by 100 sqft.\nWhich interpretation do you think is more meaningful in practice?"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-8",
    "href": "ae/ae-01-linear-regression.html#exercise-8",
    "title": "AE 01: Monte Carlo samping",
    "section": "Exercise 8",
    "text": "Exercise 8\nDoes it make sense to interpret the intercept? If so, interpret it in the context of the data. Otherwise, explain why not."
  },
  {
    "objectID": "slides/03-linear-regression.html",
    "href": "slides/03-linear-regression.html",
    "title": "Probability and Bayesian Statistics",
    "section": "",
    "text": "Suppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\).\nFor the purpose of today’s lecture, we assume \\(\\sigma\\) is known.\n\n\n\n\nThe individual likelihood contribution for subject \\(i\\) is given by, \\[Y_i|\\boldsymbol{\\beta} \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta}) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta}),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\).\n\n\n\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ Y | X ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_1\\\\\n    \\beta_2\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]\n\n\n\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)\n\n\n\n\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\).\n\n\n\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)\n\n\n\n\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\n\nBrute force: complete the square\nEasy: kernel recognition\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the multivariate normal pdf, \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]\n\n\n\n\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\\\\\n&\\sim N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\n\\end{aligned}\\]\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)\n\n\n\n\n\n\n\n\n\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n. . .\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n. . .\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0\\)\n\n\n\n\nLet’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, we define hyperparameteters.\n\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\n\nWe then compute the posterior moments and compare to OLS/MLE.\n\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\n\n\n\n\n\nparameter\ntrue\nbayes\nols\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984\n\n\n\n\n\n\n\n\n\n\n\nSummarizing a posterior (with known parametric form) is straightforward for basic quantities of interest\n\n\nPosterior means, medians, modes, and variances\nJoint, conditional, and marginal probabilities, for example: \\(P(\\beta_j &lt; c | \\mathbf{Y})\\)\n\\(\\alpha\\)-quantiles: \\(\\{q_{\\alpha} : P(\\beta_j &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n\\(\\ldots\\)\n\n\n\n\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\beta_j &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pnorm}\\))\n\n\nThese methods work well for standard posterior quantities and distributions\n\n\n\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?\n\n\n\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\n\n\n\n\nLet \\(g\\left(\\boldsymbol{\\beta}\\right)\\) be (just about) any function of \\(\\boldsymbol{\\beta}\\). The law of large numbers says that if,\n\\[\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right),\\] then, \\[\\frac{1}{S}\\sum_{s=1}^S g\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\boldsymbol{\\beta}\\right)f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)d\\boldsymbol{\\beta},\\] as \\(S\\rightarrow \\infty\\).\n\n\n\n\nImplications (as \\(S\\rightarrow \\infty\\)):\n\n\\(\\overline{\\boldsymbol{\\beta}}=\\frac{1}{S}\\sum_{s=1}^S \\boldsymbol{\\beta}^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\boldsymbol{\\beta}|\\mathbf{Y}\\right]\\)\n\\(\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\boldsymbol{\\beta}^{\\left(s\\right)}-\\overline{\\boldsymbol{\\beta}}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\n\\(\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\leq \\mathbf{c}\\right) \\rightarrow P\\left(\\boldsymbol{\\beta}\\leq \\mathbf{c} | \\mathbf{Y}\\right)\\)\n\\(\\alpha\\)-quantile of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}\\)\n\nJust about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample\n\n\n\n\nInterest in the posterior distribution of a function of \\(\\boldsymbol{\\beta}\\), \\(g\\left(\\boldsymbol{\\beta}\\right)\\)\n\nMC sampling plan:\n\nDraw \\(\\boldsymbol{\\beta}^{\\left(1\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right)\\)\n\\(\\ldots\\)\nDraw \\(\\boldsymbol{\\beta}^{\\left(S\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\)\n\n\\(\\Rightarrow g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right),\\ldots,g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right)\\)\nTherefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)"
  },
  {
    "objectID": "slides/03-linear-regression.html#defining-the-model",
    "href": "slides/03-linear-regression.html#defining-the-model",
    "title": "Bayesian Linear Regression",
    "section": "Defining the model",
    "text": "Defining the model\nSuppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\).\nFor the purpose of today’s lecture, we assume \\(\\sigma\\) is known."
  },
  {
    "objectID": "slides/03-linear-regression.html#defining-the-likelihood",
    "href": "slides/03-linear-regression.html#defining-the-likelihood",
    "title": "Bayesian Linear Regression",
    "section": "Defining the likelihood",
    "text": "Defining the likelihood\nThe individual likelihood contribution for subject \\(i\\) is given by, \\[Y_i|\\boldsymbol{\\beta} \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta}) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta}),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/03-linear-regression.html#defining-the-likelihood-matrix-version",
    "href": "slides/03-linear-regression.html#defining-the-likelihood-matrix-version",
    "title": "Bayesian Linear Regression",
    "section": "Defining the likelihood (matrix version)",
    "text": "Defining the likelihood (matrix version)\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ Y | X ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_1\\\\\n    \\beta_2\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-estimation",
    "href": "slides/03-linear-regression.html#linear-regression-estimation",
    "title": "Bayesian Linear Regression",
    "section": "Linear regression estimation",
    "text": "Linear regression estimation\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#bayesian-estimation",
    "href": "slides/03-linear-regression.html#bayesian-estimation",
    "title": "Bayesian Linear Regression",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "slides/03-linear-regression.html#prior-definition",
    "href": "slides/03-linear-regression.html#prior-definition",
    "title": "Bayesian Linear Regression",
    "section": "Prior definition",
    "text": "Prior definition\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/03-linear-regression.html#computing-the-posterior",
    "href": "slides/03-linear-regression.html#computing-the-posterior",
    "title": "Bayesian Linear Regression",
    "section": "Computing the posterior",
    "text": "Computing the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#techniques-to-find-this-posterior",
    "href": "slides/03-linear-regression.html#techniques-to-find-this-posterior",
    "title": "Bayesian Linear Regression",
    "section": "Techniques to find this posterior",
    "text": "Techniques to find this posterior\n\nBrute force: complete the square\nEasy: kernel recognition\n\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the multivariate normal pdf, \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#using-the-kernel-to-find-the-posterior",
    "href": "slides/03-linear-regression.html#using-the-kernel-to-find-the-posterior",
    "title": "Bayesian Linear Regression",
    "section": "Using the kernel to find the posterior",
    "text": "Using the kernel to find the posterior\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\\\\\n&\\sim N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\n\\end{aligned}\\]\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#back-to-the-posterior",
    "href": "slides/03-linear-regression.html#back-to-the-posterior",
    "title": "Bayesian Linear Regression",
    "section": "Back to the posterior",
    "text": "Back to the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#how-can-we-use-the-posterior",
    "href": "slides/03-linear-regression.html#how-can-we-use-the-posterior",
    "title": "Bayesian Linear Regression",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nLet’s simulate some data again:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n###Define hyperparameteters\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\na &lt;- 3\nb &lt;- 1"
  },
  {
    "objectID": "slides/03-linear-regression.html#visualize-simulated-data",
    "href": "slides/03-linear-regression.html#visualize-simulated-data",
    "title": "Bayesian Linear Regression",
    "section": "Visualize simulated data",
    "text": "Visualize simulated data"
  },
  {
    "objectID": "slides/03-linear-regression.html#comparing-to-olsmle",
    "href": "slides/03-linear-regression.html#comparing-to-olsmle",
    "title": "Bayesian Linear Regression",
    "section": "Comparing to OLS/MLE",
    "text": "Comparing to OLS/MLE\nFirst, we define hyperparameteters.\n\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\n\nWe then compute the posterior moments and compare to OLS/MLE.\n\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\n\n\n\n\n\nparameter\ntrue\nbayes\nols\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984"
  },
  {
    "objectID": "slides/03-linear-regression.html#summarizing-the-posterior",
    "href": "slides/03-linear-regression.html#summarizing-the-posterior",
    "title": "Bayesian Linear Regression",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\n\nSummarizing a posterior (with known parametric form) is straightforward for basic quantities of interest\n\n\nPosterior means, medians, modes, and variances\nJoint, conditional, and marginal probabilities, for example: \\(P(\\beta_j &lt; c | \\mathbf{Y})\\)\n\\(\\alpha\\)-quantiles: \\(\\{q_{\\alpha} : P(\\beta_j &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n\\(\\ldots\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#summarizing-the-posterior-1",
    "href": "slides/03-linear-regression.html#summarizing-the-posterior-1",
    "title": "Bayesian Linear Regression",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\beta_j &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pnorm}\\))\n\n\nThese methods work well for standard posterior quantities and distributions"
  },
  {
    "objectID": "slides/03-linear-regression.html#summarizing-the-posterior-2",
    "href": "slides/03-linear-regression.html#summarizing-the-posterior-2",
    "title": "Bayesian Linear Regression",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-linear-regression.html#monte-carlo-mc-approximation",
    "href": "slides/03-linear-regression.html#monte-carlo-mc-approximation",
    "title": "Probability and Bayesian Statistics",
    "section": "",
    "text": "Integration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#mc-approximation",
    "href": "slides/03-linear-regression.html#mc-approximation",
    "title": "Probability and Bayesian Statistics",
    "section": "",
    "text": "Let \\(g\\left(\\boldsymbol{\\beta}\\right)\\) be (just about) any function of \\(\\boldsymbol{\\beta}\\). The law of large numbers says that if,\n\\[\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right),\\] then, \\[\\frac{1}{S}\\sum_{s=1}^S g\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\boldsymbol{\\beta}\\right)f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)d\\boldsymbol{\\beta},\\] as \\(S\\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/03-linear-regression.html#mc-approximation-1",
    "href": "slides/03-linear-regression.html#mc-approximation-1",
    "title": "Probability and Bayesian Statistics",
    "section": "",
    "text": "Implications (as \\(S\\rightarrow \\infty\\)):\n\n\\(\\overline{\\boldsymbol{\\beta}}=\\frac{1}{S}\\sum_{s=1}^S \\boldsymbol{\\beta}^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\boldsymbol{\\beta}|\\mathbf{Y}\\right]\\)\n\\(\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\boldsymbol{\\beta}^{\\left(s\\right)}-\\overline{\\boldsymbol{\\beta}}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\n\\(\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\leq \\mathbf{c}\\right) \\rightarrow P\\left(\\boldsymbol{\\beta}\\leq \\mathbf{c} | \\mathbf{Y}\\right)\\)\n\\(\\alpha\\)-quantile of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}\\)\n\nJust about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample"
  },
  {
    "objectID": "slides/03-linear-regression.html#posterior-inference-for-arbitrary-functions",
    "href": "slides/03-linear-regression.html#posterior-inference-for-arbitrary-functions",
    "title": "Probability and Bayesian Statistics",
    "section": "",
    "text": "Interest in the posterior distribution of a function of \\(\\boldsymbol{\\beta}\\), \\(g\\left(\\boldsymbol{\\beta}\\right)\\)\n\nMC sampling plan:\n\nDraw \\(\\boldsymbol{\\beta}^{\\left(1\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right)\\)\n\\(\\ldots\\)\nDraw \\(\\boldsymbol{\\beta}^{\\left(S\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\)\n\n\\(\\Rightarrow g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right),\\ldots,g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right)\\)\nTherefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)"
  },
  {
    "objectID": "slides/02-probability.html#returning-to-linear-regression",
    "href": "slides/02-probability.html#returning-to-linear-regression",
    "title": "Probability and Bayesian Statistics",
    "section": "Returning to linear regression",
    "text": "Returning to linear regression\nLet’s obtain \\(S = 1,000\\) samples from our posterior.\n\nlibrary(mvtnorm) # multivariate rng\nbeta_samples &lt;- rmvnorm(1000, mean_beta, var_beta)\n\nWe can compute the posterior mean and variance.\n\nprint(apply(beta_samples, 2, mean))\n\n[1] -1.474821  3.298521\n\nprint(apply(beta_samples, 2, var))\n\n[1] 0.02316256 0.02189517"
  },
  {
    "objectID": "slides/02-probability.html#assessing-accuracy",
    "href": "slides/02-probability.html#assessing-accuracy",
    "title": "Probability and Bayesian Statistics",
    "section": "Assessing accuracy",
    "text": "Assessing accuracy"
  },
  {
    "objectID": "slides/02-probability.html#how-do-we-know-how-many-samples-to-take",
    "href": "slides/02-probability.html#how-do-we-know-how-many-samples-to-take",
    "title": "Probability and Bayesian Statistics",
    "section": "How do we know how many samples to take?",
    "text": "How do we know how many samples to take?\n\nWe can use a central limit theorem: \\(\\frac{\\sqrt{S}\\left(\\overline{\\boldsymbol{\\beta}}-\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}]\\right)}{\\sqrt{\\mathbb{V}\\left(\\overline{\\boldsymbol{\\beta}}\\right)}} \\stackrel{d}{\\rightarrow} \\text{N}\\left(0,1\\right)\\)\n\nwhere \\(\\mathbb{V}\\left(\\overline{\\boldsymbol{\\beta}}\\right) = \\mathbb{V}\\left(\\frac{1}{S}\\sum_{s=1}^S \\boldsymbol{\\beta}^{(s)}\\right) = \\frac{1}{S^2}\\sum_{s=1}^S\\mathbb{V}\\left(\\boldsymbol{\\beta}^{(s)}\\right) = \\frac{1}{S}\\mathbb{V}\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)\\).\n\\(\\theta=\\text{E}\\left[\\theta|\\boldsymbol{Y}=\\boldsymbol{y}\\right]\\),\\(\\sigma=\\sqrt{\\text{Var}\\left[\\theta|\\boldsymbol{Y}=\\boldsymbol{y}\\right]}\\) - \\(\\Rightarrow \\overline{\\theta}\\approx \\text{N}\\left(\\theta,\\sigma^2/S\\right)\\)\n\nMC standard error: \\(\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\), \\(\\widehat{\\sigma}^2=\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\theta^{\\left(s\\right)}-\\overline{\\theta}\\right)^2\\)\nApproximate 95% MC confidence interval for the posterior mean: \\(\\overline{\\theta} \\pm 2\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\)\nChoose \\(S\\) large enough to report the posterior mean with your desired precision\nReporting MC standard errors for the posterior mean is a good way to indicate that \\(S\\) is large enough"
  },
  {
    "objectID": "slides/02-probability.html#how-many-samples-to-take",
    "href": "slides/02-probability.html#how-many-samples-to-take",
    "title": "Probability and Bayesian Statistics",
    "section": "How many samples to take?",
    "text": "How many samples to take?\nWe can use a central limit theorem: \\(\\sqrt{S}\\left(\\overline{\\boldsymbol{\\beta}}-\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}]\\right)/\\sigma \\stackrel{d}{\\rightarrow} \\text{N}\\left(0,1\\right)\\),\nwhere \\(\\sigma^2 = \\mathbb{V}\\left(\\overline{\\boldsymbol{\\beta}}\\right) = \\frac{1}{S^2}\\sum_{s=1}^S\\mathbb{V}\\left(\\boldsymbol{\\beta}^{(s)}\\right) = \\frac{1}{S}\\mathbb{V}\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)\\).\n\\(\\implies \\overline{\\boldsymbol{\\beta}}\\approx N\\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}],\\sigma^2/S\\right)\\)\n\nMC standard error: \\(\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\), \\(\\widehat{\\sigma}^2=\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}-\\overline{\\boldsymbol{\\beta}}\\right)^2\\)\nApproximate 95% MC confidence interval for the posterior mean: \\(\\overline{\\boldsymbol{\\beta}} \\pm 2\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\)\nChoose \\(S\\) large enough to report the posterior mean with your desired precision\nReporting MC standard errors for the posterior mean is a good way to indicate that \\(S\\) is large enough"
  },
  {
    "objectID": "slides/02-probability.html#additional-posterior-summaries",
    "href": "slides/02-probability.html#additional-posterior-summaries",
    "title": "Probability and Bayesian Statistics",
    "section": "Additional posterior summaries",
    "text": "Additional posterior summaries\n\n# median\napply(beta_samples, 2, median)\n\n[1] -1.475117  3.292516\n\n# 95% credible intervals\napply(beta_samples, 2, function(x) quantile(x, probs = c(0.025, 0.975)))\n\n           [,1]     [,2]\n2.5%  -1.763541 3.017254\n97.5% -1.188284 3.590548\n\n# evaluating probability\nmean(beta_samples[, 1] &lt; -1.5)\n\n[1] 0.4358\n\n# summarizing arbitrary functions of the parameters\nbeta_new &lt;- beta_samples[, 1] * beta_samples[, 2]^3\nc(mean(beta_new), quantile(beta_new, probs = c(0.025, 0.975)))\n\n               2.5%     97.5% \n-53.15221 -71.43863 -38.00378 \n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/01-welcome.html#prepare-for-next-week",
    "href": "slides/01-welcome.html#prepare-for-next-week",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Prepare for next week",
    "text": "Prepare for next week\n\nComplete HW 00 tasks\nReview syllabus\nComplete reading to prepare for Tuesday’s lecture\nTuesday’s lecture: Monte Carlo Sampling"
  },
  {
    "objectID": "slides/03-linear-regression.html#review-of-last-lecture",
    "href": "slides/03-linear-regression.html#review-of-last-lecture",
    "title": "Bayesian Linear Regression",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Tuesday, we performed posterior inference for Bayesian linear regression, but we assumed the measurement error (\\(\\sigma\\)) was known!\n\nWe did this so we could sample from a closed form posterior.\nMonte Carlo approximation.\n\nWe will consider the same model, \\[\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n)\\]\n\nIn today’s lecture we will estimate both \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\). This will require a new algorithm called Gibbs sampling."
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampling-1",
    "href": "slides/03-linear-regression.html#gibbs-sampling-1",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampling",
    "text": "Gibbs sampling\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampling-for-linear-regression",
    "href": "slides/03-linear-regression.html#gibbs-sampling-for-linear-regression",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampling for linear regression",
    "text": "Gibbs sampling for linear regression\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta, \\sigma^2})d\\boldsymbol{\\beta}d\\sigma^2}.\n\\end{aligned}\\]\nNo closed form exists for the posterior. \\[f(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)\\]\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-linear-regression.html#posterior-for-linear-regression",
    "href": "slides/03-linear-regression.html#posterior-for-linear-regression",
    "title": "Bayesian Linear Regression",
    "section": "Posterior for linear regression",
    "text": "Posterior for linear regression\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)d\\boldsymbol{\\beta}d\\sigma^2}.\n\\end{aligned}\\]\nNo closed form exists for the posterior. \\[f(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression",
    "href": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\nSuppose we can sample from the following two distribution,\n\n\\(f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta})\\)\n\\(f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\sigma^2)\\)\n\nThese are called full conditional distributions.\nSet initial values for \\(\\boldsymbol{\\theta}^{(0)} = (\\boldsymbol{\\beta}^{(0)}, \\sigma^{2(0)})\\). Then, given a current state of parameters \\(\\boldsymbol{\\theta}^{(s)}\\), we can generate a new state as follows:\n\nSample \\(\\boldsymbol{\\beta}^{(s + 1)} \\sim f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^{2(s)})\\)\nSample \\(\\sigma^{2(s + 1)} \\sim f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}^{(s + 1)})\\)\nLet \\(\\boldsymbol{\\theta}^{(s+1)} = (\\boldsymbol{\\beta}^{(s + 1)}, \\sigma^{2(s + 1)})\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#why-does-this-work",
    "href": "slides/03-linear-regression.html#why-does-this-work",
    "title": "Bayesian Linear Regression",
    "section": "Why does this work?",
    "text": "Why does this work?\n\n\\(\\boldsymbol{\\theta}^{(0)}\\) isn’t a sample from the posterior, it is an arbitrarily chosen initial value\n\\(\\boldsymbol{\\theta}^{(1)}\\) likely isn’t from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\)\n\\(\\boldsymbol{\\theta}^{(2)}\\) likely isn’t from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\) and \\(\\boldsymbol{\\theta}^{(1)}\\)\nTheorem: For any initial values, the chain will eventually converge to the posterior\nTheorem: If \\(\\boldsymbol{\\theta}^{(s)}\\) is a sample from the posterior, then \\(\\boldsymbol{\\theta}^{(s+1)}\\) is too"
  },
  {
    "objectID": "slides/03-linear-regression.html#motivation-for-gibbs-sampling",
    "href": "slides/03-linear-regression.html#motivation-for-gibbs-sampling",
    "title": "Bayesian Linear Regression",
    "section": "Motivation for Gibbs sampling",
    "text": "Motivation for Gibbs sampling\n\nSuppose we were given \\(\\sigma^{2(1)}\\), a single sample from the marginal posterior distribution \\(f\\left(\\sigma^2|\\mathbf{Y}\\right)\\) (from where, who knows?)\nUse the sample to generate \\(\\boldsymbol{\\beta}^{(1)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y},\\sigma^{2(1)}\\right)\\)\n\\(\\left(\\boldsymbol{\\beta}^{(1)},\\sigma^{2(1)}\\right)\\) is a sample from \\(f\\left(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}\\right)\\)\n\\(\\boldsymbol{\\beta}^{(1)}\\) is a sample from \\(f\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)\\)\n\n\n\n\n\n\n\n\nRecall\n\n\n\\(f\\left(\\boldsymbol{\\beta}, \\sigma^{2}|\\mathbf{Y}\\right) = f\\left(\\boldsymbol{\\beta} | \\sigma^{2},\\mathbf{Y}\\right)f\\left(\\sigma^{2}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampler",
    "href": "slides/03-linear-regression.html#gibbs-sampler",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nUnder mild regulatory conditions that are generally satisfied for most statistical models, one can show that the iteration \\(\\boldsymbol{\\theta}^{(s)}\\) converges in distribution to a draw from the true joint posterior distribution\nSo for \\(s\\) sufficiently large (say, bigger than \\(s_0\\)), \\(\\left\\{\\boldsymbol{\\theta}^{(s)}, s=s_0+1,\\ldots,S\\right\\}\\) is a correlated sample from the true joint posterior (and \\(\\boldsymbol{\\beta}^{(s)}\\) and \\(\\sigma^{2(s)}\\) are samples from the marginals)\nSimilar to Monte Carlo approximation, we can use these samples to estimate posterior quantities of interest"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampler-1",
    "href": "slides/03-linear-regression.html#gibbs-sampler-1",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\n\\(\\boldsymbol{\\theta}^{(t)}\\) depends on \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-1)}\\) only through \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\boldsymbol{\\theta}^{(t)}\\) is conditionally independent of \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-2)}\\) given \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\implies\\) Markov property, so the sequence is called a Markov chain\nWe use the samples similar to MC approximation; therefore, Gibbs sampling is a form of Markov chain Monte Carlo (MCMC)\nWe will cover diagnostics for MCMC in another lecture!"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression-1",
    "href": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression-1",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nComputing the full conditionals.\n\nWe already have the full conditional for \\(\\boldsymbol{\\beta}\\):\n\n\\(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2 \\sim N \\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}], \\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y})\\right)\\)\n\n\\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression-2",
    "href": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression-2",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nComputing the full conditionals.\n\n\\(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2 \\sim N \\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}], \\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y})\\right)\\)\nFull conditional for \\(\\sigma^2\\), assuming \\(f(\\sigma^2) \\sim IG(a, b)\\):\n\n\\[\\sigma^2 |  \\mathbf{Y} , \\boldsymbol{\\beta} \\sim IG\\left(a + \\frac{n}{2},b+\\frac{\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)}{2}\\right)\\]\n\nWhy inverse-Gamma (\\(IG\\)) distribution for \\(\\sigma^2\\)?"
  },
  {
    "objectID": "slides/03-linear-regression.html#perform-gibbs-sampling",
    "href": "slides/03-linear-regression.html#perform-gibbs-sampling",
    "title": "Bayesian Linear Regression",
    "section": "Perform Gibbs sampling",
    "text": "Perform Gibbs sampling\n\nsigma2 &lt;- exp(rnorm(1)) # initial value\nsamples &lt;- NULL\nfor (s in 1:5000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n  \n  ###Sample from full conditional for sigma2\n  quadratic &lt;- as.numeric(t(Y - X %*% beta) %*% (Y - X %*% beta))\n  sigma2 &lt;- 1 / rgamma(1, shape = a + n / 2, rate = b + quadratic / 2)\n  \n  ###Save samples after a burn-in\n  samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-linear-regression.html#inspect-results",
    "href": "slides/03-linear-regression.html#inspect-results",
    "title": "Bayesian Linear Regression",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-linear-regression.html#parameter-estimation-vs.-posterior-approximation",
    "href": "slides/03-linear-regression.html#parameter-estimation-vs.-posterior-approximation",
    "title": "Bayesian Linear Regression",
    "section": "Parameter estimation vs. posterior approximation",
    "text": "Parameter estimation vs. posterior approximation\n\nModel specification: Choice of likelihood and introduction of model parameters\nPrior specification\nCalculation of the posterior\nSummarizing the posterior using MC or MCMC methods:\n\nThese are not models!\nThey do not generate more information than is in \\(\\mathbf{Y}\\) or \\(f\\left(\\boldsymbol{\\theta}\\right)\\)\nThey are simply ways of looking at \\(f\\left(\\boldsymbol{\\theta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#additional-topic-metropolis-sampling",
    "href": "slides/03-linear-regression.html#additional-topic-metropolis-sampling",
    "title": "Bayesian Linear Regression",
    "section": "Additional topic: Metropolis sampling",
    "text": "Additional topic: Metropolis sampling\n\nBefore we start using Stan for probabilistic programming, we need to understand the MCMC algorithm that is the engine for Stan’s inference, Hamiltonian Monte Carlo.\nTo get us one step closer we will quickly review the concept of Metropolis sampling, another MCMC variant"
  },
  {
    "objectID": "slides/03-linear-regression.html#intuition-behind-metropolis-samping",
    "href": "slides/03-linear-regression.html#intuition-behind-metropolis-samping",
    "title": "Bayesian Linear Regression",
    "section": "Intuition behind Metropolis samping",
    "text": "Intuition behind Metropolis samping\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let’s consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)’s in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)’s.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-linear-regression.html#metropolis-acceptance-ratio",
    "href": "slides/03-linear-regression.html#metropolis-acceptance-ratio",
    "title": "Bayesian Linear Regression",
    "section": "Metropolis acceptance ratio",
    "text": "Metropolis acceptance ratio\n\nFortunately, the comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) can be made even if we cannot compute \\(f(\\boldsymbol{\\theta} | \\mathbf{Y})\\).\n\n\\[\\begin{aligned}\nr &= \\frac{f(\\boldsymbol{\\theta}^* | \\mathbf{Y})}{f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y})}\\frac{f(\\mathbf{Y})}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\n\\end{aligned}\\]\nHaving computed \\(r\\), how should we proceed?"
  },
  {
    "objectID": "slides/03-linear-regression.html#metropolis-update",
    "href": "slides/03-linear-regression.html#metropolis-update",
    "title": "Bayesian Linear Regression",
    "section": "Metropolis update",
    "text": "Metropolis update\nGiven \\(\\boldsymbol{\\theta}^{(s)}\\), the Metropolis algorithm generates a value \\(\\boldsymbol{\\theta}^{(s + 1)}\\) as follows:\n\nSample \\(\\boldsymbol{\\theta}^*\\) from a proposal distribution, \\(\\boldsymbol{\\theta}^* ∼ J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\)\nCompute the acceptance ratio \\(r\\)\nLet \\[\\boldsymbol{\\theta}^{(s + 1)} =\n\\left\\{\n  \\begin{array}{ll}\n\\boldsymbol{\\theta}^* & \\text{with probability }\\min(r, 1) \\\\\n\\boldsymbol{\\theta}^{(s)} & \\text{with probability }1 -\\min(r, 1)\n  \\end{array}\n\\right.\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#metropolis-intuition",
    "href": "slides/03-linear-regression.html#metropolis-intuition",
    "title": "Bayesian Linear Regression",
    "section": "Metropolis intuition",
    "text": "Metropolis intuition\nMetropolis ratio: \\(r = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\)\nIf \\(r &gt; 1:\\)\n\nIntuition: Since \\(\\boldsymbol{\\theta}^{(s)}\\) is already in our set, we should include \\(\\boldsymbol{\\theta}^*\\) as it has a higher probability than \\(\\boldsymbol{\\theta}^{(s)}\\)\nProcedure: Accept \\(\\boldsymbol{\\theta}^*\\) into our set (i.e., set \\(\\boldsymbol{\\theta}^{(s + 1)} = \\boldsymbol{\\theta}^*\\))\n\nIf \\(r &lt; 1:\\)\n\nIntuition: The relative frequency of \\(\\boldsymbol{\\theta}\\)-values in our set equal to \\(\\boldsymbol{\\theta}^*\\) compared to those equal to \\(\\boldsymbol{\\theta}^{(s)}\\) should be \\(r\\). This means that for every instance of \\(\\boldsymbol{\\theta}^{(s)}\\), we should have only a “fraction” of an instance of a \\(\\boldsymbol{\\theta}^*\\) value.\nProcedure: Set \\(\\boldsymbol{\\theta}^{(s + 1)}\\) equal to either \\(\\boldsymbol{\\theta}^*\\) or \\(\\boldsymbol{\\theta}^{(s)}\\), with probability \\(r\\) and \\(1 − r\\) respectively."
  },
  {
    "objectID": "slides/03-linear-regression.html#metropolis-proposal-distribution",
    "href": "slides/03-linear-regression.html#metropolis-proposal-distribution",
    "title": "Bayesian Linear Regression",
    "section": "Metropolis proposal distribution",
    "text": "Metropolis proposal distribution\n\nThe proposal distribution is symmetric (i.e., \\(J(\\boldsymbol{\\theta}_a | \\boldsymbol{\\theta}_b) = J(\\boldsymbol{\\theta}_b | \\boldsymbol{\\theta}_a)\\)\nUsually \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) is very simple, with samples from \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) being near \\(\\boldsymbol{\\theta}\\) with high probability.\nThe most common proposal is a normal distribution\n\n\\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)}) = N(\\boldsymbol{\\theta}^{(s)}, \\boldsymbol{\\Delta})\\)\n\nThe value of the parameter \\(\\boldsymbol{\\Delta}\\) is generally chosen to make the approximation algorithm run efficiently"
  },
  {
    "objectID": "slides/03-linear-regression.html#metropolis-and-gibbs-combined",
    "href": "slides/03-linear-regression.html#metropolis-and-gibbs-combined",
    "title": "Bayesian Linear Regression",
    "section": "Metropolis and Gibbs combined",
    "text": "Metropolis and Gibbs combined\n\nThe Gibbs and Metropolis samplers are actually both algorithms within a larger class of Metropolis-Hastings algorithms\nWhen performing MCMC, one can actually choose to update a parameter using either a Gibbs or Metropolis update\nLet’s see this in action using our linear regression example"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-using-metropolisgibbs",
    "href": "slides/03-linear-regression.html#linear-regression-using-metropolisgibbs",
    "title": "Bayesian Linear Regression",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nIn this example, we will use a Metropolis update for \\(\\sigma^2\\), however we will actually focus on \\(\\log\\sigma^2\\).\n\nMetropolis requires a symmetric proposal, so it is often easier to transform parameters to be on the real line and use a normal proposal.\n\nWe will use the following proposal, \\(\\log\\sigma^{2*} \\sim N\\left(\\log\\sigma^{2(s)}, \\delta\\right)\\), where \\(\\delta = 1\\).\nWe will place \\(\\log\\sigma^2 \\sim N(0,1)\\)."
  },
  {
    "objectID": "slides/03-linear-regression.html#inspect-results-1",
    "href": "slides/03-linear-regression.html#inspect-results-1",
    "title": "Bayesian Linear Regression",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-linear-regression.html#looking-towards-stan",
    "href": "slides/03-linear-regression.html#looking-towards-stan",
    "title": "Bayesian Linear Regression",
    "section": "Looking towards Stan",
    "text": "Looking towards Stan\n\nWe can use Monte Carlo approximation, when the posterior is available in closed form.\nWe can use Gibbs sampling when the full conditional distributions are available in closed form.\nWe can always use Metropolis (or its more general form Metropolis Hastings) regardless of the form of the posterior, we only need to be able to compute \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})\\)\n\nThis amounts to specifying a likelihood and a prior (which is the fun modeling part!)\nMetropolis can be difficult to tune (i.e., finding \\(\\boldsymbol{\\Delta}\\))"
  },
  {
    "objectID": "hw/hw-01-creation.html",
    "href": "hw/hw-01-creation.html",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "",
    "text": "Due date\n\n\n\nThis assignment is due on Thursday, January 30 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#learning-goals",
    "href": "ae/ae-01-linear-regression.html#learning-goals",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nPerform Gibbs sampling for Bayesian linear regression and compute some basic summaries"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-01-linear-regression.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-01-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-01-linear-regression.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#r-and-r-studio",
    "href": "ae/ae-01-linear-regression.html#r-and-r-studio",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we may tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!"
  },
  {
    "objectID": "ae/ae-01-test.html",
    "href": "ae/ae-01-test.html",
    "title": "Welcome to BIOSTAT 725!",
    "section": "",
    "text": "\\(1+1=2 = \\int 2 dx\\)"
  },
  {
    "objectID": "hw/hw-00.html#reproducibility-checklist",
    "href": "hw/hw-00.html#reproducibility-checklist",
    "title": "HW 00",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n. . .\nNear term goals:\n✔️ Can the tables and figures be exactly reproduced from the code and data?\n✔️ Does the code actually do what you think it does?\n✔️ In addition to what was done, is it clear why it was done?\n. . .\nLong term goals:\n✔️ Can the code be used for other data?\n✔️ Can you extend the code to do other things?",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#why-is-reproducibility-important",
    "href": "hw/hw-00.html#why-is-reproducibility-important",
    "title": "HW 00",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#toolkit",
    "href": "hw/hw-00.html#toolkit",
    "title": "HW 00",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#r-and-rstudio",
    "href": "hw/hw-00.html#r-and-rstudio",
    "title": "HW 00",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\n\n\nSource: Statistical Inference via Data Science",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#rstudio-ide",
    "href": "hw/hw-00.html#rstudio-ide",
    "title": "HW 00",
    "section": "RStudio IDE",
    "text": "RStudio IDE",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#quarto",
    "href": "hw/hw-00.html#quarto",
    "title": "HW 00",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports – the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#quarto-1",
    "href": "hw/hw-00.html#quarto-1",
    "title": "HW 00",
    "section": "Quarto",
    "text": "Quarto",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#how-will-we-use-quarto",
    "href": "hw/hw-00.html#how-will-we-use-quarto",
    "title": "HW 00",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou’ll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#what-is-versioning",
    "href": "hw/hw-00.html#what-is-versioning",
    "title": "HW 00",
    "section": "What is versioning?",
    "text": "What is versioning?",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#what-is-versioning-1",
    "href": "hw/hw-00.html#what-is-versioning-1",
    "title": "HW 00",
    "section": "What is versioning?",
    "text": "What is versioning?\nwith human readable messages",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#why-do-we-need-version-control",
    "href": "hw/hw-00.html#why-do-we-need-version-control",
    "title": "HW 00",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\n\n\n\n\n\n\n\nProvides a clear record of how the analysis methods evolved. This makes analysis auditable and thus more trustworthy and reliable. [@ostblom2022]",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#git-and-github-1",
    "href": "hw/hw-00.html#git-and-github-1",
    "title": "HW 00",
    "section": "git and GitHub",
    "text": "git and GitHub\n\n\n\n\n\n\ngit is a version control system – like “Track Changes” features from Microsoft Word.\nGitHub is the home for your git-based projects on the internet (like DropBox but much better).\nThere are a lot of git commands and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, project, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username.\n\n\n\nIf you already have a GitHub account, you can move on to the next step.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "prepare/prepare-jan14.html",
    "href": "prepare/prepare-jan14.html",
    "title": "Prepare for January 14 lecture",
    "section": "",
    "text": "Occasionally during the semester we will reference material from the text book A First Course in Bayesian Statistical Methods by Peter Hoff. As a Duke student, an electronic version of the book is freely available to you through the Duke Library. We will refer to this textbook as Hoff.\n📖 Read about Bayesian inference and probability in Hoff Chapter 1 and 2\n📖 Read about Monte Carlo sampling in Hoff Chapter 4, Sections 4.1 and 4.2\n✅ Complete HW 00 tasks"
  },
  {
    "objectID": "slides/03-linear-regression.html#recall",
    "href": "slides/03-linear-regression.html#recall",
    "title": "Bayesian Linear Regression",
    "section": "Recall",
    "text": "Recall\n\\(f\\left(\\boldsymbol{\\beta}, \\sigma^{2}|\\mathbf{Y}\\right) = f\\left(\\boldsymbol{\\beta} | \\sigma^{2},\\mathbf{Y}\\right)f\\left(\\sigma^{2}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-using-metropolisgibbs-1",
    "href": "slides/03-linear-regression.html#linear-regression-using-metropolisgibbs-1",
    "title": "Bayesian Linear Regression",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nsigma2 &lt;- exp(rnorm(1))\nsamples &lt;- NULL\ndelta &lt;- 1\nfor (s in 1:10000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n\n  ###Metropolis update for sigma2\n  # Sample a proposal value\n  log_sigma2_proposal &lt;- rnorm(1, log(sigma2), delta)\n  # Compute the ratio r on the log scale for numeric stability\n  # Also, I've decided to update log(sigma2) instead of sigma2, so I can use a normal proposal distribution\n  # I've placed a normal prior on log(sigma2)\n  likelihood_proposal &lt;- sum(dnorm(Y, X %*% beta, sqrt(exp(log_sigma2_proposal)), log = TRUE))\n  likelihood_current &lt;- sum(dnorm(Y, X %*% beta, sqrt(sigma2), log = TRUE))\n  prior_proposal &lt;- dnorm(log_sigma2_proposal, 0, 1, log = TRUE)\n  prior_current &lt;- dnorm(log(sigma2), 0, 1, log = TRUE)\n  log_r &lt;- (likelihood_proposal + prior_proposal) - (likelihood_current + prior_current)\n  # Update beta using Metropolis ratio\n  if (log(runif(1)) &lt; log_r) sigma2 &lt;- exp(log_sigma2_proposal)\n\n  ###Save samples after a burn-in\n  if (s &gt; 5000) samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-linear-regression.html#looking-towards-stan-1",
    "href": "slides/03-linear-regression.html#looking-towards-stan-1",
    "title": "Bayesian Linear Regression",
    "section": "Looking towards Stan",
    "text": "Looking towards Stan\n\nStan uses an algorithm called Hamiltonian Monte Carlo, which is a form of MCMC that uses a Metropolis update\nStan does all of the MCMC tuning, allowing us to only focus on the modeling!\nThis means that our job moving forward will be to focus on specifying the\n\nlikelihood: \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})\\)\nprior: \\(f(\\boldsymbol{\\theta})\\)\n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#insepction-of-gibbs-sampler",
    "href": "ae/ae-01-linear-regression.html#insepction-of-gibbs-sampler",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "Insepction of Gibbs sampler",
    "text": "Insepction of Gibbs sampler\nWe can begin by inspecting the posterior density and traceplots.\ndat.fig &lt;- data.frame(\n  parameter = rep(c(\"beta[0]\", \"beta[1]\", \"sigma^2\"), each = 5000),\n  index = rep(1:5000, 3),\n  value = as.numeric(samples)\n)\nggplot(dat.fig, aes(x = value)) +\n  geom_density(lwd = 1.5) +\n  facet_grid(. ~ parameter, labeller = label_parsed, scales = \"free_x\") +\n  ylab(\"Density\") +\n  xlab(\"Parameter value\")\nggplot(dat.fig, aes(x = index, y = value)) + \n  geom_line(lwd = 0.5) + \n  facet_grid(. ~ parameter, labeller = label_parsed, scales = \"free_x\") + \n  ylab(\"Parameter value\") +\n  xlab(\"Sample index\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe traceplots do not exhibit much autocorrelation, indicating we are looking at samples from the posterior.\nTo recover the regression parameters on their original scale, we can use the following transformation, where \\(\\mu_Y\\), \\(\\sigma_Y\\) are the mean and standard deviation for our outcome, price, \\(\\mu_X\\), \\(\\sigma_X\\) are the mean and standard deviation for out the predictor, area, and \\(\\beta_0^*\\), \\(\\beta_1^*\\) are the regression parameters obtained from the Gibbs sampler above (i.e., on the transformed data).\n\\[\\begin{align*}\n\\beta_0 &= \\mu_Y + \\sigma_Y \\beta_0^* - \\frac{\\sigma_Y}{\\sigma_X} \\mu_X \\beta_1^*\\\\\n\\beta_1 &= \\frac{\\sigma_Y}{\\sigma_X} \\beta_1^*\n\\end{align*}\\]\nUsing this transformation, \\(\\beta_0\\) and \\(\\beta_1\\) are on the original scale.\n\nmean_y &lt;- mean(duke_forest$price)\nsd_y &lt;- sd(duke_forest$price)\nmean_x &lt;- mean(duke_forest$area)\nsd_x &lt;- sd(duke_forest$area)\nintercept &lt;- mean_y + sd_y * samples[, 1] - (sd_y / sd_x) * mean_x * samples[, 2]\nslope &lt;- (sd_y / sd_x) * samples[, 2]\n\nA histogram of the posterior parameters can be examined as follows."
  },
  {
    "objectID": "prepare/prepare-jan16.html",
    "href": "prepare/prepare-jan16.html",
    "title": "Prepare for January 16 lecture",
    "section": "",
    "text": "📖 Read Hoff Chapter 6, Sections 6.1-6.5 to learn about Gibbs sampling\n📖 Read Hoff Chapter 10, Sections 10.2-10.4 to learn about Metropolis sampling\n📖 Review simple linear regression\n✅ Complete HW 00 tasks before class"
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-10",
    "href": "hw/hw-01-creation.html#exercise-10",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nDo you think that city expenditure on residents is a useful predictor of park access? Briefly explain your response, reporting any statistics used to make your assessment.\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-1",
    "href": "hw/hw-01-creation.html#exercise-1",
    "title": "HW 01: Bayesian principles",
    "section": "Exercise 1",
    "text": "Exercise 1\nDefine a random variable \\(Y_i\\) that represents the number of hospital visits during pregnancy for each woman \\(i\\). Assume that this random variable follows a Poisson distribution with rate \\(\\lambda\\), \\(Y_i \\stackrel{iid}{\\sim} \\text{Poisson}(\\lambda)\\). We are interested in performing statistical inference on \\(\\lambda\\) using a Bayesian approach. The researchers assume a prior distribution of \\(\\lambda \\sim \\text{Gamma}(\\text{shape = }2, \\text{rate = 2})\\), with mean \\(2/2=1\\) and variance \\(2/2^2=0.5\\).\nUnder this prior specification, what is the a priori probability that \\(\\lambda\\) is greater than 1? This is equivlent to computing \\(P(\\lambda &gt; 1)\\). Use Monte Carlo sampling.\n\nAnswer:\n\nWe can use Monte Carlo sampling for this.\n\nmean(rgamma(1000, shape = 2, rate = 2) &gt; 1)\n\n[1] 0.417\n\n\nBased on the slides from the Monte Carlo sampling lecture, we know that the Gamma prior for \\(\\lambda\\) is a conjugate prior, so that the posterior is given by: \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\)."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-2",
    "href": "hw/hw-01-creation.html#exercise-2",
    "title": "HW 01: Bayesian principles",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat is the posterior mean and the 95% credible interval for the \\(\\lambda\\)?\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-3",
    "href": "hw/hw-01-creation.html#exercise-3",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat is interpretation of the slope main effect corresponding to recreation in your model at the 0.05 significance level? Is this interpretation appropriate?\n\nAnswer: Assuming a crime level of zero, we can interpret the slope corresponding to recreation, assuming all other variables are fixed. The interpretation is that for a one unit increase in recreation (i.e., number of recreational centers within a one-mile radius), exercise MET per week will increase by 2.97 MET. The p-value for this effect is significant at the 0.05 level, therefore we can conclude that for those people who live in areas of zero crime, there is an association between access to recreational facilities and exercise. This interpretation is likely not appropriate, because there are no areas with zero crime."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-4",
    "href": "hw/hw-01-creation.html#exercise-4",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the association between access to recreational facilities and exercise, for a pregnant women living in an area with 5 annual crimes/1,000 people? What about for 15 annual crimes/1,000 people?\n\nAnswer: The association between access to recreational facilities and exercise is a function of crime. In particular, this effect is given by the following: \\(\\beta_{Rec} + \\beta_{Rec \\times Crime} \\times \\texttt{crime}_i = 2.969 - 0.298 \\times  \\texttt{crime}_i\\). Plugging in for a crime value of 5, we get: 1.479. For a crime value of 15, we get a slope of -1.501."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-5",
    "href": "hw/hw-01-creation.html#exercise-5",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret the slopes from (d) and compare and contrast them. What do these slopes say about the impact of crime on the relationship between access to recreational facilities and exercise.\n\nAnswer: The interpretation of the slopes are as follows: At an annual level of crime of 5/1,000 people and keeping all of the variables constant, the association between recreational facilities and exercise has a slope of 1.479. This indicates that a one-unit increase in recreational facilities will result in a 1.5 MET increase in exercise. When crime increases to an annual level of 15/1,000 people, this slope becomes negative, -1.5. This indicates that crime is an effect modifier for the association of interest. In particular, for low levels of crime, the association is positive, however for larger values of crime the slope becomes negative."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-6",
    "href": "hw/hw-01-creation.html#exercise-6",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nAt what level of crime does the association between recreational facilities and exercise disappear?\n\nAnswer: We know that the effect of recreational facilities on exercies is a function of crime, because of the interaction term. In particular, this effect is given by the followiing: \\(\\beta_{Rec} + \\beta_{Rec \\times Crime} \\times \\texttt{crime}_i = 2.969 - 0.298 \\times  \\texttt{crime}_i\\). Therefore, we would like to set this effect to zero and solve for \\(\\texttt{crime}_i\\). Doing this, we find that \\(\\texttt{crime}_i = 2.969 / 0.298 = 9.963.\\)"
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-7",
    "href": "hw/hw-01-creation.html#exercise-7",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nTo confirm your conclusions from (f), create a visualization that demontrates the relationship between recreation and exercise, across crime levels below and above the value you found in (f). An effective figure provides a title, axis labels, and a legible legend.\n\nAnswer: We will create a scatterplot that plots recreation vs. exercise. To visualize the association across crime, we will create a new variable that is an indicator of a crime value below and above the value we found in (f), 9.963. Then, we will color code the points based on this variable. This visualization confirms our conclusions about the association between recreation and exercise, as the slope is positive for low crime and negative for high crime.\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nexercise %&gt;% \n  mutate(crime2 = 1 * (crime &gt;= 9.963)) %&gt;%\n  ggplot(aes(x = recreation, y = exercise, col = as.factor(crime2))) + \n  geom_point() + \n  scale_color_discrete(name = \"Crime &gt;= 9.963\") +\n  labs(title = \"Scatter plot of recreational facilities vs. exercise (MET)\") + \n  ylab(\"Exercises (MET/week)\") + \n  xlab(\"Number of recreational facilities within 1-mile radius\")"
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-8",
    "href": "hw/hw-01-creation.html#exercise-8",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow, assume the researchers are interested in understanding the impact of marital status on the relationship between access to recreational facililities and exercise. Setup a multivariable regression to estimate this association. There is no need to include other confounders. Does the association of interest (i.e., the relationship between access to recreational facilities and exercise) change across marital status? For a married pregnant women, what is the estimated slope of recreational facilities on exercise? What about for single pregnant women?\n\nAnswer: The regression is as follows:\n\n\nreg2 &lt;- lm(exercise ~ recreation * married, data = exercise)\nsummary(reg2)\n\n\nCall:\nlm(formula = exercise ~ recreation * married, data = exercise)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-154.734  -15.102    4.876   21.932  129.803 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         406.383     13.482  30.144   &lt;2e-16 ***\nrecreation            1.504      1.021   1.474   0.1443    \nmarried              11.693     16.742   0.698   0.4869    \nrecreation:married   -2.857      1.236  -2.311   0.0233 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 46.83 on 83 degrees of freedom\nMultiple R-squared:  0.09682,   Adjusted R-squared:  0.06417 \nF-statistic: 2.966 on 3 and 83 DF,  p-value: 0.0367\n\n\nFrom this we see that the interaction between marital status and recreational facilities is significant, with a p-value of 0.0233. The hypotheses that this corresponds to is \\(H_0:\\beta_3 = 0, H_1:\\beta_3 \\neq 0\\) at the \\(\\alpha = 0.05\\) level. Therefore, the association of interest does change across marital status. In particular, for a married pregnant woman the association (or slope) is \\(\\beta_1 + \\beta_3 = 1.504 - 2.857 = -1.353\\), and for single pregnant women, \\(\\beta_1 = 1.504\\).\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "notes/probability.html",
    "href": "notes/probability.html",
    "title": "Review of Probability",
    "section": "",
    "text": "This is foundational material that you should have already learned in a previous course. I’m reviewing important concepts that are needed for Bayesian inference.\nThe goal of Bayesian statistics is to compute the posterior distribution (i.e., the uncertainty distribution of the parameters, \\(\\boldsymbol{\\theta}\\), after observing the data, \\(\\mathbf{Y}\\))\nThis is the conditional distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{Y}\\)\nTherefore, we need to review the probability concepts that lead to the conditional distribution of one variable conditioned on another"
  },
  {
    "objectID": "notes/probability.html#review-of-probability",
    "href": "notes/probability.html#review-of-probability",
    "title": "Review of Probability",
    "section": "Review of probability",
    "text": "Review of probability\n\nThe goal of Bayesian statistics is to compute the posterior distribution (i.e., the uncertainty distribution of the parameters, \\(\\boldsymbol{\\theta}\\), after observing the data, \\(\\mathbf{Y}\\))\nThis is the conditional distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{Y}\\)\nTherefore, we need to review the probability concepts that lead to the conditional distribution of one variable conditioned on another\n\nProbability mass (pmf) and density (pdf) functions\nJoint distributions\nMarginal and conditional distributions"
  },
  {
    "objectID": "notes/probability.html#random-variables",
    "href": "notes/probability.html#random-variables",
    "title": "Review of Probability",
    "section": "Random variables",
    "text": "Random variables\n\\(X\\) (capital) is a random variable. We want to compute the probability that \\(X\\) takes on a specific value \\(x\\) (lowercase)\n\nThis is denoted \\(P(X = x)\\)\n\nWe also might want to compute the probability of \\(X\\) being in a set \\(\\mathcal A\\).\n\nThis is denoted \\(P(X \\in \\mathcal A)\\)\n\nThe set of possible values that \\(X\\) can take on is called its support, \\(\\mathcal S\\)\n\n\n\n\n\n\nRandom variables - example\n\n\n\n\nExample 1: \\(X\\) is the roll of a die\n\nThe support is \\(\\mathcal S = \\{1, 2, 3, 4, 5, 6\\}\\)\n\\(P(X = 1) = 1/6\\)\n\nExample 2: \\(X\\) is a newborn baby’s weight\n\nThe support is \\(\\mathcal S = (0, \\infty)\\)\n\\(P(X \\in [0, \\infty]) = 1\\)\n\n\n\n\n\nWhat is probability?\nObjective (associated with frequentist)\n\n\\(P(X = x)\\) as a purely mathematical statement\nIf we repeatedly sampled \\(X\\), the the proportion of draws equal to \\(x\\) converges to \\(P(X = x)\\)\n\nSubjective (associated with Bayesian)\n\n\\(P(X = x)\\) represents an individual’s degree of belief\nOften quantified as the amount an individual would be willing to wager that \\(X\\) will be \\(x\\)\n\nA Bayesian analysis makes use of both of these concepts\n\n\nWhat is uncertainty?\nAleatoric uncertainty (likelihood)\n\nUncontrollable randomness in the experiment\nFor example, the results of a fair coin flip can never be predicted with certainty\n\nEpistemic uncertainty (prior/posterior)\n\nUncertainty about a quantity that could theoretically be known\nFor example, if we flipped a coin infinitely-many times we could know the true probability of a head\n\nA Bayesian analysis makes use of both of these concepts\n\n\nProbability versus statistics\nProbability is the forward problem\n\nWe assume we know how the data are being generated and compute the probability of events\nFor example, what is the probability of flipping 5 straight heads if the coin is fair?\n\nStatistics is the inverse problem\n\nWe use data to learn about the data-generating mechanism\nFor example, if we flipped five straight head, can we conclude the coin is biased?\n\nAny statistical analysis obviously relies on probability"
  },
  {
    "objectID": "notes/probability.html#random-variables---example",
    "href": "notes/probability.html#random-variables---example",
    "title": "Review of Probability",
    "section": "Random variables - example",
    "text": "Random variables - example\n\nExample 1: \\(X\\) is the roll of a die\n\nThe support is \\(\\mathcal S = \\{1, 2, 3, 4, 5, 6\\}\\)\n\\(P(X = 1) = 1/6\\)\n\nExample 2: \\(X\\) is a newborn baby’s weight\n\nThe support is \\(\\mathcal S = (0, \\infty)\\)\n\\(P(X \\in [0, \\infty]) = 1\\)"
  },
  {
    "objectID": "notes/probability.html#what-is-probability",
    "href": "notes/probability.html#what-is-probability",
    "title": "Review of Probability",
    "section": "What is probability?",
    "text": "What is probability?\nObjective (associated with frequentist)\n\n\\(P(X = x)\\) as a purely mathematical statement\nIf we repeatedly sampled \\(X\\), the the proportion of draws equal to \\(x\\) converges to \\(P(X = x)\\)\n\nSubjective (associated with Bayesian)\n\n\\(P(X = x)\\) represents an individual’s degree of belief\nOften quantified as the amount an individual would be willing to wager that \\(X\\) will be \\(x\\)\n\nA Bayesian analysis makes use of both of these concepts"
  },
  {
    "objectID": "notes/probability.html#what-is-uncertainty",
    "href": "notes/probability.html#what-is-uncertainty",
    "title": "Review of Probability",
    "section": "What is uncertainty?",
    "text": "What is uncertainty?\nAleatoric uncertainty (likelihood)\n\nUncontrollable randomness in the experiment\nFor example, the results of a fair coin flip can never be predicted with certainty\n\nEpistemic uncertainty (prior/posterior)\n\nUncertainty about a quantity that could theoretically be known\nFor example, if we flipped a coin infinitely-many times we could know the true probability of a head\n\nA Bayesian analysis makes use of both of these concepts"
  },
  {
    "objectID": "notes/probability.html#probability-versus-statistics",
    "href": "notes/probability.html#probability-versus-statistics",
    "title": "Review of Probability",
    "section": "Probability versus statistics",
    "text": "Probability versus statistics\nProbability is the forward problem\n\nWe assume we know how the data are being generated and compute the probability of events\nFor example, what is the probability of flipping 5 straight heads if the coin is fair?\n\nStatistics is the inverse problem\n\nWe use data to learn about the data-generating mechanism\nFor example, if we flipped five straight head, can we conclude the coin is biased?\n\nAny statistical analysis obviously relies on probability"
  },
  {
    "objectID": "notes/probability.html#univariate-distributions",
    "href": "notes/probability.html#univariate-distributions",
    "title": "Review of Probability",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables\nThe random variable \\(X\\) is discrete if its support \\(\\mathcal S\\) is countable\nExamples:\n\n\\(X \\in \\{0, 1, 2, 3\\}\\) is the number of successes in 3 trials\n\n\\(X \\in \\{0, 1, 2, \\ldots\\}\\) is the number of patients with COVID in Durham County"
  },
  {
    "objectID": "notes/probability.html#univariate-distributions-1",
    "href": "notes/probability.html#univariate-distributions-1",
    "title": "Review of Probability",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables\nThe random variable \\(X\\) is continuous if its support \\(\\mathcal S\\) is uncountable\nExamples with \\(\\mathcal S = (0, \\infty)\\):\n\n\\(X &gt; 0\\) is systolic blood pressure\n\n\\(X &gt; 0\\) is a patient’s BMI"
  },
  {
    "objectID": "notes/probability.html#discrete-univariate-distributions",
    "href": "notes/probability.html#discrete-univariate-distributions",
    "title": "Review of Probability",
    "section": "Discrete univariate distributions",
    "text": "Discrete univariate distributions\n\nIf \\(X\\) is discrete we describe its distribution with its probability mass function (pmf)\nThe pmf is \\(f(x) = P(X = x)\\)\nThe domain of \\(X\\) is the set of \\(x\\) with \\(f(x) &gt; 0\\)\nWe must have \\(f(x) \\geq 0\\) and \\(\\sum_x f(x) = 1\\)\nThe mean is \\(\\mathbb E[X] = \\sum_x x f(x)\\)\nThe variance is \\(\\mathbb V(X) = \\sum_x(x − \\mathbb E[X])^2f(x)\\)\nThe last three sums are over \\(X\\)’s domain"
  },
  {
    "objectID": "notes/probability.html#parametric-families-of-distributions",
    "href": "notes/probability.html#parametric-families-of-distributions",
    "title": "Review of Probability",
    "section": "Parametric families of distributions",
    "text": "Parametric families of distributions\n\nA statistical analysis typically proceeds by selecting a pmf that seems to match the distribution of a sample\nWe rarely know the pmf exactly, but we assume it is from a parametric family of distributions\nFor example, Binomial(10, 0.5) and Binomial(4, 0.1) are different but both from the binomial family\nA family of distributions have the same equation for the pmf but differ by some unknown parameters \\(\\boldsymbol{\\theta}\\)\nWe must estimate these parameters"
  },
  {
    "objectID": "notes/probability.html#continuous-univariate-distributions",
    "href": "notes/probability.html#continuous-univariate-distributions",
    "title": "Review of Probability",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nIf \\(X\\) is continuous we describe its distribution with the probability density function (pdf) \\(f(x) \\geq 0\\)\nSince there are uncountably many possible values, \\(P(X = x) = 0\\) for all \\(x\\),\nProbabilities are computed as areas under the pdf curve \\[P(a &lt; X &lt; b) = \\int_a^b f(x)dx\\]\nTherefore, to be valid \\(f(x)\\) must satisfy \\(f(x) \\geq 0\\) and \\[P(−\\infty &lt; X &lt; \\infty) = \\int_{-\\infty}^{\\infty} f(x)dx = 1\\]"
  },
  {
    "objectID": "notes/probability.html#continuous-univariate-distributions-1",
    "href": "notes/probability.html#continuous-univariate-distributions-1",
    "title": "Review of Probability",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nThe domain is the set of \\(x\\) values with \\(f(x) &gt; 0\\)\nThe mean and the variance are defined similarly to the discrete case but with the sums replaced by integrals\nThe mean is \\(\\mathbb E[X] = \\int x f(x)dx\\)\nThe variance is \\(\\mathbb V(X) = \\int (x − \\mathbb E[X])^2 f(x)dx\\)"
  },
  {
    "objectID": "notes/probability.html#joint-distributions",
    "href": "notes/probability.html#joint-distributions",
    "title": "Review of Probability",
    "section": "Joint distributions",
    "text": "Joint distributions\n\n\\(X = (X_1, \\ldots, X_p)\\) is a random vector (vectors and matrices should be in bold).\nFor notational convenience, let’s consider only \\(p = 2\\) random variables \\(X\\) and \\(Y\\).\n\\((X, Y)\\) is discrete if it can take on a countable number of values, such as\n\\(X\\) = number of hearts and \\(Y\\) = number of face cards.\n\\((X, Y)\\) is continuous if it can take on an uncountable number of values, such as\n\\(X\\) = birthweight and \\(Y\\) = gestational age."
  },
  {
    "objectID": "notes/probability.html#discrete-random-variables",
    "href": "notes/probability.html#discrete-random-variables",
    "title": "Review of Probability",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe joint pmf is \\(f(x, y) = P(X = x, Y = y)\\)\n\n\\(\\sum_x \\sum_y f(x, y) = 1\\)\n\nThe marginal pmf for \\(X\\) is \\(f_X(x) = P(X = x) = \\sum_y f(x, y)\\)\nThe marginal pmf for \\(Y\\) is \\(f_Y(y) = P(Y = y) = \\sum_x\nf(x, y)\\)\nThe marginal distribution is the same as univariate distribution as if we ignored the other variable"
  },
  {
    "objectID": "notes/probability.html#discrete-random-variables-1",
    "href": "notes/probability.html#discrete-random-variables-1",
    "title": "Review of Probability",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe conditional pmf of \\(Y\\) given \\(X\\) is \\(f(y|x) = P(Y = y|X = x) = \\frac{P(X = x, Y = y)}{P(X = x)} = \\frac{f(x, y)}{f_X (x)}.\\)\n\\(X\\) and \\(Y\\) are independent if \\(f(x, y) = f_X(x)f_Y(y)\\) for all \\(x\\) and \\(y\\)\n\nVariables are dependent if they are not independent\n\nEquivalently, \\(X\\) and \\(Y\\) are independent if \\(f(x|y) = f_X(x)\\) for all \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "notes/probability.html#discrete-random-variables-2",
    "href": "notes/probability.html#discrete-random-variables-2",
    "title": "Review of Probability",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nNotation: \\(X_1, \\dots, X_n \\overset{\\mathrm{iid}}{\\sim} f(x)\\) means that \\(X_1, \\ldots, X_n\\) are independent and identically distributed\nThis implies the joint pmf is \\[P(X_1 = x_1, \\ldots, X_n = x_n) = \\prod_{i=1}^n f(x_i)\\]\nThe same notation and definitions of independence apply to continuous random variables\nIn this class, assume independence unless otherwise noted"
  },
  {
    "objectID": "notes/probability.html#continuous-random-variables",
    "href": "notes/probability.html#continuous-random-variables",
    "title": "Review of Probability",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nManipulating joint pdfs is similar to joint pmfs but sums are replaced by integrals\nThe joint pdf is denoted \\(f(x, y)\\)\nProbabilities are computed as volume under the pdf: \\[P((X, Y) ∈ A) = \\int_A f(x, y)dxdy\\] where \\(A \\subset \\mathbb{R}^2\\)"
  },
  {
    "objectID": "notes/probability.html#continuous-random-variables-1",
    "href": "notes/probability.html#continuous-random-variables-1",
    "title": "Review of Probability",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nThe marginal pdf of \\(X\\) is \\(f_X(x) = \\int f(x, y)dy\\)\n\\(f_X\\) is the univariate pdf for \\(X\\) as if we never considered \\(Y\\)\nThe conditional pdf of \\(Y\\) given \\(X\\) is \\[f(y|x) = \\frac{f(x, y)}{f_X (x)}\\]\nProper: \\(\\int f(y|x)dy = \\int \\frac{f(x,y)}{f_X(x)}dy = \\int \\frac{f(x,y)dy}{f_X(x)} = 1\\)"
  },
  {
    "objectID": "notes/probability.html#defining-joint-distributions-conditionally",
    "href": "notes/probability.html#defining-joint-distributions-conditionally",
    "title": "Review of Probability",
    "section": "Defining joint distributions conditionally",
    "text": "Defining joint distributions conditionally\n\nSpecifying joint distributions is hard\nEvery joint distribution can be written \\(f(x, y) = f(y|x)f(x)\\)\nTherefore, any joint distribution can be defined by\n\n\\(X\\)’s marginal distribution\nThe conditional distribution of \\(Y|X\\)\n\nThe joint problem reduces to two univariate problems\nThis idea forms the basis of hierarchical modeling"
  },
  {
    "objectID": "slides/01-welcome.html#review-of-probability",
    "href": "slides/01-welcome.html#review-of-probability",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Review of probability",
    "text": "Review of probability\n\nThe goal of Bayesian statistics is to compute the posterior distribution (i.e., the uncertainty distribution of the parameters, \\(\\boldsymbol{\\theta}\\), after observing the data, \\(\\mathbf{Y}\\)).\nThis is the conditional distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{Y}\\).\nTherefore, we need to review the probability concepts that lead to the conditional distribution of one variable conditioned on another.\n\nProbability mass (pmf) and density (pdf) functions\nJoint distributions\nMarginal and conditional distributions"
  },
  {
    "objectID": "slides/01-welcome.html#random-variables",
    "href": "slides/01-welcome.html#random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Random variables",
    "text": "Random variables\n\n\\(X\\) (capital) is a random variable.\nWe want to compute the probability that \\(X\\) takes on a specific value \\(x\\) (lowercase).\n\nThis is denoted \\(P(X = x)\\).\n\nWe also might want to compute the probability of \\(X\\) being in a set \\(\\mathcal A\\).\n\nThis is denoted \\(P(X \\in \\mathcal A)\\).\n\nThe set of possible values that \\(X\\) can take on is called its support, \\(\\mathcal S\\)."
  },
  {
    "objectID": "slides/01-welcome.html#random-variables---example",
    "href": "slides/01-welcome.html#random-variables---example",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Random variables - example",
    "text": "Random variables - example\n\nExample 1: \\(X\\) is the roll of a die.\n\nThe support is \\(\\mathcal S = \\{1, 2, 3, 4, 5, 6\\}\\).\n\\(P(X = 1) = 1/6\\).\n\nExample 2: \\(X\\) is a newborn baby’s weight.\n\nThe support is \\(\\mathcal S = (0, \\infty)\\).\n\\(P(X \\in [0, \\infty]) = 1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-probability",
    "href": "slides/01-welcome.html#what-is-probability",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is probability?",
    "text": "What is probability?\nObjective (associated with frequentist)\n\n\\(P(X = x)\\) as a purely mathematical statement.\nIf we repeatedly sampled \\(X\\), then the proportion of draws equal to \\(x\\) converges to \\(P(X = x)\\).\n\nSubjective (associated with Bayesian)\n\n\\(P(X = x)\\) represents an individual’s degree of belief.\nOften quantified as the amount an individual would be willing to wager that \\(X\\) will be \\(x\\).\n\nA Bayesian analysis makes use of both of these concepts."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-uncertainty",
    "href": "slides/01-welcome.html#what-is-uncertainty",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is uncertainty?",
    "text": "What is uncertainty?\nAleatoric uncertainty (likelihood)\n\nUncontrollable randomness in the experiment.\nFor example, the results of a fair coin flip can never be predicted with certainty.\n\nEpistemic uncertainty (prior/posterior)\n\nUncertainty about a quantity that could theoretically be known.\nFor example, if we flipped a coin infinitely-many times we could know the true probability of a head.\n\nA Bayesian analysis makes use of both of these concepts."
  },
  {
    "objectID": "slides/01-welcome.html#probability-versus-statistics",
    "href": "slides/01-welcome.html#probability-versus-statistics",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Probability versus statistics",
    "text": "Probability versus statistics\nProbability is the forward problem\n\nWe assume we know how the data are being generated and compute the probability of events\nFor example, what is the probability of flipping 5 straight heads if the coin is fair?\n\nStatistics is the inverse problem\n\nWe use data to learn about the data-generating mechanism\nFor example, if we flipped five straight head, can we conclude the coin is biased?\n\nAny statistical analysis obviously relies on probability"
  },
  {
    "objectID": "slides/01-welcome.html#univariate-distributions",
    "href": "slides/01-welcome.html#univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables.\nThe random variable \\(X\\) is discrete if its support \\(\\mathcal S\\) is countable.\nExamples:\n\n\\(X \\in \\{0, 1, 2, 3\\}\\) is the number of successes in 3 trials.\n\\(X \\in \\{0, 1, 2, \\ldots\\}\\) is the number of patients with COVID in Durham County."
  },
  {
    "objectID": "slides/01-welcome.html#univariate-distributions-1",
    "href": "slides/01-welcome.html#univariate-distributions-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables.\nThe random variable \\(X\\) is continuous if its support \\(\\mathcal S\\) is uncountable.\nExamples with \\(\\mathcal S = (0, \\infty)\\):\n\n\\(X &gt; 0\\) is systolic blood pressure.\n\\(X &gt; 0\\) is a patient’s BMI."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-univariate-distributions",
    "href": "slides/01-welcome.html#discrete-univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete univariate distributions",
    "text": "Discrete univariate distributions\n\nIf \\(X\\) is discrete we describe its distribution with its probability mass function (pmf).\nThe pmf is \\(f(x) = P(X = x)\\).\nThe domain of \\(X\\) is the set of \\(x\\) with \\(f(x) &gt; 0\\).\nWe must have \\(f(x) \\geq 0\\) and \\(\\sum_x f(x) = 1\\).\nThe mean is \\(\\mathbb E[X] = \\sum_x x f(x)\\).\nThe variance is \\(\\mathbb V(X) = \\sum_x(x − \\mathbb E[X])^2f(x)\\).\nThe last three sums are over \\(X\\)’s domain."
  },
  {
    "objectID": "slides/01-welcome.html#parametric-families-of-distributions",
    "href": "slides/01-welcome.html#parametric-families-of-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Parametric families of distributions",
    "text": "Parametric families of distributions\n\nA statistical analysis typically proceeds by selecting a pmf that seems to match the distribution of a sample.\nWe rarely know the pmf exactly, but we assume it is from a parametric family of distributions.\nFor example, Binomial(10, 0.5) and Binomial(4, 0.1) are different but both from the binomial family.\nA family of distributions have the same equation for the pmf but differ by some unknown parameters \\(\\boldsymbol{\\theta}\\).\nWe must estimate these parameters."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-univariate-distributions",
    "href": "slides/01-welcome.html#continuous-univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nIf \\(X\\) is continuous we describe its distribution with the probability density function (pdf) \\(f(x) \\geq 0\\).\nSince there are uncountably many possible values, \\(P(X = x) = 0\\) for all \\(x\\).\nProbabilities are computed as areas under the pdf curve \\[P(a &lt; X &lt; b) = \\int_a^b f(x)dx.\\]\nTherefore, to be valid \\(f(x)\\) must satisfy \\(f(x) \\geq 0\\) and \\[P(−\\infty &lt; X &lt; \\infty) = \\int_{-\\infty}^{\\infty} f(x)dx = 1.\\]"
  },
  {
    "objectID": "slides/01-welcome.html#continuous-univariate-distributions-1",
    "href": "slides/01-welcome.html#continuous-univariate-distributions-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nThe domain is the set of \\(x\\) values with \\(f(x) &gt; 0\\).\nThe mean and the variance are defined similarly to the discrete case but with the sums replaced by integrals.\nThe mean is \\(\\mathbb E[X] = \\int x f(x)dx\\).\nThe variance is \\(\\mathbb V(X) = \\int (x − \\mathbb E[X])^2 f(x)dx\\)."
  },
  {
    "objectID": "slides/01-welcome.html#joint-distributions",
    "href": "slides/01-welcome.html#joint-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Joint distributions",
    "text": "Joint distributions\n\n\\(\\mathbf{X} = (X_1, \\ldots, X_p)\\) is a random vector (vectors and matrices should be in bold).\nFor notational convenience, let’s consider only \\(p = 2\\) random variables \\(X\\) and \\(Y\\).\n\\((X, Y)\\) is discrete if it can take on a countable number of values, such as:\n\n\\(X\\) = number of hearts and \\(Y\\) = number of face cards.\n\n\\((X, Y)\\) is continuous if it can take on an uncountable number of values, such as:\n\n\\(X\\) = birthweight and \\(Y\\) = gestational age."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables",
    "href": "slides/01-welcome.html#discrete-random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe joint pmf: \\(f(x, y) = P(X = x, Y = y)\\)\n\n\\(\\sum_x \\sum_y f(x, y) = 1\\)\n\nThe marginal pmf for \\(X\\): \\(f_X(x) = P(X = x) = \\sum_y f(x, y)\\)\nThe marginal pmf for \\(Y\\): \\(f_Y(y) = P(Y = y) = \\sum_x f(x, y)\\)\nThe marginal distribution is the same as univariate distribution as if we ignored the other variable."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables-1",
    "href": "slides/01-welcome.html#discrete-random-variables-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe conditional pmf of \\(Y\\) given \\(X\\) is \\(f(y|x) = P(Y = y|X = x) = \\frac{P(X = x, Y = y)}{P(X = x)} = \\frac{f(x, y)}{f_X (x)}.\\)\n\\(X\\) and \\(Y\\) are independent if \\(f(x, y) = f_X(x)f_Y(y)\\) for all \\(x\\) and \\(y\\).\n\nVariables are dependent if they are not independent.\n\nEquivalently, \\(X\\) and \\(Y\\) are independent if \\(f(x|y) = f_X(x)\\) for all \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables-2",
    "href": "slides/01-welcome.html#discrete-random-variables-2",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nNotation: \\(X_1, \\dots, X_n \\overset{\\mathrm{iid}}{\\sim} f(x)\\) means that \\(X_1, \\ldots, X_n\\) are independent and identically distributed.\nThis implies the joint pmf is \\[P(X_1 = x_1, \\ldots, X_n = x_n) = \\prod_{i=1}^n f(x_i).\\]\nThe same notation and definitions of independence apply to continuous random variables.\nIn this class, assume independence unless otherwise noted."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-random-variables",
    "href": "slides/01-welcome.html#continuous-random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nManipulating joint pdfs is similar to joint pmfs but sums are replaced by integrals.\nThe joint pdf is denoted \\(f(x, y)\\).\nProbabilities are computed as volume under the pdf: \\[P((X, Y) ∈ A) = \\int_A f(x, y)dxdy\\] where \\(A \\subset \\mathbb{R}^2\\)."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-random-variables-1",
    "href": "slides/01-welcome.html#continuous-random-variables-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nThe marginal pdf of \\(X\\) is \\(f_X(x) = \\int f(x, y)dy\\).\n\\(f_X\\) is the univariate pdf for \\(X\\) as if we never considered \\(Y\\).\nThe conditional pdf of \\(Y\\) given \\(X\\) is \\[f(y|x) = \\frac{f(x, y)}{f_X (x)}.\\]\nProper: \\(\\int f(y|x)dy = \\int \\frac{f(x,y)}{f_X(x)}dy = \\frac{\\int f(x,y)dy}{f_X(x)} = 1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#defining-joint-distributions-conditionally",
    "href": "slides/01-welcome.html#defining-joint-distributions-conditionally",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Defining joint distributions conditionally",
    "text": "Defining joint distributions conditionally\n\nSpecifying joint distributions is hard.\nEvery joint distribution can be written \\(f(x, y) = f(y|x)f(x)\\).\nTherefore, any joint distribution can be defined by,\n\n\\(X\\)’s marginal distribution\nThe conditional distribution of \\(Y|X\\)\n\nThe joint problem reduces to two univariate problems.\nThis idea forms the basis of hierarchical modeling."
  },
  {
    "objectID": "slides/01-welcome.html#bayes-rule",
    "href": "slides/01-welcome.html#bayes-rule",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Bayes rule",
    "text": "Bayes rule\n\n\n\n\n\nThomas Bayes, 1701-1761\n\n\nPierre-Simon Laplace, 1749-1827\n\n\n\\[f(\\boldsymbol{\\theta}|\\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{\\int f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#defining-the-model",
    "href": "slides/02-monte-carlo.html#defining-the-model",
    "title": "Monte Carlo Sampling",
    "section": "Defining the model",
    "text": "Defining the model\nSuppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\).\nFor the purpose of today’s lecture, we assume \\(\\sigma\\) is known."
  },
  {
    "objectID": "slides/02-monte-carlo.html#defining-the-likelihood",
    "href": "slides/02-monte-carlo.html#defining-the-likelihood",
    "title": "Monte Carlo Sampling",
    "section": "Defining the likelihood",
    "text": "Defining the likelihood\nThe individual likelihood contribution for subject \\(i\\) is given by, \\[Y_i|\\boldsymbol{\\beta} \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta}) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta}),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#defining-the-likelihood-matrix-version",
    "href": "slides/02-monte-carlo.html#defining-the-likelihood-matrix-version",
    "title": "Monte Carlo Sampling",
    "section": "Defining the likelihood (matrix version)",
    "text": "Defining the likelihood (matrix version)\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta} ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#linear-regression-estimation",
    "href": "slides/02-monte-carlo.html#linear-regression-estimation",
    "title": "Monte Carlo Sampling",
    "section": "Linear regression estimation",
    "text": "Linear regression estimation\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#bayesian-estimation",
    "href": "slides/02-monte-carlo.html#bayesian-estimation",
    "title": "Monte Carlo Sampling",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#prior-definition",
    "href": "slides/02-monte-carlo.html#prior-definition",
    "title": "Monte Carlo Sampling",
    "section": "Prior definition",
    "text": "Prior definition\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-posterior",
    "href": "slides/02-monte-carlo.html#computing-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Computing the posterior",
    "text": "Computing the posterior\n\nIn general, computing the marginal likelihood, \\(f(Z)\\), is extremely difficulty.\nAn easier approach is to use the kernel trick.\n\n\\[\\begin{aligned}\nf(\\pi | Z) &\\propto f(Z | \\pi) f(\\pi)\\\\\n&\\propto \\pi^z (1-\\pi)^{n - z} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\\\\n&= \\pi^{\\left(\\alpha + z\\right) - 1} (1-\\pi)^{\\left(\\beta + n - z\\right) - 1}\\\\\n&= Beta\\left(\\alpha + z, \\beta + n - z\\right).\n\\end{aligned}\\]\n\nThis only works when a conjugate prior is used."
  },
  {
    "objectID": "slides/02-monte-carlo.html#techniques-to-find-this-posterior",
    "href": "slides/02-monte-carlo.html#techniques-to-find-this-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Techniques to find this posterior",
    "text": "Techniques to find this posterior\n\nBrute force: complete the square\nEasy: kernel recognition\n\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the multivariate normal pdf, \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#using-the-kernel-to-find-the-posterior",
    "href": "slides/02-monte-carlo.html#using-the-kernel-to-find-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Using the kernel to find the posterior",
    "text": "Using the kernel to find the posterior\n\\[\\begin{aligned}\nf(\\pi | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\pi) f(\\pi)\\\\\n&\\propto \\pi^{\\sum_{i=1}^n Y_i} (1-\\pi)^{n - \\sum_{i=1}^n Y_i} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\\\\n&= \\pi^{\\left(\\alpha + \\sum_{i=1}^n Y_i\\right) - 1} (1-\\pi)^{\\left(\\beta + n - \\sum_{i=1}^n Y_i\\right) - 1}\\\\\n&\\sim Beta\\left(\\alpha + \\sum_{i=1}^n Y_i, \\beta + n - \\sum_{i=1}^n Y_i\\right).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#back-to-the-posterior",
    "href": "slides/02-monte-carlo.html#back-to-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Back to the posterior",
    "text": "Back to the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-can-we-use-the-posterior",
    "href": "slides/02-monte-carlo.html#how-can-we-use-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nLet’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/02-monte-carlo.html#visualize-simulated-data",
    "href": "slides/02-monte-carlo.html#visualize-simulated-data",
    "title": "Monte Carlo Sampling",
    "section": "Visualize simulated data",
    "text": "Visualize simulated data"
  },
  {
    "objectID": "slides/02-monte-carlo.html#inspecting-the-prior",
    "href": "slides/02-monte-carlo.html#inspecting-the-prior",
    "title": "Monte Carlo Sampling",
    "section": "Inspecting the prior",
    "text": "Inspecting the prior\n\n###Define hyperparameters\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10"
  },
  {
    "objectID": "slides/02-monte-carlo.html#comparison-with-olsmle",
    "href": "slides/02-monte-carlo.html#comparison-with-olsmle",
    "title": "Monte Carlo Sampling",
    "section": "Comparison with OLS/MLE",
    "text": "Comparison with OLS/MLE\n\n###Compute posterior moments\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\nLet’s compare the posterior mean with the OLS/MLE estimate for the regression parameter \\(\\boldsymbol{\\beta}\\).\n\n\n\n\n\nparameter\ntrue\nbayes\nols/mle\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984\n\n\n\n\n\n\n\n\nWhat other summaries may we be interested in?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarizing-the-posterior",
    "href": "slides/02-monte-carlo.html#summarizing-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nWe now have a posterior distribution. What do we do now?\n. . .\n\nPosterior means, medians, modes, and variances\n\n. . .\n\nJoint, conditional, and marginal probabilities, for example: \\(P(\\pi &lt; c | Z)\\)\n\n. . .\n\n\\(\\alpha\\)-quantiles: \\(\\{q_{\\alpha} : P(\\pi &lt; q_{\\alpha} | Z) = \\alpha\\}, \\alpha \\in (0,1)\\)\n\n. . .\n\n\\(\\ldots\\)\n\n. . .\nHow do we go about computing these summaries?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarizing-the-posterior-1",
    "href": "slides/02-monte-carlo.html#summarizing-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution.\n\n\\(P(\\pi \\in A| Z)\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\pi\\), \\(g\\left(\\pi\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\pi_1 - \\pi_2|\\), \\(\\pi_1/\\pi_2\\), \\(\\max\\left\\{\\pi_1,\\ldots,\\pi_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible.\n\nWhat are our options?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-mc-approximation",
    "href": "slides/02-monte-carlo.html#monte-carlo-mc-approximation",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo (MC) approximation",
    "text": "Monte Carlo (MC) approximation\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#mc-approximation",
    "href": "slides/02-monte-carlo.html#mc-approximation",
    "title": "Monte Carlo Sampling",
    "section": "MC approximation",
    "text": "MC approximation\nLet \\(g\\left(\\pi\\right)\\) be (just about) any function of \\(\\pi\\). The law of large numbers says that if,\n\\[\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\pi|\\mathbf{Y}\\right),\\] then, \\[\\frac{1}{S}\\sum_{s=1}^S g\\left(\\pi^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\pi\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\pi\\right)f\\left(\\pi|\\mathbf{Y}\\right)d\\pi,\\] as \\(S\\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#mc-approximation-1",
    "href": "slides/02-monte-carlo.html#mc-approximation-1",
    "title": "Monte Carlo Sampling",
    "section": "MC approximation",
    "text": "MC approximation\n\nImplications (as \\(S\\rightarrow \\infty\\)):\n\n\\(\\overline{\\pi}=\\frac{1}{S}\\sum_{s=1}^S \\pi^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\pi|\\mathbf{Y}\\right]\\)\n\\(\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\pi^{\\left(s\\right)}-\\overline{\\pi}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\pi|\\mathbf{Y}\\right)\\)\n\\(\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\pi^{\\left(s\\right)}\\leq \\mathbf{c}\\right) \\rightarrow P\\left(\\pi\\leq \\mathbf{c} | \\mathbf{Y}\\right)\\)\n\\(\\alpha\\)-quantile of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}\\)\n\nJust about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample"
  },
  {
    "objectID": "slides/02-monte-carlo.html#posterior-inference-for-arbitrary-functions",
    "href": "slides/02-monte-carlo.html#posterior-inference-for-arbitrary-functions",
    "title": "Monte Carlo Sampling",
    "section": "Posterior inference for arbitrary functions",
    "text": "Posterior inference for arbitrary functions\nInterest in the posterior distribution of a function of \\(\\pi\\), \\(g\\left(\\pi\\right)\\)\n\nMC sampling plan:\n\nDraw \\(\\pi^{\\left(1\\right)}\\) from \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\pi^{\\left(1\\right)}\\right)\\)\n\\(\\ldots\\)\nDraw \\(\\pi^{\\left(S\\right)}\\) from \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\pi^{\\left(S\\right)}\\right)\\)\n\n\\(\\Rightarrow g\\left(\\pi^{\\left(1\\right)}\\right),\\ldots,g\\left(\\pi^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\pi\\right)|\\mathbf{Y}\\right)\\)\nTherefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-many-samples-to-take",
    "href": "slides/02-monte-carlo.html#how-many-samples-to-take",
    "title": "Monte Carlo Sampling",
    "section": "How many samples to take?",
    "text": "How many samples to take?\nWe can use a central limit theorem: \\(\\sqrt{S}\\left(\\overline{\\pi}-\\mathbb{E}[\\pi | \\mathbf{Y}]\\right)/\\sigma \\stackrel{d}{\\rightarrow} \\text{N}\\left(0,1\\right)\\),\n\n\\(\\overline{\\pi}=\\frac{1}{S}\\sum_{s=1}^S \\pi^{\\left(s\\right)}\\)\n\\(\\sigma^2 = \\mathbb{V}\\left(\\overline{\\pi}\\right) = \\frac{1}{S^2}\\sum_{s=1}^S\\mathbb{V}\\left(\\pi^{(s)}\\right) = \\frac{1}{S}\\mathbb{V}\\left(\\pi | \\mathbf{Y}\\right)\\).\n\n\\(\\implies \\overline{\\pi}\\approx N\\left(\\mathbb{E}[\\pi | \\mathbf{Y}],\\sigma^2/S\\right)\\)\nMC standard error: \\(\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\), \\(\\widehat{\\sigma}^2=\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\pi^{\\left(s\\right)}-\\overline{\\pi}\\right)^2\\)\nApproximate 95% MC confidence interval for the posterior mean: \\(\\overline{\\pi} \\pm 2\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\)\n\nChoose \\(S\\) large enough to report the posterior mean with your desired precision\nReporting MC standard errors for the posterior mean is a good way to indicate that \\(S\\) is large enough"
  },
  {
    "objectID": "slides/02-monte-carlo.html#returning-to-linear-regression",
    "href": "slides/02-monte-carlo.html#returning-to-linear-regression",
    "title": "Monte Carlo Sampling",
    "section": "Returning to linear regression",
    "text": "Returning to linear regression\nLet’s obtain \\(S = 1,000\\) samples from our posterior.\n\npi_samples &lt;- rbeta(1000, 1 + 120, 1 + 380)\n\nWe can compute the posterior mean and variance.\n\nprint(mean(pi_samples))\n\n[1] 0.2409367\n\nprint(var(pi_samples))\n\n[1] 0.0003463607"
  },
  {
    "objectID": "slides/02-monte-carlo.html#assessing-accuracy",
    "href": "slides/02-monte-carlo.html#assessing-accuracy",
    "title": "Monte Carlo Sampling",
    "section": "Assessing accuracy",
    "text": "Assessing accuracy"
  },
  {
    "objectID": "slides/02-monte-carlo.html#additional-posterior-summaries",
    "href": "slides/02-monte-carlo.html#additional-posterior-summaries",
    "title": "Monte Carlo Sampling",
    "section": "Additional posterior summaries",
    "text": "Additional posterior summaries\n\n# median\nmedian(pi_samples)\n\n[1] 0.2402345\n\n# 95% credible intervals\nquantile(pi_samples, probs = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.2035750 0.2784977 \n\n# evaluating probability\nmean(pi_samples &lt; 0.25)\n\n[1] 0.6992\n\n# summarizing arbitrary functions of the parameters\npi_new &lt;- pi_samples^3 - pi_samples\nc(mean(pi_new), quantile(pi_new, probs = c(0.025, 0.975)))\n\n                 2.5%      97.5% \n-0.2264329 -0.2568972 -0.1951383"
  },
  {
    "objectID": "slides/02-monte-carlo.html#matrix-likelihood-specification",
    "href": "slides/02-monte-carlo.html#matrix-likelihood-specification",
    "title": "Monte Carlo Sampling",
    "section": "Matrix likelihood specification",
    "text": "Matrix likelihood specification\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta} ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#review-of-last-lecture",
    "href": "slides/03-mcmc.html#review-of-last-lecture",
    "title": "Markov chain Monte Carlo",
    "section": "",
    "text": "On Tuesday, we performed posterior inference for a Beta-Binomial model using Monte Carlo estimation.\nToday we will discuss Bayesian estimation of linear regression:\n\n\\[\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n).\\]\n\nThere is no closed form posterior, so we cannot directly use Monte Carlo sampling.\nWe need Markov chain Monte Carlo (MCMC)!\nOur goal is to gain intuition behind MCMC, the workhorse behind Stan."
  },
  {
    "objectID": "slides/03-mcmc.html#posterior-for-linear-regression",
    "href": "slides/03-mcmc.html#posterior-for-linear-regression",
    "title": "Markov chain Monte Carlo",
    "section": "Posterior for linear regression",
    "text": "Posterior for linear regression\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)d\\boldsymbol{\\beta}d\\sigma^2}.\n\\end{aligned}\\]\nNo closed form exists for the posterior. \\[f(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#motivation-for-gibbs-sampling",
    "href": "slides/03-mcmc.html#motivation-for-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Motivation for Gibbs sampling",
    "text": "Motivation for Gibbs sampling\n\nSuppose we were given \\(\\sigma^{2(1)}\\), a single sample from the marginal posterior distribution \\(f\\left(\\sigma^2|\\mathbf{Y}\\right)\\) (from where, who knows?)\nUse the sample to generate \\(\\boldsymbol{\\beta}^{(1)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y},\\sigma^{2(1)}\\right)\\)\n\\(\\left(\\boldsymbol{\\beta}^{(1)},\\sigma^{2(1)}\\right)\\) is a sample from \\(f\\left(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}\\right)\\)\n\\(\\boldsymbol{\\beta}^{(1)}\\) is a sample from \\(f\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)\\)\n\n. . .\n\n\n\n\n\n\nRecall\n\n\n\n\\(f\\left(\\boldsymbol{\\beta}, \\sigma^{2}|\\mathbf{Y}\\right) = f\\left(\\boldsymbol{\\beta} | \\sigma^{2},\\mathbf{Y}\\right)f\\left(\\sigma^{2}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\nSuppose we can sample from the following two distributions,\n\n\\(f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta})\\)\n\\(f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\sigma^2)\\)\n\nThese are called full conditional distributions.\nSet initial values for \\(\\boldsymbol{\\theta}^{(0)} = (\\boldsymbol{\\beta}^{(0)}, \\sigma^{2(0)})\\). Then, given a current state of parameters \\(\\boldsymbol{\\theta}^{(s)}\\), we can generate a new state as follows:\n\nSample \\(\\boldsymbol{\\beta}^{(s + 1)} \\sim f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^{2(s)})\\)\nSample \\(\\sigma^{2(s + 1)} \\sim f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}^{(s + 1)})\\)\nLet \\(\\boldsymbol{\\theta}^{(s+1)} = (\\boldsymbol{\\beta}^{(s + 1)}, \\sigma^{2(s + 1)})\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#why-does-this-work",
    "href": "slides/03-mcmc.html#why-does-this-work",
    "title": "Markov chain Monte Carlo",
    "section": "Why does this work?",
    "text": "Why does this work?\n\n\\(\\boldsymbol{\\theta}^{(0)}\\) isn’t a sample from the posterior, it is an arbitrarily chosen initial value\n\\(\\boldsymbol{\\theta}^{(1)}\\) likely isn’t from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\)\n\\(\\boldsymbol{\\theta}^{(2)}\\) likely isn’t from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\) and \\(\\boldsymbol{\\theta}^{(1)}\\)\nTheorem: For any initial values, the chain will eventually converge to the posterior\nTheorem: If \\(\\boldsymbol{\\theta}^{(s)}\\) is a sample from the posterior, then \\(\\boldsymbol{\\theta}^{(s+1)}\\) is too"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler",
    "href": "slides/03-mcmc.html#gibbs-sampler",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nUnder mild regulatory conditions that are generally satisfied for most statistical models, one can show that the iteration \\(\\boldsymbol{\\theta}^{(s)}\\) converges in distribution to a draw from the true joint posterior distribution\nSo for \\(s\\) sufficiently large (say, bigger than \\(s_0\\)), \\(\\left\\{\\boldsymbol{\\theta}^{(s)}, s=s_0+1,\\ldots,S\\right\\}\\) is a correlated sample from the true joint posterior (and \\(\\boldsymbol{\\beta}^{(s)}\\) and \\(\\sigma^{2(s)}\\) are samples from the marginals)\nSimilar to Monte Carlo approximation, we can use these samples to estimate posterior quantities of interest"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-1",
    "href": "slides/03-mcmc.html#gibbs-sampler-1",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\n\\(\\boldsymbol{\\theta}^{(t)}\\) depends on \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-1)}\\) only through \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\boldsymbol{\\theta}^{(t)}\\) is conditionally independent of \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-2)}\\) given \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\implies\\) Markov property, so the sequence is called a Markov chain\nWe use the samples similar to MC approximation; therefore, Gibbs sampling is a form of Markov chain Monte Carlo (MCMC)\nWe will cover diagnostics for MCMC in another lecture!"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-1",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-1",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nWe need to compute the full conditionals. Before doing this, we require prior distributions.\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gaussian,\n\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-2",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-2",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nComputing the full conditionals.\n\nWe already have the full conditional for \\(\\boldsymbol{\\beta}\\):\n\n\\(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2 \\sim N \\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}], \\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y})\\right)\\)\n\n\\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#how-can-we-use-the-posterior",
    "href": "slides/03-mcmc.html#how-can-we-use-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nLet’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/03-mcmc.html#perform-gibbs-sampling",
    "href": "slides/03-mcmc.html#perform-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Perform Gibbs sampling",
    "text": "Perform Gibbs sampling\n\nsigma2 &lt;- exp(rnorm(1)) # initial value\nsamples &lt;- NULL\nfor (s in 1:5000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n  \n  ###Sample from full conditional for sigma2\n  quadratic &lt;- as.numeric(t(Y - X %*% beta) %*% (Y - X %*% beta))\n  sigma2 &lt;- 1 / rgamma(1, shape = a + n / 2, rate = b + quadratic / 2)\n  \n  ###Save samples after a burn-in\n  samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-mcmc.html#inspect-results",
    "href": "slides/03-mcmc.html#inspect-results",
    "title": "Markov chain Monte Carlo",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-mcmc.html#parameter-estimation-vs.-posterior-approximation",
    "href": "slides/03-mcmc.html#parameter-estimation-vs.-posterior-approximation",
    "title": "Markov chain Monte Carlo",
    "section": "Parameter estimation vs. posterior approximation",
    "text": "Parameter estimation vs. posterior approximation\n\nModel specification: Choice of likelihood and introduction of model parameters\nPrior specification\nCalculation of the posterior\nSummarizing the posterior using MC or MCMC methods:\n\nThese are not models!\nThey do not generate more information than is in \\(\\mathbf{Y}\\) or \\(f\\left(\\boldsymbol{\\theta}\\right)\\)\nThey are simply ways of looking at \\(f\\left(\\boldsymbol{\\theta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#additional-topic-metropolis-sampling",
    "href": "slides/03-mcmc.html#additional-topic-metropolis-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Additional topic: Metropolis sampling",
    "text": "Additional topic: Metropolis sampling\n\nBefore we start using Stan for probabilistic programming, we need to understand the MCMC algorithm that is the engine for Stan’s inference, Hamiltonian Monte Carlo.\nTo get us one step closer we will quickly review the concept of Metropolis sampling, another MCMC variant"
  },
  {
    "objectID": "slides/03-mcmc.html#intuition-behind-metropolis-samping",
    "href": "slides/03-mcmc.html#intuition-behind-metropolis-samping",
    "title": "Markov chain Monte Carlo",
    "section": "Intuition behind Metropolis samping",
    "text": "Intuition behind Metropolis samping\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let’s consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)’s in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)’s.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-acceptance-ratio",
    "href": "slides/03-mcmc.html#metropolis-acceptance-ratio",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis acceptance ratio",
    "text": "Metropolis acceptance ratio\n\nFortunately, the comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) can be made even if we cannot compute \\(f(\\boldsymbol{\\theta} | \\mathbf{Y})\\).\n\n\\[\\begin{aligned}\nr &= \\frac{f(\\boldsymbol{\\theta}^* | \\mathbf{Y})}{f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y})}\\frac{f(\\mathbf{Y})}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\n\\end{aligned}\\]\nHaving computed \\(r\\), how should we proceed?"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-intuition",
    "href": "slides/03-mcmc.html#metropolis-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis intuition",
    "text": "Metropolis intuition\nMetropolis ratio: \\(r = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\)\nIf \\(r &gt; 1:\\)\n\nIntuition: Since \\(\\boldsymbol{\\theta}^{(s)}\\) is already in our set, we should include \\(\\boldsymbol{\\theta}^*\\) as it has a higher probability than \\(\\boldsymbol{\\theta}^{(s)}\\)\nProcedure: Accept \\(\\boldsymbol{\\theta}^*\\) into our set (i.e., set \\(\\boldsymbol{\\theta}^{(s + 1)} = \\boldsymbol{\\theta}^*\\))\n\nIf \\(r &lt; 1:\\)\n\nIntuition: The relative frequency of \\(\\boldsymbol{\\theta}\\)-values in our set equal to \\(\\boldsymbol{\\theta}^*\\) compared to those equal to \\(\\boldsymbol{\\theta}^{(s)}\\) should be \\(r\\). This means that for every instance of \\(\\boldsymbol{\\theta}^{(s)}\\), we should have only a “fraction” of an instance of a \\(\\boldsymbol{\\theta}^*\\) value.\nProcedure: Set \\(\\boldsymbol{\\theta}^{(s + 1)}\\) equal to either \\(\\boldsymbol{\\theta}^*\\) or \\(\\boldsymbol{\\theta}^{(s)}\\), with probability \\(r\\) and \\(1 − r\\) respectively."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-update",
    "href": "slides/03-mcmc.html#metropolis-update",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis update",
    "text": "Metropolis update\nGiven \\(\\boldsymbol{\\theta}^{(s)}\\), the Metropolis algorithm generates a value \\(\\boldsymbol{\\theta}^{(s + 1)}\\) as follows:\n\nSample \\(\\boldsymbol{\\theta}^*\\) from a proposal distribution, \\(\\boldsymbol{\\theta}^* ∼ J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\)\nCompute the acceptance ratio \\(r\\)\nLet \\[\\boldsymbol{\\theta}^{(s + 1)} =\n\\left\\{\n  \\begin{array}{ll}\n\\boldsymbol{\\theta}^* & \\text{with probability }\\min(r, 1) \\\\\n\\boldsymbol{\\theta}^{(s)} & \\text{with probability }1 -\\min(r, 1)\n  \\end{array}\n\\right.\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-proposal-distribution",
    "href": "slides/03-mcmc.html#metropolis-proposal-distribution",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis proposal distribution",
    "text": "Metropolis proposal distribution\n\nThe proposal distribution is symmetric (i.e., \\(J(\\boldsymbol{\\theta}_a | \\boldsymbol{\\theta}_b) = J(\\boldsymbol{\\theta}_b | \\boldsymbol{\\theta}_a)\\)\nUsually \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) is very simple, with samples from \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) being near \\(\\boldsymbol{\\theta}\\) with high probability.\nThe most common proposal is a normal distribution\n\n\\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)}) = N(\\boldsymbol{\\theta}^{(s)}, \\boldsymbol{\\Delta})\\)\n\nThe value of the parameter \\(\\boldsymbol{\\Delta}\\) is generally chosen to make the approximation algorithm run efficiently"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-and-gibbs-combined",
    "href": "slides/03-mcmc.html#metropolis-and-gibbs-combined",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis and Gibbs combined",
    "text": "Metropolis and Gibbs combined\n\nThe Gibbs and Metropolis samplers are actually both algorithms within a larger class of Metropolis-Hastings algorithms\nWhen performing MCMC, one can actually choose to update a parameter using either a Gibbs or Metropolis update\nLet’s see this in action using our linear regression example"
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs",
    "href": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nIn this example, we will use a Metropolis update for \\(\\sigma^2\\), however we will actually focus on \\(\\log\\sigma^2\\).\n\nMetropolis requires a symmetric proposal, so it is often easier to transform parameters to be on the real line and use a normal proposal.\n\nWe will use the following proposal, \\(\\log\\sigma^{2*} \\sim N\\left(\\log\\sigma^{2(s)}, \\delta\\right)\\), where \\(\\delta = 1\\).\nWe will place the prior: \\(\\log\\sigma^2 \\sim N(0,1)\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs-1",
    "href": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs-1",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nsigma2 &lt;- exp(rnorm(1))\nsamples &lt;- NULL\ndelta &lt;- 1\nfor (s in 1:10000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n\n  ###Metropolis update for sigma2\n  # Sample a proposal value\n  log_sigma2_proposal &lt;- rnorm(1, log(sigma2), delta)\n  # Compute the ratio r on the log scale for numeric stability\n  # Also, I've decided to update log(sigma2) instead of sigma2, so I can use a normal proposal distribution\n  # I've placed a normal prior on log(sigma2)\n  likelihood_proposal &lt;- sum(dnorm(Y, X %*% beta, sqrt(exp(log_sigma2_proposal)), log = TRUE))\n  likelihood_current &lt;- sum(dnorm(Y, X %*% beta, sqrt(sigma2), log = TRUE))\n  prior_proposal &lt;- dnorm(log_sigma2_proposal, 0, 1, log = TRUE)\n  prior_current &lt;- dnorm(log(sigma2), 0, 1, log = TRUE)\n  log_r &lt;- (likelihood_proposal + prior_proposal) - (likelihood_current + prior_current)\n  # Update beta using Metropolis ratio\n  if (log(runif(1)) &lt; log_r) sigma2 &lt;- exp(log_sigma2_proposal)\n\n  ###Save samples after a burn-in\n  if (s &gt; 5000) samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-mcmc.html#inspect-results-1",
    "href": "slides/03-mcmc.html#inspect-results-1",
    "title": "Markov chain Monte Carlo",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-mcmc.html#looking-towards-stan",
    "href": "slides/03-mcmc.html#looking-towards-stan",
    "title": "Markov chain Monte Carlo",
    "section": "Looking towards Stan",
    "text": "Looking towards Stan\n\nStan uses an algorithm called Hamiltonian Monte Carlo, which is a form of MCMC that uses a Metropolis update\nStan does all of the MCMC tuning, allowing us to only focus on the modeling!\nThis means that our job moving forward will be to focus on specifying the\n\nlikelihood: \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})\\)\nprior: \\(f(\\boldsymbol{\\theta})\\)\n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-mcmc.html#looking-towards-stan-1",
    "href": "slides/03-mcmc.html#looking-towards-stan-1",
    "title": "Markov chain Monte Carlo",
    "section": "Looking towards Stan",
    "text": "Looking towards Stan\n\nStan uses an algorithm called Hamiltonian Monte Carlo, which is a form of MCMC that uses a Metropolis update\nStan does all of the MCMC tuning, allowing us to only focus on the modeling!\nThis means that our job moving forward will be to focus on specifying the\n\nlikelihood: \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})\\)\nprior: \\(f(\\boldsymbol{\\theta})\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarizing-the-posterior-2",
    "href": "slides/02-monte-carlo.html#summarizing-the-posterior-2",
    "title": "Monte Carlo Sampling",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarization-can-be-complex",
    "href": "slides/02-monte-carlo.html#summarization-can-be-complex",
    "title": "Monte Carlo Sampling",
    "section": "Summarization can be complex",
    "text": "Summarization can be complex\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\pi &lt; c | Z)\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pbeta}\\))\n\n\nThese methods work well for standard posterior quantities and distributions."
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-mc-sampling",
    "href": "slides/02-monte-carlo.html#monte-carlo-mc-sampling",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo (MC) sampling",
    "text": "Monte Carlo (MC) sampling\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\pi|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\pi|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#poisson-distribution",
    "href": "slides/02-monte-carlo.html#poisson-distribution",
    "title": "Monte Carlo Sampling",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe Poisson distribution models the number of events occurring within a fixed interval of time or space.\nR Code for Monte Carlo Approximation\n```r set.seed(123) lambda &lt;- 3 n &lt;- 10000 samples &lt;- rpois(n, lambda)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#coefficient-of-variation",
    "href": "slides/02-monte-carlo.html#coefficient-of-variation",
    "title": "Monte Carlo Sampling",
    "section": "Coefficient of Variation",
    "text": "Coefficient of Variation\nSuppose that \\(Y_i \\stackrel{iid}{\\sim} Poisson(\\lambda)\\) for \\(i = 1,\\ldots,n\\). Assume the following conjugate prior, \\(f(\\lambda) \\sim Gamma(a, b)\\), so that \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). We would like to perform inference for the \\(\\lambda\\). We take \\(S=10,000\\) samples.\n\nset.seed(54) # set seed for replicability\nS &lt;- 10000 # number of Monte Carlo samples\nlambda &lt;- 4 # true value of lambda\nn &lt;- 100 # sample size\nY &lt;- rpois(n, lambda)\na &lt;- 3 # hyperprior\nb &lt;- 1 # hyperprior\nsamples &lt;- rgamma(S, a + sum(Y), b + n) # sample from posterior\n\n# Posterior mean\ndata.frame(mean = mean(samples), median = median(samples), lower = quantile(samples, probs = c(0.025)), upper = quantile(samples, probs = c(0.975)))\n\n         mean   median    lower   upper\n2.5% 4.041415 4.038972 3.657051 4.44976\n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/02-monte-carlo.html#poisson-random-variable",
    "href": "slides/02-monte-carlo.html#poisson-random-variable",
    "title": "Monte Carlo Sampling",
    "section": "Poisson random variable",
    "text": "Poisson random variable\nSuppose that \\(Y_i \\stackrel{iid}{\\sim} Poisson(\\lambda)\\) for \\(i = 1,\\ldots,n\\). Assume the following conjugate prior, \\(f(\\lambda) \\sim Gamma(a, b)\\), so that \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). We would like to perform inference for the \\(\\lambda\\). We take \\(S=10,000\\) samples.\n\nset.seed(54) # set seed for replicability\nS &lt;- 10000 # number of Monte Carlo samples\nlambda &lt;- 4 # true value of lambda\nn &lt;- 100 # sample size\nY &lt;- rpois(n, lambda)\na &lt;- 3 # hyperprior\nb &lt;- 1 # hyperprior\nsamples &lt;- rgamma(S, a + sum(Y), b + n) # sample from posterior\n\n\n\n\n\n\nmin\nq1\nmedian\nq3\nmax\nmean\nsd\n\n\n\n\n3.269\n3.904\n4.039\n4.173\n4.773\n4.041\n0.201"
  },
  {
    "objectID": "slides/02-monte-carlo.html#binomial-random-variable",
    "href": "slides/02-monte-carlo.html#binomial-random-variable",
    "title": "Monte Carlo Sampling",
    "section": "Binomial random variable",
    "text": "Binomial random variable\nSuppose that \\(Y_i \\stackrel{iid}{\\sim} Bernoulli(\\pi)\\) for \\(i = 1,\\ldots,n\\). Assume the following conjugate prior, \\(f(\\pi) \\sim Beta(a, b)\\), so that \\(f(\\pi|\\mathbf{Y}) \\sim Beta(a + \\sum_{i=1}^nY_i,b+n - \\sum_{i=1}^n)\\). We would like to perform inference for the \\(\\pi\\). We take \\(S=10,000\\) samples.\n\nset.seed(54) # set seed for replicability\nS &lt;- 10000 # number of Monte Carlo samples\npi &lt;- 0.35 # true value of lambda\nn &lt;- 1000 # sample size\nY &lt;- rbinom(n, size = 1, prob = pi)\na &lt;- 1 # hyperprior\nb &lt;- 1 # hyperprior\nsamples &lt;- rbeta(S, a + sum(Y), b + n - sum(Y)) # sample from posterior\n\n\n\n\n\n\nmin\nq1\nmedian\nq3\nmax\nmean\nsd\n\n\n\n\n0.291\n0.334\n0.344\n0.354\n0.411\n0.344\n0.015"
  },
  {
    "objectID": "slides/02-monte-carlo.html#sampling-for-any-distribution",
    "href": "slides/02-monte-carlo.html#sampling-for-any-distribution",
    "title": "Monte Carlo Sampling",
    "section": "Sampling for any distribution",
    "text": "Sampling for any distribution\nMonte Carlo sampling does not have to be used solely for posterior inference. Suppose we are interested in computed summaries for \\(X_i \\stackrel{iid}{\\sim} N(3, 4)\\) for \\(i = 1,\\ldots,n\\). We take \\(S = 10,000\\).\n\nS &lt;- 10000\nsamples &lt;- rnorm(S, 3, 2)\nlibrary(moments)\nkurtosis(samples)\n\n[1] 3.041546\n\nskewness(samples)\n\n[1] -0.0109433\n\nmean(samples &gt; 4) # P(X_i &gt; 4)\n\n[1] 0.3087"
  },
  {
    "objectID": "slides/02-monte-carlo.html#combination-of-random-variables",
    "href": "slides/02-monte-carlo.html#combination-of-random-variables",
    "title": "Monte Carlo Sampling",
    "section": "Combination of random variables",
    "text": "Combination of random variables\nSuppose \\(X_i \\stackrel{iid}{\\sim} N(3, 4)\\) and \\(Y_i \\stackrel{iid}{\\sim} \\chi^2(df=3)\\) for \\(i = 1,\\ldots,n\\). \\(X_i\\) and \\(Y_i\\) are independent. We are interested in summaries of \\(Z_i = X_i / Y_i\\). We take \\(S = 10,000\\).\n\nS &lt;- 10000\nx &lt;- rnorm(S, 3, 2)\ny &lt;- rchisq(S, 3)\nz &lt;- x / y\nmean(z)\n\n[1] 2.961057\n\nmedian(z)\n\n[1] 1.146081\n\nmean(z &gt; 1)\n\n[1] 0.5456"
  },
  {
    "objectID": "slides/02-monte-carlo.html#prepare-for-next-class",
    "href": "slides/02-monte-carlo.html#prepare-for-next-class",
    "title": "Monte Carlo Sampling",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nComplete HW 00 which is due before Thursday’s class\nComplete reading to prepare for Thursday’s lecture\nThursday’s lecture: Markov chain Monte Carlo"
  },
  {
    "objectID": "slides/03-mcmc.html#hmc",
    "href": "slides/03-mcmc.html#hmc",
    "title": "Markov chain Monte Carlo",
    "section": "HMC",
    "text": "HMC\nThe Hamiltonian Monte Carlo (HMC) is a new MCMC approach that has been shown to work better than the usual MH algorithm. It is based on the idea of Hamiltonian dynamics.\nThe high-level idea of HMC is to generate a proposal from a better proposal distribution’ and modify the acceptance part so the it has a higher acceptance rate. In the usual MH algorithm, we are directly sample from a proposal density q(y|x). The HMC modifies this process using two components: a random momentum (velocity) vector ω and the Hamiltonian dynamics. The momentum is required fro every coordinate of the position x. Thus, if x ∈ R d, then we also need a vector of d elements for the momentum. As the name suggest, the momentum vector determines how we move x during the dynamics. The randomness is due to the random momentum vector (and the later acceptance part).\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-mcmc.html#summary-of-gibbs-sampling",
    "href": "slides/03-mcmc.html#summary-of-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Summary of Gibbs sampling",
    "text": "Summary of Gibbs sampling\n\nGibbs sampling is great when we are able to sample from the full conditional distributions.\nIt has been the main inference machine for Bayesian inference since the early 1990s.\nComputing full conditionals and coding up a Gibbs sampler can be mathematically and computationally rigorous.\nNew classes of MCMC are becoming more common to make Bayesian inference less rigorous."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-samping",
    "href": "slides/03-mcmc.html#metropolis-samping",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis samping",
    "text": "Metropolis samping\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let’s consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)’s in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)’s.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-sampling",
    "href": "slides/03-mcmc.html#metropolis-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis sampling",
    "text": "Metropolis sampling\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let’s consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)’s in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)’s.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#intuition-behind-metropolis-sampling",
    "href": "slides/03-mcmc.html#intuition-behind-metropolis-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Intuition behind Metropolis sampling",
    "text": "Intuition behind Metropolis sampling\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let’s consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)’s in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)’s.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#summary-of-metropolis-algorithm",
    "href": "slides/03-mcmc.html#summary-of-metropolis-algorithm",
    "title": "Markov chain Monte Carlo",
    "section": "Summary of Metropolis algorithm",
    "text": "Summary of Metropolis algorithm\n\nMore flexible than Gibbs sampling, because we are no longer required to compute the full conditional distribution analytically.\nPosterior samples can be obtained, however the algorithm must be properly tuned (i.e., choosing \\(\\delta\\)) and the samples may take longer to converge.\nFurthermore, choosing a proper proposal distribution can be difficult in practice.\nIn more recent years, Hamiltonian Monte Carlo has emerged as an new MCMC approach that alleviates the aforementioned issues."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-hastings-algorithm",
    "href": "slides/03-mcmc.html#metropolis-hastings-algorithm",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis-Hastings algorithm",
    "text": "Metropolis-Hastings algorithm\nThe proposal distribution is no longer assumed to be symmetric, so the acceptance ratio is, \\[\\begin{aligned}\nr &= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}^*, \\sigma^{2(s)}) f(\\boldsymbol{\\beta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\beta}^{(s)},\\sigma^{2(s)})f(\\boldsymbol{\\beta}^{(s)})} \\frac{J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*)}{J(\\boldsymbol{\\beta}^* | \\boldsymbol{\\beta}^{(s)})}.\n\\end{aligned}\\]\nBoth Metropolis and Gibbs can easily be seen as subcases of Metropolis Hastings.\n\nMetropolis assumes a symmetric proposal, so the proposal terms cancel.\nGibbs assumes that \\(J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*) = f(\\boldsymbol{\\beta}^* | \\mathbf{Y}, \\sigma^{2(s)})\\), which implies that \\(r=1\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-hastings-mh-algorithm",
    "href": "slides/03-mcmc.html#metropolis-hastings-mh-algorithm",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis-Hastings (MH) algorithm",
    "text": "Metropolis-Hastings (MH) algorithm\nThe proposal distribution is no longer assumed to be symmetric, so the acceptance ratio is, \\[\\begin{aligned}\nr &= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}^*, \\sigma^{2(s)}) f(\\boldsymbol{\\beta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\beta}^{(s)},\\sigma^{2(s)})f(\\boldsymbol{\\beta}^{(s)})} \\frac{J(\\boldsymbol{\\beta}^* | \\boldsymbol{\\beta}^{(s)})}{J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*)}.\n\\end{aligned}\\]\nBoth Metropolis and Gibbs can easily be seen as subcases of Metropolis Hastings.\n\nMetropolis assumes a symmetric proposal, so the proposal terms cancel.\nGibbs assumes that \\(J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*) = f(\\boldsymbol{\\beta}^* | \\mathbf{Y}, \\sigma^{2(s)})\\), which implies that \\(r=1\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#hamiltonian-monte-carlo-hmc-intuition",
    "href": "slides/03-mcmc.html#hamiltonian-monte-carlo-hmc-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "Hamiltonian Monte Carlo (HMC) intuition",
    "text": "Hamiltonian Monte Carlo (HMC) intuition\n\nHMC is a new MCMC approach that has been shown to work better than the usual MH algorithm.\nIt is based on the idea of Hamiltonian dynamics (a physical concept)\n\n\n\n\n\n\n\nRollercoaster Metaphor\n\n\n\nImagine you’re on a roller coaster at an amusement park. As the roller coaster moves along the track, it goes up and down hills. When the roller coaster is at the top of a hill, it has a lot of potential energy (like stored energy). When it goes down the hill, that potential energy turns into kinetic energy (energy of motion), making the roller coaster go faster. Hamiltonian dynamics is like a set of rules that tells us how the roller coaster’s energy changes as it moves along the track."
  },
  {
    "objectID": "slides/03-mcmc.html#hmc-intuition",
    "href": "slides/03-mcmc.html#hmc-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "HMC intuition",
    "text": "HMC intuition\n\nHamiltonian dynamics is used to generate a proposal from a better proposal distribution, \\(J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*)\\), and modifies the acceptance part so the it has a higher acceptance rate.\nJust like the roller coaster follows the track smoothly, Hamiltonian Monte Carlo (HMC) helps us explore different possibilities smoothly and efficiently. This way, we can make more efficient samples from the posterior, just like how the roller coaster moves quickly and smoothly along its track.\nHMC requires evaluations of \\(\\log f(\\boldsymbol{\\theta} | \\mathbf{Y})\\) and \\(\\nabla_{\\boldsymbol{\\theta}} \\log f(\\boldsymbol{\\theta} | \\mathbf{Y})\\),\n\n\\[\\log f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto \\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) + \\log f(\\boldsymbol{\\theta})\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#prepare-for-next-class",
    "href": "slides/03-mcmc.html#prepare-for-next-class",
    "title": "Markov chain Monte Carlo",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nBegin HW 01 which is due January 30\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Probabilistic Programming (Intro to Stan!)"
  },
  {
    "objectID": "slides/04-stan.html#review-of-last-lecture",
    "href": "slides/04-stan.html#review-of-last-lecture",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "",
    "text": "On Thursday, we performed posterior inference for Bayesian linear regression using Gibbs and Metropolis sampling.\n\nWe obtained correlated samples from the posterior using MCMC.\nGibbs required a lot math!\nMetropolis required tuning!\n\nToday we will introduce Stan, a probabilistic programming language that uses Hamiltonian Monte Carlo to perform general Bayesian inference."
  },
  {
    "objectID": "slides/04-stan.html#learning-objectives",
    "href": "slides/04-stan.html#learning-objectives",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this lecture you should:\n\nKnow how to start coding up a model in Stan.\nAppreciate how easy Stan makes things for us compared to coding up the algorithm ourselves.\nBe able to fit a basic linear regression in Stan."
  },
  {
    "objectID": "slides/04-stan.html#what-is-stan-and-how-do-we-use-it",
    "href": "slides/04-stan.html#what-is-stan-and-how-do-we-use-it",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "What is Stan and how do we use it?",
    "text": "What is Stan and how do we use it?\n\nStan is an intuitive yet sophisticated programming language that does the hard work for us.\nProgramming language like R, Python, Matlab, C++…\nWorks like most other languages: can use loops, conditional statements, and functions.\nCode up a model in Stan and then it implements HMC (actually something called NUTS) for us."
  },
  {
    "objectID": "slides/04-stan.html#why-should-we-use-stan",
    "href": "slides/04-stan.html#why-should-we-use-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Why should we use Stan?",
    "text": "Why should we use Stan?\n\nStan is the brainchild of Andrew Gelman at Colombia.\nStan uses an extension of HMC called NUTS that automatically tunes. It is fast.\nStan is simple to learn.\nStan has excellent documentation (a manual full of extensive examples).\nMost important: Stan has a very active and helpful user forum and development team; for example, typical question answered in less than a couple of hours."
  },
  {
    "objectID": "slides/04-stan.html#how-do-we-use-it",
    "href": "slides/04-stan.html#how-do-we-use-it",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "How do we use it?",
    "text": "How do we use it?\nCode up model in Stan code in a text editor and save as .stan file.\n\nCall Stan to run the model from:\n\nR, python, the command line, Matlab, Stata, Julia\n\nUse one of the above to analyse the data (of course you can export to another one)."
  },
  {
    "objectID": "slides/04-stan.html#bayesian-linear-regression",
    "href": "slides/04-stan.html#bayesian-linear-regression",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Bayesian linear regression",
    "text": "Bayesian linear regression\nSuppose we assume the linear regression model:\n\nLikelihood: \\(\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\)\nPriors: \\(\\boldsymbol{\\beta} \\sim N(\\mathbf{0}, \\sigma_{\\beta}^2\\mathbf{I}_p)\\) and \\(f(\\sigma^2) \\sim IG(a,b)\\)\n\nHow de we code this up in Stan?"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program",
    "href": "slides/04-stan.html#an-example-stan-program",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program",
    "text": "An example Stan program\n\ndata {\n  real Y[10]; // height for 10 people\n}\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma2;\n}\nmodel {\n  Y ~ normal(mu, sqrt(sigma2)); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma2 ~ inv_gamma(1, 1); // prior for sigma\n}"
  },
  {
    "objectID": "slides/04-stan.html#a-straightforward-example",
    "href": "slides/04-stan.html#a-straightforward-example",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "A straightforward example",
    "text": "A straightforward example\nSuppose:\n\nWe record the height, \\(Y_i\\), of 10 people.\nWe want a model to explain the variation, and choose a normal likelihood: \\[Y_i \\sim N(\\mu, \\sigma^2)\\]\nWe choose the following (independent) priors on each parameter:\n\n\\(\\mu \\sim N(0, 1)\\)\n\\(\\sigma^2 \\sim IG(1, 1)\\)\n\nQuestion: how do we code this up in Stan?"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-data-block",
    "href": "slides/04-stan.html#an-example-stan-program-data-block",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: data block",
    "text": "An example Stan program: data block\n\ndata {\n  real Y[10]; // height for 10 people\n}\n\n\nDeclare all data that you will pass to Stan to estimate your model.\nTerminate all statements with a semi-colon ;.\nUse ## or // for comments."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-data-block-1",
    "href": "slides/04-stan.html#an-example-stan-program-data-block-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: data block",
    "text": "An example Stan program: data block\n\ndata {\n  real Y[10]; // height for 10 people\n}\n\n\nWe need to tell Stan the type of data variable. For example:\n\nreal for continuous data.\nint for discrete data.\nArrays: above we specified Y as an array of continuous data of length 10."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-data-block-2",
    "href": "slides/04-stan.html#an-example-stan-program-data-block-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: data block",
    "text": "An example Stan program: data block\n\ndata {\n  real Y[10]; // height for 10 people\n}\n\nCan place limits on data, for example:\n\nreal&lt;lower = 0, upper = 1&gt; X;\nreal&lt;lower = 0&gt; Z;\n\nVectors and matrices; only contain reals and can be used for matrix operations.\n\nreal Y[10]; // array representation\nvector[10] Y; // vector representation"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-parameter-block",
    "href": "slides/04-stan.html#an-example-stan-program-parameter-block",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: parameter block",
    "text": "An example Stan program: parameter block\n\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma2;\n}\n\n\nDeclare all parameters that you use in your model.\nPlace limits on variables, for example:\n\nreal&lt;lower = 0&gt; sigma2\n\n\nA multitude of parameter types including some of the aforementioned:\n\nreal for continuous parameters.\nArrays of types, for example real beta[10]"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-parameter-block-1",
    "href": "slides/04-stan.html#an-example-stan-program-parameter-block-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: parameter block",
    "text": "An example Stan program: parameter block\n\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma2;\n}\n\n\nvector or matrix, specified by:\n\nvector[5] beta\nmatrix[5, 3] gamma\n\nsimplex for a parameter vector that must sum to 1.\nMore exotic types like corr_matrix, or ordered."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-parameter-block-2",
    "href": "slides/04-stan.html#an-example-stan-program-parameter-block-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: parameter block",
    "text": "An example Stan program: parameter block\n\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma2;\n}\n\nImportant: Stan is not developed yet to work with discrete parameters. Options for discrete parameters in Stan:\n\nMarginalize out the parameter. For example, suppose we have \\(f(\\boldsymbol{\\beta}, \\theta)\\), where \\(\\boldsymbol{\\beta}\\) is continuous and \\(\\theta\\) is discrete:\n\n\n\\(f(\\boldsymbol{\\beta}) = \\sum_{i = 1}^K f(\\boldsymbol{\\beta}, \\theta_i)\\)\n\n\nSome models can be reformulated without discrete parameters."
  },
  {
    "objectID": "slides/04-stan.html#r-packages-that-interface-with-stan",
    "href": "slides/04-stan.html#r-packages-that-interface-with-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "R packages that interface with Stan",
    "text": "R packages that interface with Stan\n\nrstan, brms, cmdstanr, rstanarm\nrstan and cmdstanr you write the Stan code, which gives you the most options.\n\nrstan has a more intuitive user interface.\ncmdstanr is more memory efficient and a lightweight interface to Stan.\n\nrstanarm and brms you don’t need to write the Stan code yourself, which makes it easier to use Stan, but is limiting.\n\nrstanarm’s biggest advantage is that the models are pre-compiled, but this is also it’s biggest limitation.\nbrms writes Stan code on the fly, so has many more models, some that are pretty advanced."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-model-block",
    "href": "slides/04-stan.html#an-example-stan-program-model-block",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: model block",
    "text": "An example Stan program: model block\n\nmodel {\n  Y ~ normal(mu, sqrt(sigma2)); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma2 ~ inv_gamma(1, 1); // prior for sigma2\n}\n\n\nUsed to define:\n\nLikelihood.\nPriors on parameters.\n\n\nIf don’t specify priors on parameters Stan assumes you are using flat priors (which can be improper)."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-model-block-1",
    "href": "slides/04-stan.html#an-example-stan-program-model-block-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: model block",
    "text": "An example Stan program: model block\n\nmodel {\n  Y ~ normal(mu, sqrt(sigma2)); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma2 ~ inv_gamma(1, 1); // prior for sigma2\n}\n\nHuge range of probability distributions covered, across a range of parameterizations. For example:\n\nDiscrete: Bernoulli, binomial, Poisson, beta-binomial, negative-binomial, categorical, multinomial.\nContinuous unbounded: normal, skew-normal, student-t, Cauchy, logistic."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-model-block-2",
    "href": "slides/04-stan.html#an-example-stan-program-model-block-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: model block",
    "text": "An example Stan program: model block\n\nmodel {\n  Y ~ normal(mu, sqrt(sigma2)); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma2 ~ inv_gamma(1, 1); // prior for sigma2\n}\n\n\nContinuous bounded: uniform, beta, log-normal, exponential, gamma, chi-squared, inverse-chi-squared, Weibull, Wiener diffusion, Pareto.\nMultivariate continuous: normal, student-t, Gaussian process.\nExotics: Dirichlet, LKJ correlation distribution, Wishart and its inverse, Von-Mises."
  },
  {
    "objectID": "slides/04-stan.html#running-stan",
    "href": "slides/04-stan.html#running-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Running Stan",
    "text": "Running Stan\nWrite model in a text editing program and save as a .stan file.\n\nTo create a .stan file from RStudio, File -&gt; New File -&gt; Stan File.\n\n\n###Load packages\nlibrary(rstan)\n\n###Generate fake data\nY &lt;- rnorm(10, mean = 0, sd = 1)\n\n###Compile and run model, and save in fit\nfit &lt;- stan(file = 'straightforward.stan', data = list(Y = Y), \n            iter = 1000, chains = 4, seed = 1)"
  },
  {
    "objectID": "slides/04-stan.html#running-stan-on-example-model",
    "href": "slides/04-stan.html#running-stan-on-example-model",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Running Stan on example model",
    "text": "Running Stan on example model\n\n###Compile and run model, and save in fit\nfit &lt;- stan(file = 'straightforward.stan', data = list(Y = Y), \n            iter = 1000, chains = 4, seed = 1)\n\nThe above R code runs NUTS for our model with the following options:\n\n\\(S=1,000\\) MCMC samples of which 500 are discarded as warm-up.\nAcross 4 chains.\nUsing a random number seed of 1 (good to ensure you can reproduce results)."
  },
  {
    "objectID": "slides/04-stan.html#example-model-results",
    "href": "slides/04-stan.html#example-model-results",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Example model: results",
    "text": "Example model: results\n\n###Print summary statistics\nprint(fit, probs = c(0.25, 0.5, 0.75))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n        mean se_mean   sd   25%   50%   75% n_eff Rhat\nmu     -0.45    0.01 0.32 -0.66 -0.46 -0.26  1361    1\nsigma2  1.23    0.02 0.62  0.81  1.09  1.48   865    1\nlp__   -6.80    0.04 1.03 -7.21 -6.49 -6.06   808    1\n\nSamples were drawn using NUTS(diag_e) at Tue Jan 21 14:36:33 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/04-stan.html#example-model-results-1",
    "href": "slides/04-stan.html#example-model-results-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Example model: results",
    "text": "Example model: results\n\n###Extract posterior samples\npars &lt;- extract(fit)\nclass(pars)\n\n[1] \"list\"\n\nnames(pars)\n\n[1] \"mu\"     \"sigma2\" \"lp__\"  \n\n###Extract samples for particular parameters\npars &lt;- extract(fit, pars = \"mu\")\nclass(pars$mu)\n\n[1] \"array\"\n\ndim(pars$mu)\n\n[1] 2000"
  },
  {
    "objectID": "slides/04-stan.html#visualize-posterior",
    "href": "slides/04-stan.html#visualize-posterior",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Visualize posterior",
    "text": "Visualize posterior\n###Extract samples for particular parameters\nlibrary(ggplot2)\ndata.frame(mu = pars$mu) |&gt;\n  ggplot(aes(x = mu)) +\n  geom_histogram() +\n  labs(x = expression(mu), y = \"Count\", \n       subtitle = bquote(\"Posterior distribution for \" ~ mu))"
  },
  {
    "objectID": "slides/04-stan.html#quick-note-what-does-sim-actually-mean",
    "href": "slides/04-stan.html#quick-note-what-does-sim-actually-mean",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Quick note: what does \\(\\sim\\) actually mean?",
    "text": "Quick note: what does \\(\\sim\\) actually mean?\n\nmodel {\n  Y ~ normal(mu, sigma); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma ~ inv_gamma(1, 1); // prior for sigma\n}\n\n\n\\(\\sim\\) doesn’t mean sampling, although often times it can be thought of as sampling\nMCMC/HMC makes use of the log-posterior\n\n\\[\\log \\prod_{i=1}^nf(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto \\log f(\\boldsymbol{\\theta}) + \\sum_{i=1}^n \\log f(\\mathbf{Y} | \\boldsymbol{\\theta})\\]\n\nAs such \\(\\sim\\) really means increment log probability"
  },
  {
    "objectID": "slides/04-stan.html#quick-note-what-does-sim-mean",
    "href": "slides/04-stan.html#quick-note-what-does-sim-mean",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Quick note: what does \\(\\sim\\) mean?",
    "text": "Quick note: what does \\(\\sim\\) mean?\n\nmodel {\n  Y ~ normal(mu, sigma); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma ~ inv_gamma(1, 1); // prior for sigma\n}\n\n\n\\(\\sim\\) doesn’t mean sampling, although often times it can be thought of as sampling\nMCMC/HMC makes use of the log-posterior\n\n\\[\\log f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto \\log f(\\boldsymbol{\\theta}) + \\sum_{i=1}^n \\log f({Y}_i | \\boldsymbol{\\theta})\\]\n\nAs such \\(\\sim\\) really means increment log probability\nAll we have to do in Stan is specify the log-posterior!"
  },
  {
    "objectID": "slides/04-stan.html#alternate-way-of-specifying-stan-models",
    "href": "slides/04-stan.html#alternate-way-of-specifying-stan-models",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Alternate way of specifying Stan models",
    "text": "Alternate way of specifying Stan models\n\nmodel {\n  target += normal_lpdf(Y | mu, sqrt(sigma2)); // likelihood\n  target += normal_lpdf(mu | 0, 1); // prior for mu\n  target += inv_gamma_lpdf(sigma2 | 1, 1); // prior for sigma\n}\n\n\ntarget is a not a variable, but a special object that represents incremental log probability.\ntarget is initialized to zero.\nnormal_lpdf is the log of the normal density of y given location mu and scale sigma:\nStan documentation for normal distribution\n\n\ntarget += std_normal_lpdf(mu) // prior for mu using standard normal"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan",
    "href": "slides/04-stan.html#linear-regression-using-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan",
    "text": "Linear regression using Stan\n\n// saved in linear_regression.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates\n  vector[n] Y; // outcome vector\n  matrix[n, p + 1] X; // covariate vector\n  real beta0; // location hyperparameter for beta\n  real&lt;lower = 0&gt; sigma_beta; // scale hyperparameter for beta\n  real&lt;lower = 0&gt; a; // shape hyperparameter for sigma2\n  real&lt;lower = 0&gt; b; // scale hyperparameter for sigma2\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma2;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, sqrt(sigma2)); // likelihood\n  target += normal_lpdf(beta | beta0, sigma_beta); // prior for beta\n  target += inv_gamma_lpdf(sigma2 | a, b); // prior for sigma2\n}"
  },
  {
    "objectID": "slides/04-stan.html#fit-model",
    "href": "slides/04-stan.html#fit-model",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Fit model",
    "text": "Fit model\n\n###Load packages\nlibrary(rstan)\n\n###Generate fake data\nY &lt;- rnorm(10, mean = 0, sd = 1)\n\n###Compile and run model, and save in fit\nfit &lt;- stan(file = 'straightforward.stan', data = list(Y= Y), \n            iter = 1000, chains = 4)"
  },
  {
    "objectID": "slides/04-stan.html#running-stan-on-example-model-1",
    "href": "slides/04-stan.html#running-stan-on-example-model-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Running Stan on example model",
    "text": "Running Stan on example model\n\n###Compile and run model, and save in fit\nfit &lt;- stan(file = 'straightforward.stan', data = list(Y = Y), \n            iter = 1000, chains = 4)"
  },
  {
    "objectID": "slides/04-stan.html#lets-simulate-some-data-again",
    "href": "slides/04-stan.html#lets-simulate-some-data-again",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Let’s simulate some data again",
    "text": "Let’s simulate some data again\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/04-stan.html#fit-linear-regression-using-stan",
    "href": "slides/04-stan.html#fit-linear-regression-using-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Fit linear regression using Stan",
    "text": "Fit linear regression using Stan\n\n###Load packages\nlibrary(rstan)\n\n###Create stan data object\nstan_data &lt;- list(n = n,\n                  p = p,\n                  Y = Y,\n                  X = X,\n                  beta0 = 0,\n                  sigma_beta = 10,\n                  a = 3, \n                  b = 1)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_fit.rds\")"
  },
  {
    "objectID": "slides/04-stan.html#example-model-results-2",
    "href": "slides/04-stan.html#example-model-results-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Example model: results",
    "text": "Example model: results\n\n###Print summary statistics\nprint(fit, probs = c(0.25, 0.5, 0.75))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n           mean se_mean   sd     25%     50%     75% n_eff Rhat\nbeta[1]   -1.48    0.00 0.15   -1.58   -1.48   -1.37  1951    1\nbeta[2]    3.30    0.00 0.15    3.20    3.30    3.40  1674    1\nsigma2     2.35    0.01 0.33    2.12    2.32    2.55  1679    1\nlp__    -196.98    0.04 1.29 -197.58 -196.63 -196.04   899    1\n\nSamples were drawn using NUTS(diag_e) at Tue Jan 21 14:38:36 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/04-stan.html#stan-plots-point-estimate-and-intervals",
    "href": "slides/04-stan.html#stan-plots-point-estimate-and-intervals",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan plots: point estimate and intervals",
    "text": "Stan plots: point estimate and intervals\n\nstan_plot(fit, pars = c(\"beta\", \"sigma2\"), include_warmup = FALSE,\n          point_est = \"median\", ci_level = 0.8, outer_level = 0.95)"
  },
  {
    "objectID": "slides/04-stan.html#stan-plots-histogram",
    "href": "slides/04-stan.html#stan-plots-histogram",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan plots: histogram",
    "text": "Stan plots: histogram\n\nstan_hist(fit)"
  },
  {
    "objectID": "slides/04-stan.html#stan-plots-density",
    "href": "slides/04-stan.html#stan-plots-density",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan plots: density",
    "text": "Stan plots: density\n\nstan_dens(fit)"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan-data-and-parameter-chunks",
    "href": "slides/04-stan.html#linear-regression-using-stan-data-and-parameter-chunks",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan: data and parameter chunks",
    "text": "Linear regression using Stan: data and parameter chunks\n\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates\n  vector[n] Y; // outcome vector\n  matrix[n, p + 1] X; // covariate vector\n  real beta0; // location hyperparameter for beta\n  real&lt;lower = 0&gt; sigma_beta; // scale hyperparameter for beta\n  real&lt;lower = 0&gt; a; // shape hyperparameter for sigma2\n  real&lt;lower = 0&gt; b; // scale hyperparameter for sigma2\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma2;\n}"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan-model-chunk",
    "href": "slides/04-stan.html#linear-regression-using-stan-model-chunk",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan: model chunk",
    "text": "Linear regression using Stan: model chunk\n\nmodel {\n  for (i in 1:n) {\n    target += normal_lpdf(Y[i] | X[i, ] * beta, sqrt(sigma2)); // likelihood\n  }\n  target += normal_lpdf(beta | beta0, sigma_beta); // prior for beta\n  target += inv_gamma_lpdf(sigma2 | a, b); // prior for sigma2\n}"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan-vectorization",
    "href": "slides/04-stan.html#linear-regression-using-stan-vectorization",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan: vectorization",
    "text": "Linear regression using Stan: vectorization\nIt is always a good idea to vectorize Stan code for faster and more efficient inference\n\nmodel {\n  target += normal_lpdf(Y | X * beta, sqrt(sigma2)); // likelihood\n  target += normal_lpdf(beta | beta0, sigma_beta); // prior for beta\n  target += inv_gamma_lpdf(sigma2 | a, b); // prior for sigma2\n}"
  },
  {
    "objectID": "slides/04-stan.html#stan-a-few-of-the-loops-and-conditions",
    "href": "slides/04-stan.html#stan-a-few-of-the-loops-and-conditions",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan: a few of the loops and conditions",
    "text": "Stan: a few of the loops and conditions\nStan has pretty much the full range of language constructs to allow pretty much any model to be coded.\nfor (i in 1:10) {something;}\n\n\nwhile (i &gt; 1) {something;}\n\n\nif (i &gt; 1) {something 1;}\nelse if (i == 0) {something2;}\nelse {something 3;}"
  },
  {
    "objectID": "slides/04-stan.html#stan-speed-concerns",
    "href": "slides/04-stan.html#stan-speed-concerns",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan speed concerns",
    "text": "Stan speed concerns\nWhile Stan is fast it pays to know the importance of each code block for efficiency.\n\ndata: called once at beginning of execution.\ntransformed data: called once at beginning of execution.\nparameters: every log probability evaluation!\ntransformed parameters: every log probability evaluation!\nmodel: every log probability evaluation!\ngenerated quantities: once per sample.\nfunctions: how many times it is called depends on the function’s nature."
  },
  {
    "objectID": "slides/04-stan.html#stan-in-parallel",
    "href": "slides/04-stan.html#stan-in-parallel",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan in parallel",
    "text": "Stan in parallel\nIn R can run chains in parallel easily using:\n\nlibrary(rstan)\noptions(mc.cores = 8)"
  },
  {
    "objectID": "slides/04-stan.html#stan-summary",
    "href": "slides/04-stan.html#stan-summary",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan summary",
    "text": "Stan summary\n\nStan works by default with a HMC-like algorithm called NUTS.\nThe Stan language is similar in nature to other common languages with loops, conditional statements and user-definable functions (didn’t cover here).\nStan makes life easier for us than coding up the MCMC algorithms ourselves."
  },
  {
    "objectID": "slides/06-model-checking.html#review-of-last-lecture",
    "href": "slides/06-model-checking.html#review-of-last-lecture",
    "title": "Model checking",
    "section": "",
    "text": "On Thursday, we learned about\n\nDifferent types of priors that can be specified\nPosterior summaries: point estimates, intervals, and probabilities\nPosterior predictive distributions\nWe learned about the generated quantities code chunk\n\nToday, we will dive into (1) methods for assessing MCMC convergence, and (2) model performance techniques."
  },
  {
    "objectID": "slides/06-model-checking.html#learning-objectives",
    "href": "slides/06-model-checking.html#learning-objectives",
    "title": "Model checking",
    "section": "Learning objectives",
    "text": "Learning objectives"
  },
  {
    "objectID": "slides/06-model-checking.html#how-to-debug-a-stan-model",
    "href": "slides/06-model-checking.html#how-to-debug-a-stan-model",
    "title": "Model checking",
    "section": "How to debug a Stan model?",
    "text": "How to debug a Stan model?\nTwo issue flavors, each with their own response.\n\nCoding errors.\nSampling issues."
  },
  {
    "objectID": "slides/06-model-checking.html#coding-errors",
    "href": "slides/06-model-checking.html#coding-errors",
    "title": "Model checking",
    "section": "Coding errors",
    "text": "Coding errors\nStan error messages are generally quite informative, however inevitably there are times when it is less clear why code fails.\n\nOne option is to debug by print.\n\n\nmodel {\n  ...\n  print(theta);\n}\n\nIn R this prints (neatly) to the console output.\nMore details on printing can be found on Stan: print statements"
  },
  {
    "objectID": "slides/06-model-checking.html#coding-errors-1",
    "href": "slides/06-model-checking.html#coding-errors-1",
    "title": "Model checking",
    "section": "Coding errors",
    "text": "Coding errors\nImportant: failing a resolution via the above go to http://mc-stan.org/ and do:\n\nLook through manual for a solution.\nLook through user forum for previous answers to similar problems.\nAsk a question; be clear, and thorough - post as simple a model that replicates the issue.\n\nOutside of this, you have an endless source of resources using Google/stackoverflow/stackexchange/ChatGPT. You can also post to Ed Discussion, go to office hours, and ask in lecture!"
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues",
    "href": "slides/06-model-checking.html#sampling-issues",
    "title": "Model checking",
    "section": "Sampling issues",
    "text": "Sampling issues\nDifferent sort of issue to a coding error, and falls into two (often related) issues:\n\nSlow convergence: still have \\(\\widehat{R} &gt; 1.1\\) after many thousands of iterations.\nDivergent iterations: get a warning in output from Stan with the number of iterations where the NUTS sampler has terminated prematurely.\n\n\nWarning messages:\n1: There were 62 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n2: There were 8 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttps://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded"
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-1",
    "href": "slides/06-model-checking.html#sampling-issues-1",
    "title": "Model checking",
    "section": "Sampling issues",
    "text": "Sampling issues\nGelman’s Folk Theorem:\n\n\nWhen you have computational problems, often there’s a problem with your model.\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-slow-convergence",
    "href": "slides/06-model-checking.html#sampling-issues-slow-convergence",
    "title": "Model checking",
    "section": "Sampling issues: slow convergence",
    "text": "Sampling issues: slow convergence\nPoor chain mixing is usually due to lack of parameter identification.\n\nA parameter is identified if it has some unique effect on the data generating process that can be separated from the effect of the other parameters.\n\nSolution: use simulated data where you know the true parameter values.\n\nInformative as to whether the model is sufficient to estimate a parameter’s value."
  },
  {
    "objectID": "slides/04-stan.html#shinystan",
    "href": "slides/04-stan.html#shinystan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "shinystan",
    "text": "shinystan\n\nlibrary(shinystan)\nmy_sso &lt;- launch_shinystan(fit)\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/05-priors-ppds.html#review-of-last-lecture",
    "href": "slides/05-priors-ppds.html#review-of-last-lecture",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "",
    "text": "On Tuesday, we learned about Stan\n\nA probabilistic programming language for Bayesian inference\nWe learned about the data, parameter, and model code chunks\nWe used Stan to fit a Bayesian linear regression\n\nToday, we will dive into priors, posterior summaries, and posterior predictive distributions (PPDs)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#learning-objectives",
    "href": "slides/05-priors-ppds.html#learning-objectives",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Learning objectives",
    "text": "Learning objectives"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-linear-regression",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-linear-regression",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: linear regression",
    "text": "Posterior predictive distribution: linear regression\n\ngenerated quantities {\n  vector[n] ppd; // store ppd samples\n  for (i in 1:n) {\n    ppd[i] = normal_rng(X[i, ] * beta, sigma);\n  }\n}\n\nThe function normal_rng generates a single independent sample from a normal distribution with parameters:\n\nmean = X[i, ] ∗ beta, where beta is a sample from the estimated posterior.\nsd = sigma, where sigma is a sample from the estimated posterior.\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-linear-regression-1",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-linear-regression-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: linear regression",
    "text": "Posterior predictive distribution: linear regression\n\ngenerated quantities {\n  vector[n] ppd; // store ppd samples\n  for (i in 1:n) {\n    ppd[i] = normal_rng(X[i, ] * beta, sigma);\n  }\n}\n\nThe function normal_rng generates a single independent sample from a normal distribution with parameters:\n\nmean = X[i, ] ∗ beta, where beta is a sample from the estimated posterior.\nsd = sigma, where sigma is a sample from the estimated posterior.\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-slow-convergence-1",
    "href": "slides/06-model-checking.html#sampling-issues-slow-convergence-1",
    "title": "Model checking",
    "section": "Sampling issues: slow convergence",
    "text": "Sampling issues: slow convergence\n\nIndicates that the stepwise integrator used to approximate Hamiltonian dynamics has likely diverged from exact trajectory.\nTherefore these samples cannot be viewed as being from the posterior.\nCauses a bias away from problem area of parameter space.\nAlmost always because the step size is too large relative to the curvature of the posterior.\nHowever can be due to placing limits on parameters that preclude an area of high probability mass.\n\nDiagnosing problem: use Shiny Stan (or otherwise) to make pairwise plots of variables =⇒ look for parameters with high bivariate correlation; indicates high curvature."
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-slow-convergence-2",
    "href": "slides/06-model-checking.html#sampling-issues-slow-convergence-2",
    "title": "Model checking",
    "section": "Sampling issues: slow convergence",
    "text": "Sampling issues: slow convergence\nSolution: If significant number of divergent iterations do the following:\n\nLower step size and increase acceptance rate in the call to Stan from R or otherwise.\nIf above doesn’t help change priors then likelihood.\n\nAgain failing all the above look at the Stan user forums, then ask a question."
  },
  {
    "objectID": "slides/06-model-checking.html#what-to-do-when-things-go-wrong-summary",
    "href": "slides/06-model-checking.html#what-to-do-when-things-go-wrong-summary",
    "title": "Model checking",
    "section": "What to do when things go wrong: summary",
    "text": "What to do when things go wrong: summary\n\nProblems with sampling are almost invariably problems with the underlying model not the sampling algorithm.\nUse fake data with all models to test for parameter identification (and that you’ve coded up correctly).\nTo debug a model that fails read error messages carefully, then try print statements.\nStan has an active developer and user forum, great documentation, and an extensive answer bank."
  },
  {
    "objectID": "slides/06-model-checking.html#shinystan",
    "href": "slides/06-model-checking.html#shinystan",
    "title": "Model checking",
    "section": "shinystan",
    "text": "shinystan\n\nlibrary(shinystan)\nmy_sso &lt;- launch_shinystan(fit)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-elicitation",
    "href": "slides/05-priors-ppds.html#prior-elicitation",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior elicitation",
    "text": "Prior elicitation\n\nSelecting the prior is one of the most important steps in a Bayesian analysis.\nThere is no “right” way to select a prior.\nThe choices often depend on the objective of the study and the nature of the data.\n\nConjugate versus non-conjugate\nInformative versus uninformative\nProper versus improper\nSubjective versus objective"
  },
  {
    "objectID": "slides/05-priors-ppds.html#conjugate-priors",
    "href": "slides/05-priors-ppds.html#conjugate-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Conjugate priors",
    "text": "Conjugate priors\n\nA prior is conjugate if the posterior is a member of the same parametric family.\nWe have seen that if the response is normal and we use a normal prior on the regression parameter, the posterior is also a normal (if we use an inverse gamma distribution for the variance, the posterior is also inverse gamma).\n\nThis requires a pairing of the likelihood and prior.\nThere is a long list of conjugate priors.\n\nThe advantage of a conjugate prior is that the posterior is available in closed form.\nNo longer critical with Stan!"
  },
  {
    "objectID": "slides/05-priors-ppds.html#informative-versus-uninformative-priors",
    "href": "slides/05-priors-ppds.html#informative-versus-uninformative-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Informative versus uninformative priors",
    "text": "Informative versus uninformative priors\n\nIn some cases informative priors are available.\nPotential sources include: literature reviews; pilot studies; expert opinions; etc.\nPrior elicitation is the process of converting expert information to prior distribution.\nFor example, the expert might not comprehend an inverse gamma pdf, but if they give you an estimate and a spread you can back out \\(a\\) and \\(b\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-1",
    "href": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Informative versus uninformative priors",
    "text": "Informative versus uninformative priors\n\nStrong priors for the parameters of interest can be hard to defend.\nStrong priors for nuisance parameters are more common.\nFor example, say you are doing a Bayesian t-test to study the mean \\(\\mu\\), you might use an informative prior for the nuisance parameter \\(\\sigma^2\\).\nAny time informative priors are used you should conduct a sensitivity analysis.\n\nThat is, compare the posterior for several priors."
  },
  {
    "objectID": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-2",
    "href": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-2",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Informative versus uninformative priors",
    "text": "Informative versus uninformative priors\n\nIn most cases prior information is not available and so uninformative priors are used.\nOther names: vague, weak, flat, diffuse, etc.\n\nThese all refer to priors with large variance.\n\nExamples: \\(\\theta \\sim Uniform(0, 1)\\) or \\(\\mu ∼ N(0, 1000^2)\\)\nUninformative priors can be conjugate or not conjugate.\nThe idea is that the likelihood overwhelms the prior.\nYou should verify this with a sensitivity analysis."
  },
  {
    "objectID": "slides/05-priors-ppds.html#improper-priors",
    "href": "slides/05-priors-ppds.html#improper-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Improper priors",
    "text": "Improper priors\n\nExtreme case: \\(\\mu \\sim N(0, \\tau^2)\\) and we set \\(\\tau = \\infty\\).\nA “prior” that doesn’t integrate to one is called improper.\nExample: \\(f(\\mu) = 1\\) for all \\(\\mu \\in \\mathbb{R}\\).\nIt’s OK to use an improper prior so long as you verify that the posterior integrates to one.\nFor example, in linear regression an improper prior can be used for the slopes as long as the number of observations exceeds the number of covariates and there are no redundant predictors."
  },
  {
    "objectID": "slides/05-priors-ppds.html#subjective-versus-objective-priors",
    "href": "slides/05-priors-ppds.html#subjective-versus-objective-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Subjective versus objective priors",
    "text": "Subjective versus objective priors\n\nA subjective Bayesian picks a prior that corresponds to their current state of knowledge before collecting data.\nOf course, if the reader does not share this prior then they might not accept the analysis, and so sensitivity analysis is common.\nAn objective analysis is one that requires no subjective decisions by the analyst.\nSubjective decisions include picking the likelihood, treatment of outliers, transformations, … and prior specification.\nA completely objective analysis may be feasible in tightly controlled experiments, but is impossible in many analyses."
  },
  {
    "objectID": "slides/05-priors-ppds.html#objective-bayes",
    "href": "slides/05-priors-ppds.html#objective-bayes",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Objective Bayes",
    "text": "Objective Bayes\n\nAn objective Bayesian attempts to replace the subjective choice of prior with an algorithm that determines the prior.\nThere are many approaches: Jeffreys, reference, probability matching, maximum entropy, empirical Bayes, penalized complexity, etc.\nJeffreys priors are the most common: \\(f(\\boldsymbol{\\theta}) \\propto \\sqrt{\\det[I(\\boldsymbol{\\theta})]}\\), where \\(I(\\boldsymbol{\\theta})_{ij} = \\mathbb{E}\\left[\\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})}{\\partial \\theta_i}\\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})}{\\partial \\theta_j}\\right]\\) is the Fisher Information matrix.\nMany of these priors are improper and so you have to check that the posterior is proper."
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-predictive-checks",
    "href": "slides/05-priors-ppds.html#prior-predictive-checks",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\n\nUse the prior distribution to obtain samples of the data, \\(\\mathbf{Y}\\).\n\n\\[\\begin{aligned}\nf(\\mathbf{Y}) &= \\int f(\\mathbf{Y}, \\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\\\\\n&= \\int f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\n\\end{aligned}\\]\n\nThis is easy to sample from using the following steps, 1. \\(\\boldsymbol{\\theta}^{sim} \\sim f(\\boldsymbol{\\theta})\\) and 2. \\(\\mathbf{Y}^{sim} \\sim f(\\mathbf{Y} | \\boldsymbol{\\theta}^{sim})\\).\nSimilar to Gibbs sampling, this gives a sample from the joint \\((\\mathbf{Y}^{sim}, \\boldsymbol{\\theta}^{sim})\\) and also the marginal \\(\\mathbf{Y}^{sim}\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-point-estimates",
    "href": "slides/05-priors-ppds.html#posterior-summaries-point-estimates",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: point estimates",
    "text": "Posterior summaries: point estimates\n\nThe output of Bayesian inference is a probability distribution. It is often convenient to summarize the posterior in various ways.\nUsually summaries are computed for individual parameters using the marginal distributions,\n\n\\[f(\\theta_i|\\mathbf{Y}) = \\int f(\\boldsymbol{\\theta} | \\mathbf{Y}) d\\boldsymbol{\\theta}_{-i}.\\]\n\nThe posterior mean is defined by,\n\n\\[\\int_{-\\infty}^{\\infty}\\theta_i f(\\theta_i|\\mathbf{Y}) d\\theta_i.\\]"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-point-estimates-1",
    "href": "slides/05-priors-ppds.html#posterior-summaries-point-estimates-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: point estimates",
    "text": "Posterior summaries: point estimates\n\nThe posterior median m is defined by,\n\n\\[\\int_{-\\infty}^{m} f(\\theta_i|\\mathbf{Y}) d\\theta_i = 0.5 = \\int_m^{\\infty} f(\\theta_i|\\mathbf{Y}) d\\theta_i.\\]\n\nThe posterior mode is given by,\n\n\\[M=\\text{argmax} f(\\theta_i|\\mathbf{Y}) \\]\n\nThe mean and mode are also well defined for the joint posterior distribution."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-intervals",
    "href": "slides/05-priors-ppds.html#posterior-summaries-intervals",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: intervals",
    "text": "Posterior summaries: intervals\n\nThe Bayesian analogue of a frequentist confidence interval is a credible interval.\nAn interval \\((a,b)\\) is a \\(100(1-\\alpha)\\%\\) posterior credible interval for \\(\\theta_i\\) if\n\n\\[\\int_a^b f(\\theta_i|\\mathbf{Y}) d\\theta_i = (1-\\alpha),\\quad 0\\leq \\alpha \\leq 1.\\]\n\nA credible region can be defined similarly for a joint distribution."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-intervals-1",
    "href": "slides/05-priors-ppds.html#posterior-summaries-intervals-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: intervals",
    "text": "Posterior summaries: intervals\n\nCredible intervals are not unique. The two most common are symmetric and highest posterior density (HPD).\n\nSymmetric: An interval \\((a,b)\\) is a symmetric \\(100(1-\\alpha)\\%\\) credible interval if,\n\n\\[\\int_{-\\infty}^a f(\\theta_i|\\mathbf{Y}) d\\theta_i = \\frac{\\alpha}{2} = \\int_b^{\\infty} f(\\theta_i|\\mathbf{Y}) d\\theta_i.\\]\n\nHighest posterior density (HPD): An interval \\((a,b)\\) is a \\(100(1-\\alpha)\\%\\) HPD interval if,\n\n\\([a,b]\\) is a \\(100(1-\\alpha)\\%\\) credible interval for \\(\\theta_i\\)\nFor all \\(\\theta_i \\in [a,b]\\) and \\(\\theta_i^* \\notin [a,b]\\), \\(f(\\theta_i|\\mathbf{Y}) \\geq f(\\theta_i^*|\\mathbf{Y})\\)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval",
    "href": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: HPD credible interval",
    "text": "Posterior summaries: HPD credible interval"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval-1",
    "href": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: HPD credible interval",
    "text": "Posterior summaries: HPD credible interval"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-probability",
    "href": "slides/05-priors-ppds.html#posterior-summaries-probability",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: probability",
    "text": "Posterior summaries: probability\n\nWe may be interested in a hypothesis test: \\(H_0: \\theta \\leq c\\) versus \\(H_1: \\theta \\geq c\\)\nWe can report the posterior probability of the null hypothesis\n\n\\(P(\\theta \\leq c | \\mathbf{Y}) = \\mathbb{E}[1(\\theta \\leq c) | \\mathbf{Y}]\\)\n\nInterpretation of the posteriour probability:\n\nProbability that the null is true \\(P(\\theta \\leq c | \\mathbf{Y})\\)\n\nInterpretation of the p-value:\n\nProbability of observing a test-statistic as or more extreme given that the null is true\nEvidence for or against the null (reject or fail to reject)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-estimation",
    "href": "slides/05-priors-ppds.html#posterior-summaries-estimation",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: estimation",
    "text": "Posterior summaries: estimation\n\nWe have already seen that we can use MC or MCMC to estimate these posterior summaries!\nTo compute the HPD interval we can use the hdi function from the ggdist R package\n\n\nlibrary(ggdist)\ny &lt;- rgamma(10000, 3, 1)\nquantile(y, probs = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.6234925 7.2014631 \n\nggdist::hdi(y)\n\n          [,1]     [,2]\n[1,] 0.3889383 6.520763"
  },
  {
    "objectID": "slides/05-priors-ppds.html#linear-regression-recall",
    "href": "slides/05-priors-ppds.html#linear-regression-recall",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Linear regression recall",
    "text": "Linear regression recall\n\nAssume we observe \\((Y_i,\\mathbf{x}_i)\\) for \\(i = 1,\\ldots,n\\), where \\[Y_i|\\boldsymbol{\\beta},\\sigma^2 \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2).\\]\nThe full data likelihood is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta},\\sigma^2),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\).\nWe have parameters \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\beta}^\\top,\\sigma^2)\\) and \\(f(\\boldsymbol{\\theta}) = f(\\boldsymbol{\\beta})f(\\sigma^2)\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#ppd-defintion",
    "href": "slides/05-priors-ppds.html#ppd-defintion",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "PPD defintion",
    "text": "PPD defintion\n\nAssume we observe a new \\(\\mathbf{x}^*\\) and we would like to make some type of prediction about \\(Y^*\\) given the data we have already observed, \\(\\mathbf{Y}\\).\nThe posterior predictive distribution is defined as \\(f(Y^*|\\mathbf{Y})\\) and can be written as, \\[\\begin{aligned}\nf(Y^* | \\mathbf{Y}) &= \\int f(Y^* , \\boldsymbol{\\theta} | \\mathbf{Y}) d\\boldsymbol{\\theta},\\quad\\text{(marginal)}\\\\\n&= \\int f(Y^* | \\boldsymbol{\\theta},\\mathbf{Y}) f(\\boldsymbol{\\theta} | \\mathbf{Y})d\\boldsymbol{\\theta},\\quad\\text{(conditional)}\\\\\n&= \\int \\underbrace{f(Y^* | \\boldsymbol{\\theta})}_{likelihood} \\underbrace{f(\\boldsymbol{\\theta} | \\mathbf{Y})}_{posterior}d\\boldsymbol{\\theta}.\\quad\\text{(independence)}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-priors-ppds.html#next",
    "href": "slides/05-priors-ppds.html#next",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Next",
    "text": "Next\n\nWe want to compute the posterior predictive distribution.\n\nPosterior predictive checks\nPrediction\n\nUse generated quantities block.\n\n\ngenerated quantities {\n  vector[n] ppd; // store ppd samples\n  for (i in 1:n) {\n    ppd[i] = normal_rng(X[i] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-definition",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-definition",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: definition",
    "text": "Posterior predictive distribution: definition\n\nAssume we observe a new \\(\\mathbf{x}'\\) and we would like to make some type of prediction about \\(Y'\\) given the data we have already observed, \\(\\mathbf{Y}\\).\nThe posterior predictive distribution is defined as \\(f(Y'|\\mathbf{Y})\\) and can be written as, \\[\\begin{aligned}\nf(Y' | \\mathbf{Y}) &= \\int f(Y' , \\boldsymbol{\\theta} | \\mathbf{Y}) d\\boldsymbol{\\theta},\\quad\\text{(marginal)}\\\\\n&= \\int f(Y' | \\boldsymbol{\\theta},\\mathbf{Y}) f(\\boldsymbol{\\theta} | \\mathbf{Y})d\\boldsymbol{\\theta},\\quad\\text{(conditional)}\\\\\n&= \\int \\underbrace{f(Y' | \\boldsymbol{\\theta})}_{likelihood} \\underbrace{f(\\boldsymbol{\\theta} | \\mathbf{Y})}_{posterior}d\\boldsymbol{\\theta}.\\quad\\text{(independence)}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-estimation",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-estimation",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: estimation",
    "text": "Posterior predictive distribution: estimation\n\nThe PPD can be written as an expectation,\n\n\\[f(Y' | \\mathbf{Y})  = \\int f(Y' | \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\mathbf{Y})d\\boldsymbol{\\theta} = \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[f(Y' | \\boldsymbol{\\theta})\\right].\\]\n\nThus, we can estimate the PPD using a Monte Carlo estimate,\n\n\\[\\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[f(Y' | \\boldsymbol{\\theta})\\right] \\approx \\frac{1}{S} \\sum_{s = 1}^S f\\left(Y' | \\boldsymbol{\\theta}^{(s)}\\right),\\]\nwhere \\(\\left\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(S)}\\right\\}\\) are samples from the posterior."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: Stan",
    "text": "Posterior predictive distribution: Stan\n\nWe want to compute the posterior predictive distribution.\nUse generated quantities block.\n\n\ngenerated quantities {\n  vector[n] in_sample;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(X[i] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-1",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: Stan",
    "text": "Posterior predictive distribution: Stan\n\ngenerated quantities {\n  vector[n] in_sample;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(X[i] * beta, sigma);\n  }\n}\n\nThe function normal_rng generates a single independent sample from a normal distribution with parameters:\n\nmean = X[i, ] * beta, where beta is a sample from the estimated posterior.\nsd = sigma, where sigma is a sample from the estimated posterior.\n\nThis computes the posterior predictive distribution for the original data."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-2",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-2",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: Stan",
    "text": "Posterior predictive distribution: Stan\nThe following additions are added to the linear regression Stan code.\n\n// saved in linear_regression_ppd.stan\ndata {\n  ...\n  int&lt;lower = 1&gt; n_pred; // number of new observations\n  matrix[n_pred, p + 1] X_pred; // covariate matrix for new observations\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n_pred] out_sample;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(X[i, ] * beta, sigma);\n  }\n  for (i in 1:n_pred) {\n    out_sample[i] = normal_rng(X_pred[i, ] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/05-priors-ppds.html#lets-simulate-some-data-again",
    "href": "slides/05-priors-ppds.html#lets-simulate-some-data-again",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Let’s simulate some data again",
    "text": "Let’s simulate some data again\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\nn_pred &lt;- 10\n  \n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\nX_pred &lt;- cbind(1, matrix(rnorm(n_pred * p), ncol = p))\nY_pred &lt;- as.numeric(X_pred %*% beta + rnorm(n_pred, 0, sigma))"
  },
  {
    "objectID": "slides/05-priors-ppds.html#fit-linear-regression-using-stan",
    "href": "slides/05-priors-ppds.html#fit-linear-regression-using-stan",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Fit linear regression using Stan",
    "text": "Fit linear regression using Stan\n\n###Load packages\nlibrary(rstan)\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_fit.rds\")"
  },
  {
    "objectID": "slides/05-priors-ppds.html#examining-prediction-performance",
    "href": "slides/05-priors-ppds.html#examining-prediction-performance",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Examining prediction performance",
    "text": "Examining prediction performance\n\n###Inspect the ppd\nppds &lt;- extract(fit, pars = c(\"in_sample\", \"out_sample\"))\nlapply(ppds, dim)\n\n$in_sample\n[1] 2000  100\n\n$out_sample\n[1] 2000   10\n\n\n. . ."
  },
  {
    "objectID": "slides/04-stan.html#prepare-for-next-class",
    "href": "slides/04-stan.html#prepare-for-next-class",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due January 30\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Priors, Posteriors, and PPDs!"
  },
  {
    "objectID": "slides/05-priors-ppds.html#prepare-for-next-class",
    "href": "slides/05-priors-ppds.html#prepare-for-next-class",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due next Thursday before class\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Model checking"
  },
  {
    "objectID": "slides/06-model-checking.html#traceplots-1",
    "href": "slides/06-model-checking.html#traceplots-1",
    "title": "Model checking",
    "section": "Traceplots",
    "text": "Traceplots\n\nlibrary(rstan)\nfit &lt;- readRDS(file = \"robjects/linear_regression_ppd_fit.rds\")\nrstan::traceplot(fit, pars = c(\"beta\", \"sigma\"), \n                 inc_warmup = TRUE)"
  },
  {
    "objectID": "slides/06-model-checking.html#autocorrelation",
    "href": "slides/06-model-checking.html#autocorrelation",
    "title": "Model checking",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nIdeally the samples would be independent across iteration.\nWhen using MCMC we are obtaining dependent samples from the posterior.\nThe autocorrelation function \\(\\rho(h)\\) is the correlation between samples \\(h\\) iterations apart.\nLower values are better, but if the chains are long enough even large values can be OK.\nHighly correlated samples have less information than independent samples."
  },
  {
    "objectID": "slides/06-model-checking.html#effective-sample-size",
    "href": "slides/06-model-checking.html#effective-sample-size",
    "title": "Model checking",
    "section": "Effective sample size",
    "text": "Effective sample size\nThe effective samples size is,\n\\[n_{eff}=ESS(\\theta_i) = \\frac{mS}{1 + 2 \\sum_{h = 1}^{\\infty} \\rho (h)},\\] where \\(m\\) is the number of chains, \\(S\\) is the number of MCMC samples, and \\(\\rho(h)\\) is the \\(h\\)th order autocorrelation for \\(\\theta_i\\).\n\nThe correlated MCMC sample of length \\(mS\\) has the same information as \\(n_{eff}\\) independent samples.\nRule of thumb: \\(n_{eff}\\) should be at least a thousand for all parameters."
  },
  {
    "objectID": "slides/06-model-checking.html#effective-sample-size-1",
    "href": "slides/06-model-checking.html#effective-sample-size-1",
    "title": "Model checking",
    "section": "Effective sample size",
    "text": "Effective sample size\n\nAs the dependence \\(\\rho\\) increases, the incremental information conveyed by each sample decreases.\n\nwhere \\(m\\) is the number of chains, and \\(S\\) is the number of sampler per chain, and \\(\\rho_{\\tau}\\) is the \\(\\tau\\)th order autocorrelation for \\(\\theta_i\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#prepare-for-next-class",
    "href": "slides/06-model-checking.html#prepare-for-next-class",
    "title": "Model checking",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due before next class\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Model Comparison"
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-in-a-few-iterations",
    "href": "slides/06-model-checking.html#convergence-in-a-few-iterations",
    "title": "Model checking",
    "section": "Convergence in a few iterations",
    "text": "Convergence in a few iterations"
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-in-a-few-hundred-iterations",
    "href": "slides/06-model-checking.html#convergence-in-a-few-hundred-iterations",
    "title": "Model checking",
    "section": "Convergence in a few hundred iterations",
    "text": "Convergence in a few hundred iterations"
  },
  {
    "objectID": "slides/06-model-checking.html#this-one-never-converged",
    "href": "slides/06-model-checking.html#this-one-never-converged",
    "title": "Model checking",
    "section": "This one never converged",
    "text": "This one never converged"
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-is-questionable",
    "href": "slides/06-model-checking.html#convergence-is-questionable",
    "title": "Model checking",
    "section": "Convergence is questionable",
    "text": "Convergence is questionable"
  },
  {
    "objectID": "slides/06-model-checking.html#traceplots-for-linear-regression",
    "href": "slides/06-model-checking.html#traceplots-for-linear-regression",
    "title": "Model checking",
    "section": "Traceplots for linear regression",
    "text": "Traceplots for linear regression\n\nlibrary(rstan)\nfit &lt;- readRDS(file = \"linear_regression_ppd_fit.rds\")\nrstan::traceplot(fit, pars = c(\"beta\", \"sigma\"), \n                 inc_warmup = TRUE)"
  },
  {
    "objectID": "slides/06-model-checking.html#autocorrelation-for-linear-regression",
    "href": "slides/06-model-checking.html#autocorrelation-for-linear-regression",
    "title": "Model checking",
    "section": "Autocorrelation for linear regression",
    "text": "Autocorrelation for linear regression\n\nstan_ac(fit, pars = c(\"beta\", \"sigma\"), \n        separate_chains = TRUE, lags = 25)"
  },
  {
    "objectID": "slides/06-model-checking.html#autocorrelation-for-linear-regression-1",
    "href": "slides/06-model-checking.html#autocorrelation-for-linear-regression-1",
    "title": "Model checking",
    "section": "Autocorrelation for linear regression",
    "text": "Autocorrelation for linear regression\n\nlibrary(bayesplot)\nmcmc_acf(fit, regex_pars = c(\"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/06-model-checking.html#effective-sample-size-for-linear-regression",
    "href": "slides/06-model-checking.html#effective-sample-size-for-linear-regression",
    "title": "Model checking",
    "section": "Effective sample size for linear regression",
    "text": "Effective sample size for linear regression\n\nprint(fit, pars = c(\"beta\", \"sigma\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -1.48       0 0.16 -1.79 -1.58 -1.48 -1.37 -1.16  2002    1\nbeta[2]  3.30       0 0.15  3.01  3.19  3.29  3.40  3.59  1980    1\nsigma    1.55       0 0.11  1.36  1.47  1.54  1.62  1.78  1909    1\n\nSamples were drawn using NUTS(diag_e) at Fri Nov 22 10:44:51 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/06-model-checking.html#assessing-mixing-using-between--and-within-sequence-variances",
    "href": "slides/06-model-checking.html#assessing-mixing-using-between--and-within-sequence-variances",
    "title": "Model checking",
    "section": "Assessing mixing using between- and within-sequence variances",
    "text": "Assessing mixing using between- and within-sequence variances\nFor a scalar parameter, \\(\\theta\\), define the MCMC samples as \\(\\theta_{ij}\\) for chain \\(j=1,\\ldots,m\\) and simulations \\(i = 1,\\ldots,n\\). We can compute the between- and within-sequence variances:\n\\[\\begin{aligned}\nB &= \\frac{n}{m-1}\\sum_{j=1}^m \\left(\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot}\\right)^2,\\quad \\bar{\\theta}_{\\cdot j} = \\frac{1}{n}\\sum_{i=1}^n \\theta_{ij},\\quad\\bar{\\theta}_{\\cdot \\cdot} = \\frac{1}{m} \\sum_{j=1}^m \\bar{\\theta}_{\\cdot j}\\\\\nW &= \\frac{1}{m}\\sum_{j=1}^m s_j^2,\\quad s_j^2=\\frac{1}{n-1}\\sum_{i=1}^n \\left(\\theta_{ij} - \\bar{\\theta}_{\\cdot j}\\right)^2\n\\end{aligned}\\]\nThe between-sequence variance, \\(B\\) contains a factor of \\(n\\) because it is based on the variance of the within-sequence means, \\(\\bar{\\theta}_{\\cdot j}\\), each of which is an average of \\(n\\) values \\(\\theta_{ij}\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#widehatr",
    "href": "slides/06-model-checking.html#widehatr",
    "title": "Model checking",
    "section": "\\(\\widehat{R}\\)",
    "text": "\\(\\widehat{R}\\)\nEstimate a total variance \\(\\mathbb{V}(\\theta | \\mathbf{Y})\\) as a weighted mean of \\(W\\) and \\(B\\) \\[\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y}) = \\frac{n-1}{n}W + \\frac{1}{n}B\\] - This overestiamtes marginal posterior variance if starting points are overdispersed.\n\nGiven finite \\(n\\), \\(W\\) underestimates marginal posterior variance.\n\nSingle chains have not yet visited all points in the distribution.\nWhen \\(n \\rightarrow \\infty\\), \\(\\mathbb{E}[W] \\rightarrow \\mathbb{V}(\\theta | \\mathbf{Y})\\)\n\nAs \\(\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})\\) overestimates and \\(W\\) underestimates, we can compute\n\n\\[\\widehat{R} = \\sqrt{\\frac{\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})}{W}}.\\]"
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-metric-widehatr",
    "href": "slides/06-model-checking.html#convergence-metric-widehatr",
    "title": "Model checking",
    "section": "Convergence metric: \\(\\widehat{R}\\)",
    "text": "Convergence metric: \\(\\widehat{R}\\)\nEstimate a total variance \\(\\mathbb{V}(\\theta | \\mathbf{Y})\\) as a weighted mean of \\(W\\), \\(B\\) \\[\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y}) = \\frac{n-1}{n}W + \\frac{1}{n}B\\]\n\nThis overestimates marginal posterior variance if starting points are overdispersed.\nGiven finite \\(n\\), \\(W\\) underestimates marginal posterior variance.\n\nSingle chains have not yet visited all points in the distribution.\nWhen \\(n \\rightarrow \\infty\\), \\(\\mathbb{E}[W] \\rightarrow \\mathbb{V}(\\theta | \\mathbf{Y})\\)\n\nAs \\(\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})\\) overestimates and \\(W\\) underestimates, we can compute\n\n\\[\\widehat{R} = \\sqrt{\\frac{\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})}{W}}, \\quad \\widehat{R} \\rightarrow 1 \\text{ as }n \\rightarrow \\infty.\\]"
  },
  {
    "objectID": "slides/06-model-checking.html#widehatr-for-linear-regression",
    "href": "slides/06-model-checking.html#widehatr-for-linear-regression",
    "title": "Model checking",
    "section": "\\(\\widehat{R}\\) for linear regression",
    "text": "\\(\\widehat{R}\\) for linear regression\n\nprint(fit, pars = c(\"beta\", \"sigma\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -1.48       0 0.16 -1.79 -1.58 -1.48 -1.37 -1.16  2002    1\nbeta[2]  3.30       0 0.15  3.01  3.19  3.29  3.40  3.59  1980    1\nsigma    1.55       0 0.11  1.36  1.47  1.54  1.62  1.78  1909    1\n\nSamples were drawn using NUTS(diag_e) at Fri Nov 22 10:44:51 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nA good rule of thumb is to want \\(\\widehat{R} \\leq 1.1\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-divergent-iterations",
    "href": "slides/06-model-checking.html#sampling-issues-divergent-iterations",
    "title": "Model checking",
    "section": "Sampling issues: divergent iterations",
    "text": "Sampling issues: divergent iterations\nYou may get a warning in output from Stan with the number of iterations where the NUTS sampler has terminated prematurely.\n\nWarning messages:\n1: There were 62 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n2: There were 8 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttps://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded \n\nSolution: If this warning appears, you can try:\n\nIncrease adapt_delta (double, between 0 and 1, defaults to 0.8).\nIncrease max_treedepth (integer, positive, defaults to 10).\nPlay with stepsize (double, positive, defaults to 1)."
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-divergent-iterations-1",
    "href": "slides/06-model-checking.html#sampling-issues-divergent-iterations-1",
    "title": "Model checking",
    "section": "Sampling issues: divergent iterations",
    "text": "Sampling issues: divergent iterations\n\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000,\n                control = list(adapt_delta = 0.8, \n                               max_treedepth = 10,\n                               stepsize = 1))\n\n\nSee help(stan) for a full list of options that can be set in control.\nComplete set of recommendations: runtime warnings and convergence problems\n\nIf the above doesn’t help change priors then likelihood!"
  },
  {
    "objectID": "slides/06-model-checking.html#what-to-do-if-your-mcmc-does-not-converge",
    "href": "slides/06-model-checking.html#what-to-do-if-your-mcmc-does-not-converge",
    "title": "Model checking",
    "section": "What to do if your MCMC does not converge?",
    "text": "What to do if your MCMC does not converge?\nGelman’s Folk Theorem:\n\n\nWhen you have computational problems, often there’s a problem with your model.\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/06-model-checking.html#debugging-with-shinystan",
    "href": "slides/06-model-checking.html#debugging-with-shinystan",
    "title": "Model checking",
    "section": "Debugging with shinystan",
    "text": "Debugging with shinystan\n\nlibrary(shinystan)\nmy_sso &lt;- launch_shinystan(fit)"
  },
  {
    "objectID": "slides/06-model-checking.html#standard-errors-of-posterior-mean-estimates",
    "href": "slides/06-model-checking.html#standard-errors-of-posterior-mean-estimates",
    "title": "Model checking",
    "section": "Standard errors of posterior mean estimates",
    "text": "Standard errors of posterior mean estimates\n\nThe sample mean from our MCMC draws is a point estimate for the posterior mean.\nThe standard error (SE) of this estimate can be used as a diagnostic.\nAssuming independence, the Monte Carlo standard error is \\(\\text{MCSE} = \\frac{s}{\\sqrt{S}},\\) where \\(s\\) is the sample standard deviation and \\(S\\) is the number of samples.\nA more realistic standard error is \\(\\text{MCSE} = \\frac{s}{\\sqrt{n_{eff}}}.\\)\n\\(\\text{MCSE} \\rightarrow 0\\) as \\(S \\rightarrow \\infty\\), whereas the standard deviation of the posterior draws approaches the standard deviation of the posterior distribution."
  },
  {
    "objectID": "slides/06-model-checking.html#posterior-predictive-checks",
    "href": "slides/06-model-checking.html#posterior-predictive-checks",
    "title": "Model checking",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\n\nLast lecture we learned about posterior predictive distributions.\nThese can be used to check the model fit in our observed data.\nThe goal is to check how well our model can generate data that matches the observed data.\nIf our model is “good”, it should be able to generate new observations that resemble the observed data.\nTo perform the posterior predictive check, we must include the generated quantities code chunk."
  },
  {
    "objectID": "slides/06-model-checking.html#compute-posterior-predictive-checks",
    "href": "slides/06-model-checking.html#compute-posterior-predictive-checks",
    "title": "Model checking",
    "section": "Compute posterior predictive checks",
    "text": "Compute posterior predictive checks\nlibrary(rstan)\nlibrary(bayesplot)\nfit &lt;- readRDS(file = \"robjects/linear_regression_ppd_fit.rds\")\nY_in_sample &lt;- extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(Y, Y_in_sample[1:100, ])"
  },
  {
    "objectID": "slides/06-model-checking.html#comparing-the-ppd-to-the-observed-data-distribution",
    "href": "slides/06-model-checking.html#comparing-the-ppd-to-the-observed-data-distribution",
    "title": "Model checking",
    "section": "Comparing the PPD to the observed data distribution",
    "text": "Comparing the PPD to the observed data distribution\nlibrary(rstan)\nlibrary(bayesplot)\nY_in_sample &lt;- extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(Y, Y_in_sample)"
  },
  {
    "objectID": "slides/06-model-checking.html#ppd-test-statistics",
    "href": "slides/06-model-checking.html#ppd-test-statistics",
    "title": "Model checking",
    "section": "PPD test statistics",
    "text": "PPD test statistics\n\nThe procedure for carrying out a posterior predictive check requires a test quantity, \\(T(\\mathbf{Y})\\), for our observed data.\nSuppose that we have samples from the posterior predictive distribution, \\(\\mathbf{Y}^{(s)} = \\left\\{Y_1^{(s)},\\ldots,Y_n^{(s)}\\right\\}\\) for \\(s = 1,\\ldots,S\\).\nWe can compute: \\(T\\left(\\mathbf{Y}^{(s)}\\right) = \\left\\{T\\left(Y_1^{(s)}\\right),\\ldots,\\left(Y_n^{(s)}\\right)\\right\\}\\)\nOur posterior predictive check will then compare the distribution of \\(T\\left(\\mathbf{Y}^{(s)}\\right)\\) to the value from our observed data, \\(T(\\mathbf{Y})\\).\n\\(T(\\cdot)\\) can be any statistics, including mean, median, etc.\nWhen the predictive distribution is not consistent with the observed statistics it indicates poor model fit."
  },
  {
    "objectID": "slides/06-model-checking.html#ploting",
    "href": "slides/06-model-checking.html#ploting",
    "title": "Model checking",
    "section": "Ploting",
    "text": "Ploting\nppc_stat(Y, Y_in_sample, stat = \"mean\")\nppc_stat(Y, Y_in_sample, stat = \"sd\")\nq25 &lt;- function(y) quantile(y, 0.25)\nq75 &lt;- function(y) quantile(y, 0.75)\nppc_stat(Y, Y_in_sample, stat = \"q25\")\nppc_stat(Y, Y_in_sample, stat = \"q75\")"
  },
  {
    "objectID": "slides/06-model-checking.html#visually-assessing-posterior-predictive-check",
    "href": "slides/06-model-checking.html#visually-assessing-posterior-predictive-check",
    "title": "Model checking",
    "section": "Visually assessing posterior predictive check",
    "text": "Visually assessing posterior predictive check\nbayesplot::ppc_stat(Y, Y_in_sample, stat = \"mean\")\nppc_stat(Y, Y_in_sample, stat = \"sd\")\nq25 &lt;- function(y) quantile(y, 0.25)\nq75 &lt;- function(y) quantile(y, 0.75)\nppc_stat(Y, Y_in_sample, stat = \"q25\")\nppc_stat(Y, Y_in_sample, stat = \"q75\")"
  },
  {
    "objectID": "slides/06-model-checking.html#visualizing-posterior-predictive-check",
    "href": "slides/06-model-checking.html#visualizing-posterior-predictive-check",
    "title": "Model checking",
    "section": "Visualizing posterior predictive check",
    "text": "Visualizing posterior predictive check\nppc_stat(Y, Y_in_sample, stat = \"mean\") # from bayesplot\nppc_stat(Y, Y_in_sample, stat = \"sd\")\nq25 &lt;- function(y) quantile(y, 0.25)\nq75 &lt;- function(y) quantile(y, 0.75)\nppc_stat(Y, Y_in_sample, stat = \"q25\")\nppc_stat(Y, Y_in_sample, stat = \"q75\")"
  },
  {
    "objectID": "slides/06-model-checking.html#posterior-predictive-p-values",
    "href": "slides/06-model-checking.html#posterior-predictive-p-values",
    "title": "Model checking",
    "section": "Posterior predictive p-values",
    "text": "Posterior predictive p-values\nplot &lt;- ppc_stat(Y, Y_in_sample, stat = \"median\") # from bayesplot\npvalue &lt;- mean(apply(Y_in_sample, 1, median) &gt; median(Y))\nplot + yaxis_text() + # just so I can see y-axis values for specifying them in annotate() below, but can remove this if you don't want the useless y-axis values displayed \n  annotate(\"text\", x = -0.75, y = 150, label = paste(\"p =\", pvalue))\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian p-value: \\(p_B = P\\left(T\\left(\\mathbf{Y}^{(s)}\\right) \\geq T\\left(\\mathbf{Y}\\right) | \\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/06-model-checking.html#watanabe-akaike-information-criteria-waic",
    "href": "slides/06-model-checking.html#watanabe-akaike-information-criteria-waic",
    "title": "Model checking",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\\[\\text{WAIC}(\\mathbf{Y},\\boldsymbol{\\theta}) = -2\\left(\\text{lppd} - p_{\\text{WAIC}}\\right),\\] where \\(p_{\\text{WAIC}} = \\sum_{i = 1}^n \\mathbb{V}_{\\boldsymbol{\\theta}}\\left(\\log f\\left(Y_i | \\boldsymbol{\\theta}\\right)\\right)\\) and log pointwise predictive density (lppd) is, \\[\\text{lppd}(\\mathbf{Y},\\boldsymbol{\\theta}) = \\sum_{i = 1}^n \\log \\frac{1}{S} \\sum_{s = 1}^S f\\left({Y}_i | \\boldsymbol{\\theta}^{(s)}\\right).\\]"
  },
  {
    "objectID": "slides/06-model-checking.html#computing-waic-using-stan",
    "href": "slides/06-model-checking.html#computing-waic-using-stan",
    "title": "Model checking",
    "section": "Computing WAIC using Stan",
    "text": "Computing WAIC using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/06-model-checking.html#lets-refit-the-stan-model",
    "href": "slides/06-model-checking.html#lets-refit-the-stan-model",
    "title": "Model checking",
    "section": "Let’s refit the Stan model",
    "text": "Let’s refit the Stan model\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_log_lik_fit.rds\")"
  },
  {
    "objectID": "slides/06-model-checking.html#we-can-now-compute-the-waic",
    "href": "slides/06-model-checking.html#we-can-now-compute-the-waic",
    "title": "Model checking",
    "section": "We can now compute the WAIC",
    "text": "We can now compute the WAIC\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", \n                           merge_chains = TRUE)\nwaic_fit &lt;- loo::waic(log_lik)\nprint(waic_fit)\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -188.4  7.4\np_waic         3.2  0.6\nwaic         376.9 14.7"
  },
  {
    "objectID": "slides/06-model-checking.html#log-pointwise-predictive-density",
    "href": "slides/06-model-checking.html#log-pointwise-predictive-density",
    "title": "Model checking",
    "section": "Log pointwise predictive density",
    "text": "Log pointwise predictive density\nThe log pointwise predictive density (lppd) is defined as:\n\\[\\text{lppd}(\\mathbf{Y},\\boldsymbol{\\theta}) = \\sum_{i = 1}^n \\log \\frac{1}{S} \\sum_{s = 1}^S f\\left({Y}_i | \\boldsymbol{\\theta}^{(s)}\\right).\\] ## Watanabe-Akaike information criteria (WAIC)\n\\[\\text{WAIC}(\\mathbf{Y},\\boldsymbol{\\theta}) = -2\\left(\\text{lppd} - p_{\\text{WAIC}}\\right),\\] where \\(p_{\\text{WAIC}} = \\sum_{i = 1}^n \\mathbb{V}_{\\boldsymbol{\\theta}}\\left(\\log f\\left(Y_i | \\boldsymbol{\\theta}\\right)\\right)\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#cross-validation",
    "href": "slides/06-model-checking.html#cross-validation",
    "title": "Model checking",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nCross-validation is a strategy for estimating a model’s predictive accuracy on another sample.\nThe sample data is divided into a number of chunks, called “folds” and the model is asked to predict each fold, after training on all the others.\nThe number of folds is given by \\(k\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#leave-one-out-cross-validation",
    "href": "slides/06-model-checking.html#leave-one-out-cross-validation",
    "title": "Model checking",
    "section": "Leave-one-out cross-validation",
    "text": "Leave-one-out cross-validation\n\nLeave-one-out cross-validation (LOO-CV) represents cross-validation at the extreme, when \\(k = n\\).\nIt is computationally costly to compute LOO-CV, since it requires fitting the model \\(n\\) times.\nLuckily, there exists an approximation to LOO-CV using Pareto smoothed importance-sampling by Vehtari, Gelman, and Gabry (2017).\nThis can be computed easily using R."
  },
  {
    "objectID": "slides/06-model-checking.html#information-criteria",
    "href": "slides/06-model-checking.html#information-criteria",
    "title": "Model checking",
    "section": "Information criteria",
    "text": "Information criteria\n\nIt is sometimes useful to consider predictive accuracy given a point estimate \\(\\hat{\\theta}(\\mathbf{Y})\\), thus,\n\n\\[\\text{expected log predictive density, given }\\hat{\\theta}(\\mathbf{Y}): \\mathbb{E}[]\\]\n\nThe frequentist Akaike information criterion (AIC) is the oldest and most restrictive,\n\n\\[AIC = -2 \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{MLE}) + 2d,\\] where \\(d\\) is the number of parameters in the model.\n\nAmong Bayesians, the deviance information criterion (DIC) has been widely used for some time, now. However, the DIC is limited in that it presumes the posterior is multivariate Gaussian, which is not always the case.\nThe widely applicable information criterion (WAIC) does not impose assumptions on the shape of the posterior distribution.\n\nNote: WAIC provides an estimate of out-of-sample deviance and converges with LOO-CV as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#hyperpriors",
    "href": "slides/05-priors-ppds.html#hyperpriors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Hyperpriors",
    "text": "Hyperpriors\nPrior is the name for the distribution of model parameters that show up in the likelihood.\n\nFor example: In linear regression, \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are parameters in the likelihood, thus \\(f(\\boldsymbol{\\beta})\\) and \\(f(\\sigma^2)\\) are called priors.\n\nHyperprior is the name for the distribution of model parameters not in the likelihood.\nHyperparameter is the name for the parameters in a hyperprior."
  },
  {
    "objectID": "slides/05-priors-ppds.html#hyperpriors-1",
    "href": "slides/05-priors-ppds.html#hyperpriors-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Hyperpriors",
    "text": "Hyperpriors\n\nFor example: Suppose in linear regression, we place the following prior for \\(\\boldsymbol{\\beta}\\), \\(f(\\boldsymbol{\\beta} | \\boldsymbol{\\beta}_0, \\sigma_{\\beta}^2) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I})\\) and\n\n\\(f(\\boldsymbol{\\beta}_0) = N(\\mathbf{0}, \\mathbf{I})\\)\n\\(f(\\sigma_{\\beta}^2) = IG(a_{\\beta}, b_{\\beta})\\)\n\n\\(f(\\boldsymbol{\\beta}_0)\\) and \\(f(\\sigma_{\\beta}^2)\\) are hyperpriors.\n\\(a_{\\beta}, b_{\\beta}\\) are hyperparameters.\n\n. . .\nQuestion: Why would a researcher be interested in a hyperprior?"
  },
  {
    "objectID": "slides/07-workflow.html#review-of-last-lecture",
    "href": "slides/07-workflow.html#review-of-last-lecture",
    "title": "Bayesian Workflow",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Tuesday, we learned about various ways to check MCMC convergence and model fit.\n\nTraceplots, effective sample size (\\(n_{eff}\\)), MC standard error, \\(\\hat{R}\\), sampling issues\nPosterior predictive checks\nModel checks using shinystan\n\nToday, we will put these concepts within the larger framework of the Bayesian workflow."
  },
  {
    "objectID": "slides/07-workflow.html#cross-validation",
    "href": "slides/07-workflow.html#cross-validation",
    "title": "Bayesian Workflow",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nCross-validation is a strategy for estimating a model’s predictive accuracy on another sample.\nCross-validation methods capture out-of-sample prediction error by fitting the model to training data and evaluating this predictive accuracy on a holdout set.\nThey can be computationally expensive but avoid the problem of overfitting.\nThe sample data is divided into a number of chunks, called “folds” and the model is asked to predict each fold, after training on all the others. The number of folds is given by \\(k\\).\nLeave-one-out cross-validation (LOO-CV) represents cross-validation at the extreme, when \\(k = n\\)."
  },
  {
    "objectID": "slides/07-workflow.html#leave-one-out-cross-validation",
    "href": "slides/07-workflow.html#leave-one-out-cross-validation",
    "title": "Bayesian Workflow",
    "section": "Leave-one-out cross-validation",
    "text": "Leave-one-out cross-validation\n\nIt is computationally costly to compute LOO-CV, since it requires fitting the model \\(n\\) times.\nLuckily, there exists an approximation to LOO-CV using Pareto smoothed importance-sampling by Vehtari, Gelman, and Gabry (2017).\nThis can be computed easily using R."
  },
  {
    "objectID": "slides/07-workflow.html#information-criteria",
    "href": "slides/07-workflow.html#information-criteria",
    "title": "Bayesian Workflow",
    "section": "Information criteria",
    "text": "Information criteria\n\nSeveral information criteria have been proposed that do not require fitting the model several times.\nMany are functions of the deviance, i.e., twice the negative log likelihood, \\(D(\\mathbf{Y}|\\boldsymbol{\\theta}) = −2 \\log f(\\mathbf{Y}|\\boldsymbol{\\theta}).\\)\nIdeally, models will have small deviance.\nHowever, if a model is too complex it will have small deviance but be unstable (over-fitting)\nThe Akaike information criteria has a complexity penalty \\(AIC = D(\\mathbf{Y}|\\hat{\\boldsymbol{\\theta}})+ 2p\\), where \\(\\hat{\\boldsymbol{\\theta}}\\) is the MLE.\nModel with smaller AIC are preferred."
  },
  {
    "objectID": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic",
    "href": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic",
    "title": "Bayesian Workflow",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC is an alternative to DIC\nIt is motivated as an approximation to leave-one-out CV\nIn the end WAIC has model-fit and model-complexity components\nIt is used the same as DIC with smaller WAIC preferred\nIn practice the two often give similar results, but WAIC is arguably more theoretically justified"
  },
  {
    "objectID": "slides/07-workflow.html#computing-waic-using-stan",
    "href": "slides/07-workflow.html#computing-waic-using-stan",
    "title": "Bayesian Workflow",
    "section": "Computing WAIC using Stan",
    "text": "Computing WAIC using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/07-workflow.html#lets-refit-the-stan-model",
    "href": "slides/07-workflow.html#lets-refit-the-stan-model",
    "title": "Bayesian Workflow",
    "section": "Let’s refit the Stan model",
    "text": "Let’s refit the Stan model\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_log_lik_fit.rds\")"
  },
  {
    "objectID": "slides/07-workflow.html#we-can-now-compute-the-waic",
    "href": "slides/07-workflow.html#we-can-now-compute-the-waic",
    "title": "Bayesian Workflow",
    "section": "We can now compute the WAIC",
    "text": "We can now compute the WAIC\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", \n                           merge_chains = TRUE)\nwaic_fit &lt;- loo::waic(log_lik)\nprint(waic_fit)\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -188.4  7.4\np_waic         3.2  0.6\nwaic         376.9 14.7"
  },
  {
    "objectID": "slides/07-workflow.html#debugging-with-shinystan",
    "href": "slides/07-workflow.html#debugging-with-shinystan",
    "title": "Bayesian Workflow",
    "section": "Debugging with shinystan",
    "text": "Debugging with shinystan\n\nlibrary(shinystan)\nmy_sso &lt;- launch_shinystan(fit)"
  },
  {
    "objectID": "slides/07-workflow.html#prepare-for-next-class",
    "href": "slides/07-workflow.html#prepare-for-next-class",
    "title": "Bayesian Workflow",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due before next class\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Bayesian Workflow\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/06-model-checking.html#posterior-predictive-checks-1",
    "href": "slides/06-model-checking.html#posterior-predictive-checks-1",
    "title": "Model checking",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\n\nLast lecture we learned about posterior predictive distributions.\nThese can be used to check the model fit in our observed data.\nThe goal is to check how well our model can generate data that matches the observed data.\nIf our model is “good”, it should be able to generate new observations that resemble the observed data.\nTo perform the posterior predictive check, we must include the generated quantities code chunk."
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-predictive-checks-1",
    "href": "slides/05-priors-ppds.html#prior-predictive-checks-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\n\nSuppose that I am interested in modeling the number of patients who are in the waiting room during an hour in the Duke ED, \\(Y_i\\).\nWe can model this random variable as \\(Y_i \\stackrel{iid}{\\sim}Poisson(\\lambda)\\) for \\(i = 1,\\ldots,n\\). A conjugate prior for \\(\\lambda\\) is \\(\\lambda \\sim Gamma(a,b)\\).\nWe know from experience that on average there are 20 patients waiting during an hour, so we want \\(\\mathbb{E}[Y_i]=\\lambda = 20\\).\nThus, we place a prior on \\(\\lambda\\) that is centered at 20, which requires that \\(a/b=20\\).\nThere are infinitely many priors specifications."
  },
  {
    "objectID": "slides/05-priors-ppds.html#visualizing-prior-predictive-checks",
    "href": "slides/05-priors-ppds.html#visualizing-prior-predictive-checks",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Visualizing prior predictive checks",
    "text": "Visualizing prior predictive checks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the samples of \\(Y_i\\) under different prior specifications to compute a summary statstic, for example for the maximum value."
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-predictive-checks-2",
    "href": "slides/05-priors-ppds.html#prior-predictive-checks-2",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\n\nPrior predictive checks can be used to determine a realistic specification for \\(a\\) and \\(b\\).\n\n\ndata {\n  int&lt;lower = 1&gt; n;\n  real&lt;lower = 0&gt; a;\n  real&lt;lower = 0&gt; b;\n}\ngenerated quantities {\n  real lambda = gamma_rng(a, b);\n  vector[n] y_sim;\n  for (i in 1:n) y_sim[i] = poisson_rng(lambda);\n}\n\n\nWhen running Stan for prior predictive checks you must specify algorithm = \"Fixed_param\""
  },
  {
    "objectID": "slides/07-workflow.html#bayes-theorem",
    "href": "slides/07-workflow.html#bayes-theorem",
    "title": "Bayesian Workflow",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{f(\\mathbf{Y})}\\]\n\n\nRethinking Bayes theorem:\n\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto f(\\mathbf{Y}, \\boldsymbol{\\theta}) = f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) \\]\n\n\n\nIn Stan:\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) + \\log f(\\boldsymbol{\\theta})\\]"
  },
  {
    "objectID": "slides/07-workflow.html#rethinking-bayes-theorem",
    "href": "slides/07-workflow.html#rethinking-bayes-theorem",
    "title": "Bayesian Workflow",
    "section": "Rethinking Bayes theorem",
    "text": "Rethinking Bayes theorem\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto f(\\mathbf{Y}, \\boldsymbol{\\theta}) = f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) \\]"
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-statistics",
    "href": "slides/07-workflow.html#bayesian-statistics",
    "title": "Bayesian Workflow",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\nAdvantages:\n\nNatural approach to expressing uncertainty\nAbility to incorporate prior information\nIncreased modeling flexibility\nFull posterior distribution of parameters\nNatural propagation of uncertainty\n\nDisadvantages:\n\nSlow speed of model estimation"
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-workflow",
    "href": "slides/07-workflow.html#bayesian-workflow",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\n\n\n\n\nGelman A., Vehtari A., Simpson D., Margossian, C., Carpenter, B. and Yao, Y., Kennedy, L., Gabry, J., Bürkner P. C., & Modrák M. (2020). Bayesian Workflow."
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-workflow-1",
    "href": "slides/07-workflow.html#bayesian-workflow-1",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\n\n\nTaken from Bayesian workflow by Francesca Capel\n\n\n\nToday we will talk about a general strategy for taking a question and data to a robust conclusion."
  },
  {
    "objectID": "slides/07-workflow.html#a-simplified-workflow",
    "href": "slides/07-workflow.html#a-simplified-workflow",
    "title": "Bayesian Workflow",
    "section": "A simplified workflow",
    "text": "A simplified workflow\n\nSetting up a full probability model: a joint probability distribution for all observable and unobservable quantities in a problem. The model should be consistent with knowledge about the underlying scientific problem and the data collection process.\nConditioning on observed data: calculating and interpreting the appropriate posterior distribution — the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data.\nEvaluating the fit of the model and the implications of the resulting posterior distribution: how well does the model fit the data, are the substantive conclusions reasonable, and how sensitive are the results to the modeling assumptions in step 1? In response, one can alter or expand the model and repeat the three steps.\n\n\nFrom BDA3."
  },
  {
    "objectID": "slides/07-workflow.html#motivating-example",
    "href": "slides/07-workflow.html#motivating-example",
    "title": "Bayesian Workflow",
    "section": "Motivating example",
    "text": "Motivating example\nWe will use the bdims dataset from the openintro package. \nResearch question: How much people’s weight tends to increase when height increases, and how certain we can be about the magnitude of the increase. In particular, we might be interested in predicting a person’s weight based on their height.\n\nlibrary(openintro)\nglimpse(bdims)\n\nRows: 507\nColumns: 25\n$ bia_di &lt;dbl&gt; 42.9, 43.7, 40.1, 44.3, 42.5, 43.3, 43.5, 44.4, 43.5, 42.0, 40.…\n$ bii_di &lt;dbl&gt; 26.0, 28.5, 28.2, 29.9, 29.9, 27.0, 30.0, 29.8, 26.5, 28.0, 29.…\n$ bit_di &lt;dbl&gt; 31.5, 33.5, 33.3, 34.0, 34.0, 31.5, 34.0, 33.2, 32.1, 34.0, 33.…\n$ che_de &lt;dbl&gt; 17.7, 16.9, 20.9, 18.4, 21.5, 19.6, 21.9, 21.8, 15.5, 22.5, 20.…\n$ che_di &lt;dbl&gt; 28.0, 30.8, 31.7, 28.2, 29.4, 31.3, 31.7, 28.8, 27.5, 28.0, 30.…\n$ elb_di &lt;dbl&gt; 13.1, 14.0, 13.9, 13.9, 15.2, 14.0, 16.1, 15.1, 14.1, 15.6, 13.…\n$ wri_di &lt;dbl&gt; 10.4, 11.8, 10.9, 11.2, 11.6, 11.5, 12.5, 11.9, 11.2, 12.0, 10.…\n$ kne_di &lt;dbl&gt; 18.8, 20.6, 19.7, 20.9, 20.7, 18.8, 20.8, 21.0, 18.9, 21.1, 19.…\n$ ank_di &lt;dbl&gt; 14.1, 15.1, 14.1, 15.0, 14.9, 13.9, 15.6, 14.6, 13.2, 15.0, 14.…\n$ sho_gi &lt;dbl&gt; 106.2, 110.5, 115.1, 104.5, 107.5, 119.8, 123.5, 120.4, 111.0, …\n$ che_gi &lt;dbl&gt; 89.5, 97.0, 97.5, 97.0, 97.5, 99.9, 106.9, 102.5, 91.0, 93.5, 9…\n$ wai_gi &lt;dbl&gt; 71.5, 79.0, 83.2, 77.8, 80.0, 82.5, 82.0, 76.8, 68.5, 77.5, 81.…\n$ nav_gi &lt;dbl&gt; 74.5, 86.5, 82.9, 78.8, 82.5, 80.1, 84.0, 80.5, 69.0, 81.5, 81.…\n$ hip_gi &lt;dbl&gt; 93.5, 94.8, 95.0, 94.0, 98.5, 95.3, 101.0, 98.0, 89.5, 99.8, 98…\n$ thi_gi &lt;dbl&gt; 51.5, 51.5, 57.3, 53.0, 55.4, 57.5, 60.9, 56.0, 50.0, 59.8, 60.…\n$ bic_gi &lt;dbl&gt; 32.5, 34.4, 33.4, 31.0, 32.0, 33.0, 42.4, 34.1, 33.0, 36.5, 34.…\n$ for_gi &lt;dbl&gt; 26.0, 28.0, 28.8, 26.2, 28.4, 28.0, 32.3, 28.0, 26.0, 29.2, 27.…\n$ kne_gi &lt;dbl&gt; 34.5, 36.5, 37.0, 37.0, 37.7, 36.6, 40.1, 39.2, 35.5, 38.3, 38.…\n$ cal_gi &lt;dbl&gt; 36.5, 37.5, 37.3, 34.8, 38.6, 36.1, 40.3, 36.7, 35.0, 38.6, 40.…\n$ ank_gi &lt;dbl&gt; 23.5, 24.5, 21.9, 23.0, 24.4, 23.5, 23.6, 22.5, 22.0, 22.2, 23.…\n$ wri_gi &lt;dbl&gt; 16.5, 17.0, 16.9, 16.6, 18.0, 16.9, 18.8, 18.0, 16.5, 16.9, 16.…\n$ age    &lt;int&gt; 21, 23, 28, 23, 22, 21, 26, 27, 23, 21, 23, 22, 20, 26, 23, 22,…\n$ wgt    &lt;dbl&gt; 65.6, 71.8, 80.7, 72.6, 78.8, 74.8, 86.4, 78.4, 62.0, 81.6, 76.…\n$ hgt    &lt;dbl&gt; 174.0, 175.3, 193.5, 186.5, 187.2, 181.5, 184.0, 184.5, 175.0, …\n$ sex    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …"
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-workflow-2",
    "href": "slides/07-workflow.html#bayesian-workflow-2",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\nResearch question: What are your dependent and indepednent variables? What associations are you interested in? EDA.\n\n\n\nSpecify likelihood & priors: Use knowledge of the problem to construct a generative model.\n\n\n\n\nCheck the model with simulated data: Generate data from the model and evaluate fit as a sanity check (prior predictive checks).\n\n\n\n\nFit the model to real data: Estimate parameters in the model using MCMC."
  },
  {
    "objectID": "slides/07-workflow.html#motivating-example-predicting-weight",
    "href": "slides/07-workflow.html#motivating-example-predicting-weight",
    "title": "Bayesian Workflow",
    "section": "Motivating example: predicting weight",
    "text": "Motivating example: predicting weight\nResearch question: We would like to understand the relationship between a person’s height and weight. A few particular questions we have are:\n\nHow much does a person’s weight increase when their height increases?\nHow certain we can be about the magnitude of the increase?\nCan we predict a person’s weight based on their height?\n\nData: We will use the bdims dataset from the openintro package. This dataset contains body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender."
  },
  {
    "objectID": "slides/07-workflow.html#asdf",
    "href": "slides/07-workflow.html#asdf",
    "title": "Bayesian Workflow",
    "section": "asdf",
    "text": "asdf\n\nlibrary(openintro)\nglimpse(bdims)\n\nRows: 507\nColumns: 25\n$ bia_di &lt;dbl&gt; 42.9, 43.7, 40.1, 44.3, 42.5, 43.3, 43.5, 44.4, 43.5, 42.0, 40.…\n$ bii_di &lt;dbl&gt; 26.0, 28.5, 28.2, 29.9, 29.9, 27.0, 30.0, 29.8, 26.5, 28.0, 29.…\n$ bit_di &lt;dbl&gt; 31.5, 33.5, 33.3, 34.0, 34.0, 31.5, 34.0, 33.2, 32.1, 34.0, 33.…\n$ che_de &lt;dbl&gt; 17.7, 16.9, 20.9, 18.4, 21.5, 19.6, 21.9, 21.8, 15.5, 22.5, 20.…\n$ che_di &lt;dbl&gt; 28.0, 30.8, 31.7, 28.2, 29.4, 31.3, 31.7, 28.8, 27.5, 28.0, 30.…\n$ elb_di &lt;dbl&gt; 13.1, 14.0, 13.9, 13.9, 15.2, 14.0, 16.1, 15.1, 14.1, 15.6, 13.…\n$ wri_di &lt;dbl&gt; 10.4, 11.8, 10.9, 11.2, 11.6, 11.5, 12.5, 11.9, 11.2, 12.0, 10.…\n$ kne_di &lt;dbl&gt; 18.8, 20.6, 19.7, 20.9, 20.7, 18.8, 20.8, 21.0, 18.9, 21.1, 19.…\n$ ank_di &lt;dbl&gt; 14.1, 15.1, 14.1, 15.0, 14.9, 13.9, 15.6, 14.6, 13.2, 15.0, 14.…\n$ sho_gi &lt;dbl&gt; 106.2, 110.5, 115.1, 104.5, 107.5, 119.8, 123.5, 120.4, 111.0, …\n$ che_gi &lt;dbl&gt; 89.5, 97.0, 97.5, 97.0, 97.5, 99.9, 106.9, 102.5, 91.0, 93.5, 9…\n$ wai_gi &lt;dbl&gt; 71.5, 79.0, 83.2, 77.8, 80.0, 82.5, 82.0, 76.8, 68.5, 77.5, 81.…\n$ nav_gi &lt;dbl&gt; 74.5, 86.5, 82.9, 78.8, 82.5, 80.1, 84.0, 80.5, 69.0, 81.5, 81.…\n$ hip_gi &lt;dbl&gt; 93.5, 94.8, 95.0, 94.0, 98.5, 95.3, 101.0, 98.0, 89.5, 99.8, 98…\n$ thi_gi &lt;dbl&gt; 51.5, 51.5, 57.3, 53.0, 55.4, 57.5, 60.9, 56.0, 50.0, 59.8, 60.…\n$ bic_gi &lt;dbl&gt; 32.5, 34.4, 33.4, 31.0, 32.0, 33.0, 42.4, 34.1, 33.0, 36.5, 34.…\n$ for_gi &lt;dbl&gt; 26.0, 28.0, 28.8, 26.2, 28.4, 28.0, 32.3, 28.0, 26.0, 29.2, 27.…\n$ kne_gi &lt;dbl&gt; 34.5, 36.5, 37.0, 37.0, 37.7, 36.6, 40.1, 39.2, 35.5, 38.3, 38.…\n$ cal_gi &lt;dbl&gt; 36.5, 37.5, 37.3, 34.8, 38.6, 36.1, 40.3, 36.7, 35.0, 38.6, 40.…\n$ ank_gi &lt;dbl&gt; 23.5, 24.5, 21.9, 23.0, 24.4, 23.5, 23.6, 22.5, 22.0, 22.2, 23.…\n$ wri_gi &lt;dbl&gt; 16.5, 17.0, 16.9, 16.6, 18.0, 16.9, 18.8, 18.0, 16.5, 16.9, 16.…\n$ age    &lt;int&gt; 21, 23, 28, 23, 22, 21, 26, 27, 23, 21, 23, 22, 20, 26, 23, 22,…\n$ wgt    &lt;dbl&gt; 65.6, 71.8, 80.7, 72.6, 78.8, 74.8, 86.4, 78.4, 62.0, 81.6, 76.…\n$ hgt    &lt;dbl&gt; 174.0, 175.3, 193.5, 186.5, 187.2, 181.5, 184.0, 184.5, 175.0, …\n$ sex    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …"
  },
  {
    "objectID": "slides/07-workflow.html#the-dataset",
    "href": "slides/07-workflow.html#the-dataset",
    "title": "Bayesian Workflow",
    "section": "The dataset",
    "text": "The dataset\n\nlibrary(openintro)\nglimpse(bdims)\n\nRows: 507\nColumns: 25\n$ bia_di &lt;dbl&gt; 42.9, 43.7, 40.1, 44.3, 42.5, 43.3, 43.5, 44.4, 43.5, 42.0, 40.…\n$ bii_di &lt;dbl&gt; 26.0, 28.5, 28.2, 29.9, 29.9, 27.0, 30.0, 29.8, 26.5, 28.0, 29.…\n$ bit_di &lt;dbl&gt; 31.5, 33.5, 33.3, 34.0, 34.0, 31.5, 34.0, 33.2, 32.1, 34.0, 33.…\n$ che_de &lt;dbl&gt; 17.7, 16.9, 20.9, 18.4, 21.5, 19.6, 21.9, 21.8, 15.5, 22.5, 20.…\n$ che_di &lt;dbl&gt; 28.0, 30.8, 31.7, 28.2, 29.4, 31.3, 31.7, 28.8, 27.5, 28.0, 30.…\n$ elb_di &lt;dbl&gt; 13.1, 14.0, 13.9, 13.9, 15.2, 14.0, 16.1, 15.1, 14.1, 15.6, 13.…\n$ wri_di &lt;dbl&gt; 10.4, 11.8, 10.9, 11.2, 11.6, 11.5, 12.5, 11.9, 11.2, 12.0, 10.…\n$ kne_di &lt;dbl&gt; 18.8, 20.6, 19.7, 20.9, 20.7, 18.8, 20.8, 21.0, 18.9, 21.1, 19.…\n$ ank_di &lt;dbl&gt; 14.1, 15.1, 14.1, 15.0, 14.9, 13.9, 15.6, 14.6, 13.2, 15.0, 14.…\n$ sho_gi &lt;dbl&gt; 106.2, 110.5, 115.1, 104.5, 107.5, 119.8, 123.5, 120.4, 111.0, …\n$ che_gi &lt;dbl&gt; 89.5, 97.0, 97.5, 97.0, 97.5, 99.9, 106.9, 102.5, 91.0, 93.5, 9…\n$ wai_gi &lt;dbl&gt; 71.5, 79.0, 83.2, 77.8, 80.0, 82.5, 82.0, 76.8, 68.5, 77.5, 81.…\n$ nav_gi &lt;dbl&gt; 74.5, 86.5, 82.9, 78.8, 82.5, 80.1, 84.0, 80.5, 69.0, 81.5, 81.…\n$ hip_gi &lt;dbl&gt; 93.5, 94.8, 95.0, 94.0, 98.5, 95.3, 101.0, 98.0, 89.5, 99.8, 98…\n$ thi_gi &lt;dbl&gt; 51.5, 51.5, 57.3, 53.0, 55.4, 57.5, 60.9, 56.0, 50.0, 59.8, 60.…\n$ bic_gi &lt;dbl&gt; 32.5, 34.4, 33.4, 31.0, 32.0, 33.0, 42.4, 34.1, 33.0, 36.5, 34.…\n$ for_gi &lt;dbl&gt; 26.0, 28.0, 28.8, 26.2, 28.4, 28.0, 32.3, 28.0, 26.0, 29.2, 27.…\n$ kne_gi &lt;dbl&gt; 34.5, 36.5, 37.0, 37.0, 37.7, 36.6, 40.1, 39.2, 35.5, 38.3, 38.…\n$ cal_gi &lt;dbl&gt; 36.5, 37.5, 37.3, 34.8, 38.6, 36.1, 40.3, 36.7, 35.0, 38.6, 40.…\n$ ank_gi &lt;dbl&gt; 23.5, 24.5, 21.9, 23.0, 24.4, 23.5, 23.6, 22.5, 22.0, 22.2, 23.…\n$ wri_gi &lt;dbl&gt; 16.5, 17.0, 16.9, 16.6, 18.0, 16.9, 18.8, 18.0, 16.5, 16.9, 16.…\n$ age    &lt;int&gt; 21, 23, 28, 23, 22, 21, 26, 27, 23, 21, 23, 22, 20, 26, 23, 22,…\n$ wgt    &lt;dbl&gt; 65.6, 71.8, 80.7, 72.6, 78.8, 74.8, 86.4, 78.4, 62.0, 81.6, 76.…\n$ hgt    &lt;dbl&gt; 174.0, 175.3, 193.5, 186.5, 187.2, 181.5, 184.0, 184.5, 175.0, …\n$ sex    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …"
  },
  {
    "objectID": "slides/07-workflow.html#visualize-data",
    "href": "slides/07-workflow.html#visualize-data",
    "title": "Bayesian Workflow",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/07-workflow.html#prepare-data",
    "href": "slides/07-workflow.html#prepare-data",
    "title": "Bayesian Workflow",
    "section": "Prepare data",
    "text": "Prepare data\n\nlibrary(openintro)\ndat &lt;- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))\nhead(dat)\n\n    weight   height  sex\n1 144.6231 68.50397 Male\n2 158.2917 69.01579 Male\n3 177.9128 76.18114 Male\n4 160.0554 73.42524 Male\n5 173.7241 73.70083 Male\n6 164.9056 71.45673 Male"
  },
  {
    "objectID": "slides/07-workflow.html#visualize-data-1",
    "href": "slides/07-workflow.html#visualize-data-1",
    "title": "Bayesian Workflow",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/07-workflow.html#visualize-data-2",
    "href": "slides/07-workflow.html#visualize-data-2",
    "title": "Bayesian Workflow",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/07-workflow.html#motivating-example-predicting-weight-from-height",
    "href": "slides/07-workflow.html#motivating-example-predicting-weight-from-height",
    "title": "Bayesian Workflow",
    "section": "Motivating example: predicting weight from height",
    "text": "Motivating example: predicting weight from height\nResearch question: We would like to understand the relationship between a person’s height and weight. A few particular questions we have are:\n\nHow much does a person’s weight increase when their height increases?\nHow certain we can be about the magnitude of the increase?\nCan we predict a person’s weight based on their height?\n\nData: We will use the bdims dataset from the openintro package. This dataset contains body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender."
  },
  {
    "objectID": "slides/07-workflow.html#scope-out-your-problem",
    "href": "slides/07-workflow.html#scope-out-your-problem",
    "title": "Bayesian Workflow",
    "section": "1. Scope out your problem",
    "text": "1. Scope out your problem"
  },
  {
    "objectID": "slides/07-workflow.html#scope-out-your-problem-1",
    "href": "slides/07-workflow.html#scope-out-your-problem-1",
    "title": "Bayesian Workflow",
    "section": "1. Scope out your problem",
    "text": "1. Scope out your problem"
  },
  {
    "objectID": "slides/07-workflow.html#scope-out-your-problem-2",
    "href": "slides/07-workflow.html#scope-out-your-problem-2",
    "title": "Bayesian Workflow",
    "section": "1. Scope out your problem",
    "text": "1. Scope out your problem"
  },
  {
    "objectID": "slides/07-workflow.html#specify-likelihood-priors",
    "href": "slides/07-workflow.html#specify-likelihood-priors",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\ndata {\n  int&lt;lower = 1&gt; n;\n  vector[n] height;\n  vector[n] weight;\n}"
  },
  {
    "objectID": "slides/07-workflow.html#specify-likelihood-priors-1",
    "href": "slides/07-workflow.html#specify-likelihood-priors-1",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\nparameter {\n  real alpha;\n  real beta;\n  real&lt;lower = 0&gt; sigma;\n}"
  },
  {
    "objectID": "slides/07-workflow.html#specify-likelihood-priors-2",
    "href": "slides/07-workflow.html#specify-likelihood-priors-2",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\nmodel {\n  target += normal_lpdf(weight | alpha + beta * height, sigma);\n}"
  },
  {
    "objectID": "slides/07-workflow.html#specify-likelihood-priors-3",
    "href": "slides/07-workflow.html#specify-likelihood-priors-3",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha\\) is the intercept, or average weight for someone who is zero inches (not a particularly useful number on its own)\n\\(\\beta\\) measures the association between weight and height, in pounds/inch\n\\(\\sigma\\) is the measurement error for the population\n\n\nmodel {\n  target += normal_lpdf(weight | alpha + beta * height, sigma);\n  target += normal_lpdf(alpha | 0, 100);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}"
  },
  {
    "objectID": "slides/07-workflow.html#fake-data",
    "href": "slides/07-workflow.html#fake-data",
    "title": "Bayesian Workflow",
    "section": "3. Fake data",
    "text": "3. Fake data\nSanity check:\n\nDraw parameter values from priors.\nGenerate data based on those parameter values.\nFit model to generated data.\nCheck fit is reasonable.\n\n\ngenerated quantities {\n  vector[n] weight;\n  real alpha = normal_rng(150, 5);\n  real beta = normal_rng(0, 10);\n  real sigma = fabs(normal_rng(0, 5));\n  for (i in 1:n) {\n    weight[i] = normal_rng(alpha + beta * height_c[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/07-workflow.html#quick-aside",
    "href": "slides/07-workflow.html#quick-aside",
    "title": "Bayesian Workflow",
    "section": "Quick aside",
    "text": "Quick aside\nWhat does it mean to use the prior sigma ~ normal(0, 5)?\n\nWhen a parameter is truncated, for example real&lt;lower = 0&gt; sigma, priors can still be placed across the real line, \\(\\mathbb{R}\\).\n\n\nparameters {\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(sigma | 0, 5);\n}\n\n\nThis specification induces a prior on the truncated space \\(\\mathbb{R}^+\\).\nThe induced prior for sigma is a half-normal distribution."
  },
  {
    "objectID": "slides/07-workflow.html#quick-aside-1",
    "href": "slides/07-workflow.html#quick-aside-1",
    "title": "Bayesian Workflow",
    "section": "Quick aside",
    "text": "Quick aside\n\nThe half-normal is a useful prior for nonnegative parameters that should not be too large and may be very close to zero.\nSimilar distributions for scale parameters are half-t and half-Cauchy priors, these have heavier tales."
  },
  {
    "objectID": "slides/07-workflow.html#specify-likelihood-priors-4",
    "href": "slides/07-workflow.html#specify-likelihood-priors-4",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[weight_i] = \\alpha + \\beta \\times (height_i - \\bar{x)},\\quad\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n height_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha\\) is the intercept, or average weight for someone who is an average height\n\n\nmodel {\n  target += normal_lpdf(weight | alpha + beta * height_c, sigma);\n  target += normal_lpdf(alpha | 150, 5);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}"
  },
  {
    "objectID": "slides/07-workflow.html#fake-data-1",
    "href": "slides/07-workflow.html#fake-data-1",
    "title": "Bayesian Workflow",
    "section": "3. Fake data",
    "text": "3. Fake data"
  },
  {
    "objectID": "slides/07-workflow.html#fit-model-to-real-data",
    "href": "slides/07-workflow.html#fit-model-to-real-data",
    "title": "Bayesian Workflow",
    "section": "4. Fit model to real data",
    "text": "4. Fit model to real data\n\n// saved in linear_regression_workflow.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  vector[n] weight; // outcome vector\n  vector[n] height_c; // covariate vector\n  int&lt;lower = 1&gt; n_pred; // number of new observations\n  vector[n_pred] height_c_pred; // vector for new observations\n}\nparameters {\n  real alpha;\n  real beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(weight | alpha + height_c * beta, sigma); // likelihood\n  target += normal_lpdf(alpha | 150, 5);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n_pred] out_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(alpha + height_c[i] * beta, sigma);\n    log_lik[i] = normal_lpdf(weight[i] | alpha + height_c[i] * beta, sigma);\n  }\n  for (i in 1:n_pred) {\n    out_sample[i] = normal_rng(alpha + height_c_pred[i] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/07-workflow.html#fit-model-to-real-data-1",
    "href": "slides/07-workflow.html#fit-model-to-real-data-1",
    "title": "Bayesian Workflow",
    "section": "4. Fit model to real data",
    "text": "4. Fit model to real data\n\nstan_data &lt;- list(n = nrow(dat), \n                  height_c = (dat$height - mean(dat$height)), \n                  weight = dat$weight)\nregression_model &lt;- stan_model(file = \"linear_regression_workflow.stan\")\nfit &lt;- sampling(regression_model, data = stan_data)\nprint(fit)\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd   2.5%    50%  97.5% n_eff Rhat\nalpha 152.35    0.01 0.88 150.63 152.35 154.08  3773    1\nbeta    5.69    0.00 0.25   5.21   5.69   6.18  4215    1\nsigma  20.25    0.01 0.60  19.13  20.23  21.50  3571    1\n\nSamples were drawn using NUTS(diag_e) at Wed Nov 27 14:08:19 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/07-workflow.html#diagnostics",
    "href": "slides/07-workflow.html#diagnostics",
    "title": "Bayesian Workflow",
    "section": "5. Diagnostics",
    "text": "5. Diagnostics\n\nrstan::traceplot(fit, pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/07-workflow.html#diagnostics-1",
    "href": "slides/07-workflow.html#diagnostics-1",
    "title": "Bayesian Workflow",
    "section": "5. Diagnostics",
    "text": "5. Diagnostics\n\nlibrary(bayesplot)\nmcmc_acf(fit, regex_pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/07-workflow.html#graph-fit",
    "href": "slides/07-workflow.html#graph-fit",
    "title": "Bayesian Workflow",
    "section": "6. Graph fit",
    "text": "6. Graph fit"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-checking",
    "href": "slides/07-workflow.html#posterior-predictive-checking",
    "title": "Bayesian Workflow",
    "section": "Posterior predictive checking",
    "text": "Posterior predictive checking"
  },
  {
    "objectID": "slides/07-workflow.html#graph-fit-1",
    "href": "slides/07-workflow.html#graph-fit-1",
    "title": "Bayesian Workflow",
    "section": "6. Graph fit",
    "text": "6. Graph fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior mean and 95% credible interval for \\(\\mu = \\alpha + \\beta \\times height_i\\)."
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-fit",
    "href": "slides/07-workflow.html#posterior-predictive-fit",
    "title": "Bayesian Workflow",
    "section": "Posterior predictive fit",
    "text": "Posterior predictive fit"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive",
    "href": "slides/07-workflow.html#posterior-predictive",
    "title": "Bayesian Workflow",
    "section": "Posterior predictive",
    "text": "Posterior predictive\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(dat$weight, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/07-workflow.html#visualizing-posterior-predictive-check",
    "href": "slides/07-workflow.html#visualizing-posterior-predictive-check",
    "title": "Bayesian Workflow",
    "section": "Visualizing posterior predictive check",
    "text": "Visualizing posterior predictive check\nppc_stat(dat$weight, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(dat$weight, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(dat$weight, y_pred, stat = \"q025\")\nppc_stat(dat$weight, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-checks",
    "href": "slides/07-workflow.html#posterior-predictive-checks",
    "title": "Bayesian Workflow",
    "section": "7. Posterior predictive checks",
    "text": "7. Posterior predictive checks\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(dat$weight, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-checks-1",
    "href": "slides/07-workflow.html#posterior-predictive-checks-1",
    "title": "Bayesian Workflow",
    "section": "7. Posterior predictive checks",
    "text": "7. Posterior predictive checks\nppc_stat(dat$weight, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(dat$weight, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(dat$weight, y_pred, stat = \"q025\")\nppc_stat(dat$weight, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-regression-fit",
    "href": "slides/07-workflow.html#posterior-predictive-regression-fit",
    "title": "Bayesian Workflow",
    "section": "Posterior predictive regression fit",
    "text": "Posterior predictive regression fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior predictive distribution mean and 95% credible interval, \\(f(weight_i' | weight_{1:n})\\)."
  },
  {
    "objectID": "slides/07-workflow.html#shinystan",
    "href": "slides/07-workflow.html#shinystan",
    "title": "Bayesian Workflow",
    "section": "shinystan",
    "text": "shinystan\n\nlibrary(shinystan)\ny &lt;- dat$weight # need to define outcome as a global variable to be accessible\nsso &lt;- shinystan::launch_shinystan(fit)"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-checks-2",
    "href": "slides/07-workflow.html#posterior-predictive-checks-2",
    "title": "Bayesian Workflow",
    "section": "7. Posterior predictive checks",
    "text": "7. Posterior predictive checks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior predictive distribution mean and 95% credible interval, \\(f(weight_i' | weight_{1:n})\\)."
  },
  {
    "objectID": "slides/07-workflow.html#compare-models",
    "href": "slides/07-workflow.html#compare-models",
    "title": "Bayesian Workflow",
    "section": "8. Compare models:",
    "text": "8. Compare models:\n\nIterate on model design, choose a model.\nWe’ve talked about performing sensitivity analyses to choice of prior.\nWe have not introduced formal methods for model comparison."
  },
  {
    "objectID": "slides/07-workflow.html#model-comparison",
    "href": "slides/07-workflow.html#model-comparison",
    "title": "Bayesian Workflow",
    "section": "Model comparison",
    "text": "Model comparison\n\nModel selection criteria are designed to help comparing several models.\nAn ideal criteria will not just based on their fit with training data, but on an estimation of their prediction accuracy with new data.\nCriteria often reward models that offer a good compromise between simplicity and accuracy.\nExamples: Likelihood ratio test, AIC"
  },
  {
    "objectID": "slides/07-workflow.html#likelihood-ratio",
    "href": "slides/07-workflow.html#likelihood-ratio",
    "title": "Bayesian Workflow",
    "section": "Likelihood ratio",
    "text": "Likelihood ratio\n\nThe likelihood ratio method consists in assessing whether increasing the complexity of a model results in a significant improvement of likelihood which justifies this increased complexity.\nIt is a computationally inexpensive method, since it only relies on the value of the total likelihood function, but can only be used to compare nested models.\nNot often used for Bayesian models."
  },
  {
    "objectID": "slides/07-workflow.html#waic",
    "href": "slides/07-workflow.html#waic",
    "title": "Bayesian Workflow",
    "section": "WAIC",
    "text": "WAIC\n(Gelman, Hwang, and Vehtari (2014)). A fully Bayesian criterion is the Widely Applicable Information Criterion (WAIC) (Watanabe and Opper (2010)), asymptotically equal to the Bayesian leave-one-out cross validation criterion."
  },
  {
    "objectID": "slides/07-workflow.html#information-criteria-1",
    "href": "slides/07-workflow.html#information-criteria-1",
    "title": "Bayesian Workflow",
    "section": "Information criteria",
    "text": "Information criteria\n\nIt is sometimes useful to consider predictive accuracy given a point estimate \\(\\hat{\\theta}(\\mathbf{Y})\\), thus,\n\n\\[\\text{expected log predictive density, given }\\hat{\\theta}(\\mathbf{Y}): \\mathbb{E}[]\\]\n\nThe frequentist Akaike information criterion (AIC) is the oldest and most restrictive,\n\n\\[AIC = -2 \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{MLE}) + 2d,\\] where \\(d\\) is the number of parameters in the model.\n\nAmong Bayesians, the deviance information criterion (DIC) has been widely used for some time, now. However, the DIC is limited in that it presumes the posterior is multivariate Gaussian, which is not always the case.\nThe widely applicable information criterion (WAIC) does not impose assumptions on the shape of the posterior distribution.\n\nNote: WAIC provides an estimate of out-of-sample deviance and converges with LOO-CV as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/07-workflow.html#compare-models-1",
    "href": "slides/07-workflow.html#compare-models-1",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\nSuppose we would like to predict weight and we would like to compare our original model with a model that also includes sex. We can compare these models using WAIC."
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-information-criteria-bic",
    "href": "slides/07-workflow.html#bayesian-information-criteria-bic",
    "title": "Bayesian Workflow",
    "section": "Bayesian information criteria (BIC)",
    "text": "Bayesian information criteria (BIC)\n\nThe Bayesian information criteria is similar\n\n\\[BIC = D(\\mathbf{Y}|\\hat{\\boldsymbol{\\theta}})+ \\log(n)p\\]\n\nThis is motivated as an approximation to the log Bayes factor of the model compared to the null model.\nHowever, this is only an asymptotic (large \\(n\\)) approximation.\nWith large \\(n\\) the prior is irrelevant, and so this is not satisfying to a subjective Bayesian."
  },
  {
    "objectID": "slides/07-workflow.html#deviance-information-criteria-dic",
    "href": "slides/07-workflow.html#deviance-information-criteria-dic",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nDIC is a popular Bayesian analog of AIC or BIC.\nUnlike CV, DIC requires only one model fit.\nHowever, proceed with caution.\nDIC really only applies when the posterior is approximately normal, and will give misleading results when the posterior far from normality (e.g., bimodal).\nDIC is also criticized for selecting overly-complex models."
  },
  {
    "objectID": "slides/07-workflow.html#deviance-information-criteria-dic-1",
    "href": "slides/07-workflow.html#deviance-information-criteria-dic-1",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nLet \\(\\bar{D} = \\mathbb{E}[D(\\mathbf{Y}|\\boldsymbol{\\theta})|\\mathbf{Y}]\\) be the posterior mean of the deviance.\nDenote \\(\\hat{\\boldsymbol{\\theta}}\\) as the posterior mean of \\(\\boldsymbol{\\theta}\\).\nThe effective number of parameters is \\(p_D = \\bar{D} − D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}})\\).\nDIC can be written like AIC,\n\n\\[DIC = \\bar{D} + p_D =D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}}) + 2p_D\\]\n\nModels with small \\(\\bar{D}\\) fit the data well.\nModels with small \\(p_D\\) are simple.\nWe select the model with smallest DIC."
  },
  {
    "objectID": "slides/07-workflow.html#deviance-information-criteria-dic-2",
    "href": "slides/07-workflow.html#deviance-information-criteria-dic-2",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nThe effective number of parameters is a useful measure of model complexity\nIntuitively, if there are \\(p\\) parameters and we have uninformative priors then \\(p_D \\approx p\\)\nHowever, \\(p_D \\ll p\\) if there are strong priors.\nAs with AIC or BIC, the actual value is meaningless, only differences are relevant.\nDIC can only be used to compare models with the same likelihood."
  },
  {
    "objectID": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic-1",
    "href": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic-1",
    "title": "Bayesian Workflow",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC is written in terms of the posterior of the likelihood rather than parameters\nLet \\(m_i\\) be the posterior mean of \\(f(Y_i|\\boldsymbol{\\theta})\\) and \\(v_i\\) be the posterior variance of \\(\\log f(Y_i|\\boldsymbol{\\theta})\\)\nThe effective model size is \\(p_W = \\sum_{i=1}^n v_i\\)\nThe criteria is\n\n\\[WAIC = −2 \\sum_{i=1}^n \\log (m_i) + 2p_W\\]"
  },
  {
    "objectID": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic-2",
    "href": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic-2",
    "title": "Bayesian Workflow",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\\[\\text{WAIC}(\\mathbf{Y},\\boldsymbol{\\theta}) = -2\\left(\\text{lppd} - p_{\\text{WAIC}}\\right),\\] where \\(p_{\\text{WAIC}} = \\sum_{i = 1}^n \\mathbb{V}_{\\boldsymbol{\\theta}}\\left(\\log f\\left(Y_i | \\boldsymbol{\\theta}\\right)\\right)\\) and log pointwise predictive density (lppd) is, \\[\\text{lppd}(\\mathbf{Y},\\boldsymbol{\\theta}) = \\sum_{i = 1}^n \\log \\frac{1}{S} \\sum_{s = 1}^S f\\left({Y}_i | \\boldsymbol{\\theta}^{(s)}\\right).\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html",
    "href": "slides/02-monte-carlo.html",
    "title": "Monte Carlo Sampling",
    "section": "",
    "text": "Monte Carlo (MC) sampling\nBayesian inference using MC sampling"
  },
  {
    "objectID": "slides/02-monte-carlo.html#simulating-pi-using-monte-carlo",
    "href": "slides/02-monte-carlo.html#simulating-pi-using-monte-carlo",
    "title": "Monte Carlo Sampling",
    "section": "Simulating \\(\\pi\\) using Monte Carlo",
    "text": "Simulating \\(\\pi\\) using Monte Carlo\nSuppose we are interested in estimating \\(\\pi\\).\n\n\n\nWe can formulate \\(\\pi\\) as a function of the area of a square and circle.\nArea of a circle: \\(A_c = \\pi r^2\\)\nArea of a square: \\(A_s = 4 r^2\\)\nThe ratio of the two areas is: \\(\\frac{A_c}{A_s} = \\frac{\\pi r^2}{4 r^2} \\implies \\pi = \\frac{4 A_c}{A_s}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we have an estimate for the ratio we can solve for \\(\\pi\\). The challenge becomes estimating this ratio."
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-simulations",
    "href": "slides/02-monte-carlo.html#monte-carlo-simulations",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo Simulations",
    "text": "Monte Carlo Simulations\nThis is where we can take advantage of how quickly a computer can generate pseudorandom numbers. There is a whole class of algorithms called Monte Carlo simulations that exploit randomness to estimate real world scenarios that would otherwise be difficult to explicitly calculate. We can use a Monte Carlo simulation to estimate the area ratio of the circle to the square. Imagine you randomly drop grains of sand into the area of the square. By counting the total number of sand grains in the square (all of them since you’re an accurate dropper) to the number of sand grains inside the circle we get this estimate. Multiply the estimated ratio by four and you get an estimate for π. The more sand grains you use the more accurate your estimate of π.\nThe algorithm for this method is straightforward:\n\nGenerate a random point \\((x, y)\\) inside a square of side 2 centered at the origin. This is equivalent to assuming the following:\n\n\n\\(x \\sim Uniform(-1, 1)\\)\n\\(y \\sim Uniform(-1, 1)\\)\n\n\nDetermine whether the point falls inside the unit circle inscribed in the square by checking whether \\(x^2 + y^2 \\leq 1\\).\nRepeat steps 1 and 2 for a large number of points (\\(n\\)).\nCalculate the ratio of the number of points that fell inside the circle to the total number of points generated.\nMultiply the ratio by 4 to estimate the value of pi."
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-big-of-a-difference-does-n-make",
    "href": "slides/02-monte-carlo.html#how-big-of-a-difference-does-n-make",
    "title": "Monte Carlo Sampling",
    "section": "How big of a difference does \\(n\\) make",
    "text": "How big of a difference does \\(n\\) make\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n###Computing pi with differenct \nestimate_pi &lt;- function(n) {\n  count &lt;- 0\n  x &lt;- runif(n, -1, 1)\n  y &lt;- runif(n, -1, 1)\n  4 * mean(x^2 + y^2 &lt;= 1)\n}\nn_seq_log &lt;- seq(1, 7, length.out = 50)\nn_seq &lt;- 10^n_seq_log\npis &lt;- unlist(lapply(n_seq, estimate_pi))\npar(mfcol = c(1, 1))\nplot(n_seq_log, pis, type = \"b\", ylab = expression(paste(\"Estimated value of \", pi)), pch = 16, xaxt = \"n\", xlab = \"Number of points\")\naxis(1, at = 1:7, labels = c(expression(10^1), expression(10^2), expression(10^3), expression(10^4), expression(10^5), expression(10^6), expression(10^7)))\nabline(h = pi, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", legend = \"True value\", lwd = 2, lty = 2, col = \"red\", bty = \"n\")\n\n\n\n\n\n\n\npar(mfcol = c(1, 1))\nplot(n_seq_log, abs(pis - pi), type = \"b\", ylab = expression(paste(\"Estimated value of \", pi)), pch = 16, xaxt = \"n\", xlab = \"Number of points\")\naxis(1, at = 1:7, labels = c(expression(10^1), expression(10^2), expression(10^3), expression(10^4), expression(10^5), expression(10^6), expression(10^7)))\nabline(h = pi, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", legend = \"True value\", lwd = 2, lty = 2, col = \"red\", bty = \"n\")"
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-sampling-1",
    "href": "slides/02-monte-carlo.html#monte-carlo-sampling-1",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo sampling",
    "text": "Monte Carlo sampling\n\nWe can take advantage of how quickly a computer can generate pseudo-random numbers.\nThere is a class of algorithms called Monte Carlo sampling that exploit randomness to estimate real world scenarios that would otherwise be difficult to explicitly calculate.\nThe name comes from the Monte Carlo Casino in Monaco, where the primary developer of the method, mathematician Stanislaw Ulam, was inspired by his uncle’s gambling habits. (This is who Stan was named after!)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#algorithm-for-estimating-pi",
    "href": "slides/02-monte-carlo.html#algorithm-for-estimating-pi",
    "title": "Monte Carlo Sampling",
    "section": "Algorithm for estimating \\(\\pi\\)",
    "text": "Algorithm for estimating \\(\\pi\\)\n\nGenerate a random point \\((x, y)\\) inside a square centered at the origin with length 2.\n\n\nThis is equivalent to assuming:\n\n\\(f_{X,Y}(x,y) = f_X(x)f_Y(y)\\)\n\\(f_X(x) = Uniform(-1, 1)\\), \\(f_Y(y) = Uniform(-1, 1)\\)\n\n\n. . .\n\nDetermine whether the point falls inside the unit circle inscribed in the square by checking whether \\(x^2 + y^2 \\leq 1\\).\n\n. . .\n\nRepeat steps 1 and 2 for a large number of points (\\(S\\))."
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-big-of-a-difference-does-s-make",
    "href": "slides/02-monte-carlo.html#how-big-of-a-difference-does-s-make",
    "title": "Monte Carlo Sampling",
    "section": "How big of a difference does \\(S\\) make",
    "text": "How big of a difference does \\(S\\) make"
  },
  {
    "objectID": "slides/02-monte-carlo.html#estimating-pi-with-increasing-s",
    "href": "slides/02-monte-carlo.html#estimating-pi-with-increasing-s",
    "title": "Monte Carlo Sampling",
    "section": "Estimating \\(\\pi\\) with increasing \\(S\\)",
    "text": "Estimating \\(\\pi\\) with increasing \\(S\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#error-in-estimating-pi-using-monte-carlo",
    "href": "slides/02-monte-carlo.html#error-in-estimating-pi-using-monte-carlo",
    "title": "Monte Carlo Sampling",
    "section": "Error in estimating \\(\\pi\\) using Monte Carlo",
    "text": "Error in estimating \\(\\pi\\) using Monte Carlo"
  },
  {
    "objectID": "slides/02-monte-carlo.html#error-in-estimating-pi",
    "href": "slides/02-monte-carlo.html#error-in-estimating-pi",
    "title": "Monte Carlo Sampling",
    "section": "Error in estimating \\(\\pi\\)",
    "text": "Error in estimating \\(\\pi\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-estimation-of-the-ratio",
    "href": "slides/02-monte-carlo.html#monte-carlo-estimation-of-the-ratio",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo estimation of the ratio",
    "text": "Monte Carlo estimation of the ratio\n\nWe can use a Monte Carlo simulation to estimate the area ratio of the circle to the square.\nImagine you randomly drop grains of sand into the area of the square. By counting the total number of sand grains in the square (all of them since you’re an accurate dropper) to the number of sand grains inside the circle we get this estimate.\nMultiply the estimated ratio by 4 and you get an estimate for \\(\\pi\\).\nThe more grains of sand that are used the more accurate your estimate of \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling",
    "href": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling",
    "title": "Monte Carlo Sampling",
    "section": "Intuition behing Monte Carlo sampling",
    "text": "Intuition behing Monte Carlo sampling\n\nAssume \\(X \\sim Uniform(-1,1)\\) and \\(Y \\sim Uniform(-1,1)\\).\nWe can write our problem as, \\(\\pi = 4P(X^2 + Y^2 \\leq 1)\\).\nHow could we do this without Monte Carlo?\nDefine, \\(Z = X^2 + Y^2\\). We could then use change-of-variables to compute the density of \\(Z\\) and then compute \\(P(Z \\leq 1)\\).\n\nThis is generally difficult!"
  },
  {
    "objectID": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling-1",
    "href": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling-1",
    "title": "Monte Carlo Sampling",
    "section": "Intuition behing Monte Carlo sampling",
    "text": "Intuition behing Monte Carlo sampling\n\nInstead, we could write our problem as an expectation and use the law of large numbers,\n\n\\[P(X^2 + Y^2 \\leq 1) = \\mathbb{E}_{X,Y}\\left[1\\left(X^2 + Y^2 \\leq 1\\right)\\right].\\]\n\nRecall: \\(\\mathbb{E}_X[1(A)] = \\int_A f_X(x)dx = P(X \\in A)\\).\nWe then have that, \\[\\frac{1}{S}\\sum_{i = 1}^S 1\\left(X_i^2 + Y_i^2 \\leq 1\\right) \\rightarrow \\mathbb{E}_{X,Y}\\left[1\\left(X^2 + Y^2 \\leq 1\\right)\\right],\\]\n\nwhere \\((X_i,Y_i) \\sim f(X,Y)\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#a-motivating-example",
    "href": "slides/02-monte-carlo.html#a-motivating-example",
    "title": "Monte Carlo Sampling",
    "section": "A motivating example",
    "text": "A motivating example\n\nSuppose we are interested in estimating the prevalence of diabetes in Durham County. We aim to estimate this prevalence by taking a sample of \\(n\\) individuals in Durham County and we record whether or not they have diabetes, \\(Y_i\\).\nWe assume that \\(Z = \\sum_{i=1}^n Y_i \\sim Binomial(n, \\pi)\\) for \\(i = 1,\\ldots,n\\).\nOur goal is to estimate \\(\\pi\\) and perform statistical inference (e.g., point estimation, interval estimation, etc.)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#posterior-inference",
    "href": "slides/02-monte-carlo.html#posterior-inference",
    "title": "Monte Carlo Sampling",
    "section": "Posterior inference",
    "text": "Posterior inference\n\nIn Bayesian statistics, inference is encoded through the posterior distribution,\n\n\\[\\begin{aligned}\nf(\\pi | Z) &= \\frac{f(Z | \\pi)f(\\pi)}{f(Z)},\\\\\n&= \\frac{f(Z | \\pi)f(\\pi)}{\\int f(Z | \\pi)f(\\pi)d\\pi}.\n\\end{aligned}\\]\n\nAll we have to do is specify the likelihood and prior."
  },
  {
    "objectID": "slides/02-monte-carlo.html#likelihood-specification",
    "href": "slides/02-monte-carlo.html#likelihood-specification",
    "title": "Monte Carlo Sampling",
    "section": "Likelihood specification",
    "text": "Likelihood specification\n\\(Z\\) is a Binomial distribution with pmf,\n\\[f(Z | \\pi) = P(Z = z) = {n \\choose z} \\pi^z(1-\\pi)^{n-z},\\]\nwhere \\(z \\in \\{0, 1, \\ldots, n\\}\\).\n\n\\({n \\choose z} = \\frac{n!}{z!(n-z)!} = \\frac{\\Gamma(n+1)}{\\Gamma(z+1)\\Gamma(n-z+1)}\\)\n\\(\\Gamma(x) = (x-1)!\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#prior-specification",
    "href": "slides/02-monte-carlo.html#prior-specification",
    "title": "Monte Carlo Sampling",
    "section": "Prior specification",
    "text": "Prior specification\nWhat do we know about \\(\\pi\\)?\n\n\\(\\pi\\) is continuous.\n\\(\\pi \\in (0,1)\\).\n\nWe should place a distribution on \\(\\pi\\) that permits these properties.\n. . .\nOne option is the Beta distribution, \\(\\pi \\sim Beta(\\alpha,\\beta)\\), \\[f(\\pi) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}.\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-posterior-1",
    "href": "slides/02-monte-carlo.html#computing-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Computing the posterior",
    "text": "Computing the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#using-the-kernel-to-find-the-posterior-1",
    "href": "slides/02-monte-carlo.html#using-the-kernel-to-find-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Using the kernel to find the posterior",
    "text": "Using the kernel to find the posterior\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{aligned}\\]\nThis is the kernel of a multivariate normal for \\(\\boldsymbol{\\beta}\\), with \\(\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}\\) and \\(\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\). It’s easy to see then that, \\(f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\\)\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#suppose-we-have",
    "href": "slides/02-monte-carlo.html#suppose-we-have",
    "title": "Monte Carlo Sampling",
    "section": "Suppose we have",
    "text": "Suppose we have\nSuppose we conducted a simple random sample of 500 individuals in Durham County and 120 responsed that they had diabetes and 380 that they did not."
  },
  {
    "objectID": "slides/02-monte-carlo.html#lets-inspect-the-posterior",
    "href": "slides/02-monte-carlo.html#lets-inspect-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Let’s inspect the posterior",
    "text": "Let’s inspect the posterior\n\nSuppose we conducted a simple random sample of 500 individuals in Durham County and 120 responsed that they had diabetes and 380 that they did not.\nThe posterior becomes, \\(Beta\\left(\\alpha + 120, \\beta + 380\\right)\\).\nWe must choose our prior distribution wisely.\nNote that:\n\n\\(\\mathbb{E}[\\pi] = \\alpha/(\\alpha + \\beta)\\)\n\\(\\mathbb{V}(\\pi) = (\\alpha\\beta)/[(\\alpha + \\beta)^2(\\alpha + \\beta + 1)]\\).\n\nTypically, \\(\\alpha = \\beta = 1\\), which corresponds to a uniform prior on \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#lets-inspect-the-posterior-1",
    "href": "slides/02-monte-carlo.html#lets-inspect-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Let’s inspect the posterior",
    "text": "Let’s inspect the posterior"
  },
  {
    "objectID": "slides/02-monte-carlo.html#suppose-my-prior-changes",
    "href": "slides/02-monte-carlo.html#suppose-my-prior-changes",
    "title": "Monte Carlo Sampling",
    "section": "Suppose my prior changes",
    "text": "Suppose my prior changes"
  },
  {
    "objectID": "slides/02-monte-carlo.html#returning-to-our-posterior",
    "href": "slides/02-monte-carlo.html#returning-to-our-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Returning to our posterior",
    "text": "Returning to our posterior\nLet’s obtain \\(S = 1,000\\) samples from our posterior.\n\npi_samples &lt;- rbeta(1000, 1 + 120, 1 + 380)\n\nWe can compute the posterior mean and variance.\n\nmean(pi_samples)\n\n[1] 0.2404674\n\nvar(pi_samples)\n\n[1] 0.0003437017"
  },
  {
    "objectID": "slides/02-monte-carlo.html#algorithm-for-estimating-pi-1",
    "href": "slides/02-monte-carlo.html#algorithm-for-estimating-pi-1",
    "title": "Monte Carlo Sampling",
    "section": "Algorithm for estimating \\(\\pi\\)",
    "text": "Algorithm for estimating \\(\\pi\\)\n\nGenerate a random point \\((x, y)\\) inside a square centered at the origin with length 2.\nDetermine whether the point falls inside the unit circle inscribed in the square by checking whether \\(x^2 + y^2 \\leq 1\\).\nRepeat steps 1 and 2 for a large number of points (\\(S\\)).\nCalculate the ratio of the number of points that fell inside the circle to the total number of points generated.\n\n. . .\n\nMultiply the ratio by 4 to estimate the value of \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-marginal-likelihood",
    "href": "slides/02-monte-carlo.html#computing-the-marginal-likelihood",
    "title": "Monte Carlo Sampling",
    "section": "Computing the marginal likelihood",
    "text": "Computing the marginal likelihood\nUnder our prior specification, we can compute the marginal likelihood, \\(f(Z)\\):\n\\[\\begin{aligned}\nf(Z) &= \\int f(Z | \\pi) f(\\pi) d\\pi\\\\\n&= \\int {n \\choose z} \\pi^z(1-\\pi)^{n-z} \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} d\\pi\\\\\n&= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\int \\underbrace{\\pi^{(\\alpha + z) - 1}(1-\\pi)^{(\\beta + n - z) - 1}}_{Beta\\text{ }kernel} d\\pi\n\\end{aligned}\\]\n\n\n\n\n\n\nDefinition\n\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample: The kernel of the Beta pdf is \\(\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-marginal-likelihood-1",
    "href": "slides/02-monte-carlo.html#computing-the-marginal-likelihood-1",
    "title": "Monte Carlo Sampling",
    "section": "Computing the marginal likelihood",
    "text": "Computing the marginal likelihood\nUnder our prior specification, we can compute the marginal likelihood, \\(f(Z)\\):\n\\[\\begin{aligned}\nf(Z) &= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\int \\pi^{(\\alpha + z) - 1}(1-\\pi)^{(\\beta + n - z) - 1} d\\pi\\\\\n&= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta + n)}{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#we-can-then-compute-the-posetrior",
    "href": "slides/02-monte-carlo.html#we-can-then-compute-the-posetrior",
    "title": "Monte Carlo Sampling",
    "section": "We can then compute the posetrior",
    "text": "We can then compute the posetrior\n\\[\\begin{aligned}\nf(\\pi | Z) &\\propto f(Z | \\pi) f(\\pi)\\\\\n&= \\frac{{n \\choose z} \\pi^z(1-\\pi)^{n-z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}}{{n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta) }{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#we-can-then-compute-the-posterior",
    "href": "slides/02-monte-carlo.html#we-can-then-compute-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "We can then compute the posterior",
    "text": "We can then compute the posterior\n\\[\\begin{aligned}\nf(\\pi | Z) &= \\frac{f(Z | \\pi) f(\\pi)}{f(Z)}\\\\\n&= \\frac{{n \\choose z} \\pi^z(1-\\pi)^{n-z} \\times \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}}{{n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta) }{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}}\\\\\n&=\\frac{\\Gamma(\\alpha+z)\\Gamma(\\beta+n-z)}{\\Gamma(\\alpha + \\beta + n)}\\pi^{(\\alpha + z) - 1} (1 - \\pi)^{(\\beta + n - z) - 1}\\\\\n&=Beta\\left(\\alpha + z, \\beta + n - z\\right).\n\\end{aligned}\\]\n\nA prior that is considered conjugate yields a posterior with the same distribution.\nThe Beta distribution is conjugate for the Bernoulli/Binomial distributions."
  },
  {
    "objectID": "slides/03-mcmc.html#defining-the-model",
    "href": "slides/03-mcmc.html#defining-the-model",
    "title": "Markov chain Monte Carlo",
    "section": "Defining the model",
    "text": "Defining the model\nSuppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#defining-the-likelihood",
    "href": "slides/03-mcmc.html#defining-the-likelihood",
    "title": "Markov chain Monte Carlo",
    "section": "Defining the likelihood",
    "text": "Defining the likelihood\nThe individual likelihood contribution for subject \\(i\\) is given by,\n\\[Y_i|\\boldsymbol{\\beta},\\sigma^2 \\stackrel{ind}{\\sim} N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta},\\sigma^2) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta},\\sigma^2),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#matrix-likelihood-specification",
    "href": "slides/03-mcmc.html#matrix-likelihood-specification",
    "title": "Markov chain Monte Carlo",
    "section": "Matrix likelihood specification",
    "text": "Matrix likelihood specification\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2 ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-estimation",
    "href": "slides/03-mcmc.html#linear-regression-estimation",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression estimation",
    "text": "Linear regression estimation\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} \\log f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#bayesian-estimation",
    "href": "slides/03-mcmc.html#bayesian-estimation",
    "title": "Markov chain Monte Carlo",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#prior-definition",
    "href": "slides/03-mcmc.html#prior-definition",
    "title": "Markov chain Monte Carlo",
    "section": "Prior definition",
    "text": "Prior definition\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/03-mcmc.html#computing-the-posterior",
    "href": "slides/03-mcmc.html#computing-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Computing the posterior",
    "text": "Computing the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#techniques-to-find-this-posterior",
    "href": "slides/03-mcmc.html#techniques-to-find-this-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Techniques to find this posterior",
    "text": "Techniques to find this posterior\n\nBrute force: complete the square\nEasy: kernel recognition\n\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the multivariate normal pdf, \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#using-the-kernel-to-find-the-posterior",
    "href": "slides/03-mcmc.html#using-the-kernel-to-find-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Using the kernel to find the posterior",
    "text": "Using the kernel to find the posterior\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{aligned}\\]\nThis is the kernel of a multivariate normal for \\(\\boldsymbol{\\beta}\\), with \\(\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}\\) and \\(\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\). It’s easy to see then that, \\(f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\\)\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#back-to-the-posterior",
    "href": "slides/03-mcmc.html#back-to-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Back to the posterior",
    "text": "Back to the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#visualize-simulated-data",
    "href": "slides/03-mcmc.html#visualize-simulated-data",
    "title": "Markov chain Monte Carlo",
    "section": "Visualize simulated data",
    "text": "Visualize simulated data"
  },
  {
    "objectID": "slides/03-mcmc.html#inspecting-the-prior",
    "href": "slides/03-mcmc.html#inspecting-the-prior",
    "title": "Markov chain Monte Carlo",
    "section": "Inspecting the prior",
    "text": "Inspecting the prior\n\n###Define hyperparameters\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\na &lt;- 3\nb &lt;- 1"
  },
  {
    "objectID": "slides/03-mcmc.html#comparison-with-olsmle",
    "href": "slides/03-mcmc.html#comparison-with-olsmle",
    "title": "Markov chain Monte Carlo",
    "section": "Comparison with OLS/MLE",
    "text": "Comparison with OLS/MLE\n\n###Compute posterior moments\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\nLet’s compare the posterior mean with the OLS/MLE estimate for the regression parameter \\(\\boldsymbol{\\beta}\\).\n\n\n\n\n\nparameter\ntrue\nbayes\nols/mle\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984"
  },
  {
    "objectID": "slides/03-mcmc.html#summarizing-the-posterior",
    "href": "slides/03-mcmc.html#summarizing-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\n\nPosterior means, medians, modes, and variances\nJoint, conditional, and marginal probabilities, for example: \\(P(\\beta_j &lt; c | \\mathbf{Y})\\)\n\\(\\alpha\\)-quantiles: \\(\\{q_{\\alpha} : P(\\beta_j &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n\\(\\ldots\\)\n\nSummarizing a posterior (with known parametric form) is straightforward for basic quantities of interest."
  },
  {
    "objectID": "slides/03-mcmc.html#summarization-can-be-complex",
    "href": "slides/03-mcmc.html#summarization-can-be-complex",
    "title": "Markov chain Monte Carlo",
    "section": "Summarization can be complex",
    "text": "Summarization can be complex\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\beta_j &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pnorm}\\))\n\n\nThese methods work well for standard posterior quantities and distributions."
  },
  {
    "objectID": "slides/03-mcmc.html#summarizing-the-posterior-1",
    "href": "slides/03-mcmc.html#summarizing-the-posterior-1",
    "title": "Markov chain Monte Carlo",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution.\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible.\n\nWhat are our options?"
  },
  {
    "objectID": "slides/03-mcmc.html#how-can-we-use-the-posterior-1",
    "href": "slides/03-mcmc.html#how-can-we-use-the-posterior-1",
    "title": "Markov chain Monte Carlo",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nLet’s simulate some data again:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n###Define hyperparameteters\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\na &lt;- 3\nb &lt;- 1"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta",
    "href": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\boldsymbol{\\beta}\\)",
    "text": "Full conditional for \\(\\boldsymbol{\\beta}\\)\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta},\\sigma^2 | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{aligned}\\]\nThis is the kernel of a multivariate normal for \\(\\boldsymbol{\\beta}\\), with \\(\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}\\) and \\(\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\). It’s easy to see then that, \\(f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\\)\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-3",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-3",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nComputing the full conditionals.\n\n\\(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2 \\sim N \\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}], \\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y})\\right)\\)\nFull conditional for \\(\\sigma^2\\), assuming \\(f(\\sigma^2) \\sim IG(a, b)\\):\n\n\\[\\sigma^2 |  \\mathbf{Y} , \\boldsymbol{\\beta} \\sim IG\\left(a + \\frac{n}{2},b+\\frac{\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)}{2}\\right)\\]\n\nWhy inverse-Gamma (\\(IG\\)) distribution for \\(\\sigma^2\\)?"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta-1",
    "href": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta-1",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\boldsymbol{\\beta}\\)",
    "text": "Full conditional for \\(\\boldsymbol{\\beta}\\)\nThe full conditional can be found in closed-form and is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n. . .\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n. . .\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-sigma2",
    "href": "slides/03-mcmc.html#full-conditional-for-sigma2",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\sigma^2\\)",
    "text": "Full conditional for \\(\\sigma^2\\)\nFull conditional for \\(\\sigma^2\\), assuming \\(f(\\sigma^2) \\sim IG(a, b)\\):\n\\[\\sigma^2 |  \\mathbf{Y} , \\boldsymbol{\\beta} \\sim IG\\left(a + \\frac{n}{2},b+\\frac{\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)}{2}\\right)\\]\n\nWhy inverse-Gamma (\\(IG\\)) distribution for \\(\\sigma^2\\)?"
  },
  {
    "objectID": "slides/03-mcmc.html#sampling-from-the-posterior",
    "href": "slides/03-mcmc.html#sampling-from-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Sampling from the posterior",
    "text": "Sampling from the posterior\nLet’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-workflow-3",
    "href": "slides/07-workflow.html#bayesian-workflow-3",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\nCheck diagnostics: Use MCMC diagnostics to guarentee that the algorithm converged.\n\n\n\nExamine posterior fit: Create posterior summaries that are relevant to the research question.\n\n\n\n\nCheck predictions: Examing posterior predictive checks.\n\n\n\n\nCompare models: Iterate on model design and choose a model."
  },
  {
    "objectID": "slides/07-workflow.html#research-question",
    "href": "slides/07-workflow.html#research-question",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/07-workflow.html#research-question-1",
    "href": "slides/07-workflow.html#research-question-1",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/07-workflow.html#research-question-2",
    "href": "slides/07-workflow.html#research-question-2",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/07-workflow.html#check-the-model-with-simulated-data",
    "href": "slides/07-workflow.html#check-the-model-with-simulated-data",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\nSanity check:\n\nDraw parameter values from priors.\nGenerate data based on those parameter values.\nFit model to generated data.\nCheck fit is reasonable.\n\n\ngenerated quantities {\n  vector[n] weight;\n  real alpha = normal_rng(150, 5);\n  real beta = normal_rng(0, 10);\n  real sigma = fabs(normal_rng(0, 5));\n  for (i in 1:n) {\n    weight[i] = normal_rng(alpha + beta * height_c[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/07-workflow.html#check-the-model-with-simulated-data-1",
    "href": "slides/07-workflow.html#check-the-model-with-simulated-data-1",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:"
  },
  {
    "objectID": "slides/07-workflow.html#fit-the-model-to-real-data",
    "href": "slides/07-workflow.html#fit-the-model-to-real-data",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\n// saved in linear_regression_workflow.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  vector[n] weight; // outcome vector\n  vector[n] height_c; // covariate vector\n  int&lt;lower = 1&gt; n_pred; // number of new observations\n  vector[n_pred] height_c_pred; // vector for new observations\n}\nparameters {\n  real alpha;\n  real beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(weight | alpha + height_c * beta, sigma); // likelihood\n  target += normal_lpdf(alpha | 150, 5);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n_pred] out_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(alpha + height_c[i] * beta, sigma);\n    log_lik[i] = normal_lpdf(weight[i] | alpha + height_c[i] * beta, sigma);\n  }\n  for (i in 1:n_pred) {\n    out_sample[i] = normal_rng(alpha + height_c_pred[i] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/07-workflow.html#fit-the-model-to-real-data-1",
    "href": "slides/07-workflow.html#fit-the-model-to-real-data-1",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\nstan_data &lt;- list(n = nrow(dat), \n                  height_c = (dat$height - mean(dat$height)), \n                  weight = dat$weight)\nregression_model &lt;- stan_model(file = \"linear_regression_workflow.stan\")\nfit &lt;- sampling(regression_model, data = stan_data)\nprint(fit)\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd   2.5%    50%  97.5% n_eff Rhat\nalpha 152.35    0.01 0.88 150.63 152.35 154.08  3773    1\nbeta    5.69    0.00 0.25   5.21   5.69   6.18  4215    1\nsigma  20.25    0.01 0.60  19.13  20.23  21.50  3571    1\n\nSamples were drawn using NUTS(diag_e) at Wed Nov 27 14:08:19 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/07-workflow.html#check-diagnostics",
    "href": "slides/07-workflow.html#check-diagnostics",
    "title": "Bayesian Workflow",
    "section": "5. Check diagnostics:",
    "text": "5. Check diagnostics:\n\nrstan::traceplot(fit, pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/07-workflow.html#check-diagnostics-1",
    "href": "slides/07-workflow.html#check-diagnostics-1",
    "title": "Bayesian Workflow",
    "section": "5. Check diagnostics:",
    "text": "5. Check diagnostics:\n\nlibrary(bayesplot)\nmcmc_acf(fit, regex_pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/07-workflow.html#examine-posterior-fit",
    "href": "slides/07-workflow.html#examine-posterior-fit",
    "title": "Bayesian Workflow",
    "section": "6.Examine posterior fit:",
    "text": "6.Examine posterior fit:"
  },
  {
    "objectID": "slides/07-workflow.html#examine-posterior-fit-1",
    "href": "slides/07-workflow.html#examine-posterior-fit-1",
    "title": "Bayesian Workflow",
    "section": "6.Examine posterior fit:",
    "text": "6.Examine posterior fit:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior mean and 95% credible interval for \\(\\mu = \\alpha + \\beta \\times height_i\\)."
  },
  {
    "objectID": "slides/07-workflow.html#check-predictions",
    "href": "slides/07-workflow.html#check-predictions",
    "title": "Bayesian Workflow",
    "section": "7.Check predictions:",
    "text": "7.Check predictions:\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(dat$weight, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/07-workflow.html#check-predictions-1",
    "href": "slides/07-workflow.html#check-predictions-1",
    "title": "Bayesian Workflow",
    "section": "7. Check predictions:",
    "text": "7. Check predictions:\nppc_stat(dat$weight, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(dat$weight, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(dat$weight, y_pred, stat = \"q025\")\nppc_stat(dat$weight, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/07-workflow.html#check-predictions-2",
    "href": "slides/07-workflow.html#check-predictions-2",
    "title": "Bayesian Workflow",
    "section": "7. Check predictions:",
    "text": "7. Check predictions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior predictive distribution mean and 95% credible interval, \\(f(weight_i' | weight_{1:n})\\)."
  },
  {
    "objectID": "slides/07-workflow.html#model-comparison-1",
    "href": "slides/07-workflow.html#model-comparison-1",
    "title": "Bayesian Workflow",
    "section": "Model comparison",
    "text": "Model comparison\n\nModel selection criteria are designed to help comparing several models.\nAn ideal criteria will not just based on their fit with training data, but on an estimation of their prediction accuracy with new data.\nCriteria often reward models that offer a good compromise between simplicity and accuracy.\nExamples: Likelihood ratio test, AIC"
  },
  {
    "objectID": "slides/07-model-comparison.html#review-of-last-lecture",
    "href": "slides/07-model-comparison.html#review-of-last-lecture",
    "title": "Model Comparison",
    "section": "",
    "text": "On Tuesday, we learned about various ways to check MCMC convergence and model fit.\n\nTraceplots, effective sample size (\\(n_{eff}\\)), MC standard error, \\(\\hat{R}\\), sampling issues\nPosterior predictive checks\nModel checks using shinystan\n\nToday, we will learn about model comparisons."
  },
  {
    "objectID": "slides/07-model-comparison.html#model-comparison-1",
    "href": "slides/07-model-comparison.html#model-comparison-1",
    "title": "Model Comparison",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nWhen comparing models, we prefer models that are closer to the true data-generating process.\nWe need some ways to quantify the degree of closeness to the true model. Note that in this context models refer to the distributional family as well as the parameter values.\nFor example, the model \\(Y_i \\sim N(5,2)\\) is a different model than \\(Y_i \\sim N(3,2)\\), which is a different model than \\(Y_i \\sim Gamma(2,2)\\).\n\nThe first two have the same family but different parameter values (different means, same SD), whereas the last two have different distributional families (Normal vs. Gamma).\n\nOne way to quantify the degree of closeness to the true model is using Kullback-Leibler (KL) divergence."
  },
  {
    "objectID": "slides/07-model-comparison.html#likelihood-ratio",
    "href": "slides/07-model-comparison.html#likelihood-ratio",
    "title": "Model Comparison",
    "section": "Likelihood ratio",
    "text": "Likelihood ratio\n\nThe likelihood ratio method consists in assessing whether increasing the complexity of a model results in a significant improvement of likelihood which justifies this increased complexity.\nIt is a computationally inexpensive method, since it only relies on the value of the total likelihood function, but can only be used to compare nested models.\nNot often used for Bayesian models."
  },
  {
    "objectID": "slides/07-model-comparison.html#information-criteria",
    "href": "slides/07-model-comparison.html#information-criteria",
    "title": "Model Comparison",
    "section": "Information criteria",
    "text": "Information criteria\n\nSeveral information criteria have been proposed that do not require fitting the model several times, including AIC, DIC, and WAIC.\nWe will introduce the information criteria, assuming a likelihood \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})\\) for observed data \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\) with population parameter \\(\\boldsymbol{\\theta}\\).\nInformation criteria are often presented as deviance, defined as, \\(D(\\mathbf{Y}|\\boldsymbol{\\theta}) = −2 \\log f(\\mathbf{Y}|\\boldsymbol{\\theta})\\).\nIdeally, models will have small deviance.\nHowever, if a model is too complex it will have small deviance but be unstable (overfitting)."
  },
  {
    "objectID": "slides/07-model-comparison.html#bayesian-information-criteria-bic",
    "href": "slides/07-model-comparison.html#bayesian-information-criteria-bic",
    "title": "Model Comparison",
    "section": "Bayesian information criteria (BIC)",
    "text": "Bayesian information criteria (BIC)\n\nThe Bayesian information criteria is similar\n\n\\[BIC = D(\\mathbf{Y}|\\hat{\\boldsymbol{\\theta}})+ \\log(n)p\\]\n\nThis is motivated as an approximation to the log Bayes factor of the model compared to the null model.\nHowever, this is only an asymptotic (large \\(n\\)) approximation.\nWith large \\(n\\) the prior is irrelevant, and so this is not satisfying to a subjective Bayesian."
  },
  {
    "objectID": "slides/07-model-comparison.html#deviance-information-criteria-dic",
    "href": "slides/07-model-comparison.html#deviance-information-criteria-dic",
    "title": "Model Comparison",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\nDeviance information criteria (DIC) estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{DIC}} = \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) - p_{\\text{DIC}},\\] where \\(\\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}\\) is a Bayesian point estimate, typically a posterior mean, and \\(p_{\\text{DIC}}\\) is an estimate of the complexity penalty,\n\\[p_{\\text{DIC}} = 2 \\left(\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) - \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[\\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) \\right]\\right).\\]\n\nThe second term can be estimated as a MC integral.\n\\(\\text{DIC} = -2\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) + 2p_{\\text{DIC}}.\\)"
  },
  {
    "objectID": "slides/07-model-comparison.html#deviance-information-criteria-dic-1",
    "href": "slides/07-model-comparison.html#deviance-information-criteria-dic-1",
    "title": "Model Comparison",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nAdvantages of DIC:\n\nThe effective number of parameters is a useful measure of model complexity.\nIntuitively, if there are \\(p\\) parameters and we have uninformative priors then \\(p_D \\approx p\\).\nHowever, \\(p_D \\ll p\\) if there are strong priors.\n\nDisadvantages of DIC:\n\nDIC can only be used to compare models with the same likelihood.\nDIC really only applies when the posterior is approximately normal, and will give misleading results when the posterior is far from normality (e.g., bimodal)."
  },
  {
    "objectID": "slides/07-model-comparison.html#deviance-information-criteria-dic-2",
    "href": "slides/07-model-comparison.html#deviance-information-criteria-dic-2",
    "title": "Model Comparison",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nLet \\(\\bar{D} = \\mathbb{E}[D(\\mathbf{Y}|\\boldsymbol{\\theta})|\\mathbf{Y}]\\) be the posterior mean of the deviance.\nDenote \\(\\hat{\\boldsymbol{\\theta}}\\) as the posterior mean of \\(\\boldsymbol{\\theta}\\).\nThe effective number of parameters is \\(p_D = \\bar{D} − D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}})\\).\nDIC can be written like AIC,\n\n\\[DIC = \\bar{D} + p_D =D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}}) + 2p_D\\]\n\nModels with small \\(\\bar{D}\\) fit the data well.\nModels with small \\(p_D\\) are simple.\nWe select the model with smallest DIC."
  },
  {
    "objectID": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic",
    "href": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic",
    "title": "Model Comparison",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\nWatanabe-Akaike or widely available information criteria (WAIC) estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{WAIC}} = \\text{lppd} - p_{\\text{WAIC}}.\\]\n\nThe log pointwise predictive density (lppd) is given by, \\[\\text{lppd} = \\log \\prod_{i=1}^n f(Y_i | \\mathbf{Y}) =  \\sum_{i=1}^n \\log \\int f\\left(Y_i | \\boldsymbol{\\theta}\\right)f(\\boldsymbol{\\theta}| \\mathbf{Y}) d\\boldsymbol{\\theta}.\\]\nlppd can be estimated as, \\(\\sum_{i=1}^n \\log \\left(\\frac{1}{S} \\sum_{s = 1}^S f\\left(Y_i | \\boldsymbol{\\theta}^{(s)}\\right)\\right)\\), where \\(\\boldsymbol{\\theta}^{(s)}\\) are drawn from the posterior."
  },
  {
    "objectID": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-1",
    "href": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-1",
    "title": "Model Comparison",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC has the desirable property of averaging over the posterior distribution, instead of conditioning on a point estimate.\n\\(p_{\\text{WAIC}}\\) can be thought of as an approximation to the number of unconstrained parameters in the model.\nIn practice, \\(p_{\\text{WAIC}_2}\\) is often used, since it is theoretically closer to LOO-CV."
  },
  {
    "objectID": "slides/07-model-comparison.html#cross-validation",
    "href": "slides/07-model-comparison.html#cross-validation",
    "title": "Model Comparison",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nA common approach to compare models is using cross-validation.\nThis is exactly the same procedure used in classical statistics.\nThis operates under the assumption that the true model likely produces better out-of-sample predictions than competing models.\nAdvantages: Simple, intuitive, and broadly applicable.\nDisadvantages: Slow because it requires several model fits and it is hard to say a difference is statistically significant."
  },
  {
    "objectID": "slides/07-model-comparison.html#leave-one-out-cross-validation",
    "href": "slides/07-model-comparison.html#leave-one-out-cross-validation",
    "title": "Model Comparison",
    "section": "Leave-one-out cross-validation",
    "text": "Leave-one-out cross-validation\n\nIt is computationally costly to compute LOO-CV, since it requires fitting the model \\(n\\) times.\nLuckily, there exists an approximation to LOO-CV using Pareto smoothed importance-sampling by Vehtari, Gelman, and Gabry (2017).\nThis can be computed easily using R. We will get to this!"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-using-stan",
    "href": "slides/07-model-comparison.html#computing-waic-using-stan",
    "title": "Model Comparison",
    "section": "Computing WAIC using Stan",
    "text": "Computing WAIC using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/07-model-comparison.html#lets-refit-the-stan-model",
    "href": "slides/07-model-comparison.html#lets-refit-the-stan-model",
    "title": "Model Comparison",
    "section": "Let’s refit the Stan model",
    "text": "Let’s refit the Stan model\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_log_lik_fit.rds\")"
  },
  {
    "objectID": "slides/07-model-comparison.html#we-can-now-compute-the-waic",
    "href": "slides/07-model-comparison.html#we-can-now-compute-the-waic",
    "title": "Model Comparison",
    "section": "We can now compute the WAIC",
    "text": "We can now compute the WAIC\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", \n                           merge_chains = TRUE)\nwaic_fit &lt;- loo::waic(log_lik)\nprint(waic_fit)\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -188.4  7.4\np_waic         3.2  0.6\nwaic         376.9 14.7"
  },
  {
    "objectID": "slides/07-model-comparison.html#compare-models",
    "href": "slides/07-model-comparison.html#compare-models",
    "title": "Model Comparison",
    "section": "8. Compare models",
    "text": "8. Compare models\n\nSuppose we would like to predict weight and we would like to compare our original model with a model that also includes sex. We can compare these models using WAIC."
  },
  {
    "objectID": "slides/07-model-comparison.html#prepare-for-next-class",
    "href": "slides/07-model-comparison.html#prepare-for-next-class",
    "title": "Model Comparison",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02, which was just assigned\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Bayesian Workflow"
  },
  {
    "objectID": "slides/08-workflow.html#review-of-last-lecture",
    "href": "slides/08-workflow.html#review-of-last-lecture",
    "title": "Bayesian Workflow",
    "section": "",
    "text": "On Thursday, we learned about various ways compare models.\n\nAIC, DIC, WAIC\nLOO-CV/LOO-IC\n\nToday, we will put these concepts within the larger framework of the Bayesian workflow."
  },
  {
    "objectID": "slides/08-workflow.html#bayes-theorem",
    "href": "slides/08-workflow.html#bayes-theorem",
    "title": "Bayesian Workflow",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{f(\\mathbf{Y})}\\]\n. . .\n\nRethinking Bayes theorem:\n\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto f(\\mathbf{Y}, \\boldsymbol{\\theta}) = f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) \\]\n. . .\n\nIn Stan:\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) + \\log f(\\boldsymbol{\\theta})\\]"
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow",
    "href": "slides/08-workflow.html#bayesian-workflow",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\n\n\n\n\nGelman A., Vehtari A., Simpson D., Margossian, C., Carpenter, B. and Yao, Y., Kennedy, L., Gabry, J., Bürkner P. C., & Modrák M. (2020). Bayesian Workflow."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow-1",
    "href": "slides/08-workflow.html#bayesian-workflow-1",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\n\n\nTaken from Bayesian workflow by Francesca Capel\n\n\n\nToday we will talk about a general strategy for taking a question and data to a robust conclusion."
  },
  {
    "objectID": "slides/08-workflow.html#a-simplified-workflow",
    "href": "slides/08-workflow.html#a-simplified-workflow",
    "title": "Bayesian Workflow",
    "section": "A simplified workflow",
    "text": "A simplified workflow\n\nSetting up a full probability model: a joint probability distribution for all observable and unobservable quantities in a problem. The model should be consistent with knowledge about the underlying scientific problem and the data collection process.\nConditioning on observed data: calculating and interpreting the appropriate posterior distribution — the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data.\nEvaluating the fit of the model and the implications of the resulting posterior distribution: how well does the model fit the data, are the substantive conclusions reasonable, and how sensitive are the results to the modeling assumptions in step 1? In response, one can alter or expand the model and repeat the three steps.\n\nFrom BDA3."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow-2",
    "href": "slides/08-workflow.html#bayesian-workflow-2",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\nResearch question: What are your dependent and indepednent variables? What associations are you interested in? EDA.\n\n. . .\n\nSpecify likelihood & priors: Use knowledge of the problem to construct a generative model.\n\n. . .\n\nCheck the model with simulated data: Generate data from the model and evaluate fit as a sanity check (prior predictive checks).\n\n. . .\n\nFit the model to real data: Estimate parameters using MCMC."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow-3",
    "href": "slides/08-workflow.html#bayesian-workflow-3",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\nCheck diagnostics: Use MCMC diagnostics to guarentee that the algorithm converged.\n\n. . .\n\nExamine posterior fit: Create posterior summaries that are relevant to the research question.\n\n. . .\n\nCheck predictions: Examing posterior predictive checks.\n\n. . .\n\nCompare models: Iterate on model design and choose a model."
  },
  {
    "objectID": "slides/08-workflow.html#motivating-example-predicting-weight-from-height",
    "href": "slides/08-workflow.html#motivating-example-predicting-weight-from-height",
    "title": "Bayesian Workflow",
    "section": "Motivating example: predicting weight from height",
    "text": "Motivating example: predicting weight from height\nResearch question: We would like to understand the relationship between a person’s height and weight. A few particular questions we have are:\n\nHow much does a person’s weight increase when their height increases?\nHow certain we can be about the magnitude of the increase?\nCan we predict a person’s weight based on their height?\n\nData: We will use the bdims dataset from the openintro package. This dataset contains body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender."
  },
  {
    "objectID": "slides/08-workflow.html#prepare-data",
    "href": "slides/08-workflow.html#prepare-data",
    "title": "Bayesian Workflow",
    "section": "Prepare data",
    "text": "Prepare data\n\nlibrary(openintro)\ndat &lt;- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))\nhead(dat)\n\n    weight   height  sex\n1 144.6231 68.50397 Male\n2 158.2917 69.01579 Male\n3 177.9128 76.18114 Male\n4 160.0554 73.42524 Male\n5 173.7241 73.70083 Male\n6 164.9056 71.45673 Male"
  },
  {
    "objectID": "slides/08-workflow.html#research-question",
    "href": "slides/08-workflow.html#research-question",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/08-workflow.html#research-question-1",
    "href": "slides/08-workflow.html#research-question-1",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/08-workflow.html#research-question-2",
    "href": "slides/08-workflow.html#research-question-2",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors",
    "href": "slides/08-workflow.html#specify-likelihood-priors",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\nDefine, \\(Y_i\\) as the weight of observation \\(i\\) and \\(\\mathbf{x}_i\\) as a vector of covariates (here only height).\n\n\\[Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates (excluding intercept)\n  vector[n] Y;      // outcome variable\n  matrix[n, p] X;   // covariate matrix\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-1",
    "href": "slides/08-workflow.html#specify-likelihood-priors-1",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\nparameter {\n  real alpha;            // intercept on the original scale\n  vector[p] beta;             // regression parameters\n  real&lt;lower = 0&gt; sigma; // measurement error\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-2",
    "href": "slides/08-workflow.html#specify-likelihood-priors-2",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-3",
    "href": "slides/08-workflow.html#specify-likelihood-priors-3",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha\\) is the intercept, or average weight for someone who is zero inches (not a particularly useful number on its own)\n\\(\\beta\\) measures the association between weight and height, in pounds/inch\n\\(\\sigma\\) is the measurement error for the population\n\n\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, sigma);\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 10);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-4",
    "href": "slides/08-workflow.html#specify-likelihood-priors-4",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[Y_i] = \\alpha^+ + (\\mathbf{x}_i - \\bar{\\mathbf{x}}) \\boldsymbol{\\beta},\\quad\\bar{\\mathbf{x}}=\\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha^+\\) is the intercept, or average weight for someone who is an average height\n\n\ntransformed data {\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n}"
  },
  {
    "objectID": "slides/08-workflow.html#quick-aside",
    "href": "slides/08-workflow.html#quick-aside",
    "title": "Bayesian Workflow",
    "section": "Quick aside",
    "text": "Quick aside\nWhat does it mean to use the prior sigma ~ normal(0, 5)?\n\nWhen a parameter is truncated, for example real&lt;lower = 0&gt; sigma, priors can still be placed across the real line, \\(\\mathbb{R}\\).\n\n\nparameters {\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(sigma | 0, 10);\n}\n\n\nThis specification induces a prior on the truncated space \\(\\mathbb{R}^+\\).\nThe induced prior for sigma is a half-normal distribution."
  },
  {
    "objectID": "slides/08-workflow.html#quick-aside-1",
    "href": "slides/08-workflow.html#quick-aside-1",
    "title": "Bayesian Workflow",
    "section": "Quick aside",
    "text": "Quick aside\n\nThe half-normal is a useful prior for nonnegative parameters that should not be too large and may be very close to zero.\nSimilar distributions for scale parameters are half-t and half-Cauchy priors, these have heavier tales."
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\n\nDraw parameter values from priors.\nGenerate data based on those parameter values.\nCheck simulated data summaries and compare to observed data."
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-1",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-1",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\n\n// stored in workflow_prior_pred_check.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  real Y_bar;\n  matrix[n, p] X;\n  real&lt;lower = 0&gt; sigma_alpha;\n  real&lt;lower = 0&gt; sigma_beta;\n  real&lt;lower = 0&gt; sigma_sigma;\n}\ntransformed data {\n  row_vector[p] X_bar;\n  for (i in 1:p) X_bar[i] = mean(X[, i]);\n}\ngenerated quantities {\n  // Sample from the priors\n  real alpha_star = normal_rng(0, sigma_alpha);\n  real alpha_plus = alpha_star + Y_bar;\n  real alpha = alpha_plus - X_bar * beta;\n  vector[p] beta;\n  for (i in 1:p) beta[i] = normal_rng(0, sigma_beta);\n  real sigma = fabs(normal_rng(0, sigma_sigma));\n  // Simulate data from the prior\n  vector[n] Y;\n  for (i in 1:n) {\n    Y[i] = normal_rng(alpha + X[i, ] * beta, sigma);\n  }\n  // Compute summaries from the prior\n  real Y_min = min(Y);\n  real Y_max = max(Y);\n  real Y_mean = mean(Y);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#fit-the-model-to-real-data",
    "href": "slides/08-workflow.html#fit-the-model-to-real-data",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\n// saved in linear_regression_workflow.stan\ndata {\n  int&lt;lower = 1&gt; n;        // number of observations\n  int&lt;lower = 1&gt; p;        // number of covariates (excluding intercept)\n  vector[n] Y;             // outcome vector\n  matrix[n, p] X;          // covariate matrix\n  int&lt;lower = 1&gt; n_pred;   // number of new observations to predict\n  matrix[n_pred, p] X_new; // covariate matrix for new observations\n}\ntransformed data {\n  vector[n] Y_centered;\n  real Y_bar;\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n  Y_bar = mean(Y);\n  Y_centered = Y - Y_bar;\n}\nparameters {\n  real alpha_star;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(Y_centered | alpha_star + X_centered * beta, sigma); // likelihood\n  target += normal_lpdf(alpha_star | 0, 10);\n  target += normal_lpdf(beta | 0, 5);\n  target += normal_lpdf(sigma | 0, 4);\n}\ngenerated quantities {\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  vector[n_pred] Y_new;\n  real alpha = Y_bar + alpha_star - X_bar * beta;\n  for (i in 1:n) {\n    Y_pred[i] = normal_rng(alpha + X[i, ] * beta, sigma);\n    log_lik[i] = normal_lpdf(Y_centered[i] | alpha_star + X_centered[i, ] * beta, sigma);\n  }\n  for (i in 1:n_pred) Y_new[i] = normal_rng(alpha + X_new[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#fit-the-model-to-real-data-1",
    "href": "slides/08-workflow.html#fit-the-model-to-real-data-1",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\n###Compile model\nregression_model &lt;- stan_model(file = \"linear_regression_workflow.stan\")\n\n###Create data\nY &lt;- dat$weight\nX &lt;- matrix(dat$height)\nn_new &lt;- 1000\nX_new &lt;- matrix(seq(min(dat$height), max(dat$height), length.out = n_new))\nstan_data &lt;- list(\n  n = nrow(dat), \n  p = ncol(X),\n  Y = Y,\n  X = X,\n  n_new = n_new,\n  X_new = X_new\n)"
  },
  {
    "objectID": "slides/08-workflow.html#check-diagnostics",
    "href": "slides/08-workflow.html#check-diagnostics",
    "title": "Bayesian Workflow",
    "section": "5. Check diagnostics:",
    "text": "5. Check diagnostics:\n\nrstan::traceplot(fit_workflow, pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/08-workflow.html#check-diagnostics-1",
    "href": "slides/08-workflow.html#check-diagnostics-1",
    "title": "Bayesian Workflow",
    "section": "5. Check diagnostics:",
    "text": "5. Check diagnostics:\n\nlibrary(bayesplot)\nmcmc_acf(fit_workflow, regex_pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/08-workflow.html#examine-posterior-fit",
    "href": "slides/08-workflow.html#examine-posterior-fit",
    "title": "Bayesian Workflow",
    "section": "6. Examine posterior fit:",
    "text": "6. Examine posterior fit:"
  },
  {
    "objectID": "slides/08-workflow.html#examine-posterior-fit-1",
    "href": "slides/08-workflow.html#examine-posterior-fit-1",
    "title": "Bayesian Workflow",
    "section": "6. Examine posterior fit:",
    "text": "6. Examine posterior fit:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior mean and 95% credible interval for \\(\\mu = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/08-workflow.html#check-predictions",
    "href": "slides/08-workflow.html#check-predictions",
    "title": "Bayesian Workflow",
    "section": "7.Check predictions:",
    "text": "7.Check predictions:\nY_pred &lt;- rstan::extract(fit_workflow, pars = \"Y_pred\")$Y_pred\nppc_dens_overlay(Y, Y_pred[1:100, ])"
  },
  {
    "objectID": "slides/08-workflow.html#check-predictions-1",
    "href": "slides/08-workflow.html#check-predictions-1",
    "title": "Bayesian Workflow",
    "section": "7. Check predictions:",
    "text": "7. Check predictions:\nppc_stat(Y, Y_pred, stat = \"mean\") # from bayesplot\nppc_stat(Y, Y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(Y, Y_pred, stat = \"q025\")\nppc_stat(Y, Y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/08-workflow.html#check-predictions-2",
    "href": "slides/08-workflow.html#check-predictions-2",
    "title": "Bayesian Workflow",
    "section": "7. Check predictions:",
    "text": "7. Check predictions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior predictive distribution mean and 95% credible interval, \\(f(Y_{i'} | Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/08-workflow.html#shinystan",
    "href": "slides/08-workflow.html#shinystan",
    "title": "Bayesian Workflow",
    "section": "shinystan",
    "text": "shinystan\n\nlibrary(shinystan)\nY &lt;- dat$weight # need to define outcome as a global variable to be accessible\nsso &lt;- shinystan::launch_shinystan(fit_workflow)"
  },
  {
    "objectID": "slides/08-workflow.html#compare-models",
    "href": "slides/08-workflow.html#compare-models",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\nSuppose we would like to compare our original model with models that also includes sex and an interaction between sex and height.\n\n\\[\\begin{aligned}\n\\mathbb{E}[weight_i] &= \\alpha + \\beta_1 height_i\\\\\n\\mathbb{E}[weight_i] &= \\alpha + \\beta_1 height_i + \\beta_2 sex_i\\\\\n\\mathbb{E}[weight_i] &= \\alpha + \\beta_1 height_i + \\beta_2 sex_i + \\beta_3 height_i sex_i\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08-workflow.html#model-comparison-1",
    "href": "slides/08-workflow.html#model-comparison-1",
    "title": "Bayesian Workflow",
    "section": "Model comparison",
    "text": "Model comparison\n\nModel selection criteria are designed to help comparing several models.\nAn ideal criteria will not just based on their fit with training data, but on an estimation of their prediction accuracy with new data.\nCriteria often reward models that offer a good compromise between simplicity and accuracy.\nExamples: Likelihood ratio test, AIC"
  },
  {
    "objectID": "slides/08-workflow.html#likelihood-ratio",
    "href": "slides/08-workflow.html#likelihood-ratio",
    "title": "Bayesian Workflow",
    "section": "Likelihood ratio",
    "text": "Likelihood ratio\n\nThe likelihood ratio method consists in assessing whether increasing the complexity of a model results in a significant improvement of likelihood which justifies this increased complexity.\nIt is a computationally inexpensive method, since it only relies on the value of the total likelihood function, but can only be used to compare nested models.\nNot often used for Bayesian models."
  },
  {
    "objectID": "slides/08-workflow.html#information-criteria",
    "href": "slides/08-workflow.html#information-criteria",
    "title": "Bayesian Workflow",
    "section": "Information criteria",
    "text": "Information criteria\n\nSeveral information criteria have been proposed that do not require fitting the model several times.\nMany are functions of the deviance, i.e., twice the negative log likelihood, \\(D(\\mathbf{Y}|\\boldsymbol{\\theta}) = −2 \\log f(\\mathbf{Y}|\\boldsymbol{\\theta}).\\)\nIdeally, models will have small deviance.\nHowever, if a model is too complex it will have small deviance but be unstable (over-fitting)\nThe Akaike information criteria has a complexity penalty \\(AIC = D(\\mathbf{Y}|\\hat{\\boldsymbol{\\theta}})+ 2p\\), where \\(\\hat{\\boldsymbol{\\theta}}\\) is the MLE.\nModel with smaller AIC are preferred."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-information-criteria-bic",
    "href": "slides/08-workflow.html#bayesian-information-criteria-bic",
    "title": "Bayesian Workflow",
    "section": "Bayesian information criteria (BIC)",
    "text": "Bayesian information criteria (BIC)\n\nThe Bayesian information criteria is similar\n\n\\[BIC = D(\\mathbf{Y}|\\hat{\\boldsymbol{\\theta}})+ \\log(n)p\\]\n\nThis is motivated as an approximation to the log Bayes factor of the model compared to the null model.\nHowever, this is only an asymptotic (large \\(n\\)) approximation.\nWith large \\(n\\) the prior is irrelevant, and so this is not satisfying to a subjective Bayesian."
  },
  {
    "objectID": "slides/08-workflow.html#deviance-information-criteria-dic",
    "href": "slides/08-workflow.html#deviance-information-criteria-dic",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nDIC is a popular Bayesian analog of AIC or BIC.\nUnlike CV, DIC requires only one model fit.\nHowever, proceed with caution.\nDIC really only applies when the posterior is approximately normal, and will give misleading results when the posterior far from normality (e.g., bimodal).\nDIC is also criticized for selecting overly-complex models."
  },
  {
    "objectID": "slides/08-workflow.html#deviance-information-criteria-dic-1",
    "href": "slides/08-workflow.html#deviance-information-criteria-dic-1",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nLet \\(\\bar{D} = \\mathbb{E}[D(\\mathbf{Y}|\\boldsymbol{\\theta})|\\mathbf{Y}]\\) be the posterior mean of the deviance.\nDenote \\(\\hat{\\boldsymbol{\\theta}}\\) as the posterior mean of \\(\\boldsymbol{\\theta}\\).\nThe effective number of parameters is \\(p_D = \\bar{D} − D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}})\\).\nDIC can be written like AIC,\n\n\\[DIC = \\bar{D} + p_D =D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}}) + 2p_D\\]\n\nModels with small \\(\\bar{D}\\) fit the data well.\nModels with small \\(p_D\\) are simple.\nWe select the model with smallest DIC."
  },
  {
    "objectID": "slides/08-workflow.html#deviance-information-criteria-dic-2",
    "href": "slides/08-workflow.html#deviance-information-criteria-dic-2",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nThe effective number of parameters is a useful measure of model complexity\nIntuitively, if there are \\(p\\) parameters and we have uninformative priors then \\(p_D \\approx p\\)\nHowever, \\(p_D \\ll p\\) if there are strong priors.\nAs with AIC or BIC, the actual value is meaningless, only differences are relevant.\nDIC can only be used to compare models with the same likelihood."
  },
  {
    "objectID": "slides/08-workflow.html#watanabe-akaike-information-criteria-waic",
    "href": "slides/08-workflow.html#watanabe-akaike-information-criteria-waic",
    "title": "Bayesian Workflow",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC is an alternative to DIC\nIt is motivated as an approximation to leave-one-out CV\nIn the end WAIC has model-fit and model-complexity components\nIt is used the same as DIC with smaller WAIC preferred\nIn practice the two often give similar results, but WAIC is arguably more theoretically justified"
  },
  {
    "objectID": "slides/08-workflow.html#watanabe-akaike-information-criteria-waic-1",
    "href": "slides/08-workflow.html#watanabe-akaike-information-criteria-waic-1",
    "title": "Bayesian Workflow",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC is written in terms of the posterior of the likelihood rather than parameters\nLet \\(m_i\\) be the posterior mean of \\(f(Y_i|\\boldsymbol{\\theta})\\) and \\(v_i\\) be the posterior variance of \\(\\log f(Y_i|\\boldsymbol{\\theta})\\)\nThe effective model size is \\(p_W = \\sum_{i=1}^n v_i\\)\nThe criteria is\n\n\\[WAIC = −2 \\sum_{i=1}^n \\log (m_i) + 2p_W\\]"
  },
  {
    "objectID": "slides/08-workflow.html#cross-validation",
    "href": "slides/08-workflow.html#cross-validation",
    "title": "Bayesian Workflow",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nCross-validation is a strategy for estimating a model’s predictive accuracy on another sample.\nCross-validation methods capture out-of-sample prediction error by fitting the model to training data and evaluating this predictive accuracy on a holdout set.\nThey can be computationally expensive but avoid the problem of overfitting.\nThe sample data is divided into a number of chunks, called “folds” and the model is asked to predict each fold, after training on all the others. The number of folds is given by \\(k\\).\nLeave-one-out cross-validation (LOO-CV) represents cross-validation at the extreme, when \\(k = n\\)."
  },
  {
    "objectID": "slides/08-workflow.html#leave-one-out-cross-validation",
    "href": "slides/08-workflow.html#leave-one-out-cross-validation",
    "title": "Bayesian Workflow",
    "section": "Leave-one-out cross-validation",
    "text": "Leave-one-out cross-validation\n\nIt is computationally costly to compute LOO-CV, since it requires fitting the model \\(n\\) times.\nLuckily, there exists an approximation to LOO-CV using Pareto smoothed importance-sampling by Vehtari, Gelman, and Gabry (2017).\nThis can be computed easily using R."
  },
  {
    "objectID": "slides/08-workflow.html#computing-waic-using-stan",
    "href": "slides/08-workflow.html#computing-waic-using-stan",
    "title": "Bayesian Workflow",
    "section": "Computing WAIC using Stan",
    "text": "Computing WAIC using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#lets-refit-the-stan-model",
    "href": "slides/08-workflow.html#lets-refit-the-stan-model",
    "title": "Bayesian Workflow",
    "section": "Let’s refit the Stan model",
    "text": "Let’s refit the Stan model\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_log_lik_fit.rds\")"
  },
  {
    "objectID": "slides/08-workflow.html#we-can-now-compute-the-waic",
    "href": "slides/08-workflow.html#we-can-now-compute-the-waic",
    "title": "Bayesian Workflow",
    "section": "We can now compute the WAIC",
    "text": "We can now compute the WAIC\n\nlibrary(loo)\nlog_lik_model1 &lt;- loo::extract_log_lik(fit_model1, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model2 &lt;- loo::extract_log_lik(fit_model2, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model3 &lt;- loo::extract_log_lik(fit_model3, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model1 &lt;- loo::waic(log_lik_model1)\nwaic_model2 &lt;- loo::waic(log_lik_model2)\nwaic_model3 &lt;- loo::waic(log_lik_model3)\nloo::loo_compare(waic_model1, waic_model2, waic_model3)\n\n       elpd_diff se_diff\nmodel2   0.0       0.0  \nmodel3  -0.8       2.4  \nmodel1 -25.3       5.8"
  },
  {
    "objectID": "slides/08-workflow.html#compare-models-1",
    "href": "slides/08-workflow.html#compare-models-1",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\n###Compute individual WAIC\nlibrary(loo)\nlog_lik_model1 &lt;- loo::extract_log_lik(fit_model1, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model2 &lt;- loo::extract_log_lik(fit_model2, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model3 &lt;- loo::extract_log_lik(fit_model3, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model1 &lt;- loo::waic(log_lik_model1)\nwaic_model2 &lt;- loo::waic(log_lik_model2)\nwaic_model3 &lt;- loo::waic(log_lik_model3)\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"height_only\" = waic_model1, \"height_sex\" = waic_model2, \"interaction\" = waic_model3))\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n            elpd_diff se_diff  elpd_waic se_elpd_waic p_waic   se_p_waic\ninteraction     0.00      0.00 -2225.62     24.22         5.08     0.96 \nheight_sex     -1.19      1.66 -2226.81     23.78         4.80     0.87 \nheight_only   -28.15      8.43 -2253.77     21.55         3.57     0.63 \n            waic     se_waic \ninteraction  4451.23    48.44\nheight_sex   4453.62    47.55\nheight_only  4507.53    43.10"
  },
  {
    "objectID": "slides/08-workflow.html#prepare-for-next-class",
    "href": "slides/08-workflow.html#prepare-for-next-class",
    "title": "Bayesian Workflow",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Nonlinear Regression"
  },
  {
    "objectID": "slides/01-welcome.html",
    "href": "slides/01-welcome.html",
    "title": "Welcome to BIOSTAT 725!",
    "section": "",
    "text": "Education and career journey\n\nBS in Statistical Science from Duke University\nPhD in Biostatistics from University of North Carolina - Chapel Hill\nPostdoc in Duke Forge: Duke’s Center for Actionable Health Data Science\nNIH/NEI Pathway to Independence Fellow (K99/R00)\nAssistant Professor, Department of Biostatistics & Bioinformatics and Statistical Science at Duke; Faculty Affiliate of Duke AI Health\n\nWork focuses on developing data science tools to improve patient experience using biomedical data (including EHR)\nDad of 4 and 6 year old daughters 🙂\n\n\n\n\n\n\n\nDr. Youngsoo Baek (PhD)\n\nPhd in Statistical Science from Duke University\nPostdoc in Biostatistics & Bioinformatics\n\nBraden Scherting\n\nPhd candidate in Statistical Science\n\n\n\n\n\n\nClick on the link or scan the QR code to answer the Ed Discussion poll\nhttps://edstem.org/us/courses/68995/discussion/5942168\n\n\n\n\n\n\n\n\n\n\nIntroduction to the course\nSyllabus activity\nReview of probability"
  },
  {
    "objectID": "slides/07-model-comparison.html#model-selection",
    "href": "slides/07-model-comparison.html#model-selection",
    "title": "Model Comparison",
    "section": "Model selection",
    "text": "Model selection\n\nOften we have many potential models in our arsenal.\nFor a given dataset, how do determine whether a simple model is sufficient or if we need to bring out the “big guns”?\nIs there a “right” model? Probably not.\nA statistical model is a mathematical representation of the system that includes errors and biases in the observation process.\nAll models are simplifications of reality.\nWe want a model that is as simple as possible yet seems to fit the data reasonably well.\nAn ideal criteria will not just based on their fit with training data, but on an estimation of their prediction accuracy with new data."
  },
  {
    "objectID": "slides/07-model-comparison.html#k-fold-cross-validation",
    "href": "slides/07-model-comparison.html#k-fold-cross-validation",
    "title": "Model Comparison",
    "section": "K-fold cross-validation",
    "text": "K-fold cross-validation\n\nSplit the data into \\(K\\) equally-sized groups.\nSet aside group \\(k\\) as test set and fit the model to the remaining \\(K − 1\\) groups.\nMake predictions for the test set \\(k\\) based on the model fit to the training data.\nRepeat steps 1 and 2 for \\(k = 1, \\dots, K\\) giving a predicted value \\(\\widehat{Y}_i\\) for all \\(n\\) observations.\nMeasure prediction accuracy, e.g.,\n\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2.\\]"
  },
  {
    "objectID": "slides/07-model-comparison.html#variants-of-cross-validation",
    "href": "slides/07-model-comparison.html#variants-of-cross-validation",
    "title": "Model Comparison",
    "section": "Variants of cross-validation",
    "text": "Variants of cross-validation\n\nUsually \\(K\\) is either 5 or 10.\n\\(K = n\\) is called leave-one-out cross-validation (LOO-CV), which is great but slow.\nThe predicted value \\(\\widehat{Y}_i\\) can be either the posterior predictive mean or median.\nMean squared error (MSE) can be replaced with mean absolute deviation (MAD),\n\n\\[MAD = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\widehat{Y}_i|.\\]"
  },
  {
    "objectID": "slides/07-model-comparison.html#cross-validation-1",
    "href": "slides/07-model-comparison.html#cross-validation-1",
    "title": "Model Comparison",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nCross-validation is a strategy for estimating a model’s predictive accuracy on another sample.\nCross-validation methods capture out-of-sample prediction error by fitting the model to training data and evaluating this predictive accuracy on a holdout set.\nThey can be computationally expensive but avoid the problem of overfitting.\nThe sample data is divided into a number of chunks, called “folds” and the model is asked to predict each fold, after training on all the others. The number of folds is given by \\(k\\).\nLeave-one-out cross-validation (LOO-CV) represents cross-validation at the extreme, when \\(k = n\\)."
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-and-loo-cv-using-stan",
    "href": "slides/07-model-comparison.html#computing-waic-and-loo-cv-using-stan",
    "title": "Model Comparison",
    "section": "Computing WAIC and LOO-CV using Stan",
    "text": "Computing WAIC and LOO-CV using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/07-model-comparison.html#lets-simulate-some-data",
    "href": "slides/07-model-comparison.html#lets-simulate-some-data",
    "title": "Model Comparison",
    "section": "Let’s simulate some data:",
    "text": "Let’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3, 1), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\nn_pred &lt;- 10 # number of predicted observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\nX_pred &lt;- cbind(1, matrix(rnorm(n_pred * p), ncol = p))\nY_pred &lt;- as.numeric(X_pred %*% beta + rnorm(n_pred, 0, sigma))"
  },
  {
    "objectID": "slides/07-model-comparison.html#an-example-model-comparison",
    "href": "slides/07-model-comparison.html#an-example-model-comparison",
    "title": "Model Comparison",
    "section": "An example model comparison",
    "text": "An example model comparison\nTrue Model: \\(\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}\\)\nModel 1: \\(\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1}\\)\nModel 2: \\(\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}\\)"
  },
  {
    "objectID": "slides/07-model-comparison.html#fit-model-1",
    "href": "slides/07-model-comparison.html#fit-model-1",
    "title": "Model Comparison",
    "section": "Fit model 1",
    "text": "Fit model 1\n\n###Create stan data object\nstan_data_model1 &lt;- list(n = n, p = p - 1, Y = Y, X = X[, -3],\n                         beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                         n_pred = n_pred, X_pred = X_pred[, -3])\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model 1 and save\nfit_model1 &lt;- sampling(stan_model, data = stan_data_model1, \n                chains = 4, iter = 1000)\nsaveRDS(fit_model1, file = \"linear_regression_ppd_log_lik_fit_model1.rds\")"
  },
  {
    "objectID": "slides/07-model-comparison.html#fit-model-2",
    "href": "slides/07-model-comparison.html#fit-model-2",
    "title": "Model Comparison",
    "section": "Fit model 2",
    "text": "Fit model 2\n\n###Create stan data object\nstan_data_model2 &lt;- list(n = n, p = p, Y = Y, X = X,\n                         beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                         n_pred = n_pred, X_pred = X_pred)\n\n###Run model 2 and save\nfit_model2 &lt;- sampling(stan_model, data = stan_data_model2, \n                chains = 4, iter = 1000)\nsaveRDS(fit_model2, file = \"linear_regression_ppd_log_lik_fit_model2.rds\")"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic",
    "href": "slides/07-model-comparison.html#computing-waic",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\nBegin by extracting the log-likelihood values from the model.\nWe will use the loo package.\n\n\n###Load loo package\nlibrary(loo)\n\n###Extract log likelihood\nlog_lik_model1 &lt;- loo::extract_log_lik(fit_model1, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model2 &lt;- loo::extract_log_lik(fit_model2, parameter_name = \"log_lik\", merge_chains = TRUE)\n\n###Explore the object\nclass(log_lik_model1)\n\n[1] \"matrix\" \"array\" \n\ndim(log_lik_model1)\n\n[1] 2000  100"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-1",
    "href": "slides/07-model-comparison.html#computing-waic-1",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\n###Compute WAIC for the two models\nwaic_model1 &lt;- loo::waic(log_lik_model1)\nwaic_model2 &lt;- loo::waic(log_lik_model2)\n\n###Inspect WAIC for model 1\nwaic_model1\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -201.0  6.6\np_waic         3.0  0.5\nwaic         401.9 13.1"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-loo-ic",
    "href": "slides/07-model-comparison.html#computing-loo-ic",
    "title": "Model Comparison",
    "section": "Computing LOO-IC",
    "text": "Computing LOO-IC\n\n###Compute LOO-IC for the two models\nloo_model1 &lt;- loo::loo(log_lik_model1)\nloo_model2 &lt;- loo::loo(log_lik_model2)\n\n###Make a comparison\ncomp &lt;- loo::loo_compare(list(\"true\" = loo_model2, \"misspec\" = loo_model1))\nprint(comp, digits = 2)\n\n        elpd_diff se_diff\ntrue      0.00      0.00 \nmisspec -22.08      5.76 \n\nprint(comp, digits = 2, simplify = FALSE)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic  \ntrue       0.00      0.00 -178.88     6.57        3.90    0.58   357.77\nmisspec  -22.08      5.76 -200.96     6.56        3.00    0.52   401.93\n        se_looic\ntrue      13.14 \nmisspec   13.12"
  },
  {
    "objectID": "slides/07-model-comparison.html#example-data",
    "href": "slides/07-model-comparison.html#example-data",
    "title": "Model Comparison",
    "section": "Example data",
    "text": "Example data\nData: We will use the bdims dataset from the openintro package. This dataset contains body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender."
  },
  {
    "objectID": "slides/07-model-comparison.html#prepare-data",
    "href": "slides/07-model-comparison.html#prepare-data",
    "title": "Model Comparison",
    "section": "Prepare data",
    "text": "Prepare data\n\nlibrary(openintro)\ndat &lt;- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))"
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting"
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting-1",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting-1",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting"
  },
  {
    "objectID": "slides/07-model-comparison.html#model-comparison",
    "href": "slides/07-model-comparison.html#model-comparison",
    "title": "Model Comparison",
    "section": "Model comparison",
    "text": "Model comparison\n\nIn statistical modeling, a more complex model almost always results in a better fit to the data.\n\nA more complex model means one with more parameters.\n\nIf one has 10 observations, one can have a model with 10 parameters that can perfectly predict every single data point (by just having a parameter to predict each data point).\nThere are two problems with overly complex models.\n\nThey become increasingly hard to interpret (think a straight line versus a polynomial).\nThey are more at risk of overfitting, such that it does not work for future observations."
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting-2",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting-2",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\nWith more complex models, out-of-sample prediction becomes worse.\nThis is because when you use a complex model in a data set, it tailors the coefficients to any sampling errors and noise in the data such that it will not generalize to new observations.\nTherefore, our goal in model comparison is to choose a model with the following two properties:\n\nIt is complex enough to capture the essence of the data generation process (and thus avoid underfitting),\nIt avoids overfitting to make the model usefull for predicting new observations."
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting-3",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting-3",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\nWith more complex models, out-of-sample prediction becomes worse.\nThis is because when you use a complex model in a data set, it tailors the coefficients to any sampling errors and noise in the data such that it will not generalize to new observations.\nTherefore, our goal in model comparison is to choose a model with the following two properties:\n\nIt is complex enough to capture the essence of the data generation process (and thus avoid underfitting),\nIt avoids overfitting to make the model usefull for predicting new observations."
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting-4",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting-4",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\nWith more complex models, out-of-sample prediction becomes worse.\nThis is because when you use a complex model in a data set, it tailors the coefficients to any sampling errors and noise in the data such that it will not generalize to new observations.\nTherefore, our goal in model comparison is to choose a model with the following two properties:\n\nIt is complex enough to capture the essence of the data generation process (and thus avoid underfitting),\nIt avoids overfitting to make the model usefull for predicting new observations."
  },
  {
    "objectID": "slides/07-model-comparison.html#finding-an-optimal-model",
    "href": "slides/07-model-comparison.html#finding-an-optimal-model",
    "title": "Model Comparison",
    "section": "Finding an optimal model",
    "text": "Finding an optimal model\n\nTrade-off between overfitting and underfitting (in machine learning this is commonly called bias-variance trade-off).\n\nA simple model tends to produce biased predictions because it does not capture the essence of the data generating process.\nA model that is overly complex is unbiased but results in a lot of uncertainty in the prediction.\n\nPolynomials are merely one example of comparing simple to complex models. You can think about:\n\nModels with and without interactions,\nModels with a few predictors versus hundreds of predictors,\nRegression analyses versus hierarchical models, etc."
  },
  {
    "objectID": "slides/07-model-comparison.html#kullback-leibler-divergence",
    "href": "slides/07-model-comparison.html#kullback-leibler-divergence",
    "title": "Model Comparison",
    "section": "Kullback-Leibler divergence",
    "text": "Kullback-Leibler divergence\n\nFor two models, \\(M_0\\) and \\(M_1\\), the KL divergence is given by,\n\n\\[\\begin{aligned}\nD_{KL}\\left(M_0 | M_1\\right) &= \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log\\frac{f_{M_0}(\\mathbf{Y})}{f_{M_1}(\\mathbf{Y})} d\\mathbf{Y}\\\\\n&\\hspace{-1.5in}= \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_0}(\\mathbf{Y})d\\mathbf{Y} - \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_1}(\\mathbf{Y})d\\mathbf{Y}\n\\end{aligned}\\]\n\nNote that \\(D_{KL}\\) is not considered a distance, because it is not strictly symmetric, \\(D_{KL}\\left(M_0 | M_1\\right) \\neq D_{KL}\\left(M_1 | M_0\\right)\\)."
  },
  {
    "objectID": "slides/07-model-comparison.html#kullback-leibler-divergence-1",
    "href": "slides/07-model-comparison.html#kullback-leibler-divergence-1",
    "title": "Model Comparison",
    "section": "Kullback-Leibler divergence",
    "text": "Kullback-Leibler divergence\nAs an example, assume that the data are generated by a true model \\(M_0\\), and we have two candidate models \\(M_1\\) and \\(M_2\\), where\n\n\n\n\\(M_0: Y_i \\sim N(3,2)\\)\n\\(M_1: Y_i \\sim N(3.5, 2.5)\\)\n\\(M_2: Y_i \\sim Cauchy(3,2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{KL}(M_0 |M_1) = 0.063\\), \\(D_{KL}(M_0 | M_1) = 0.259\\), so \\(M_1\\) is a better model than \\(M_2\\)."
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-divergence",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-divergence",
    "title": "Model Comparison",
    "section": "Comparing models using KL divergence",
    "text": "Comparing models using KL divergence\n\nNote that in the expression of \\(D_{KL}\\), when talking about the same target model, the first term is always the same and describes the “true” model, \\(M_0\\).\nTherefore, it is sufficient to compare models on the second term,\n\\[\\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_1}(\\mathbf{Y})d\\mathbf{Y},\\] which can also be written as, \\(\\mathbb{E}_{M_0}\\left[\\log f_{M_1}(\\mathbf{Y})\\right].\\)\nThis term is the expected log predictive density (elpd).\nA larger elpd is preferred."
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nNote that in the expression of \\(D_{KL}\\), when talking about the same target model, the first term is always the same and describes the true model, \\(M_0\\).\nTherefore, it is sufficient to compare models on the second term,\n\\[\\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_1}(\\mathbf{Y})d\\mathbf{Y},\\] which can also be written as, \\(\\mathbb{E}_{M_0}\\left[\\log f_{M_1}(\\mathbf{Y})\\right].\\)\nThis term is the expected log predictive density (elpd).\nA larger elpd is preferred. Why?"
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-1",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-1",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nIn the real world, we do not know \\(M_0\\).\n\nIf we knew, then we would just need to choose \\(M_0\\) as our model and there will be no problem about model comparisons.\nEven if we knew the true model, we would still need to estimate the parameter values.\n\nThus, we cannot compute elpd, since the expectation is over \\(f_{M_0}(\\mathbf{Y})\\).\nWe need to estimate elpd!"
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-2",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-2",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nelpd is an expectation, so we can think about estimating it using Monte Carlo sampling, \\[\\frac{1}{S}\\sum_{s = 1}^S\\log f_{M_1}\\left(\\mathbf{Y}^{(s)}\\right)\\rightarrow \\mathbb{E}_{M_0}\\left[\\log f_{M_1}(\\mathbf{Y})\\right], \\quad \\mathbf{Y}^{(s)} \\sim f_{M_0}(\\mathbf{Y}).\\]\n\nWe need to find a way to approximate, \\(f_{M_0}\\left(\\mathbf{Y}^{(s)}\\right)\\).\n\nA naive way to approximate \\(f(\\mathbf{Y}^{(s)})\\) is to assume that the distribution of the observed data is the true model.\n\nThis is equivalent to assuming that \\(\\mathbf{Y}^{(s)} \\sim \\{\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n\\}\\).\nThis leads to an overly optimistic estimate and favors complex models."
  },
  {
    "objectID": "slides/07-model-comparison.html#overview-of-comparison-methods",
    "href": "slides/07-model-comparison.html#overview-of-comparison-methods",
    "title": "Model Comparison",
    "section": "Overview of comparison methods",
    "text": "Overview of comparison methods\n\nInformation criteria: AIC, DIC, and WAIC, which estimate the elpd in the current sample, minus a correction factor.\nCross validation: A method that splits the current sample into \\(K\\) parts, estimates the parameters in \\(K − 1\\) parts, and estimates the elpd in the remaining 1 part.\n\n\nA special case is when \\(K = n\\) so that each time one uses \\(n-1\\) data points to estimate the model parameters, and estimates the elpd for the observation that was left out. This is called leave-one-out cross-validation (LOO-CV)."
  },
  {
    "objectID": "slides/07-model-comparison.html#model-fit-an-example-data-set",
    "href": "slides/07-model-comparison.html#model-fit-an-example-data-set",
    "title": "Model Comparison",
    "section": "Model fit: an example data set",
    "text": "Model fit: an example data set\n\nLet’s explore the idea of model fit using an example dataset from the openintro package called bdims.\nThis dataset contains body girth measurements and skeletal diameter measurements.\nToday we will explore the association between height and weight."
  },
  {
    "objectID": "slides/07-model-comparison.html#models-of-increasing-complexity",
    "href": "slides/07-model-comparison.html#models-of-increasing-complexity",
    "title": "Model Comparison",
    "section": "Models of increasing complexity",
    "text": "Models of increasing complexity\n\nWhen using height to predict weight, we can models of increasing complexity using higher order polynomials.\nLet’s fit the following models to the subset of 10 data points:\n\n\\[\\begin{aligned}\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2 + \\beta_3 height_i^3\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2 + \\beta_3 height_i^3 + \\beta_4 height_i^4\n\\end{aligned}\\]\n\nWe can compare these models using standard measures of goodness-of-fit, including \\(R^2\\) and root mean squared error (RMSE)."
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-3",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-3",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nA better way to estimate elpd is to collect data on a new independent sample that is believed to share the same data generating process as the current sample, and estimate elpd on the new sample.\n\nThis is called out-of-sample validation.\nThe problem, of course, is we usually do not have the resources to collect a new sample.\n\nTherefore, statisticians have worked hard to find ways to estimate elpd from the current sample, and there are two broad approaches, information criteria and cross-validation."
  },
  {
    "objectID": "slides/07-model-comparison.html#akaike-information-criteria-aic",
    "href": "slides/07-model-comparison.html#akaike-information-criteria-aic",
    "title": "Model Comparison",
    "section": "Akaike information criteria (AIC)",
    "text": "Akaike information criteria (AIC)\nAkaike information criteria (AIC) estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{AIC}} = \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}) - p,\\] where \\(p\\) is the number of parameters estimated in the model and \\(\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}\\) is the MLE point estimate.\n\n\\(\\text{AIC} = -2\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}) + 2p\\)\n\\(p\\) is an adjustment for overfitting, but once we go beyond linear models, we cannot simply add \\(p\\).\nInformative priors tend to reduce the amount of overfitting.\nModel with smaller AIC are preferred."
  },
  {
    "objectID": "slides/07-model-comparison.html#deviance-information-criteria-dic-3",
    "href": "slides/07-model-comparison.html#deviance-information-criteria-dic-3",
    "title": "Model Comparison",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nThe effective number of parameters is a useful measure of model complexity.\nIntuitively, if there are \\(p\\) parameters and we have uninformative priors then \\(p_D \\approx p\\).\nHowever, \\(p_D \\ll p\\) if there are strong priors.\nAs with AIC or BIC, the actual value is meaningless, only differences are relevant.\nDIC can only be used to compare models with the same likelihood."
  },
  {
    "objectID": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-2",
    "href": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-2",
    "title": "Model Comparison",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC has the desirable property of averaging over the posterior distribution, instead of conditioning on a point estimate.\n\\(p_{\\text{WAIC}}\\) can be thought of as an approximation to the number of unconstrained parameters in the model.\nIn practice, \\(p_{\\text{WAIC}_2}\\) is often used, since it is theoretically closer to LOO-CV."
  },
  {
    "objectID": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-3",
    "href": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-3",
    "title": "Model Comparison",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC is written in terms of the posterior of the likelihood rather than parameters.\nLet \\(m_i\\) be the posterior mean of \\(f(Y_i|\\boldsymbol{\\theta})\\) and \\(v_i\\) be the posterior variance of \\(\\log f(Y_i|\\boldsymbol{\\theta})\\)\nThe effective model size is \\(p_W = \\sum_{i=1}^n v_i\\)\nThe criteria is,\n\n\\[WAIC = −2 \\sum_{i=1}^n \\log (m_i) + 2p_W.\\]"
  },
  {
    "objectID": "slides/07-model-comparison.html#leave-one-out-cross-validation-loo-cv",
    "href": "slides/07-model-comparison.html#leave-one-out-cross-validation-loo-cv",
    "title": "Model Comparison",
    "section": "Leave-one-out cross-validation (LOO-CV)",
    "text": "Leave-one-out cross-validation (LOO-CV)\n\nAssume the data are partitioned into a training set, \\(\\mathbf{Y}_{\\text{train}}\\) and a holdout set \\(\\mathbf{Y}_{\\text{test}}\\), thus yielding a posterior distribution \\(f(\\boldsymbol{\\theta} | \\mathbf{Y}_{\\text{train}})\\).\nIn the setting of LOO-CV, we have \\(n\\) different \\(f\\left(\\boldsymbol{\\theta} | \\mathbf{Y}_{-i}\\right)\\).\nThe Bayesian LOO-CV estimate of out-of-sample predictive fit is \\[\\text{lppd}_{\\text{LOO-CV}} = \\sum_{i=1}^n \\log f\\left(\\boldsymbol{\\theta} | \\mathbf{Y}_{-i}\\right),\\]\nThe estimated number of parameters can be computed as,\n\n\\[p_{\\text{LOO-CV}} = \\text{lppd} - \\text{lppd}_{\\text{LOO-CV}}.\\]\n\nThis also referred to as leave-one-out information criteria (LOO-IC)"
  },
  {
    "objectID": "slides/07-model-comparison.html#leave-one-out-cross-validation-loo-cv-1",
    "href": "slides/07-model-comparison.html#leave-one-out-cross-validation-loo-cv-1",
    "title": "Model Comparison",
    "section": "Leave-one-out cross-validation (LOO-CV)",
    "text": "Leave-one-out cross-validation (LOO-CV)\nLOO-CV estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{LOO-CV}} = \\text{lppd}_{\\text{LOO-CV}} - p_{\\text{WAIC}} = \\text{lppd}_{\\text{LOO-CV}}.\\]\n\nUnder some common models there are shortcuts for computing it, however in general these do not exist.\nWAIC can be treated as a fast approximation of LOO-CV.\nIn Stan, LOO-CV is approximated using the Pareto smoothed importance sampling (PSIS) to make the process faster, without having to repeat the process \\(n\\) times."
  },
  {
    "objectID": "slides/07-model-comparison.html#model-fit-an-example-data-set-1",
    "href": "slides/07-model-comparison.html#model-fit-an-example-data-set-1",
    "title": "Model Comparison",
    "section": "Model fit: an example data set",
    "text": "Model fit: an example data set\n\nlibrary(openintro)\ndat &lt;- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))"
  },
  {
    "objectID": "slides/07-model-comparison.html#waic",
    "href": "slides/07-model-comparison.html#waic",
    "title": "Model Comparison",
    "section": "WAIC",
    "text": "WAIC\nThere are two common estimates of \\(p_{\\text{WAIC}}\\), both of which can be estimated using MC samples of the posterior. \\[\\begin{aligned}\np_{\\text{WAIC}_1} &= 2 \\sum_{i=1}^n\\left(\\log \\left( \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[f(Y_i | \\boldsymbol{\\theta})\\right]\\right) - \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[\\log f(Y_i | \\boldsymbol{\\theta}) \\right]\\right)\\\\\np_{\\text{WAIC}_2} &= \\sum_{i=1}^n \\mathbb{V}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left(\\log f\\left(Y_i | \\mathbf{\\theta}\\right)\\right)\n\\end{aligned}\\]\n\n\\(\\text{WAIC} = -2 \\text{lppd} + 2p_{\\text{WAIC}}.\\)"
  },
  {
    "objectID": "slides/07-model-comparison.html#waic-1",
    "href": "slides/07-model-comparison.html#waic-1",
    "title": "Model Comparison",
    "section": "WAIC",
    "text": "WAIC\n\nWAIC has the desirable property of averaging over the posterior distribution, instead of conditioning on a point estimate.\n\\(p_{\\text{WAIC}}\\) can be thought of as an approximation to the number of unconstrained parameters in the model.\nIn practice, \\(p_{\\text{WAIC}_2}\\) is often used, since it is theoretically closer to LOO-CV."
  },
  {
    "objectID": "slides/07-model-comparison.html#loo-cv",
    "href": "slides/07-model-comparison.html#loo-cv",
    "title": "Model Comparison",
    "section": "LOO-CV",
    "text": "LOO-CV\nLOO-CV estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{LOO-CV}} = \\text{lppd}_{\\text{LOO-CV}} - p_{\\text{WAIC}} = \\text{lppd}_{\\text{LOO-CV}}.\\]\n\nUnder some common models there are shortcuts for computing it, however in general these do not exist.\nWAIC can be treated as a fast approximation of LOO-CV.\nIn Stan, LOO-CV is approximated using the Pareto smoothed importance sampling (PSIS) to make the process faster, without having to repeat the process \\(n\\) times."
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-2",
    "href": "slides/07-model-comparison.html#computing-waic-2",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\n###Inspect WAIC for model 2\nwaic_model2\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -178.9  6.6\np_waic         3.9  0.6\nwaic         357.7 13.1"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-3",
    "href": "slides/07-model-comparison.html#computing-waic-3",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"true\" = waic_model2, \"misspec\" = waic_model1))\nprint(comp_waic, digits = 2)\n\n        elpd_diff se_diff\ntrue      0.00      0.00 \nmisspec -22.09      5.76 \n\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \ntrue       0.00      0.00 -178.86      6.57         3.88    0.58    357.72\nmisspec  -22.09      5.76 -200.95      6.56         2.99    0.52    401.90\n        se_waic\ntrue      13.14\nmisspec   13.12"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-loo-cvloo-ci",
    "href": "slides/07-model-comparison.html#computing-loo-cvloo-ci",
    "title": "Model Comparison",
    "section": "Computing LOO-CV/LOO-CI",
    "text": "Computing LOO-CV/LOO-CI\n\n###Compute LOO-IC for the two models\nloo_model1 &lt;- loo::loo(log_lik_model1)\nloo_model2 &lt;- loo::loo(log_lik_model2)\n\n###Make a comparison\ncomp &lt;- loo::loo_compare(list(\"true\" = loo_model2, \"misspec\" = loo_model1))\nprint(comp, digits = 2)\n\n        elpd_diff se_diff\ntrue      0.00      0.00 \nmisspec -22.08      5.76 \n\nprint(comp, digits = 2, simplify = FALSE)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic  \ntrue       0.00      0.00 -178.88     6.57        3.90    0.58   357.77\nmisspec  -22.08      5.76 -200.96     6.56        3.00    0.52   401.93\n        se_looic\ntrue      13.14 \nmisspec   13.12"
  },
  {
    "objectID": "slides/08-workflow.html#compare-models-2",
    "href": "slides/08-workflow.html#compare-models-2",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\n###Compute individual LOO-CV/LOO-IC\nloo_model1 &lt;- loo::loo(log_lik_model1)\nloo_model2 &lt;- loo::loo(log_lik_model2)\nloo_model3 &lt;- loo::loo(log_lik_model3)\n\n###Make a comparison\ncomp_loo &lt;- loo::loo_compare(list(\"height_only\" = loo_model1, \"height_sex\" = loo_model2, \"interaction\" = loo_model3))\nprint(comp_loo, digits = 2, simplify = FALSE)\n\n            elpd_diff se_diff  elpd_loo se_elpd_loo p_loo    se_p_loo looic   \ninteraction     0.00      0.00 -2225.63    24.23        5.10     0.96  4451.27\nheight_sex     -1.18      1.66 -2226.81    23.78        4.80     0.87  4453.63\nheight_only   -28.13      8.43 -2253.77    21.55        3.57     0.63  4507.54\n            se_looic\ninteraction    48.46\nheight_sex     47.56\nheight_only    43.10"
  },
  {
    "objectID": "slides/08-workflow.html#the-plan-moving-forward",
    "href": "slides/08-workflow.html#the-plan-moving-forward",
    "title": "Bayesian Workflow",
    "section": "The plan moving forward",
    "text": "The plan moving forward\n\nWe have now learned all of the skills needed to implement a Bayesian workflow.\nThe remainder of the course will be focused on introducing new models for types of data that are common when working in the biomedical data setting."
  },
  {
    "objectID": "slides/03-mcmc.html",
    "href": "slides/03-mcmc.html",
    "title": "Markov chain Monte Carlo",
    "section": "",
    "text": "On Tuesday, we performed posterior inference for a Beta-Binomial model using Monte Carlo estimation.\nToday we will discuss Bayesian estimation of linear regression:\n\n\\[\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n).\\]\n\nThere is no closed form posterior, so we cannot directly use Monte Carlo sampling.\nWe need Markov chain Monte Carlo (MCMC)!\nOur goal is to gain intuition behind MCMC, the workhorse behind Stan."
  },
  {
    "objectID": "slides/07-model-comparison.html",
    "href": "slides/07-model-comparison.html",
    "title": "Model Comparison",
    "section": "",
    "text": "On Tuesday, we learned about various ways to check MCMC convergence and model fit.\n\nTraceplots, effective sample size (\\(n_{eff}\\)), MC standard error, \\(\\hat{R}\\), sampling issues\nPosterior predictive checks\nModel checks using shinystan\n\nToday, we will learn about model comparisons."
  },
  {
    "objectID": "slides/05-priors-ppds.html",
    "href": "slides/05-priors-ppds.html",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "",
    "text": "On Tuesday, we learned about Stan\n\nA probabilistic programming language for Bayesian inference\nWe learned about the data, parameter, and model code chunks\nWe used Stan to fit a Bayesian linear regression\n\nToday, we will dive into priors, posterior summaries, and posterior predictive distributions (PPDs)"
  },
  {
    "objectID": "slides/09-nonlinear.html#review-of-last-lecture",
    "href": "slides/09-nonlinear.html#review-of-last-lecture",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "On Tuesday, we put all of our skills together and learned about the Bayesian workflow.\nWe have now learned all the skills needed to perform Bayesian inference.\nThe rest of the course we will introduce new models and data types that are useful for performing biomedical data science."
  },
  {
    "objectID": "slides/09-nonlinear.html#prepare-for-next-class",
    "href": "slides/09-nonlinear.html#prepare-for-next-class",
    "title": "Nonlinear Regression",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02.\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Robust regression"
  },
  {
    "objectID": "slides/09-robust.html#review-of-last-lecture",
    "href": "slides/09-robust.html#review-of-last-lecture",
    "title": "Robust Regression",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we put all of our skills together and learned about the Bayesian workflow.\nWe have now learned all the skills needed to perform Bayesian inference.\nThe rest of the course we will introduce new models and data types that are useful for performing biomedical data science."
  },
  {
    "objectID": "slides/09-robust.html#prepare-for-next-class",
    "href": "slides/09-robust.html#prepare-for-next-class",
    "title": "Robust Regression",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Regularization\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/09-robust.html#student-t-distribution",
    "href": "slides/09-robust.html#student-t-distribution",
    "title": "Robust Regression",
    "section": "Student-t distribution",
    "text": "Student-t distribution"
  },
  {
    "objectID": "slides/09-robust.html#visualizing-mean-deviation-across-time",
    "href": "slides/09-robust.html#visualizing-mean-deviation-across-time",
    "title": "Robust Regression",
    "section": "Visualizing mean deviation across time",
    "text": "Visualizing mean deviation across time"
  },
  {
    "objectID": "slides/09-robust.html#ols-regression",
    "href": "slides/09-robust.html#ols-regression",
    "title": "Robust Regression",
    "section": "OLS Regression",
    "text": "OLS Regression\n\\[\\begin{aligned}\nMD_i &= \\beta_0 + \\beta_1 Time_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(MD_i\\) are independent observations (independence).\n\\(MD_i\\) is linearly related to \\(Time_i\\) (linearity).\n\\(r_i = MD_i - \\mu_i\\) is normally distributed (normality).\n\\(r_i\\) has constant variance across \\(Time_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/09-robust.html#a-motivating-research-question",
    "href": "slides/09-robust.html#a-motivating-research-question",
    "title": "Robust Regression",
    "section": "A motivating research question",
    "text": "A motivating research question\n\nIn today’s lecture, we will look at data on serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years.\n\nA detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994).\n\nFor an example patient, we define \\(Y_i\\) as the serum concentration value and \\(X_i\\) as a child’s age, given in years."
  },
  {
    "objectID": "slides/09-robust.html#disease-progression",
    "href": "slides/09-robust.html#disease-progression",
    "title": "Robust Regression",
    "section": "Disease progression",
    "text": "Disease progression\n\nOLS can be written as follows for \\(i = 1,\\ldots,n\\),\n\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2).\\]\n\n\\(\\beta_1\\) represent the the change in IgG serum concentration a one year increase in age.\nOften the following hypothesis is tested: \\[H_0: \\beta_1=0,H_1: \\beta_1 &lt; 0.\\]"
  },
  {
    "objectID": "slides/09-robust.html#ols-regression-assumptions",
    "href": "slides/09-robust.html#ols-regression-assumptions",
    "title": "Robust Regression",
    "section": "OLS regression assumptions",
    "text": "OLS regression assumptions\n\\[\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(Y_i\\) are independent observations (independence).\n\\(Y_i\\) is linearly related to \\(X_i\\) (linearity).\n\\(\\epsilon_i = Y_i - \\mu_i\\) is normally distributed (normality).\n\\(\\epsilon_i\\) has constant variance across \\(X_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/09-robust.html#assessing-assumptions",
    "href": "slides/09-robust.html#assessing-assumptions",
    "title": "Robust Regression",
    "section": "Assessing assumptions",
    "text": "Assessing assumptions"
  },
  {
    "objectID": "slides/09-robust.html#heteroskedasticity",
    "href": "slides/09-robust.html#heteroskedasticity",
    "title": "Robust Regression",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nHeteroskedasticity is the violation of the assumption of constant variance.\nHow can we handle this?\nIn OLS, there are approaches like heteroskedastic consistent errors, but this is not a generative model.\nIn the Bayesian framework, we generally like to write down generative models."
  },
  {
    "objectID": "slides/09-robust.html#weighted-regression",
    "href": "slides/09-robust.html#weighted-regression",
    "title": "Robust Regression",
    "section": "Weighted regression",
    "text": "Weighted regression\n\nA common case is weighted regression, where each \\(Y_i\\) represents the mean of \\(n_i\\) observations. Then the scale of each observation is, \\[\\tau_i^2 = \\sigma^2/n_i,\\] where \\(\\sigma^2\\) is a global scale parameter.\nAlternatively, suppose each observation represents the sum of each \\(n_i\\) observations. Then the scale of each observation is, \\[\\tau_i^2 = n_i \\sigma^2.\\]"
  },
  {
    "objectID": "slides/09-robust.html#modeling-the-scale-with-covariates",
    "href": "slides/09-robust.html#modeling-the-scale-with-covariates",
    "title": "Robust Regression",
    "section": "Modeling the scale with covariates",
    "text": "Modeling the scale with covariates\n\nThe scale can also be modeled with covariates.\nIt is common to model the log-transformation of the scale or variance to transform it to \\(\\mathbb{R}\\),\n\n\\[\\log \\tau_i^2 = \\mathbf{z}_i \\boldsymbol{\\gamma},\\]\nwhere \\(\\mathbf{z}_i = (z_{i1},\\ldots,z_{ip})\\) are a \\(p\\)-dimensional vector of covariates and \\(\\boldsymbol{\\gamma}\\) are parameters that regress the covariates onto the log variance.\n\nOther options include: \\(\\log \\tau_i^2 = \\mathbf{z}_i \\boldsymbol{\\gamma} + \\nu_i,\\quad \\nu_i \\sim N(0, \\sigma^2)\\)\nOther options include: \\(\\log \\tau_i^2 = f(\\mu_i)\\)\nAny plausible generative model can be specified!"
  },
  {
    "objectID": "slides/09-robust.html#heteroskedastic-variance",
    "href": "slides/09-robust.html#heteroskedastic-variance",
    "title": "Robust Regression",
    "section": "Heteroskedastic variance",
    "text": "Heteroskedastic variance\n\nWe can write the regression model using a observation specific variance, \\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\tau_i^2).\\]\nOne way of writing the variance is: \\(\\tau_i^2 = \\sigma^2 \\lambda_i\\).\n\n\\(\\sigma^2\\) is a global scale parameter.\n\\(\\lambda_i\\) is an observation specific scale parameter.\n\nIn the Bayesian framework, we must place a prior on \\(\\lambda_i\\)."
  },
  {
    "objectID": "slides/09-robust.html#median-regression",
    "href": "slides/09-robust.html#median-regression",
    "title": "Robust Regression",
    "section": "Median regression",
    "text": "Median regression\nLeast absolute deviation (LAD) regression minimizes the following objective function,\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{LAD}} = \\arg \\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n |Y_i - \\mathbf{x}_i\\boldsymbol{\\beta}|.\\]\nThe Bayesian analog is the Laplace distribution, \\[f(x | \\mu, \\sigma) = \\frac{1}{2\\sigma} \\exp\\left\\{-\\frac{|x - \\mu|}{\\sigma}\\right\\}\\] where \\(\\mu\\) and \\(\\sigma\\) are the location and scale.\n\nThe Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is \\(|x−\\mu|\\), so minimizing the likelihood will also minimize the least absolute distances.\nLaplace distribution is also known as the double-exponential distribution (symmetric exponential distributions around \\(\\mu\\) with scale \\(\\sigma\\)).\nThus, a linear regression with Laplace errors is analogous to a median regression,\n\n\\[Y_i = \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\stackrel{ind}{\\sim}\\text{Laplace}(0, \\sigma).\\]\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += double_exponential_lpdf(Y | X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/09-robust.html#laplace-as-a-mixture",
    "href": "slides/09-robust.html#laplace-as-a-mixture",
    "title": "Robust Regression",
    "section": "Laplace as a mixture",
    "text": "Laplace as a mixture\nIf \\(V \\sim \\text{Exponential}(1)\\) and \\(Z \\sim N(0,1)\\) independent of \\(V\\), then \\(X = μ + \\sigma \\sqrt{2 V} Z ∼ \\text{Laplace}(\\mu,\\sigma)\\)."
  },
  {
    "objectID": "slides/09-robust.html#laplace-in-stan",
    "href": "slides/09-robust.html#laplace-in-stan",
    "title": "Robust Regression",
    "section": "Laplace in Stan",
    "text": "Laplace in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] V;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, sigma * sqrt(2 * V));\n  target += exponential_lpdf(to_vector(V));\n}"
  },
  {
    "objectID": "slides/09-robust.html#general-quantile-regression",
    "href": "slides/09-robust.html#general-quantile-regression",
    "title": "Robust Regression",
    "section": "General quantile regression",
    "text": "General quantile regression\n\nfunctions {\n  real asym_laplace_lpdf(real y, real mu, real sigma, real tau) {\n    return log(tau) + log1m(tau)\n      - log(sigma)\n      - 2 * ((y &lt; mu) ? (1 - tau) * (mu - y) : tau * (y - mu)) / sigma;\n  }\n}\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0, upper = 1&gt; tau;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  for (i in 1:n) target += asym_laplace_lpdf(Y[i] | X[i, ] * beta, sigma, tau);\n}"
  },
  {
    "objectID": "slides/09-robust.html#heteroskedastic-variance-1",
    "href": "slides/09-robust.html#heteroskedastic-variance-1",
    "title": "Robust Regression",
    "section": "Heteroskedastic variance",
    "text": "Heteroskedastic variance\n\nWe can write the regression model using a observation specific variance, \\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\tau_i^2).\\]\nOne way of writing the variance is: \\(\\tau_i^2 = \\sigma^2 \\lambda_i\\).\n\n\\(\\sigma^2\\) is a global scale parameter.\n\\(\\lambda_i\\) is an observation specific scale parameter.\n\nIn the Bayesian framework, we must place a prior on \\(\\lambda_i\\)."
  },
  {
    "objectID": "slides/09-robust.html#heteroskedastic-variance-2",
    "href": "slides/09-robust.html#heteroskedastic-variance-2",
    "title": "Robust Regression",
    "section": "Heteroskedastic variance",
    "text": "Heteroskedastic variance\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\begin{aligned}\nY_i &= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\\[f(Y_i) = \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\]"
  },
  {
    "objectID": "slides/09-robust.html#equivalnence-with-heavy-tail-distribution",
    "href": "slides/09-robust.html#equivalnence-with-heavy-tail-distribution",
    "title": "Robust Regression",
    "section": "Equivalnence with heavy-tail distribution",
    "text": "Equivalnence with heavy-tail distribution\n\\[\\begin{aligned}\nT_i &= \\frac{Z_i}{\\sqrt{\\frac{W_i}{\\nu}}},\\quad Z_i \\stackrel{iid}{\\sim} N(0,1), W_i \\stackrel{iid}{\\sim}\\chi^2_{\\nu}\\\\\n&= \\frac{Z_i}{\\sqrt{\\frac{1}{\\nu V_i}}},\\quad V_i \\stackrel{iid}{\\sim} \\text{Inv-}\\chi^2_{\\nu}\\\\\n&= \\sqrt{\\nu V_i} Z_i,\\quad \\lambda_i = \\nu V_i\\\\\n&= \\sqrt{\\lambda_i} Z_i, \\quad \\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\\\\\n&\\sim t_{\\nu}\n\\end{aligned}\\]\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma^2\\right)\\]"
  },
  {
    "objectID": "slides/09-robust.html#how",
    "href": "slides/09-robust.html#how",
    "title": "Robust Regression",
    "section": "How?",
    "text": "How?\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).\\]\n\\[\\iff\\]\n\\[\\begin{aligned}\nY_i &= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\\[\\begin{aligned}\nY_i &= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\\[f(Y_i) = \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\]"
  },
  {
    "objectID": "slides/09-robust.html#understanding-the-equivalence",
    "href": "slides/09-robust.html#understanding-the-equivalence",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\nHeteroskedastic variances assumption is equivalent to assuming a heavy-tailed distribution.\n\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).\\]\n\\[\\iff\\]\n\\[\\begin{aligned}\nY_i &= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\nNote that since the number of \\(\\lambda_i\\) parameters is equal to the number of observations, this model will not have a proper posterior distribution without a proper prior distribution."
  },
  {
    "objectID": "slides/09-robust.html#understanding-the-equivalence-1",
    "href": "slides/09-robust.html#understanding-the-equivalence-1",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\\[\\begin{aligned}\nf(Y_i) &= \\int_0^{\\infty} f(Y_i , \\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} N(Y_i ; \\mu_i, \\sigma^2 \\lambda_i) \\text{Inverse-Gamma}\\left(\\lambda_i ; \\frac{\\nu}{2},\\frac{\\nu}{2}\\right) d\\lambda_i\\\\\n&= t_{\\nu}\\left(\\mu_i,\\sigma\\right).\n\\end{aligned}\\]\n\nThe marginal likelihood can be viewed as a mixture of a Gaussian likelihood with an Inverse-Gamma scale parameter."
  },
  {
    "objectID": "slides/09-robust.html#understanding-the-equivalence-2",
    "href": "slides/09-robust.html#understanding-the-equivalence-2",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\\[\\begin{aligned}\nT_i &= \\frac{Z_i}{\\sqrt{\\frac{W_i}{\\nu}}},\\quad Z_i \\stackrel{iid}{\\sim} N(0,1), W_i \\stackrel{iid}{\\sim}\\chi^2_{\\nu}\\\\\n&= \\frac{Z_i}{\\sqrt{\\frac{1}{\\nu V_i}}},\\quad V_i \\stackrel{iid}{\\sim} \\text{Inv-}\\chi^2_{\\nu}\\\\\n&= \\sqrt{\\nu V_i} Z_i,\\quad \\lambda_i = \\nu V_i\\\\\n&= \\sqrt{\\lambda_i} Z_i, \\quad \\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\\\\\n&\\sim t_{\\nu}\n\\end{aligned}\\]\n\nWe then have: \\(Y_i = \\mu_i + \\sigma T_i \\sim t_{\\nu}(\\mu_i, \\sigma).\\)"
  },
  {
    "objectID": "slides/09-robust.html#robust-regression-as-heteroskedasticity",
    "href": "slides/09-robust.html#robust-regression-as-heteroskedasticity",
    "title": "Robust Regression",
    "section": "Robust regression as heteroskedasticity",
    "text": "Robust regression as heteroskedasticity\nThus, robust regression models with Student-t errors can be derived from a particular model of heteroskedastic normal errors."
  },
  {
    "objectID": "slides/09-robust.html#other-heteroskedastic-variance-specification",
    "href": "slides/09-robust.html#other-heteroskedastic-variance-specification",
    "title": "Robust Regression",
    "section": "Other heteroskedastic variance specification",
    "text": "Other heteroskedastic variance specification\nAs a Bayesian, we have complete flexibility over the specification \\(f(\\lambda_i)\\).\n\nAnother specification could be,\n\n\\[\\boldsymbol{\\lambda} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha}),\\]\nwhere \\(\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots,\\lambda_n)\\) and \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_n)\\).\n\nThis prior forces \\(\\sum_{i=1}^n \\lambda_i = 1\\), \\(\\lambda_i &gt;0\\)."
  },
  {
    "objectID": "slides/09-robust.html#one-option-for-a-prior",
    "href": "slides/09-robust.html#one-option-for-a-prior",
    "title": "Robust Regression",
    "section": "One option for a prior",
    "text": "One option for a prior\n\nWe could specify the following,\n\n\\[\\boldsymbol{\\lambda} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha}),\\]\nwhere \\(\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots,\\lambda_n)\\) and \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_n)\\).\n\nThis prior forces \\(\\sum_{i=1}^n \\lambda_i = 1\\), \\(\\lambda_i &gt;0\\).\nA prior probabilities are given by, \\(\\mathbb{E}[\\lambda_i] = \\alpha_i / \\alpha_0\\), where \\(\\alpha_0 = \\sum_{i=1}^n \\alpha_i.\\)"
  },
  {
    "objectID": "slides/09-robust.html#bayesian-prior-to-induce-structure",
    "href": "slides/09-robust.html#bayesian-prior-to-induce-structure",
    "title": "Robust Regression",
    "section": "Bayesian prior to induce structure",
    "text": "Bayesian prior to induce structure\n\nSuppse we would like \\(\\sum_{i=1}^n \\lambda_i = 1\\), \\(\\lambda_i &gt;0\\).\nWe could specify the following,\n\n\\[\\boldsymbol{\\lambda} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha}),\\]\nwhere \\(\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots,\\lambda_n)\\) and \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_n)\\).\n\nThe prior mean is \\(\\mathbb{E}[\\lambda_i] = \\alpha_i / \\alpha_0\\), where \\(\\alpha_0 = \\sum_{i=1}^n \\alpha_i.\\)\nTypically, \\(\\alpha_i = 1 \\forall i\\)."
  },
  {
    "objectID": "slides/09-robust.html#a-prior-to-induce-a-heavy-tail-distribution",
    "href": "slides/09-robust.html#a-prior-to-induce-a-heavy-tail-distribution",
    "title": "Robust Regression",
    "section": "A prior to induce a heavy-tail distribution",
    "text": "A prior to induce a heavy-tail distribution\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\\]\n\nUnder this prior, the marginal likelihood for \\(Y_i\\) is equivalent to a Student-t distribution,\n\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).\\]"
  },
  {
    "objectID": "slides/09-robust.html#robust-regression",
    "href": "slides/09-robust.html#robust-regression",
    "title": "Robust Regression",
    "section": "Robust regression",
    "text": "Robust regression\n\nToday we will learn about regression techniques that are robust to the assumptions of linear regression.\nWe will introduce the idea of robust regression by exploring ways to generalize the homoskedastic variance assumption in linear regression.\nWe will touch on heteroskedasticity, heavy-tailed distributions, and median regression (more generally quantile regression)."
  },
  {
    "objectID": "slides/09-robust.html#examples-of-heavy-tailed-distributions",
    "href": "slides/09-robust.html#examples-of-heavy-tailed-distributions",
    "title": "Robust Regression",
    "section": "Examples of heavy-tailed distributions",
    "text": "Examples of heavy-tailed distributions"
  },
  {
    "objectID": "slides/09-robust.html#a-prior-to-induce-a-heavy-tail",
    "href": "slides/09-robust.html#a-prior-to-induce-a-heavy-tail",
    "title": "Robust Regression",
    "section": "A prior to induce a heavy-tail",
    "text": "A prior to induce a heavy-tail\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\\]\n\nUnder this prior, the marginal likelihood for \\(Y_i\\) is equivalent to a Student-t distribution,\n\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).\\]"
  },
  {
    "objectID": "slides/09-robust.html#student-t-in-stan",
    "href": "slides/09-robust.html#student-t-in-stan",
    "title": "Robust Regression",
    "section": "Student-t in Stan",
    "text": "Student-t in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; nu;\n}\nmodel {\n  target += student_t_lpdf(Y | nu, X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/09-robust.html#student-t-in-stan-mixture",
    "href": "slides/09-robust.html#student-t-in-stan-mixture",
    "title": "Robust Regression",
    "section": "Student-t in Stan: mixture",
    "text": "Student-t in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += inv_gamma_lpdf(lambda | 0.5 * nu, 0.5 * nu);\n}"
  },
  {
    "objectID": "slides/09-robust.html#another-example-of-robust-regression",
    "href": "slides/09-robust.html#another-example-of-robust-regression",
    "title": "Robust Regression",
    "section": "Another example of robust regression",
    "text": "Another example of robust regression\n\nLet’s revisit our general heteroskedastic regression, \\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\sigma^2 \\lambda_i).\\]\nWe can induce another form of robust regression using the following prior for \\(\\lambda_i\\), \\(\\lambda_i \\sim \\text{Exponential}(1/2)\\).\nUnder this prior, the induced marginal model is, \\[Y_i = \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\stackrel{ind}{\\sim} \\text{Laplace}(\\mu, \\sigma).\\]\n\\(f(\\epsilon_i | \\mu, \\sigma) = \\frac{1}{2\\sigma} \\exp\\left\\{-\\frac{|\\epsilon_i - \\mu|}{\\sigma}\\right\\}\\)"
  },
  {
    "objectID": "slides/09-robust.html#modeling-the-scale-with-covariates-1",
    "href": "slides/09-robust.html#modeling-the-scale-with-covariates-1",
    "title": "Robust Regression",
    "section": "Modeling the scale with covariates",
    "text": "Modeling the scale with covariates\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1&gt; q;\n  vector[n] Y;\n  matrix[n, p] X;\n  matrix[n, q] Z;\n}\nparameters {\n  vector[p] beta;\n  vector[q] gamma;\n}\ntransformed parameters {\n  vector[n] tau = exp(Z * gamma);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n}"
  },
  {
    "objectID": "slides/09-robust.html#dirchlet-prior",
    "href": "slides/09-robust.html#dirchlet-prior",
    "title": "Robust Regression",
    "section": "Dirchlet prior",
    "text": "Dirchlet prior\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  vector&lt;lower = 0&gt;[n] alpha;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  simplex[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += dirichlet_lpdf(lambda | alpha);\n}"
  },
  {
    "objectID": "slides/09-robust.html#dirchlet-prior-in-stan",
    "href": "slides/09-robust.html#dirchlet-prior-in-stan",
    "title": "Robust Regression",
    "section": "Dirchlet prior in Stan",
    "text": "Dirchlet prior in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  vector&lt;lower = 0&gt;[n] alpha;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  simplex[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += dirichlet_lpdf(lambda | alpha);\n}"
  },
  {
    "objectID": "slides/09-robust.html#student-t-in-stan-1",
    "href": "slides/09-robust.html#student-t-in-stan-1",
    "title": "Robust Regression",
    "section": "Student-t in Stan",
    "text": "Student-t in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real&lt;lower = 0&gt; nu;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += student_t_lpdf(Y | nu, X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/09-robust.html#student-t-in-stan-mixture-1",
    "href": "slides/09-robust.html#student-t-in-stan-mixture-1",
    "title": "Robust Regression",
    "section": "Student-t in Stan: mixture",
    "text": "Student-t in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, sigma * sqrt(lambda));\n  target += inv_gamma_lpdf(lambda | 0.5 * nu, 0.5 * nu);\n}"
  },
  {
    "objectID": "slides/09-robust.html#why-heavy-tailed-distributions",
    "href": "slides/09-robust.html#why-heavy-tailed-distributions",
    "title": "Robust Regression",
    "section": "Why heavy-tailed distributions?",
    "text": "Why heavy-tailed distributions?\n\nReplacing the normal distribution with a distribution with heavy-tails (e.g., Student-t, Laplace) is a common approach to robust regression.\nRobust regression refers to regression methods which are less sensitive to outliers or small sample sizes.\nLinear regression, including Bayesian regression with normally distributed errors is sensitive to outliers, because the normal distribution has narrow tail probabilities.\nOur heteroskedastic model that we just explored is only one example of a robust regression model."
  },
  {
    "objectID": "slides/09-robust.html#vizualizing-heavy-tail-distributions",
    "href": "slides/09-robust.html#vizualizing-heavy-tail-distributions",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/09-robust.html#vizualizing-heavy-tail-distributions-1",
    "href": "slides/09-robust.html#vizualizing-heavy-tail-distributions-1",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/09-robust.html#median-regression-using-laplace",
    "href": "slides/09-robust.html#median-regression-using-laplace",
    "title": "Robust Regression",
    "section": "Median regression using Laplace",
    "text": "Median regression using Laplace\nLeast absolute deviation (LAD) regression minimizes the following objective function,\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{LAD}} = \\arg \\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n |Y_i - \\mathbf{x}_i\\boldsymbol{\\beta}|.\\]\nThe Bayesian analog is the Laplace distribution,\n\\(f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma) = \\left(\\frac{1}{2\\sigma}\\right)^n \\exp\\left\\{-\\sum_{i=1}^n\\frac{|Y_i - \\mathbf{x}_i \\boldsymbol{\\beta}|}{\\sigma}\\right\\}.\\)"
  },
  {
    "objectID": "slides/09-robust.html#median-regression-using-laplace-1",
    "href": "slides/09-robust.html#median-regression-using-laplace-1",
    "title": "Robust Regression",
    "section": "Median regression using Laplace",
    "text": "Median regression using Laplace\n\nThe Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is \\(|x−\\mu|\\), so minimizing the likelihood will also minimize the least absolute distances.\nLaplace distribution is also known as the double-exponential distribution (symmetric exponential distributions around \\(\\mu\\) with scale \\(\\sigma\\)).\nThus, a linear regression with Laplace errors is analogous to a median regression,\nWhy is median regression considered more robust than regression of the mean?"
  },
  {
    "objectID": "slides/09-robust.html#laplace-regression-in-stan",
    "href": "slides/09-robust.html#laplace-regression-in-stan",
    "title": "Robust Regression",
    "section": "Laplace regression in Stan",
    "text": "Laplace regression in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += double_exponential_lpdf(Y | X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/09-robust.html#laplace-regression-in-stan-mixture",
    "href": "slides/09-robust.html#laplace-regression-in-stan-mixture",
    "title": "Robust Regression",
    "section": "Laplace regression in Stan: mixture",
    "text": "Laplace regression in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += exponential_lpdf(lambda | 0.5);\n}"
  },
  {
    "objectID": "slides/09-robust.html#example-data",
    "href": "slides/09-robust.html#example-data",
    "title": "Robust Regression",
    "section": "Example data",
    "text": "Example data\nThis data set refers to the serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years. A detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994). The specific data set that we have used for our analysis is from the latter reference. The relationship of IgG with age is quite weak, with some visual evidence of positive skewness. We took the response variable Y to be the IgG concentration and used a quadratic model in age, x, to fit the quantile regression,\n\\[Quantile = \\beta_0 + \\beta_1 age + \\beta_2 age^2\\]\n\nlibrary(Brq)\ndata(\"ImmunogG\")\nhead(ImmunogG)\n\n  IgG Age\n1 1.5 0.5\n2 2.7 0.5\n3 1.9 0.5\n4 4.0 0.5\n5 1.9 0.5\n6 4.4 0.5\n\nplot(ImmunogG$Age, ImmunogG$IgG)"
  },
  {
    "objectID": "slides/09-robust.html#visualizing-mean-deviation-across-time-1",
    "href": "slides/09-robust.html#visualizing-mean-deviation-across-time-1",
    "title": "Robust Regression",
    "section": "Visualizing mean deviation across time",
    "text": "Visualizing mean deviation across time"
  },
  {
    "objectID": "slides/09-robust.html#assessing-assumptions-1",
    "href": "slides/09-robust.html#assessing-assumptions-1",
    "title": "Robust Regression",
    "section": "Assessing assumptions",
    "text": "Assessing assumptions"
  },
  {
    "objectID": "slides/09-robust.html#pulling-the-data",
    "href": "slides/09-robust.html#pulling-the-data",
    "title": "Robust Regression",
    "section": "Pulling the data",
    "text": "Pulling the data\n\nlibrary(Brq)\ndata(\"ImmunogG\")\nhead(ImmunogG)\n\n  IgG Age\n1 1.5 0.5\n2 2.7 0.5\n3 1.9 0.5\n4 4.0 0.5\n5 1.9 0.5\n6 4.4 0.5"
  },
  {
    "objectID": "slides/09-robust.html#visualizing-igg-data",
    "href": "slides/09-robust.html#visualizing-igg-data",
    "title": "Robust Regression",
    "section": "Visualizing IgG data",
    "text": "Visualizing IgG data"
  },
  {
    "objectID": "slides/09-robust.html#asymmetric-laplace-distribution",
    "href": "slides/09-robust.html#asymmetric-laplace-distribution",
    "title": "Robust Regression",
    "section": "Asymmetric Laplace distribution",
    "text": "Asymmetric Laplace distribution\nA random variable, \\(Y \\sim ALD_p(\\mu,\\sigma)\\) is said to follow an asymmetric Laplace distribution (ALD) if the pdf is given by,\n\\[f(Y) = \\frac{p(1-p)}{\\sigma} \\exp \\left\\{-\\rho_p\\left(\\frac{Y - \\mu}{\\sigma}\\right)\\right\\},\\]\nwhere \\(p \\in (0,1)\\) is the percentile and \\[\\rho_p(x) = x\\left(p - 1(u &lt; 0)\\right) = \\frac{|x| + (2p - 1)x}{2}.\\]\n\nWhen \\(p = 0.5\\) it reduces to a regular Laplace distribution."
  },
  {
    "objectID": "slides/09-robust.html#quantile-regression",
    "href": "slides/09-robust.html#quantile-regression",
    "title": "Robust Regression",
    "section": "Quantile regression",
    "text": "Quantile regression"
  },
  {
    "objectID": "slides/09-robust.html#scale-mixture-representation",
    "href": "slides/09-robust.html#scale-mixture-representation",
    "title": "Robust Regression",
    "section": "Scale-mixture representation",
    "text": "Scale-mixture representation\nThe above may also be written as a mixture of exponential and normal distributions. Letting, \\(Z_i \\sim Exponential(1)\\) and \\(\\sigma \\sim Exponential(1)\\).\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\sigma \\theta Z_i + \\epsilon_i,\\quad \\epsilon_i \\sim N\\left(0, \\tau^2 \\sigma^2 Z_i\\right),\\]\nwhere \\[\\theta = \\frac{1-2p}{p(1-p)}\\] and \\[\\tau \\sqrt{\\frac{2}{p(1-p)}}.\\]"
  },
  {
    "objectID": "slides/09-robust.html#scale-mixture-in-stan",
    "href": "slides/09-robust.html#scale-mixture-in-stan",
    "title": "Robust Regression",
    "section": "Scale-mixture in Stan",
    "text": "Scale-mixture in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0, upper = 1&gt; q;\n}\ntransformed data {\n  real theta = (1 - 2 * q) / (q * (1 - q));\n  real tau = sqrt(2 / (q * (1 - q)));\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector&lt;lower=0&gt;[n] z;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta + sigma * theta * z, tau * sqrt(z) * sigma);\n  target += exponential_lpdf(sigma | 1);\n  target += exponential_lpdf(z | 1);\n}"
  },
  {
    "objectID": "slides/09-robust.html#posterior-of-boldsymbolbeta",
    "href": "slides/09-robust.html#posterior-of-boldsymbolbeta",
    "title": "Robust Regression",
    "section": "Posterior of \\(\\boldsymbol{\\beta}\\)",
    "text": "Posterior of \\(\\boldsymbol{\\beta}\\)\n\n\n\n\n\nquantile\nmean\nlower\nupper\n\n\n\n\n0.025\n0.3291804\n0.2639468\n0.3899443\n\n\n0.250\n0.5130600\n0.3788182\n0.6523600\n\n\n0.500\n0.7268102\n0.5705129\n0.8836400\n\n\n0.750\n0.8093408\n0.6919899\n0.9363650\n\n\n0.975\n1.1741866\n0.9783041\n1.3936860"
  },
  {
    "objectID": "slides/09-robust.html#posterior-of-beta_1",
    "href": "slides/09-robust.html#posterior-of-beta_1",
    "title": "Robust Regression",
    "section": "Posterior of \\(\\beta_1\\)",
    "text": "Posterior of \\(\\beta_1\\)\n\n\n\n\n\nquantile\nmean\nlower\nupper\n\n\n\n\n0.025\n0.3291804\n0.2639468\n0.3899443\n\n\n0.250\n0.5130600\n0.3788182\n0.6523600\n\n\n0.500\n0.7268102\n0.5705129\n0.8836400\n\n\n0.750\n0.8093408\n0.6919899\n0.9363650\n\n\n0.975\n1.1741866\n0.9783041\n1.3936860"
  },
  {
    "objectID": "slides/10-regularization.html#review-of-last-lecture",
    "href": "slides/10-regularization.html#review-of-last-lecture",
    "title": "Regularization",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Thursday, we learned about robust regression.\n\nHeteroskedasticity\nHeavy-tailed distributions\nQuantile regression\n\nThese were all models for the observed data \\(Y_i\\).\nToday, we will focus on prior specifications for \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/10-regularization.html#ridge-regression",
    "href": "slides/10-regularization.html#ridge-regression",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nRidge regression aims to yield a parsimonious regularized regression model in the presence of highly correlated predictor variables.\nRidge regression solves the following loss function,\n\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right) + \\lambda \\sum_{j = 1}^p \\beta_j^2.\\]\n\n\\(\\lambda \\geq 0\\) controls the amount of regularization."
  },
  {
    "objectID": "slides/10-regularization.html#a-motivating-research-question",
    "href": "slides/10-regularization.html#a-motivating-research-question",
    "title": "Regularization",
    "section": "A motivating research question",
    "text": "A motivating research question\n\nIn today’s lecture, we will look at data on serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years.\n\nA detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994).\n\nFor an example patient, we define \\(Y_i\\) as the serum concentration value and \\(X_i\\) as a child’s age, given in years."
  },
  {
    "objectID": "slides/10-regularization.html#prepare-for-next-class",
    "href": "slides/10-regularization.html#prepare-for-next-class",
    "title": "Regularization",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWe are going to jump into an AE on body fat, but first some reminders.\nWork on HW 02, which is due before next class.\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Classification\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/10-regularization.html#orindary-least-square-regression",
    "href": "slides/10-regularization.html#orindary-least-square-regression",
    "title": "Regularization",
    "section": "Orindary least square regression",
    "text": "Orindary least square regression\n\nConsider the linear regression model, \\(Y_i \\stackrel{ind}{\\sim}N\\left(\\mathbf{x}_i^\\top\\boldsymbol{\\beta},\\sigma^2\\right)\\).\n\nFor simplicity lets assume that \\(\\sigma = 1\\).\n\nWe can write this in matrix form, \\(\\mathbf{Y} \\stackrel{}{\\sim}N\\left(\\mathbf{X}\\boldsymbol{\\beta},\\mathbf{I}_n\\right)\\).\nOrdinary least squares (OLS) regression solves the following loss function,\n\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{ols}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right).\\]\n\nIn this simple setting: \\(\\hat{\\boldsymbol{\\beta}}_{\\text{ols}} = \\left(\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)."
  },
  {
    "objectID": "slides/10-regularization.html#ridge-regression-1",
    "href": "slides/10-regularization.html#ridge-regression-1",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nThe term, \\(\\lambda \\sum_{j = 1}^p \\beta_j^2\\), is sometimes called the L2 - norm.\nIn can be seen that: \\(\\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\left(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\).\nUnder the setting of correlated predictors, the inverse of \\(\\mathbf{X}^\\top\\mathbf{X}\\) often approaches singularity.\nAdding positive elements to the diagonals improves stability of the inverse.\nWhen \\(\\lambda = 0\\), \\(\\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\hat{\\boldsymbol{\\beta}}_{\\text{ols}}\\)\nWhen \\(\\lambda \\rightarrow \\infty\\), \\(\\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} \\rightarrow 0\\)."
  },
  {
    "objectID": "slides/10-regularization.html#bayesian-ridge-regression",
    "href": "slides/10-regularization.html#bayesian-ridge-regression",
    "title": "Regularization",
    "section": "Bayesian ridge regression",
    "text": "Bayesian ridge regression\n\nFrom a Bayesian framework, ridge regression can be derived using the following prior, \\(\\beta_j | \\lambda \\stackrel{ind}{\\sim}N\\left(0,\\frac{1}{\\lambda}\\right)\\).\nWhy?\n\n\n\n\\(f(\\beta_j | \\lambda) \\propto \\exp\\left\\{-\\frac{\\lambda\\beta_j^2}{2}\\right\\}\\)\n\n\n\n\n\\(\\log \\prod_{i=1}^n f(\\beta_j | \\lambda) = -0.5 \\lambda\\sum_{i=1}^n\\beta_j^2\\)"
  },
  {
    "objectID": "slides/10-regularization.html#prior-for-lambda",
    "href": "slides/10-regularization.html#prior-for-lambda",
    "title": "Regularization",
    "section": "Prior for \\(\\lambda\\)",
    "text": "Prior for \\(\\lambda\\)\n\nRidge regression: \\(\\beta_j | \\lambda \\stackrel{ind}{\\sim}N\\left(0,\\frac{1}{\\lambda}\\right)\\).\nSometimes \\(\\lambda\\) is fixed (or learned using a grid search).\nSince we are Bayesians, we can place a prior on \\(\\lambda\\).\nLarger values of \\(\\lambda\\) induce greater regularization, because the variance of the regression coefficients becomes smaller.\n\nThis inspires a prior with a heavy-tail, so a half-Cauchy distribution is often used, \\(\\lambda \\sim \\mathcal C^+(0,1)\\)."
  },
  {
    "objectID": "slides/10-regularization.html#half-cauchy-distribution",
    "href": "slides/10-regularization.html#half-cauchy-distribution",
    "title": "Regularization",
    "section": "Half-Cauchy distribution",
    "text": "Half-Cauchy distribution\nA random variable \\(X \\sim \\mathcal C^+(\\mu,\\sigma)\\) follows a half-Cauchy distribution with location \\(\\mu\\) and scale \\(\\sigma &gt; 0\\) and has the following density,\n\\[f(X | \\mu, \\sigma) = \\frac{2}{\\pi \\sigma}\\frac{1}{1 + (X - \\mu)^2 / \\sigma^2},\\quad X \\geq \\mu\\]\n\nThe Half-Cauchy distribution with \\(\\mu = 0\\) is a useful prior for non-negative parameters that may be very large, as allowed by the very heavy tails of the Half-Cauchy distribution."
  },
  {
    "objectID": "slides/10-regularization.html#half-cauchy-distribution-in-stan",
    "href": "slides/10-regularization.html#half-cauchy-distribution-in-stan",
    "title": "Regularization",
    "section": "Half-Cauchy distribution in Stan",
    "text": "Half-Cauchy distribution in Stan\nIn Stan, the half-Cauchy distribution can be specified by putting a constraint on the parameter definition.\n\nparameters {\n  real&lt;lower = 0&gt; lambda;\n}\nmodel {\n  target += cauchy_lpdf(lambda | 0, 1);\n}"
  },
  {
    "objectID": "slides/10-regularization.html#lasso-regression",
    "href": "slides/10-regularization.html#lasso-regression",
    "title": "Regularization",
    "section": "Lasso regression",
    "text": "Lasso regression\n\nA drawback to ridge regression is that it does not improve parsimony, because all parameters stay in the final model (i.e., no \\(\\beta_j\\) is set exactly to 0).\nLeast absolute shrinkage and selection operator (Lasso) regression is an alternative method.\nLasso regression solves the following loss function,\n\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{lasso}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right) + \\lambda \\sum_{j = 1}^p |\\beta_j|.\\]"
  },
  {
    "objectID": "slides/10-regularization.html#lasso-regression-1",
    "href": "slides/10-regularization.html#lasso-regression-1",
    "title": "Regularization",
    "section": "Lasso regression",
    "text": "Lasso regression\n\nThe term, \\(\\lambda \\sum_{j = 1}^p |\\beta_j|\\), is sometimes called the L1 - norm.\nThis specification allows less important predictors to be set to zero, meaning it performs both shrinkage and variable selection.\nThere is no analytic form for \\(\\hat{\\boldsymbol{\\beta}}_{\\text{lasso}}\\), since the solution is non-linear in \\(\\mathbf{Y}\\).\nLasso pulls \\(\\beta_j\\) to zero faster than ridge regression."
  },
  {
    "objectID": "slides/10-regularization.html#bayesian-lasso-regression",
    "href": "slides/10-regularization.html#bayesian-lasso-regression",
    "title": "Regularization",
    "section": "Bayesian lasso regression",
    "text": "Bayesian lasso regression\n\nFrom a Bayesian framework, lasso regression can be derived using the following prior, \\(\\beta_j | \\lambda \\stackrel{ind}{\\sim}\\text{Laplace}\\left(0,\\frac{1}{\\lambda}\\right)\\).\nWhy?\n\n\n\n\\(f(\\beta_j | \\lambda) \\propto \\exp\\left\\{-\\lambda|\\beta_j|\\right\\}.\\)\n\n\n\n\n\\(\\log \\prod_{i=1}^n f(\\beta_j | \\lambda) = -\\lambda\\sum_{i=1}^n|\\beta_j|\\)"
  },
  {
    "objectID": "slides/10-regularization.html#sparsity-in-regression-problems",
    "href": "slides/10-regularization.html#sparsity-in-regression-problems",
    "title": "Regularization",
    "section": "Sparsity in regression problems",
    "text": "Sparsity in regression problems\n\nSupervised learning can be cast as the problem of estimating a set of coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_j\\}_{j=1}^{p}\\) that determines some functional relationship between a set of \\(\\{x_j\\}_{j = 1}^p\\) and a target variable \\(y\\).\nThis is a central focus of statistics and machine learning.\nChallenges arise in “large-\\(p\\)” problems where, in order to avoid overly complex models that predict poorly, some form of dimension reduction is needed.\nFinding a sparse solution, where some \\(\\beta_j\\) are zero, is desirable."
  },
  {
    "objectID": "slides/10-regularization.html#bayesian-sparse-estimation",
    "href": "slides/10-regularization.html#bayesian-sparse-estimation",
    "title": "Regularization",
    "section": "Bayesian sparse estimation",
    "text": "Bayesian sparse estimation\n\nFrom a Bayesian-learning perspective, there are two main sparse-estimation alternatives: discrete mixtures and shrinkage priors.\nDiscrete mixtures have been very popular, with the spike-and-slab prior being the gold standard.\n\nEasy to force \\(\\beta_j\\) to exactly zero, but require discrete parameter specification.\n\nShrinkage priors force \\(\\beta_j\\) to zero using regularization, but struggle to get exact zeros.\n\nIn recent years, shrinkage priors have become dominant in Bayesian sparsity priors."
  },
  {
    "objectID": "slides/10-regularization.html#horseshoe-prior",
    "href": "slides/10-regularization.html#horseshoe-prior",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\n\nLet’s assume \\(\\mathbf{Y} \\stackrel{}{\\sim}N\\left(\\mathbf{X}\\boldsymbol{\\beta},\\sigma^2\\mathbf{I}_n\\right)\\), where \\(\\boldsymbol{\\beta}\\) is assumed to be sparse.\nThe horseshoe prior is specified as,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\sim \\mathcal C^+(0, 1),\n\\end{aligned}\\] where \\(\\mathcal C^+(0, 1)\\) is a half-Cauchy distribution for the standard deviation \\(\\lambda_j\\)."
  },
  {
    "objectID": "slides/10-regularization.html#half-cauchy-distribution-1",
    "href": "slides/10-regularization.html#half-cauchy-distribution-1",
    "title": "Regularization",
    "section": "Half-Cauchy distribution",
    "text": "Half-Cauchy distribution\nA random variable \\(X \\sim \\mathcal C^+(\\mu,\\sigma)\\) follows a half-Cauchy distribution with location \\(\\mu\\) and scale \\(\\sigma &gt; 0\\) and has the following density,\n\\[f(X | \\mu, \\sigma) = \\frac{2}{\\pi \\sigma}\\frac{1}{1 + (X - \\mu)^2 / \\sigma^2},\\quad X \\geq \\mu\\]\n\nThe Half-Cauchy distribution with \\(\\mu = 0\\) is a useful prior for non-negative parameters that may be very large, as allowed by the very heavy tails of the Half-Cauchy distribution."
  },
  {
    "objectID": "slides/10-regularization.html#half-cauchy-distribution-in-stan-1",
    "href": "slides/10-regularization.html#half-cauchy-distribution-in-stan-1",
    "title": "Regularization",
    "section": "Half-Cauchy distribution in Stan",
    "text": "Half-Cauchy distribution in Stan\nIn Stan, the half-Cauchy distribution can be specified by putting a constraint on the parameter definition.\n\nparameters {\n  real&lt;lower = 0&gt; lambda;\n}\nmodel {\n  target += cauchy_lpdf(lambda | 0, 1);\n}"
  },
  {
    "objectID": "slides/10-regularization.html#horseshoe-prior-1",
    "href": "slides/10-regularization.html#horseshoe-prior-1",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\n\nThe horseshoe prior is specified as,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\sim \\mathcal C^+(0, 1),\n\\end{aligned}\\] where \\(\\mathcal C^+(0, 1)\\) is a half-Cauchy distribution for the standard deviation \\(\\lambda_j\\).\n\n\\(\\lambda_j\\)’s are the local shrinkage parameters.\n\\(\\tau\\) is the global shrinkage parameter."
  },
  {
    "objectID": "slides/10-regularization.html#horsehoe-density",
    "href": "slides/10-regularization.html#horsehoe-density",
    "title": "Regularization",
    "section": "Horsehoe density",
    "text": "Horsehoe density\n\n\n\n\n\nFrom Carvalho 2009"
  },
  {
    "objectID": "slides/10-regularization.html#horseshoe-prior-2",
    "href": "slides/10-regularization.html#horseshoe-prior-2",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\nThe horseshoe prior has two interesting features that make it particularly useful as a shrinkage prior for sparse problems.\n\nIt has flat, Cauchy-like tails that allow strong signals to remain large (that is, un-shrunk) a posteriori.\nIt has an infinitely tall spike at the origin that provides severe shrinkage for the zero elements of \\(\\boldsymbol{\\beta}\\).\n\nAs we will see, these are key elements that make the horseshoe an attractive choice for handling sparse vectors."
  },
  {
    "objectID": "slides/10-regularization.html#relation-to-other-shrinkage-priors",
    "href": "slides/10-regularization.html#relation-to-other-shrinkage-priors",
    "title": "Regularization",
    "section": "Relation to other shrinkage priors",
    "text": "Relation to other shrinkage priors\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\sim f(\\lambda_j)\n\\end{aligned}\\]\n\n\\(\\lambda_j = \\lambda\\), implies a Gaussian prior for \\(\\beta_j\\) (Ridge regression).\n\\(f(\\lambda_j) = \\text{Exponential}(2)\\), implies independent Laplacian priors for \\(\\beta_j\\) (LASSO).\n\\(f(\\lambda_j) = \\text{Inverse-Gamma}(a,b)\\), implies independent Student-t priors for \\(\\beta_j\\) (relevance vector machine)."
  },
  {
    "objectID": "slides/10-regularization.html#shrinkage-of-each-prior",
    "href": "slides/10-regularization.html#shrinkage-of-each-prior",
    "title": "Regularization",
    "section": "Shrinkage of each prior",
    "text": "Shrinkage of each prior\n\nDefine the posterior mean of \\(\\beta_j\\) as \\(\\bar{\\beta}_j\\) and the maximum likelihood estimator for \\(\\beta_j\\) as \\(\\hat{\\beta}_j\\).\nThe following relationship holds: \\(\\bar{\\beta}_j = (1 - \\kappa_j) \\hat{\\beta}_j\\),\n\n\\[\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}s_j^2\\lambda_j^2}.\\]\n\n\\(\\kappa_j\\) is called the shrinkage factor for \\(\\beta_j\\).\n\\(s_j^2 = \\mathbb{V}(x_j)\\) is the variance for each predictor."
  },
  {
    "objectID": "slides/10-regularization.html#standardization-of-predictors",
    "href": "slides/10-regularization.html#standardization-of-predictors",
    "title": "Regularization",
    "section": "Standardization of predictors",
    "text": "Standardization of predictors\n\nIn regularization problems, predictors are standardized (to mean zero and standard deviation one).\nThis means that so that \\(s_j = 1\\).\nShrinkage parameter:\n\n\\[\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}\\lambda_j^2}.\\]\n\n\\(\\kappa_j = 1\\), implies complete shrinkage.\n\\(\\kappa_j = 0\\), implies no shrinkage."
  },
  {
    "objectID": "slides/10-regularization.html#shrinkage-parameter",
    "href": "slides/10-regularization.html#shrinkage-parameter",
    "title": "Regularization",
    "section": "Shrinkage parameter",
    "text": "Shrinkage parameter\n\n\n\n\n\nFrom Carvalho 2009"
  },
  {
    "objectID": "slides/10-regularization.html#horseshoe-shrinkage-parameter",
    "href": "slides/10-regularization.html#horseshoe-shrinkage-parameter",
    "title": "Regularization",
    "section": "Horseshoe shrinkage parameter",
    "text": "Horseshoe shrinkage parameter\n\nChoosing \\(\\lambda_j ∼ \\mathcal C^+(0, 1)\\) implies \\(\\kappa_j ∼ \\text{Beta}(0.5, 0.5)\\), a density that is symmetric and unbounded at both 0 and 1.\nThis horseshoe-shaped shrinkage profile expects to see two things a priori:\n\nStrong signals (\\(\\kappa \\approx 0\\), no shrinkage), and\nZeros (\\(\\kappa \\approx 1\\), total shrinkage)."
  },
  {
    "objectID": "slides/10-regularization.html#spike-and-slab-prior",
    "href": "slides/10-regularization.html#spike-and-slab-prior",
    "title": "Regularization",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nThe prior is often written as a two-component mixture of Gaussians,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, c^2, \\epsilon &\\sim \\lambda_j N(0, c^2) + (1-\\lambda_j) N(0,\\omega^2)\\\\\n\\lambda_j &\\sim \\text{Bernoulli}(\\pi).\n\\end{aligned}\\]\n\n\\(\\omega \\ll c\\) and the indicator variable \\(\\lambda_j \\in \\{0, 1\\}\\) denotes whether \\(\\beta_j\\) is close to zero (comes from the “spike”, \\(\\lambda_j = 0\\)) or nonzero (comes from the “slab”, \\(\\lambda_j = 1\\))."
  },
  {
    "objectID": "slides/10-regularization.html#spike-and-slab-prior-1",
    "href": "slides/10-regularization.html#spike-and-slab-prior-1",
    "title": "Regularization",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nOften \\(\\omega = 0\\) (the spike is a true spike), and the prior can be written as,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, c^2 &\\sim N(0, \\lambda_j^2 c^2)\\\\\n\\lambda_j &\\sim \\text{Bernoulli}(\\pi).\n\\end{aligned}\\]\n\nInstead of giving continuous priors for \\(\\lambda_j\\)’s as in the horseshoe, here only two values are allowed (0,1).\nThe shrinkage factor \\(\\kappa_j\\) only has mass at \\(\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}}\\) and \\(\\kappa_j = 1\\) with probabilities \\(\\pi\\) and \\(1-\\pi\\),"
  },
  {
    "objectID": "slides/10-regularization.html#similarity-to-horseshoe",
    "href": "slides/10-regularization.html#similarity-to-horseshoe",
    "title": "Regularization",
    "section": "Similarity to horseshoe",
    "text": "Similarity to horseshoe\n\nLetting \\(c \\rightarrow \\infty\\), all the mass is concentrated at the extremes \\(\\kappa_j = 0\\) and \\(\\kappa_j = 1\\) (this resembles the horseshoe).\n\n\n\n\n\n\nFrom Piironena and Vehtari 2017\n\nThe horseshoe can be seen as a continuous approximation to the spike-and-slab prior as \\(c \\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/10-regularization.html#global-shrinkage-parameter-tau",
    "href": "slides/10-regularization.html#global-shrinkage-parameter-tau",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\nThe prior mean can be shown to be,\n\n\\[\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = \\frac{\\tau \\sigma^{-1} \\sqrt{n}}{1 + \\tau \\sigma^{-1} \\sqrt{n}}p.\\]\n\nSetting \\(\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = p_0\\) (prior guess for the number of non-zero coefficients) yields for \\(\\tau\\),\n\n\\[\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/10-regularization.html#global-shrinkage-parameter-tau-1",
    "href": "slides/10-regularization.html#global-shrinkage-parameter-tau-1",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/10-regularization.html#regularized-horseshoe-prior",
    "href": "slides/10-regularization.html#regularized-horseshoe-prior",
    "title": "Regularization",
    "section": "Regularized horseshoe prior",
    "text": "Regularized horseshoe prior\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau, c &\\sim N\\left(0, \\tau^2 \\tilde{\\lambda}_j^2\\right),\\quad \\tilde{\\lambda}_j^2 = \\frac{c^2 \\lambda_j^2}{c^2 + \\tau^2 \\lambda_j^2},\\\\\n\\lambda_j &\\sim \\mathcal C^+(0,1).\n\\end{aligned}\\]\n\nWhen \\(\\tau^2 \\lambda_j^2 \\ll c^2\\) (i.e., \\(\\beta_j\\) close to zero), \\(\\beta_j \\sim  N\\left(0, \\tau^2\\lambda_j^2\\right)\\)\nWhen \\(\\tau^2 \\lambda_j^2 \\gg c^2\\), (i.e., \\(\\beta_j\\) far from zero), \\(\\beta_j \\sim  N\\left(0, c^2\\right)\\)\n\\(c \\rightarrow \\infty\\) recovers the original horseshoe.\n\nWhy is this an appealing extension?"
  },
  {
    "objectID": "slides/10-regularization.html#global-shrinkage-parameter-tau-2",
    "href": "slides/10-regularization.html#global-shrinkage-parameter-tau-2",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\nSetting \\(\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = p_0\\) (prior guess for the number of non-zero coefficients) yields for \\(\\tau\\),\n\n\\[\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/10-regularization.html#regularized-horseshoe-compared-to-spike-and-slab",
    "href": "slides/10-regularization.html#regularized-horseshoe-compared-to-spike-and-slab",
    "title": "Regularization",
    "section": "Regularized horseshoe compared to spike-and-slab",
    "text": "Regularized horseshoe compared to spike-and-slab\n\nThe regularized horseshoe prior is comparable to the spike-and-slab with finite \\(c\\).\n\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/10-regularization.html#choosing-a-prior-for-c2",
    "href": "slides/10-regularization.html#choosing-a-prior-for-c2",
    "title": "Regularization",
    "section": "Choosing a prior for \\(c^2\\)",
    "text": "Choosing a prior for \\(c^2\\)\n\nUnless substantial knowledge about the scale of the relevant coefficients exists, it is recommended to place a prior for \\(c\\) instead of fixing it.\nOften a reasonable choice is, \\[c^2 \\sim \\text{Inv-Gamma}(\\alpha, \\beta), \\quad \\alpha = \\nu/2, \\beta = \\nu s^2 / 2,\\]\nThis translates to a \\(t_{\\nu}(0,s^2)\\) slab for the coefficients far from 0.\nAnother motivation for using inverse-Gamma is that it has a heavy right tail accompanied by a light left tail thereby preventing much mass from accumulating near zero."
  },
  {
    "objectID": "slides/10-regularization.html#choosing-a-prior-for-tau",
    "href": "slides/10-regularization.html#choosing-a-prior-for-tau",
    "title": "Regularization",
    "section": "Choosing a prior for \\(\\tau\\)",
    "text": "Choosing a prior for \\(\\tau\\)\n\nCarvalho et al. 2009 suggest \\(\\tau \\sim \\mathcal C^+(0,1)\\).\nPolson and Scott 2011 recommend \\(\\tau | \\sigma \\sim \\mathcal C^+(0, \\sigma^2)\\).\nAnother prior comes from a quantity called the effective number of nonzero coefficients,\n\\[m_{eff} = \\sum_{j=1}^p (1 - \\kappa_j).\\]"
  },
  {
    "objectID": "slides/11-classification.html#review-of-last-lecture",
    "href": "slides/11-classification.html#review-of-last-lecture",
    "title": "Classification",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we learned about Bayesian approaches to regularization.\n\nGlobal-local shrinage priors\n\nToday, we will focus on classification: logistic regression, multinomial regression, ordinal regression."
  },
  {
    "objectID": "slides/11-classification.html#models-for-binary-outcomes",
    "href": "slides/11-classification.html#models-for-binary-outcomes",
    "title": "Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nSuppose we have a binary outcome (e.g., \\(Y = 1\\) if a condition is satisfied and \\(Y = 0\\) if not) and predictors on a variety of scales.\nIf the predictors are discrete and the binary outcomes are independent, we can use the Bernoulli distribution for individual 0-1 data or the binomial distribution for grouped data that are counts of successes in each group."
  },
  {
    "objectID": "slides/11-classification.html#models-for-binary-outcomes-1",
    "href": "slides/11-classification.html#models-for-binary-outcomes-1",
    "title": "Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nLet’s suppose we want to model \\(P(Y = 1)\\).\nOne strategy might be to simply fit a linear regression model to the probabilities.\nFor example,\n\n\\[\\begin{aligned}\nP(Y_i = 1) &= \\alpha + \\beta_1x_{i1} + \\cdots + \\beta_px_{ip} + \\epsilon_i\\\\\n&= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11-classification.html#prepare-for-next-class",
    "href": "slides/11-classification.html#prepare-for-next-class",
    "title": "Classification",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03, which was just assigned.\nComplete reading to prepare for next Tuesdays’s lecture\nTuesday’s lecture: Missing data\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/11-classification.html#primary-biliary-cirrhosis",
    "href": "slides/11-classification.html#primary-biliary-cirrhosis",
    "title": "Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nThe Mayo Clinic conducted a trial for primary biliary cirrhosis, comparing the drug D-penicillamine vs. placebo. Patients were followed for a specified duration, and their status at the end of follow-up (whether they died) was recorded.\nResearchers are interested in predicting whether a patient died based on the following variables:\n\nascites: whether the patient had ascites (1 = yes, 0 = no)\nbilirubin: serum bilirubin in mg/dL\nstage: histologic stage of disease (ordinal categorical variable with stages 1, 2, 3, and 4)"
  },
  {
    "objectID": "slides/11-classification.html#what-can-go-wrong",
    "href": "slides/11-classification.html#what-can-go-wrong",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nSuppose we fit the following model:\n\n\\[\\begin{aligned}\nP(Y_i = 1) &= \\alpha + \\beta_1(ascites)_i + \\beta_2(bilirubin)_i\\\\\n&\\quad+\\beta_3(stage = 2)_i + \\beta_4(stage = 3)_i\\\\\n&\\quad+\\beta_5(stage = 4)_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\n\\end{aligned}\\]\n\nWhat can go wrong?"
  },
  {
    "objectID": "slides/11-classification.html#what-can-go-wrong-1",
    "href": "slides/11-classification.html#what-can-go-wrong-1",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "slides/11-classification.html#what-can-go-wrong-2",
    "href": "slides/11-classification.html#what-can-go-wrong-2",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nAdditionally, as a probability, \\(P(Y_i = 1)\\) must be in the interval [0, 1], but there is nothing in the model that enforces this constraint, so that you could be estimating probabilities that are negative or that are greater than 1!"
  },
  {
    "objectID": "slides/11-classification.html#from-probabilities-to-log-odds",
    "href": "slides/11-classification.html#from-probabilities-to-log-odds",
    "title": "Classification",
    "section": "From probabilities to log-odds",
    "text": "From probabilities to log-odds\n\nSuppose the probability of an event is \\(\\pi\\).\nThen the odds that the event occurs is \\(\\frac{\\pi}{1 - \\pi}\\).\nTaking the (natural) log of the odds, we have the logit of \\(\\pi\\): the log-odds:\n\n\\[\\text{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).\\]\n\nNote that although \\(\\pi\\) is constrained to lie between 0 and 1, the logit of \\(\\pi\\) is unconstrained - it can be anything from \\(-\\infty\\) to \\(\\infty\\)."
  },
  {
    "objectID": "slides/11-classification.html#logistic-regression-model",
    "href": "slides/11-classification.html#logistic-regression-model",
    "title": "Classification",
    "section": "Logistic regression model",
    "text": "Logistic regression model\n\nLet’s create a model for the logit of \\(\\pi\\): \\(\\text{logit}(\\pi_i)= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\\)\nThis is a linear model for a transformation of the outcome of interest, and is also equivalent to,\n\n\\[\\pi_i = \\frac{\\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})}{1 + \\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})} = \\text{expit}(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}).\\]\n\nThe expression on the right is called a logistic function and cannot yield a value that is negative or a value that is &gt;1. Fitting a model of this form is known as logistic regression."
  },
  {
    "objectID": "slides/11-classification.html#other-models-for-binary-data",
    "href": "slides/11-classification.html#other-models-for-binary-data",
    "title": "Classification",
    "section": "Other models for binary data",
    "text": "Other models for binary data\n\nOther transformations (also called link functions) can be used to ensure the probabilities lie in [0, 1], including the Probit (popular in Bayesian statistics)."
  },
  {
    "objectID": "slides/11-classification.html#logistic-regression",
    "href": "slides/11-classification.html#logistic-regression",
    "title": "Classification",
    "section": "Logistic regression",
    "text": "Logistic regression\n\\[\\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nNegative logits represent probabilities less than one-half, and positive logits represent probabilities above one-half."
  },
  {
    "objectID": "slides/11-classification.html#interpreting-parameters-in-logistic-regression",
    "href": "slides/11-classification.html#interpreting-parameters-in-logistic-regression",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nTypically we interpret functions of parameters in logistic regression rather than the parameters themselves.\n\nFor the simple model: \\(\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\beta x_{i},\\) we note that the probability that \\(Y_i = 1\\) when \\(X_i = 0\\) is\n\\[P(Y_i = 1 | X_{i} = 0) = \\frac{\\exp(\\alpha)}{1 + \\exp(\\alpha)}.\\]"
  },
  {
    "objectID": "slides/11-classification.html#interpreting-parameters-in-logistic-regression-1",
    "href": "slides/11-classification.html#interpreting-parameters-in-logistic-regression-1",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nSuppose that \\(X\\) is a binary (0/1) variable (e.g., \\(X\\) = 1 for males and 0 for non-males).\n\nIn this case, we interpret \\(\\exp(\\beta)\\) as the odds ratio (OR) of the response for the two possible levels of \\(X\\).\nFor \\(X\\) on other scales, \\(\\exp(\\beta)\\) is interpreted as the odds ratio of the response comparing two values of \\(X\\) one unit apart.\n\nWhy?"
  },
  {
    "objectID": "slides/11-classification.html#interpreting-parameters-in-logistic-regression-2",
    "href": "slides/11-classification.html#interpreting-parameters-in-logistic-regression-2",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nThe log odds of response for \\(X = 1\\) is given by \\(\\alpha + \\beta\\), and the log odds of response for \\(X = 0\\) is \\(\\alpha\\).\nSo the odds ratio of response comparing \\(X = 1\\) to \\(X = 0\\) is given by \\(\\frac{\\exp(\\alpha + \\beta)}{\\exp(\\alpha)} = \\exp(\\beta)\\).\nIn a multivariable logistic regression model with more than one predictor, this OR is interpreted conditionally on values of other variables (i.e., controlling for them)."
  },
  {
    "objectID": "slides/11-classification.html#bayesian-logistic-regression",
    "href": "slides/11-classification.html#bayesian-logistic-regression",
    "title": "Classification",
    "section": "Bayesian logistic regression",
    "text": "Bayesian logistic regression\n\nWe start with observations \\(Y_i \\in \\{0,1\\}\\) for \\(i = 1,\\ldots,n\\), where \\(Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\), \\(\\pi_i = P(Y_i = 1)\\).\nThe log-odds are modeled as \\(\\text{logit}(\\pi_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nTo complete the Bayesian model specification, we must place priors on \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\).\n\nAll priors we have discussed up-to-this point apply!\n\nHistorically, this was a difficult model to fit, but can be easily implemented in Stan."
  },
  {
    "objectID": "slides/11-classification.html#logistic-regression-in-stan",
    "href": "slides/11-classification.html#logistic-regression-in-stan",
    "title": "Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\n// Saved in logistic_regression.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n}\nmodel {\n  target += bernoulli_logit_lpmf(Y | alpha + X * beta);\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = bernoulli_logit_rng(alpha + height_c[i] * beta);\n    log_lik[i] = bernoulli_logit_lmpf(Y[i] | alpha + X[i, ] * beta);\n  }\n}"
  },
  {
    "objectID": "slides/11-classification.html#primary-biliary-cirrhosis-1",
    "href": "slides/11-classification.html#primary-biliary-cirrhosis-1",
    "title": "Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nhead(pbc)\n\n  id trt      age sex ascites hepato spiders edema bili chol albumin copper\n1  1   1 58.76523   f       1      1       1   1.0 14.5  261    2.60    156\n2  2   1 56.44627   f       0      1       1   0.0  1.1  302    4.14     54\n3  3   1 70.07255   m       0      0       0   0.5  1.4  176    3.48    210\n4  4   1 54.74059   f       0      1       1   0.5  1.8  244    2.54     64\n5  5   2 38.10541   f       0      1       1   0.0  3.4  279    3.53    143\n6  7   2 55.53457   f       0      1       0   0.0  1.0  322    4.09     52\n  alk.phos    ast trig platelet protime stage outcome\n1   1718.0 137.95  172      190    12.2     4       1\n2   7394.8 113.52   88      221    10.6     3       0\n3    516.0  96.10   55      151    12.0     4       1\n4   6121.8  60.63   92      183    10.3     4       1\n5    671.0 113.15   72      136    10.9     3       1\n6    824.0  60.45  213      204     9.7     3       0"
  },
  {
    "objectID": "slides/11-classification.html#logistic-regression-in-stan-1",
    "href": "slides/11-classification.html#logistic-regression-in-stan-1",
    "title": "Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\nlibrary(rstan)\ncompiled_model &lt;- stan_model(file = \"logistic_regression.stan\")\nfit &lt;- sampling(compiled_model, data = stan_data)\nprint(fit, pars = c(\"alpha\", \"beta\"), probs = c(0.025, 0.5, 0.975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n         mean se_mean   sd  2.5%   50% 97.5% n_eff Rhat\nalpha   -3.35    0.04 1.21 -6.13 -3.24 -1.41   791    1\nbeta[1]  2.24    0.04 1.32  0.18  2.05  5.48  1252    1\nbeta[2]  0.38    0.00 0.08  0.24  0.38  0.54  2005    1\nbeta[3]  1.71    0.04 1.25 -0.34  1.59  4.55   783    1\nbeta[4]  2.19    0.04 1.22  0.20  2.07  4.96   806    1\nbeta[5]  2.61    0.04 1.23  0.59  2.48  5.46   798    1\n\nSamples were drawn using NUTS(diag_e) at Mon Dec 30 14:36:11 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/11-classification.html#prepare-data-for",
    "href": "slides/11-classification.html#prepare-data-for",
    "title": "Classification",
    "section": "Prepare data for",
    "text": "Prepare data for"
  },
  {
    "objectID": "slides/11-classification.html#prepare-data-for-stan",
    "href": "slides/11-classification.html#prepare-data-for-stan",
    "title": "Classification",
    "section": "Prepare data for Stan",
    "text": "Prepare data for Stan\n\nX &lt;- model.matrix(object = ~ ascites + bili + as.factor(stage), data = pbc)[, -1]\nY &lt;- pbc$outcome\nstan_data &lt;- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X)\nhead(X)\n\n  ascites bili as.factor(stage)2 as.factor(stage)3 as.factor(stage)4\n1       1 14.5                 0                 0                 1\n2       0  1.1                 0                 1                 0\n3       0  1.4                 0                 0                 1\n4       0  1.8                 0                 0                 1\n5       0  3.4                 0                 1                 0\n6       0  1.0                 0                 1                 0"
  },
  {
    "objectID": "slides/11-classification.html#convergence-diagnostics",
    "href": "slides/11-classification.html#convergence-diagnostics",
    "title": "Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/11-classification.html#convergence-diagnostics-1",
    "href": "slides/11-classification.html#convergence-diagnostics-1",
    "title": "Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/11-classification.html#back-to-the-pbc-data",
    "href": "slides/11-classification.html#back-to-the-pbc-data",
    "title": "Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nFitting a logistic regression model, we obtain\n\n\n\n\n\n\n\nvariable\nmean\nsd\n2.5%\n97.5%\n\n\n\n\nalpha\nintercept\n-3.35\n0.04\n-6.13\n-1.41\n\n\nbeta[1]\nascites\n2.24\n0.04\n0.18\n5.48\n\n\nbeta[2]\nbilirubin\n0.38\n0.00\n0.24\n0.54\n\n\nbeta[3]\nstage == 2\n1.71\n0.04\n-0.34\n4.55\n\n\nbeta[4]\nstage == 3\n2.19\n0.04\n0.20\n4.96\n\n\nbeta[5]\nstage == 4\n2.61\n0.04\n0.59\n5.46\n\n\n\n\n\n\nHow might we interpret these coefficients as odds ratios?"
  },
  {
    "objectID": "slides/11-classification.html#back-to-the-pbc-data-1",
    "href": "slides/11-classification.html#back-to-the-pbc-data-1",
    "title": "Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nRemember, we are interested in the probability that a patient died during follow-up (a “success”). We are predicting the log-odds of this event happening.\n\nThe posterior mean for ascites was 2.24. Thus, the odds ratio for dying is \\(\\exp(2.24) \\approx 9.4\\). That is, patients with ascites have 9.4 times the odds of dying compared to patients that do not, holding all other variables constant.\nThe posterior mean for bilirubin was 0.38. Thus, the odds ratio for dying for a patient with 1 additional mg/dL serum bilirubin compared to another is \\(\\exp(0.38) \\approx 1.46\\), holding all other variables constant.\nThe baseline stage was 1. The posterior mean for stage 3 was 2.19. Thus, patients in stage 3 have approximately 8.93 times the odds of dying compared to patients that do not, holding all other variables constant."
  },
  {
    "objectID": "slides/11-classification.html#predicted-probabilities",
    "href": "slides/11-classification.html#predicted-probabilities",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nThere is a one-to-one relationship between \\(\\pi\\) and \\(\\text{logit}(\\pi)\\). So, if we predict \\(\\text{logit}(\\pi)\\), we can “back-transform” to get back to a predicted probability.\nFor instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\nx_i &lt;- matrix(c(0, 5, 1, 0, 0), ncol = 1)\npars &lt;- rstan::extract(fit, pars = c(\"alpha\", \"beta\"))\nlog_odds &lt;- pars$alpha + as.numeric(pars$beta %*% x_i)\npi &lt;- exp(log_odds) / (1 + exp(log_odds))"
  },
  {
    "objectID": "slides/11-classification.html#predicted-probabilities-1",
    "href": "slides/11-classification.html#predicted-probabilities-1",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior mean of the predicted probabilities is 0.56."
  },
  {
    "objectID": "slides/11-classification.html#posterior-predictive-checks",
    "href": "slides/11-classification.html#posterior-predictive-checks",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/11-classification.html#posterior-predictive-checks-1",
    "href": "slides/11-classification.html#posterior-predictive-checks-1",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_bars(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/11-classification.html#posterior-predictive-checks-2",
    "href": "slides/11-classification.html#posterior-predictive-checks-2",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_stat(stan_data$Y, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(stan_data$Y, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(stan_data$Y, y_pred, stat = \"q025\")\nppc_stat(stan_data$Y, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/11-classification.html#model-comparison",
    "href": "slides/11-classification.html#model-comparison",
    "title": "Classification",
    "section": "Model comparison",
    "text": "Model comparison\n\nComparing our model to a baseline that removed stage.\n\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_baseline &lt;- loo::extract_log_lik(fit_baseline, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model &lt;- loo::waic(log_lik)\nwaic_model_baseline &lt;- loo::waic(log_lik_baseline)\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"full\" = waic_model, \"baseline\" = waic_model_baseline))\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nfull        0.00      0.00 -155.94      9.51         7.69    2.09    311.88\nbaseline   -1.69      3.09 -157.63      9.36         4.86    1.98    315.25\n         se_waic\nfull       19.01\nbaseline   18.73"
  },
  {
    "objectID": "slides/11-classification.html#generalized-linear-models",
    "href": "slides/11-classification.html#generalized-linear-models",
    "title": "Classification",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nPreviously, we have focused on linear regression. Other forms of regression follow naturally from linear regression."
  },
  {
    "objectID": "slides/11-classification.html#steps-to-selecting-a-bayesian-glm",
    "href": "slides/11-classification.html#steps-to-selecting-a-bayesian-glm",
    "title": "Classification",
    "section": "Steps to selecting a Bayesian GLM",
    "text": "Steps to selecting a Bayesian GLM\n\nIdentify the support of the response distribution.\nSelect the likelihood by picking a parametric family of distributions with this support.\nChoose a link function \\(g\\) that transforms the range of parameters to the whole real line.\nSpecify a linear model on the transformed parameters.\nSelect priors for the regression coefficients."
  },
  {
    "objectID": "slides/11-classification.html#example-of-selecting-a-bayesian-glm",
    "href": "slides/11-classification.html#example-of-selecting-a-bayesian-glm",
    "title": "Classification",
    "section": "Example of selecting a Bayesian GLM",
    "text": "Example of selecting a Bayesian GLM\n\nSupport: \\(Y_i \\in \\{0, 1, 2, \\ldots\\}\\).\nLikelihood family: \\(Y_i \\stackrel{ind}{\\sim} \\text{Poisson}(\\lambda_i)\\).\nLink: \\(g(\\lambda_i) = \\log(\\lambda_i) \\in (−\\infty, \\infty)\\).\nRegression model: \\(\\log(\\lambda_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nPriors: \\(\\alpha, \\beta_j \\sim N(0, 10^2)\\)."
  },
  {
    "objectID": "slides/11-classification.html#multinomial-regression",
    "href": "slides/11-classification.html#multinomial-regression",
    "title": "Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nAssume an outcome \\(Y_i \\in \\{1,\\ldots,K\\}\\). You can imagine running \\(K\\) independent binary logistic regression models, in which one outcome is chosen as a “pivot” and then the other K − 1 outcomes are separately regressed against the pivot outcome. If outcome K (the last outcome) is chosen as the pivot, the K − 1 regression equations are:\n\\[\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}\\]\n\nIt can be seen that:\n\n\\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\] and \\(P(Y_i = K) = 1 - \\sum_{j=1}^{K-1}P(Y_i = j)\\).\n\nThis is known as the additive log-ratio model."
  },
  {
    "objectID": "slides/11-classification.html#multinomial-regression-1",
    "href": "slides/11-classification.html#multinomial-regression-1",
    "title": "Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nLet \\(Y_i \\in \\{1,\\ldots,K\\}\\) be an ordinal outcome with \\(K\\) categories. Then \\(P(Y \\leq k)\\) is the cumulative probability of \\(Y\\) less than or equal to a specific category \\(k=1,\\ldots,K-1\\). The odds of being less than or equal a particular category can be defined as, \\[\\frac{P(Y\\leq k)}{P(Y &gt; k)}\\] for \\(k=1,\\ldots,K-1\\), since \\(P(Y &gt; K) = 0\\) and dividing by zero is undefined. The log odds is also known as the logit, so that \\[\\log \\frac{P(Y\\leq k)}{P(Y &gt; k)} = \\text{logit}P(Y\\leq k) = \\alpha_k + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nProportional odds regression."
  },
  {
    "objectID": "slides/11-classification.html#ordinal-regression-using-stan",
    "href": "slides/11-classification.html#ordinal-regression-using-stan",
    "title": "Classification",
    "section": "Ordinal regression using Stan",
    "text": "Ordinal regression using Stan\n\ndata {\n  int&lt;lower = 2&gt; K;\n  int&lt;lower = 0&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1, upper = K&gt; Y[n]\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  ordered[K - 1] alpha;\n}\nmodel {\n  target += ordered_logistic_glm_lpmf(Y | X, beta, alpha);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(alpha | 0, 10);\n}"
  },
  {
    "objectID": "slides/10-robust.html#review-of-last-lecture",
    "href": "slides/10-robust.html#review-of-last-lecture",
    "title": "Robust Regression",
    "section": "",
    "text": "On Thursday, we started to branch out from linear regression.\nWe learned about approaches for nonlinear regression.\nToday we will address approaches for robust regression, which will generalize the assumption of homoskedasticity (and also the normality assumption)."
  },
  {
    "objectID": "slides/10-robust.html#a-motivating-research-question",
    "href": "slides/10-robust.html#a-motivating-research-question",
    "title": "Robust Regression",
    "section": "A motivating research question",
    "text": "A motivating research question\n\nIn today’s lecture, we will look at data on serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years.\n\nA detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994).\n\nFor an example patient, we define \\(Y_i\\) as the serum concentration value and \\(X_i\\) as a child’s age, given in years."
  },
  {
    "objectID": "slides/10-robust.html#pulling-the-data",
    "href": "slides/10-robust.html#pulling-the-data",
    "title": "Robust Regression",
    "section": "Pulling the data",
    "text": "Pulling the data\n\nlibrary(Brq)\ndata(\"ImmunogG\")\nhead(ImmunogG)\n\n  IgG Age\n1 1.5 0.5\n2 2.7 0.5\n3 1.9 0.5\n4 4.0 0.5\n5 1.9 0.5\n6 4.4 0.5"
  },
  {
    "objectID": "slides/10-robust.html#visualizing-igg-data",
    "href": "slides/10-robust.html#visualizing-igg-data",
    "title": "Robust Regression",
    "section": "Visualizing IgG data",
    "text": "Visualizing IgG data"
  },
  {
    "objectID": "slides/10-robust.html#disease-progression",
    "href": "slides/10-robust.html#disease-progression",
    "title": "Robust Regression",
    "section": "Disease progression",
    "text": "Disease progression\n\nOLS can be written as follows for \\(i = 1,\\ldots,n\\),\n\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2).\\]\n\n\\(\\beta_1\\) represent the the change in IgG serum concentration a one year increase in age.\nOften the following hypothesis is tested: \\[H_0: \\beta_1=0,H_1: \\beta_1 &lt; 0.\\]"
  },
  {
    "objectID": "slides/10-robust.html#ols-regression-assumptions",
    "href": "slides/10-robust.html#ols-regression-assumptions",
    "title": "Robust Regression",
    "section": "OLS regression assumptions",
    "text": "OLS regression assumptions\n\\[\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(Y_i\\) are independent observations (independence).\n\\(Y_i\\) is linearly related to \\(X_i\\) (linearity).\n\\(\\epsilon_i = Y_i - \\mu_i\\) is normally distributed (normality).\n\\(\\epsilon_i\\) has constant variance across \\(X_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/10-robust.html#assessing-assumptions",
    "href": "slides/10-robust.html#assessing-assumptions",
    "title": "Robust Regression",
    "section": "Assessing assumptions",
    "text": "Assessing assumptions"
  },
  {
    "objectID": "slides/10-robust.html#robust-regression",
    "href": "slides/10-robust.html#robust-regression",
    "title": "Robust Regression",
    "section": "Robust regression",
    "text": "Robust regression\n\nToday we will learn about regression techniques that are robust to the assumptions of linear regression.\nWe will introduce the idea of robust regression by exploring ways to generalize the homoskedastic variance assumption in linear regression.\nWe will touch on heteroskedasticity, heavy-tailed distributions, and median regression (more generally quantile regression)."
  },
  {
    "objectID": "slides/10-robust.html#heteroskedasticity",
    "href": "slides/10-robust.html#heteroskedasticity",
    "title": "Robust Regression",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nHeteroskedasticity is the violation of the assumption of constant variance.\nHow can we handle this?\nIn OLS, there are approaches like heteroskedastic consistent errors, but this is not a generative model.\nIn the Bayesian framework, we generally like to write down generative models."
  },
  {
    "objectID": "slides/10-robust.html#weighted-regression",
    "href": "slides/10-robust.html#weighted-regression",
    "title": "Robust Regression",
    "section": "Weighted regression",
    "text": "Weighted regression\n\nA common case is weighted regression, where each \\(Y_i\\) represents the mean of \\(n_i\\) observations. Then the scale of each observation is, \\[\\tau_i^2 = \\sigma^2/n_i,\\] where \\(\\sigma^2\\) is a global scale parameter.\nAlternatively, suppose each observation represents the sum of each \\(n_i\\) observations. Then the scale of each observation is, \\[\\tau_i^2 = n_i \\sigma^2.\\]"
  },
  {
    "objectID": "slides/10-robust.html#modeling-the-scale-with-covariates",
    "href": "slides/10-robust.html#modeling-the-scale-with-covariates",
    "title": "Robust Regression",
    "section": "Modeling the scale with covariates",
    "text": "Modeling the scale with covariates\n\nOne option is to allow the sale to be modeled as a function of covariates.\nIt is common to model the log-transformation of the scale or variance to transform it to \\(\\mathbb{R}\\),\n\n\\[\\log \\tau_i = \\mathbf{z}_i \\boldsymbol{\\gamma},\\]\nwhere \\(\\mathbf{z}_i = (z_{i1},\\ldots,z_{ip})\\) are a \\(p\\)-dimensional vector of covariates and \\(\\boldsymbol{\\gamma}\\) are parameters that regress the covariates onto the log standard deviation.\n\nOther options include: \\(\\log \\tau_i = \\mathbf{z}_i \\boldsymbol{\\gamma} + \\nu_i,\\quad \\nu_i \\sim N(0, \\sigma^2)\\)\nOther options include: \\(\\log \\tau_i = f(\\mu_i)\\)\nAny plausible generative model can be specified!"
  },
  {
    "objectID": "slides/10-robust.html#modeling-the-scale-with-covariates-1",
    "href": "slides/10-robust.html#modeling-the-scale-with-covariates-1",
    "title": "Robust Regression",
    "section": "Modeling the scale with covariates",
    "text": "Modeling the scale with covariates\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1&gt; q;\n  vector[n] Y;\n  matrix[n, p] X;\n  matrix[n, q] Z;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  vector[q] gamma;\n}\ntransformed parameters {\n  vector[n] tau = exp(Z * gamma);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n}"
  },
  {
    "objectID": "slides/10-robust.html#heteroskedastic-variance",
    "href": "slides/10-robust.html#heteroskedastic-variance",
    "title": "Robust Regression",
    "section": "Heteroskedastic variance",
    "text": "Heteroskedastic variance\n\nWe can write the regression model using a observation specific variance, \\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\tau_i^2).\\]\nOne way of writing the variance is: \\(\\tau_i^2 = \\sigma^2 \\lambda_i\\).\n\n\\(\\sigma^2\\) is a global scale parameter.\n\\(\\lambda_i\\) is an observation specific scale parameter.\n\nIn the Bayesian framework, we must place a prior on \\(\\lambda_i\\)."
  },
  {
    "objectID": "slides/10-robust.html#bayesian-prior-to-induce-structure",
    "href": "slides/10-robust.html#bayesian-prior-to-induce-structure",
    "title": "Robust Regression",
    "section": "Bayesian prior to induce structure",
    "text": "Bayesian prior to induce structure\n\nSuppse we would like \\(\\sum_{i=1}^n \\lambda_i = 1\\), \\(\\lambda_i &gt;0\\).\nWe could specify the following,\n\n\\[\\boldsymbol{\\lambda} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha}),\\]\nwhere \\(\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots,\\lambda_n)\\) and \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_n)\\).\n\nThe prior mean is \\(\\mathbb{E}[\\lambda_i] = \\alpha_i / \\alpha_0\\), where \\(\\alpha_0 = \\sum_{i=1}^n \\alpha_i.\\)\nTypically, \\(\\alpha_i = 1 \\forall i\\)."
  },
  {
    "objectID": "slides/10-robust.html#dirchlet-prior-in-stan",
    "href": "slides/10-robust.html#dirchlet-prior-in-stan",
    "title": "Robust Regression",
    "section": "Dirchlet prior in Stan",
    "text": "Dirchlet prior in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  vector&lt;lower = 0&gt;[n] alpha;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  simplex[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += dirichlet_lpdf(lambda | alpha);\n}"
  },
  {
    "objectID": "slides/10-robust.html#a-prior-to-induce-a-heavy-tail",
    "href": "slides/10-robust.html#a-prior-to-induce-a-heavy-tail",
    "title": "Robust Regression",
    "section": "A prior to induce a heavy-tail",
    "text": "A prior to induce a heavy-tail\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\\]"
  },
  {
    "objectID": "slides/10-robust.html#understanding-the-equivalence",
    "href": "slides/10-robust.html#understanding-the-equivalence",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\nHeteroskedastic variances assumption is equivalent to assuming a heavy-tailed distribution.\n\n\\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} t_{\\nu}\\left(0, \\sigma\\right).\\]\n\\[\\iff\\]\n\\[\\begin{aligned}\nY_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\nNote that since the number of \\(\\lambda_i\\) parameters is equal to the number of observations, this model will not have a proper posterior distribution without a proper prior distribution."
  },
  {
    "objectID": "slides/10-robust.html#understanding-the-equivalence-1",
    "href": "slides/10-robust.html#understanding-the-equivalence-1",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\\[\\begin{aligned}\nf(Y_i) &= \\int_0^{\\infty} f(Y_i , \\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} N(Y_i ; \\mu_i, \\sigma^2 \\lambda_i) \\text{Inverse-Gamma}\\left(\\lambda_i ; \\frac{\\nu}{2},\\frac{\\nu}{2}\\right) d\\lambda_i\\\\\n&= t_{\\nu}\\left(\\mu_i,\\sigma\\right).\n\\end{aligned}\\]\n\nThe marginal likelihood can be viewed as a mixture of a Gaussian likelihood with an Inverse-Gamma scale parameter."
  },
  {
    "objectID": "slides/10-robust.html#understanding-the-equivalence-2",
    "href": "slides/10-robust.html#understanding-the-equivalence-2",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\nA random variable \\(T_i \\stackrel{iid}{\\sim} t_{\\nu}\\) can be written as a function of Gaussian and \\(\\chi^2\\) random variables, \\[\\begin{aligned}\nT_i &= \\frac{Z_i}{\\sqrt{\\frac{W_i}{\\nu}}},\\quad Z_i \\stackrel{iid}{\\sim} N(0,1), \\quad W_i \\stackrel{iid}{\\sim}\\chi^2_{\\nu}\\\\\n&= \\frac{Z_i}{\\sqrt{\\frac{1}{\\nu V_i}}},\\quad V_i \\stackrel{iid}{\\sim} \\text{Inv-}\\chi^2_{\\nu},\\quad V_i=W_i^{-1}\\\\\n&= \\sqrt{\\nu V_i} Z_i,\\quad \\lambda_i = \\nu V_i\\\\\n&= \\sqrt{\\lambda_i} Z_i, \\quad \\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\n\\end{aligned}\\]\n\nWe then have: \\(Y_i = \\mu_i + \\sigma T_i \\sim t_{\\nu}(\\mu_i, \\sigma).\\)"
  },
  {
    "objectID": "slides/10-robust.html#student-t-in-stan",
    "href": "slides/10-robust.html#student-t-in-stan",
    "title": "Robust Regression",
    "section": "Student-t in Stan",
    "text": "Student-t in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; nu;\n}\nmodel {\n  target += student_t_lpdf(Y | nu, alpha + X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/10-robust.html#student-t-in-stan-mixture",
    "href": "slides/10-robust.html#student-t-in-stan-mixture",
    "title": "Robust Regression",
    "section": "Student-t in Stan: mixture",
    "text": "Student-t in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n  real&lt;lower = 0&gt; nu;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n  target += inv_gamma_lpdf(lambda | 0.5 * nu, 0.5 * nu);\n}"
  },
  {
    "objectID": "slides/10-robust.html#why-heavy-tailed-distributions",
    "href": "slides/10-robust.html#why-heavy-tailed-distributions",
    "title": "Robust Regression",
    "section": "Why heavy-tailed distributions?",
    "text": "Why heavy-tailed distributions?\n\nReplacing the normal distribution with a distribution with heavy-tails (e.g., Student-t, Laplace) is a common approach to robust regression.\nRobust regression refers to regression methods which are less sensitive to outliers or small sample sizes.\nLinear regression, including Bayesian regression with normally distributed errors is sensitive to outliers, because the normal distribution has narrow tail probabilities.\nOur heteroskedastic model that we just explored is only one example of a robust regression model."
  },
  {
    "objectID": "slides/10-robust.html#vizualizing-heavy-tail-distributions",
    "href": "slides/10-robust.html#vizualizing-heavy-tail-distributions",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/10-robust.html#vizualizing-heavy-tail-distributions-1",
    "href": "slides/10-robust.html#vizualizing-heavy-tail-distributions-1",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/10-robust.html#another-example-of-robust-regression",
    "href": "slides/10-robust.html#another-example-of-robust-regression",
    "title": "Robust Regression",
    "section": "Another example of robust regression",
    "text": "Another example of robust regression\n\nLet’s revisit our general heteroskedastic regression, \\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\sigma^2 \\lambda_i).\\]\nWe can induce another form of robust regression using the following prior for \\(\\lambda_i\\), \\(\\lambda_i \\sim \\text{Exponential}(1/2)\\)."
  },
  {
    "objectID": "slides/10-robust.html#median-regression-using-laplace",
    "href": "slides/10-robust.html#median-regression-using-laplace",
    "title": "Robust Regression",
    "section": "Median regression using Laplace",
    "text": "Median regression using Laplace\nLeast absolute deviation (LAD) regression minimizes the following objective function,\n\\[\\hat{{\\alpha}}_{\\text{LAD}},\\hat{\\boldsymbol{\\beta}}_{\\text{LAD}} = \\arg \\min_{\\alpha,\\boldsymbol{\\beta}} \\sum_{i=1}^n |Y_i - \\mu_i|, \\quad \\mu_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta}.\\]\nThe Bayesian analog is the Laplace distribution,\n\\[f(\\mathbf{Y} | \\alpha, \\boldsymbol{\\beta}, \\sigma) = \\left(\\frac{1}{2\\sigma}\\right)^n \\exp\\left\\{-\\sum_{i=1}^n\\frac{|Y_i - \\mu_i|}{\\sigma}\\right\\}.\\]"
  },
  {
    "objectID": "slides/10-robust.html#median-regression-using-laplace-1",
    "href": "slides/10-robust.html#median-regression-using-laplace-1",
    "title": "Robust Regression",
    "section": "Median regression using Laplace",
    "text": "Median regression using Laplace\n\nThe Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is \\(|x−\\mu|\\), so minimizing the likelihood will also minimize the least absolute distances.\nLaplace distribution is also known as the double-exponential distribution (symmetric exponential distributions around \\(\\mu\\) with scale \\(\\sigma\\)).\nThus, a linear regression with Laplace errors is analogous to a median regression,\nWhy is median regression considered more robust than regression of the mean?"
  },
  {
    "objectID": "slides/10-robust.html#laplace-regression-in-stan",
    "href": "slides/10-robust.html#laplace-regression-in-stan",
    "title": "Robust Regression",
    "section": "Laplace regression in Stan",
    "text": "Laplace regression in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += double_exponential_lpdf(Y | alpha + X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/10-robust.html#laplace-regression-in-stan-mixture",
    "href": "slides/10-robust.html#laplace-regression-in-stan-mixture",
    "title": "Robust Regression",
    "section": "Laplace regression in Stan: mixture",
    "text": "Laplace regression in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n  target += exponential_lpdf(lambda | 0.5);\n}"
  },
  {
    "objectID": "slides/10-robust.html#asymmetric-laplace-distribution",
    "href": "slides/10-robust.html#asymmetric-laplace-distribution",
    "title": "Robust Regression",
    "section": "Asymmetric Laplace distribution",
    "text": "Asymmetric Laplace distribution\nA random variable, \\(Y \\sim ALD_p(\\mu,\\sigma)\\) is said to follow an asymmetric Laplace distribution (ALD) if the pdf is given by,\n\\[f(Y) = \\frac{p(1-p)}{\\sigma} \\exp \\left\\{-\\rho_p\\left(\\frac{Y - \\mu}{\\sigma}\\right)\\right\\},\\]\nwhere \\(p \\in (0,1)\\) is the percentile and \\[\\rho_p(x) = x\\left(p - 1(u &lt; 0)\\right) = \\frac{|x| + (2p - 1)x}{2}.\\]\n\nWhen \\(p = 0.5\\) it reduces to a regular Laplace distribution."
  },
  {
    "objectID": "slides/10-robust.html#general-quantile-regression",
    "href": "slides/10-robust.html#general-quantile-regression",
    "title": "Robust Regression",
    "section": "General quantile regression",
    "text": "General quantile regression\n\nfunctions {\n  real asym_laplace_lpdf(real y, real mu, real sigma, real tau) {\n    return log(tau) + log1m(tau)\n      - log(sigma)\n      - 2 * ((y &lt; mu) ? (1 - tau) * (mu - y) : tau * (y - mu)) / sigma;\n  }\n}\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0, upper = 1&gt; tau;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  for (i in 1:n) target += asym_laplace_lpdf(Y[i] | alpha + X[i, ] * beta, sigma, tau);\n}"
  },
  {
    "objectID": "slides/10-robust.html#quantile-regression",
    "href": "slides/10-robust.html#quantile-regression",
    "title": "Robust Regression",
    "section": "Quantile regression",
    "text": "Quantile regression"
  },
  {
    "objectID": "slides/10-robust.html#posterior-of-beta_1",
    "href": "slides/10-robust.html#posterior-of-beta_1",
    "title": "Robust Regression",
    "section": "Posterior of \\(\\beta_1\\)",
    "text": "Posterior of \\(\\beta_1\\)\n\n\n\n\n\nquantile\nmean\nlower\nupper\n\n\n\n\n0.025\n0.3291804\n0.2639468\n0.3899443\n\n\n0.250\n0.5130600\n0.3788182\n0.6523600\n\n\n0.500\n0.7268102\n0.5705129\n0.8836400\n\n\n0.750\n0.8093408\n0.6919899\n0.9363650\n\n\n0.975\n1.1741866\n0.9783041\n1.3936860"
  },
  {
    "objectID": "slides/10-robust.html#scale-mixture-representation",
    "href": "slides/10-robust.html#scale-mixture-representation",
    "title": "Robust Regression",
    "section": "Scale-mixture representation",
    "text": "Scale-mixture representation\nThe above may also be written as a mixture of exponential and normal distributions. Letting, \\(Z_i \\sim Exponential(1)\\) and \\(\\sigma \\sim Exponential(1)\\).\n\\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\sigma \\theta Z_i + \\epsilon_i,\\quad \\epsilon_i \\sim N\\left(0, \\tau^2 \\sigma^2 Z_i\\right),\\]\nwhere \\[\\theta = \\frac{1-2p}{p(1-p)},\\quad\\tau = \\sqrt{\\frac{2}{p(1-p)}}.\\]"
  },
  {
    "objectID": "slides/10-robust.html#scale-mixture-in-stan",
    "href": "slides/10-robust.html#scale-mixture-in-stan",
    "title": "Robust Regression",
    "section": "Scale-mixture in Stan",
    "text": "Scale-mixture in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0, upper = 1&gt; q;\n}\ntransformed data {\n  real theta = (1 - 2 * q) / (q * (1 - q));\n  real tau = sqrt(2 / (q * (1 - q)));\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector&lt;lower=0&gt;[n] z;\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta + sigma * theta * z, tau * sqrt(z) * sigma);\n  target += exponential_lpdf(sigma | 1);\n  target += exponential_lpdf(z | 1);\n}"
  },
  {
    "objectID": "slides/10-robust.html#prepare-for-next-class",
    "href": "slides/10-robust.html#prepare-for-next-class",
    "title": "Robust Regression",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02, which is due before next class.\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Regularization"
  },
  {
    "objectID": "slides/12-classification.html#review-of-last-lecture",
    "href": "slides/12-classification.html#review-of-last-lecture",
    "title": "Classification",
    "section": "",
    "text": "Last week, we learned about Bayesian approaches to robust regression and regularization.\n\nGlobal-local shrinkage priors.\n\nThis week, we will focus on classification models.\n\nToday: Binary classification (logistic regression).\nThursday: Multiclass classification (multinomial, ordinal regression)."
  },
  {
    "objectID": "slides/12-classification.html#generalized-linear-models",
    "href": "slides/12-classification.html#generalized-linear-models",
    "title": "Classification",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nPreviously, we have focused on linear regression. Other forms of regression follow naturally from linear regression."
  },
  {
    "objectID": "slides/12-classification.html#models-for-binary-outcomes",
    "href": "slides/12-classification.html#models-for-binary-outcomes",
    "title": "Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nBernoulli random variable: Used for binary outcomes (success/failure), e.g., whether a patient responds to a treatment (yes/no).\nBinomial random variable: Used when there are multiple trials (e.g., 10 patients), and you want to model the number of successes (e.g., how many out of 10 patients experience a treatment response)."
  },
  {
    "objectID": "slides/12-classification.html#models-for-binary-outcomes-1",
    "href": "slides/12-classification.html#models-for-binary-outcomes-1",
    "title": "Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nSuppose \\(Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\) for \\(i = 1,\\ldots,n\\). The pmf is,\n\n\\[f(Y_i) = P(Y_i = y) = \\pi_i^y (1 - \\pi_i)^{1 - y}, \\quad y \\in\\{0,1\\}.\\]\n\nWe only need to specify \\(\\pi_i = P(Y_i = 1)\\).\nOne strategy might be to simply fit a linear regression model,\n\n\\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\quad\\epsilon_i \\sim N(0, \\sigma^2).\\]\n\nWe can set \\(P(Y_i = 1) = \\hat{Y}_i\\)."
  },
  {
    "objectID": "slides/12-classification.html#primary-biliary-cirrhosis",
    "href": "slides/12-classification.html#primary-biliary-cirrhosis",
    "title": "Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nThe Mayo Clinic conducted a trial for primary biliary cirrhosis, comparing the drug D-penicillamine vs. placebo. Patients were followed for a specified duration, and their status at the end of follow-up (whether they died) was recorded.\nResearchers are interested in predicting whether a patient died based on the following variables:\n\nascites: whether the patient had ascites (1 = yes, 0 = no)\nbilirubin: serum bilirubin in mg/dL\nstage: histologic stage of disease (ordinal categorical variable with stages 1, 2, 3, and 4)"
  },
  {
    "objectID": "slides/12-classification.html#what-can-go-wrong",
    "href": "slides/12-classification.html#what-can-go-wrong",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nSuppose we fit the following model:\n\n\\[\\begin{aligned}\nY_i &= \\alpha + \\beta_1(ascites)_i + \\beta_2(bilirubin)_i\\\\\n&\\quad+\\beta_3(stage = 2)_i + \\beta_4(stage = 3)_i\\\\\n&\\quad+\\beta_5(stage = 4)_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\n\\end{aligned}\\]\n. . .\nWhat can go wrong?"
  },
  {
    "objectID": "slides/12-classification.html#what-can-go-wrong-1",
    "href": "slides/12-classification.html#what-can-go-wrong-1",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "slides/12-classification.html#what-can-go-wrong-2",
    "href": "slides/12-classification.html#what-can-go-wrong-2",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nAdditionally, as a probability, \\(P(Y_i = 1)\\) must be in the interval [0, 1], but there is nothing in the model that enforces this constraint, so that you could be estimating probabilities that are negative or that are greater than 1!"
  },
  {
    "objectID": "slides/12-classification.html#from-probabilities-to-log-odds",
    "href": "slides/12-classification.html#from-probabilities-to-log-odds",
    "title": "Classification",
    "section": "From probabilities to log-odds",
    "text": "From probabilities to log-odds\n\nSuppose the probability of an event is \\(\\pi\\).\nThen the odds that the event occurs is \\(\\frac{\\pi}{1 - \\pi}\\).\nTaking the (natural) log of the odds, we have the logit of \\(\\pi\\): the log-odds:\n\n\\[\\text{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).\\]\n\nNote that although \\(\\pi\\) is constrained to lie between 0 and 1, the logit of \\(\\pi\\) is unconstrained - it can be anything from \\(-\\infty\\) to \\(\\infty\\)."
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression-model",
    "href": "slides/12-classification.html#logistic-regression-model",
    "title": "Classification",
    "section": "Logistic regression model",
    "text": "Logistic regression model\n\nLet’s create a model for the logit of \\(\\pi\\): \\(\\text{logit}(\\pi_i)= \\eta_i\\), where \\(\\eta_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\\)\nThis is a linear model for a transformation of the outcome of interest, and is also equivalent to,\n\n\\[\\pi_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\text{expit}(\\eta_i).\\]\n\nThe expression on the right is called a logistic function and cannot yield a value that is negative or a value that is &gt;1. Fitting a model of this form is known as logistic regression."
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression",
    "href": "slides/12-classification.html#logistic-regression",
    "title": "Classification",
    "section": "Logistic regression",
    "text": "Logistic regression\n\\[\\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\eta_i\\]\n\nNegative logits represent probabilities less than one-half.\n\n\\(\\eta_i &lt; 0 \\implies \\pi_i &lt; 0.5\\).\n\nPositive logits represent probabilities greater than one-half.\n\n\\(\\eta_i &gt; 0 \\implies \\pi_i &gt; 0.5\\)."
  },
  {
    "objectID": "slides/12-classification.html#interpreting-parameters-in-logistic-regression",
    "href": "slides/12-classification.html#interpreting-parameters-in-logistic-regression",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\nTypically we interpret functions of parameters in logistic regression rather than the parameters themselves.\nFor the simple model: \\(\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\beta X_{i},\\) we note that the probability that \\(Y_i = 1\\) when \\(X_i = 0\\) is\n\\[P(Y_i = 1 | X_{i} = 0) = \\frac{\\exp(\\alpha)}{1 + \\exp(\\alpha)}.\\]"
  },
  {
    "objectID": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-1",
    "href": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-1",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nSuppose that \\(X\\) is a binary (0/1) variable (e.g., \\(X = 1\\) for males and 0 for non-males).\n\nIn this case, we interpret \\(\\exp(\\beta)\\) as the odds ratio (OR) of the response for the two possible levels of \\(X\\).\nFor \\(X\\) on other scales, \\(\\exp(\\beta)\\) is interpreted as the odds ratio of the response comparing two values of \\(X\\) one unit apart.\n\nWhy?"
  },
  {
    "objectID": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-2",
    "href": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-2",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nThe log odds of response for \\(X = 1\\) is given by \\(\\alpha + \\beta\\), and the log odds of response for \\(X = 0\\) is \\(\\alpha\\).\nSo the odds ratio of response comparing \\(X = 1\\) to \\(X = 0\\) is given by \\(\\frac{\\exp(\\alpha + \\beta)}{\\exp(\\alpha)} = \\exp(\\beta)\\).\nIn a multivariable logistic regression model with more than one predictor, this OR is interpreted conditionally on values of other variables (i.e., controlling for them)."
  },
  {
    "objectID": "slides/12-classification.html#bayesian-logistic-regression",
    "href": "slides/12-classification.html#bayesian-logistic-regression",
    "title": "Classification",
    "section": "Bayesian logistic regression",
    "text": "Bayesian logistic regression\n\nWe start with observations \\(Y_i \\in \\{0,1\\}\\) for \\(i = 1,\\ldots,n\\), where \\(Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\), \\(\\pi_i = P(Y_i = 1)\\).\nThe log-odds are modeled as \\(\\text{logit}(\\pi_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} = \\eta_i\\).\nTo complete the Bayesian model specification, we must place priors on \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\).\n\nAll priors we have discussed up-to-this point apply!\n\nHistorically, this was a difficult model to fit, but can be easily implemented in Stan."
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression-in-stan",
    "href": "slides/12-classification.html#logistic-regression-in-stan",
    "title": "Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\n// Saved in logistic_regression.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int Y[n];                              // Y is now type int\n  matrix[n, p] X;\n}\ntransformed data {\n  matrix[n, p] X_centered;               // We are only centering X!\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n}\nmodel {\n  target += bernoulli_logit_lpmf(Y | alpha + X_centered * beta); // bernoulli likelihood parameterized in logits\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n}\ngenerated quantities {\n  real pi_average = exp(alpha) / (1 + exp(alpha));\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = bernoulli_logit_rng(alpha + X_centered[i, ] * beta);\n    log_lik[i] = bernoulli_logit_lpmf(Y[i] | alpha + X_centered[i, ] * beta);\n  }\n}\n\nbernoulli_logit_lpmf"
  },
  {
    "objectID": "slides/12-classification.html#primary-biliary-cirrhosis-1",
    "href": "slides/12-classification.html#primary-biliary-cirrhosis-1",
    "title": "Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nhead(pbc)\n\n  id trt      age sex ascites hepato spiders edema bili chol albumin copper\n1  1   1 58.76523   f       1      1       1   1.0 14.5  261    2.60    156\n2  2   1 56.44627   f       0      1       1   0.0  1.1  302    4.14     54\n3  3   1 70.07255   m       0      0       0   0.5  1.4  176    3.48    210\n4  4   1 54.74059   f       0      1       1   0.5  1.8  244    2.54     64\n5  5   2 38.10541   f       0      1       1   0.0  3.4  279    3.53    143\n6  7   2 55.53457   f       0      1       0   0.0  1.0  322    4.09     52\n  alk.phos    ast trig platelet protime stage outcome\n1   1718.0 137.95  172      190    12.2     4       1\n2   7394.8 113.52   88      221    10.6     3       0\n3    516.0  96.10   55      151    12.0     4       1\n4   6121.8  60.63   92      183    10.3     4       1\n5    671.0 113.15   72      136    10.9     3       1\n6    824.0  60.45  213      204     9.7     3       0"
  },
  {
    "objectID": "slides/12-classification.html#prepare-data-for-stan",
    "href": "slides/12-classification.html#prepare-data-for-stan",
    "title": "Classification",
    "section": "Prepare data for Stan",
    "text": "Prepare data for Stan\n\nX &lt;- model.matrix(object = ~ ascites + bili + as.factor(stage), data = pbc)[, -1]\nY &lt;- pbc$outcome\nstan_data &lt;- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X)\nhead(X)\n\n  ascites bili as.factor(stage)2 as.factor(stage)3 as.factor(stage)4\n1       1 14.5                 0                 0                 1\n2       0  1.1                 0                 1                 0\n3       0  1.4                 0                 0                 1\n4       0  1.8                 0                 0                 1\n5       0  3.4                 0                 1                 0\n6       0  1.0                 0                 1                 0"
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression-in-stan-1",
    "href": "slides/12-classification.html#logistic-regression-in-stan-1",
    "title": "Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\nlibrary(rstan)\ncompiled_model &lt;- stan_model(file = \"logistic_regression.stan\")\nfit &lt;- sampling(compiled_model, data = stan_data)\nprint(fit, pars = c(\"alpha\", \"beta\", \"pi_average\"), probs = c(0.025, 0.5, 0.975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n           mean se_mean   sd  2.5%  50% 97.5% n_eff Rhat\nalpha      0.21    0.00 0.19 -0.14 0.20  0.60  1819 1.00\nbeta[1]    2.24    0.03 1.32  0.12 2.12  5.30  2328 1.00\nbeta[2]    0.38    0.00 0.08  0.24 0.38  0.55  2027 1.00\nbeta[3]    1.79    0.04 1.31 -0.43 1.65  4.86  1097 1.01\nbeta[4]    2.26    0.04 1.30  0.10 2.14  5.27  1073 1.01\nbeta[5]    2.69    0.04 1.31  0.52 2.54  5.74  1069 1.01\npi_average 0.55    0.00 0.05  0.47 0.55  0.65  1826 1.00\n\nSamples were drawn using NUTS(diag_e) at Mon Feb 17 09:17:32 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/12-classification.html#convergence-diagnostics",
    "href": "slides/12-classification.html#convergence-diagnostics",
    "title": "Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/12-classification.html#convergence-diagnostics-1",
    "href": "slides/12-classification.html#convergence-diagnostics-1",
    "title": "Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/12-classification.html#back-to-the-pbc-data",
    "href": "slides/12-classification.html#back-to-the-pbc-data",
    "title": "Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nFitting a logistic regression model, we obtain\n\n\n\n\n\n\n\nvariable\nmean\nsd\n2.5%\n97.5%\n\n\n\n\nalpha\nintercept\n0.21\n0.00\n-0.14\n0.60\n\n\nbeta[1]\nascites\n2.24\n0.03\n0.12\n5.30\n\n\nbeta[2]\nbilirubin\n0.38\n0.00\n0.24\n0.55\n\n\nbeta[3]\nstage == 2\n1.79\n0.04\n-0.43\n4.86\n\n\nbeta[4]\nstage == 3\n2.26\n0.04\n0.10\n5.27\n\n\nbeta[5]\nstage == 4\n2.69\n0.04\n0.52\n5.74\n\n\n\n\n\n\nHow might we interpret these coefficients as odds ratios?"
  },
  {
    "objectID": "slides/12-classification.html#back-to-the-pbc-data-1",
    "href": "slides/12-classification.html#back-to-the-pbc-data-1",
    "title": "Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nRemember, we are interested in the probability that a patient died during follow-up (a “success”). We are predicting the log-odds of this event happening.\n\nThe posterior mean for ascites was 2.24. Thus, the odds ratio for dying is \\(\\exp(2.24) \\approx 9.40\\). That is, patients with ascites have 9 times the odds of dying compared to patients that do not, holding all other variables constant.\nThe posterior mean for bilirubin was 0.38. Thus, the odds ratio for dying for a patient with 1 additional mg/dL serum bilirubin compared to another is \\(\\exp(0.38) \\approx 1.46\\), holding all other variables constant.\nThe baseline stage was 1. The posterior mean for stage 3 was 2.26. Thus, patients in stage 3 have approximately 9.58 times the odds of dying compared to patients that do not, holding all other variables constant."
  },
  {
    "objectID": "slides/12-classification.html#predicted-probabilities",
    "href": "slides/12-classification.html#predicted-probabilities",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nThere is a one-to-one relationship between \\(\\pi\\) and \\(\\text{logit}(\\pi)\\). So, if we predict \\(\\text{logit}(\\pi)\\), we can “back-transform” to get back to a predicted probability.\n\n\n// stored in logistic_regression_new.stan\ndata {\n  row_vector[p] X_new;\n}\ngenerated quantities {\n  real eta_new = (alpha + (X_new - X_bar) * beta);\n  real pi_new = inv_logit(eta_new); // expit function\n  real Y_new = bernoulli_logit_rng(eta_new); // posterior predictive distribution\n}"
  },
  {
    "objectID": "slides/12-classification.html#predicted-probabilities-1",
    "href": "slides/12-classification.html#predicted-probabilities-1",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nFor instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\ncompiled_model &lt;- stan_model(file = \"logistic_regression_new.stan\")\nstan_data &lt;- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X,\n                  X_new = c(0, 5, 1, 0, 0))\nfit &lt;- sampling(compiled_model, data = stan_data)"
  },
  {
    "objectID": "slides/12-classification.html#posterior-predictive-checks",
    "href": "slides/12-classification.html#posterior-predictive-checks",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\ny_pred &lt;- rstan::extract(fit, pars = \"Y_pred\")$Y_pred\nppc_dens_overlay(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/12-classification.html#posterior-predictive-checks-1",
    "href": "slides/12-classification.html#posterior-predictive-checks-1",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_bars(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/12-classification.html#posterior-predictive-checks-2",
    "href": "slides/12-classification.html#posterior-predictive-checks-2",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_stat(stan_data$Y, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(stan_data$Y, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(stan_data$Y, y_pred, stat = \"q025\")\nppc_stat(stan_data$Y, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/12-classification.html#model-comparison",
    "href": "slides/12-classification.html#model-comparison",
    "title": "Classification",
    "section": "Model comparison",
    "text": "Model comparison\n\nComparing our model to a baseline that removed stage.\n\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_baseline &lt;- loo::extract_log_lik(fit_baseline, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model &lt;- loo::waic(log_lik)\nwaic_model_baseline &lt;- loo::waic(log_lik_baseline)\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"full\" = waic_model, \"baseline\" = waic_model_baseline))\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nfull        0.00      0.00 -156.08      9.60         7.83    2.20    312.17\nbaseline   -1.38      3.15 -157.46      9.29         4.68    1.82    314.92\n         se_waic\nfull       19.21\nbaseline   18.58"
  },
  {
    "objectID": "slides/12-classification.html#other-models-for-binary-data",
    "href": "slides/12-classification.html#other-models-for-binary-data",
    "title": "Classification",
    "section": "Other models for binary data",
    "text": "Other models for binary data\nAn alternative approach is Probit regression, where we use the CDF of the standard normal distribution instead of the logit link: \\(\\Phi^{-1}(\\pi) = \\alpha + \\beta X\\)\nWhere \\(\\Phi^{-1}\\) is the inverse normal CDF (also called the probit link function).\n\ndata {\n  int&lt;lower = 1&gt; n;               // number of observations\n  int&lt;lower = 1&gt; p;               // number of predictors\n  int&lt;lower = 0, upper = 1&gt; Y[n]; // binary outcome (0 or 1)\n  matrix[n, p] X;                 // design matrix (predictors)\n}\nparameters {\n  real alpha;               // intercept\n  vector[p] beta;           // coefficients\n}\n\nmodel {\n  target += bernoulli_lpmf(Phi(alpha + X * beta)); // Probit model\n}"
  },
  {
    "objectID": "slides/12-classification.html#steps-to-selecting-a-bayesian-glm",
    "href": "slides/12-classification.html#steps-to-selecting-a-bayesian-glm",
    "title": "Classification",
    "section": "Steps to selecting a Bayesian GLM",
    "text": "Steps to selecting a Bayesian GLM\n\nIdentify the support of the response distribution.\nSelect the likelihood by picking a parametric family of distributions with this support.\nChoose a link function \\(g\\) that transforms the range of parameters to the whole real line.\nSpecify a linear model on the transformed parameters.\nSelect priors for the regression coefficients."
  },
  {
    "objectID": "slides/12-classification.html#example-of-selecting-a-bayesian-glm",
    "href": "slides/12-classification.html#example-of-selecting-a-bayesian-glm",
    "title": "Classification",
    "section": "Example of selecting a Bayesian GLM",
    "text": "Example of selecting a Bayesian GLM\n\nSupport: \\(Y_i \\in \\{0, 1, 2, \\ldots\\}\\).\nLikelihood family: \\(Y_i \\stackrel{ind}{\\sim} \\text{Poisson}(\\lambda_i)\\).\nLink: \\(g(\\lambda_i) = \\log(\\lambda_i) \\in (−\\infty, \\infty)\\).\nRegression model: \\(\\log(\\lambda_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nPriors: \\(\\alpha, \\beta_j \\sim N(0, 10^2)\\)."
  },
  {
    "objectID": "slides/12-classification.html#multinomial-regression",
    "href": "slides/12-classification.html#multinomial-regression",
    "title": "Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nAssume an outcome \\(Y_i \\in \\{1,\\ldots,K\\}\\). You can imagine running \\(K\\) independent binary logistic regression models, in which one outcome is chosen as a “pivot” and then the other K − 1 outcomes are separately regressed against the pivot outcome. If outcome K (the last outcome) is chosen as the pivot, the K − 1 regression equations are:\n\\[\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}\\]\n\nIt can be seen that:\n\n\\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\] and \\(P(Y_i = K) = 1 - \\sum_{j=1}^{K-1}P(Y_i = j)\\).\n\nThis is known as the additive log-ratio model."
  },
  {
    "objectID": "slides/12-classification.html#multinomial-regression-1",
    "href": "slides/12-classification.html#multinomial-regression-1",
    "title": "Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nLet \\(Y_i \\in \\{1,\\ldots,K\\}\\) be an ordinal outcome with \\(K\\) categories. Then \\(P(Y \\leq k)\\) is the cumulative probability of \\(Y\\) less than or equal to a specific category \\(k=1,\\ldots,K-1\\). The odds of being less than or equal a particular category can be defined as, \\[\\frac{P(Y\\leq k)}{P(Y &gt; k)}\\] for \\(k=1,\\ldots,K-1\\), since \\(P(Y &gt; K) = 0\\) and dividing by zero is undefined. The log odds is also known as the logit, so that \\[\\log \\frac{P(Y\\leq k)}{P(Y &gt; k)} = \\text{logit}P(Y\\leq k) = \\alpha_k + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nProportional odds regression."
  },
  {
    "objectID": "slides/12-classification.html#ordinal-regression-using-stan",
    "href": "slides/12-classification.html#ordinal-regression-using-stan",
    "title": "Classification",
    "section": "Ordinal regression using Stan",
    "text": "Ordinal regression using Stan\n\ndata {\n  int&lt;lower = 2&gt; K;\n  int&lt;lower = 0&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1, upper = K&gt; Y[n]\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  ordered[K - 1] alpha;\n}\nmodel {\n  target += ordered_logistic_lpmf(Y | X, beta, alpha);\n  target += ordered_logistic_glm_lpmf(Y | X * beta, alpha);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(alpha | 0, 10);\n}"
  },
  {
    "objectID": "slides/12-classification.html#prepare-for-next-class",
    "href": "slides/12-classification.html#prepare-for-next-class",
    "title": "Classification",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03.\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Multiclass classification"
  },
  {
    "objectID": "slides/09-nonlinear.html#a-motivating-example",
    "href": "slides/09-nonlinear.html#a-motivating-example",
    "title": "Nonlinear Regression",
    "section": "A motivating example",
    "text": "A motivating example\n\nPatients with glaucoma are follow…"
  },
  {
    "objectID": "slides/09-nonlinear.html#a-motivating-example-1",
    "href": "slides/09-nonlinear.html#a-motivating-example-1",
    "title": "Nonlinear Regression",
    "section": "A motivating example",
    "text": "A motivating example"
  },
  {
    "objectID": "slides/09-nonlinear.html#an-example-patient",
    "href": "slides/09-nonlinear.html#an-example-patient",
    "title": "Nonlinear Regression",
    "section": "An example patient",
    "text": "An example patient"
  },
  {
    "objectID": "slides/09-nonlinear.html#polynomials",
    "href": "slides/09-nonlinear.html#polynomials",
    "title": "Nonlinear Regression",
    "section": "Polynomials",
    "text": "Polynomials\n\nModel for the mean process becomes nonlinear:\n\n\\[\\mu_i = \\alpha + \\beta_1 X_i + \\cdots + \\beta_J X_i^J\\]\n\n\\(X_i\\) is years from baseline visit.\n\\(J\\) is chosen depending on the degree of non-linearity.\nWhen fitting non-linear regression in Bayesian context it is useful to standardize the data."
  },
  {
    "objectID": "slides/09-nonlinear.html#splines",
    "href": "slides/09-nonlinear.html#splines",
    "title": "Nonlinear Regression",
    "section": "Splines",
    "text": "Splines\n\nOur goal is to approximate the blossom trend with a wiggly function. With B-splines, just like with polynomial regression, we do this by generating new predictor variables and using those in the linear model, \\(\\mu_i\\).\nUnlike polynomial regression, B-splines do not directly transform the predictor by squaring or cubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of the synthetic variables is called a basis function. The linear model ends up looking very familiar:\n\n\\[\\mu_i = \\alpha + \\beta_1 B_{i,1} + \\beta_2 B_{i,2} + \\cdots \\] where \\(B_{i,n}\\) is the \\(n\\)-th basis function’s value on row \\(i\\), and the \\(\\beta\\) parameters are corresponding weights for each. The parameters act like slopes, adjusting the influence of each basis function on the mean \\(\\mu_i\\).\n\nSo really this is just another linear regression, but with some fancy, synthetic predictor variables."
  },
  {
    "objectID": "slides/09-nonlinear.html#change-points",
    "href": "slides/09-nonlinear.html#change-points",
    "title": "Nonlinear Regression",
    "section": "Change points",
    "text": "Change points"
  },
  {
    "objectID": "slides/09-nonlinear.html#glaucoma-disease-progression",
    "href": "slides/09-nonlinear.html#glaucoma-disease-progression",
    "title": "Nonlinear Regression",
    "section": "Glaucoma disease progression",
    "text": "Glaucoma disease progression\n\nToday we will use data from the Rotterdam Ophthalmic Data Repository.\nGlaucoma is the leading cause of irreversible blindness world wide with over 60 million glaucoma patients as of 2012. Since impairment caused by glaucoma is irreversible, early detection of disease progression is crucial for effective treatment.\nPatients with glaucoma are routinely followed up and administered visual fields, a functional assessment of their vision.\nAfter each visual field test their current disease status is reported as a mean deviation (MD) value, measured in decibels (dB). A lower mean deviation indicates worse vision.\nCentral clinical challenges are i) identifying disease progression of MD, and ii) predicting future MD."
  },
  {
    "objectID": "slides/09-nonlinear.html#linear-regression",
    "href": "slides/09-nonlinear.html#linear-regression",
    "title": "Nonlinear Regression",
    "section": "Linear regression",
    "text": "Linear regression\n\nConsider the classic parametric model:\n\n\\[Y_i = \\alpha + X_i \\beta + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2).\\]\n\nAssumptions:\n\n\\(\\epsilon_i\\) are independent.\n\\(\\epsilon_i\\) are Gaussian.\nThe mean of \\(Y_i\\) is linear in \\(X_i\\).\nThe residual distribution does not depend on \\(X_i\\).\n\n\nToday we will generalize the linearity assumption."
  },
  {
    "objectID": "slides/09-nonlinear.html#linear-regression-1",
    "href": "slides/09-nonlinear.html#linear-regression-1",
    "title": "Nonlinear Regression",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#cubic-regression",
    "href": "slides/09-nonlinear.html#cubic-regression",
    "title": "Nonlinear Regression",
    "section": "Cubic regression",
    "text": "Cubic regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#quadratic-regression",
    "href": "slides/09-nonlinear.html#quadratic-regression",
    "title": "Nonlinear Regression",
    "section": "Quadratic regression",
    "text": "Quadratic regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#quadtratic-regression",
    "href": "slides/09-nonlinear.html#quadtratic-regression",
    "title": "Nonlinear Regression",
    "section": "Quadtratic regression",
    "text": "Quadtratic regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#what-is-the-point",
    "href": "slides/09-nonlinear.html#what-is-the-point",
    "title": "Nonlinear Regression",
    "section": "What is the point?",
    "text": "What is the point?\n\nChoice of model is highly dependent on the context.\nAs we learned in the model comparison lecture, a better fit to the sample might not actually be a better model.\nThese basis models are difficult to interpret and are not particularly useful for a clinical setting (they may be useful for prediction!)."
  },
  {
    "objectID": "slides/09-nonlinear.html#model-comparison",
    "href": "slides/09-nonlinear.html#model-comparison",
    "title": "Nonlinear Regression",
    "section": "Model comparison",
    "text": "Model comparison"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-splines",
    "href": "slides/09-nonlinear.html#b-splines",
    "title": "Nonlinear Regression",
    "section": "B-splines",
    "text": "B-splines"
  },
  {
    "objectID": "slides/09-nonlinear.html#learning-objectives-today",
    "href": "slides/09-nonlinear.html#learning-objectives-today",
    "title": "Nonlinear Regression",
    "section": "Learning objectives today",
    "text": "Learning objectives today\n\nThus far, we have focused on linear regression models.\nToday we will focus on approaches that use linear regression to build nonlinear associations. For example: polynomial regression and b-splines.\nThese approaches work by transforming a single predictor variable into several synthetic variables.\nWe will also look at a change point model, that encodes clinical context into a nonlinear framework."
  },
  {
    "objectID": "slides/09-nonlinear.html#splines-1",
    "href": "slides/09-nonlinear.html#splines-1",
    "title": "Nonlinear Regression",
    "section": "Splines",
    "text": "Splines\nFirst, we choose the knots. Remember, the knots are just values of year that serve as pivots for our spline. Where should the knots go? There are different ways to answer this question. You can, in principle, put the knots wherever you like. Their locations are part of the model, and you are responsible for them. Let’s do what we did in the simple example above, place the knots at different evenly spaced quantiles of the predictor variable. This gives you more knots where there are more observations."
  },
  {
    "objectID": "slides/09-nonlinear.html#splines-2",
    "href": "slides/09-nonlinear.html#splines-2",
    "title": "Nonlinear Regression",
    "section": "Splines",
    "text": "Splines\nThe next choice is polynomial degree. This determines how basis functions combine, which determines how the parameters interact to produce the spline. For degree 1, as in Figure 4.12, two basis functions combine at each point. For degree 2, three functions combine at each point. For degree 3, four combine. R already has a nice function that will build basis functions for any list of knots and degree. This code will construct the necessary basis functions for a degree 3 (cubic) spline: (p. 117)\n\nlibrary(splines)\nnum_knots &lt;- 10\nknot_list &lt;- quantile(dat_pat$X, probs = seq(from = 0, to = 1, length.out = num_knots))\n\nB &lt;- bs(dat_pat$X,\n        knots = knot_list[-c(1, num_knots)], \n        degree = 3, \n        intercept = TRUE)\nplot(1, 1, type = \"n\", xlim = c(0, max(dat_pat$X)), ylim = c(0, 1))\nfor (i in 1:ncol(B)) lines(dat_pat$X, B[, i])"
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression",
    "href": "slides/09-nonlinear.html#nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nDefine: \\(\\mu_i = \\mathbb{E}[Y_i] = \\alpha + X_i\\beta.\\)\nThe mean process can be modeled flexibly, \\(\\mu_i = g(X_i)\\), where \\(g\\) is some function that relates \\(X_i\\) to \\(\\mathbb{E}[Y_i].\\)\nA form of nonlinear regression approximates the function \\(g\\) using a finite basis expansion, \\[g(X_i) = \\alpha + \\sum_{j=1}^J B_j(X_i)\\beta_j,\\] where \\(B_j(X)\\) are known basis functions and \\(\\beta_j\\) are unknown parameters that determine the shape of \\(g\\)."
  },
  {
    "objectID": "slides/09-nonlinear.html#linear-regression-2",
    "href": "slides/09-nonlinear.html#linear-regression-2",
    "title": "Nonlinear Regression",
    "section": "Linear regression",
    "text": "Linear regression\n\nLinear regression is simple.\nLinear regression is highly interpretable. It encodes disease progression into a slope, which is the amount of MD loss (dB) per year.\n\nInterpretability is important!\n\nA linear relationship may be an oversimplification.\nOften in prediction contexts, a nonlinear approach is preferred."
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression-1",
    "href": "slides/09-nonlinear.html#nonlinear-regression-1",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nExample: Polynomial regression takes \\(B_j(X_i) = X_i^j\\).\nExample: Gaussian radial basis functions: \\[B_j(X_i) = \\exp\\left\\{-\\frac{|X_i - \\nu_j|^2}{l^2}\\right\\},\\] where \\(\\nu_j\\) are centers of the basis functions and \\(l\\) is a common width parameter.\nThe number of of basis functions and the width parameter \\(l\\) controls the scale at which the model can vary as a function of \\(X_i\\)."
  },
  {
    "objectID": "slides/09-nonlinear.html#model-fitting",
    "href": "slides/09-nonlinear.html#model-fitting",
    "title": "Nonlinear Regression",
    "section": "Model fitting",
    "text": "Model fitting\n\nThe model is Yi ∼ N(BTi β, σ2), where βj ∼ N(0, τ 2) and Bi is comprised of the known basis functions Bj(Xi)\nTherefore, the model is usual linear regression model and is straightforward to fit using MCMC\nBayesian methods are excellent for quantifying uncertainty in the fitted model and predictions\nHow to pick J? Can we J &gt; n?"
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression-2",
    "href": "slides/09-nonlinear.html#nonlinear-regression-2",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nExample: The cubic B-spline basis function is the following piecewise cubic polynomial:\n\n\\[B_j(X_i) = \\left\\{\n\\begin{matrix*}[l]\n\\frac{1}{6}u^3 & \\text{for }X_i \\in (\\nu_j,\\nu_{j+1}), & u = (X_i - \\nu_j) / \\delta\\\\\n\\frac{1}{6}(1 + 3u + 3u^2 - 3u^3) & \\text{for }X_i \\in (\\nu_{j+1},\\nu_{j+2}), & u = (X_i - \\nu_{j+1}) / \\delta\\\\\n\\frac{1}{6}(4 - 6u^2 + 3u^3) & \\text{for }X_i \\in (\\nu_{j+2},\\nu_{j+3}), & u = (X_i - \\nu_{j+2}) / \\delta\\\\\n\\frac{1}{6}(1 - 3u + 3u^2 - u^3) & \\text{for }X_i \\in (\\nu_{j+3},\\nu_{j+4}), & u = (X_i - \\nu_{j+3}) / \\delta\\\\\n0 & \\text{otherwise.}\n\\end{matrix*}\n\\right.\\]\n\nB-splines are a piecewise continuous function defined conditional on some set of knots.\nHere we assume a uniform knot locations \\(\\nu_{j + k} = \\nu_j + \\delta k\\).\nB-splines have compact support, so the design matrix is sparse."
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression-3",
    "href": "slides/09-nonlinear.html#nonlinear-regression-3",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nConditionally on the selected bases \\(B\\), the model is linear in the parameters. Hence we can write, \\[Y_i = \\mu_i + \\epsilon_i = \\mathbf{w}_i \\boldsymbol{\\beta} + \\epsilon_i,\\] with \\(\\mathbf{w}_i = (B_1(X_i),\\ldots,B_J(X_i))\\).\nModel fitting can proceed as in linear regression models, since the resulting model is linear in \\(\\boldsymbol{\\beta}\\).\nIt is often useful to center the basis function model around the linear model, \\(\\mu_i = \\alpha + X_i \\beta + \\mathbf{w}_i\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/09-nonlinear.html#glaucoma-data",
    "href": "slides/09-nonlinear.html#glaucoma-data",
    "title": "Nonlinear Regression",
    "section": "Glaucoma data",
    "text": "Glaucoma data\n\n### Load and process data to obtain data for an example patient\ndat &lt;- read.csv(file = \"LongGlaucVF_20150216/VisualFields.csv\")\ndat &lt;- dat[order(dat$STUDY_ID, dat$SITE), ]\ndat$EYE_ID &lt;- cumsum(!duplicated(dat[, c(\"STUDY_ID\", \"SITE\")]))\ndat_pat &lt;- dat[dat$EYE_ID == \"4\", ] # 4\ndat_pat$time &lt;- (dat_pat$AGE - dat_pat$AGE[1]) / 365\ndat_pat &lt;- dat_pat[, c(\"time\", \"MD\")]\ncolnames(dat_pat) &lt;- c(\"X\", \"Y\")\nglimpse(dat_pat)\n\n\n\nRows: 18\nColumns: 2\n$ X &lt;dbl&gt; 0.0000000, 0.6136986, 1.1315068, 1.6520548, 2.1671233, 2.6794521, 3.…\n$ Y &lt;dbl&gt; -2.76, -2.08, -1.91, -2.63, -5.13, -2.14, -1.97, -0.83, -1.75, -1.61…"
  },
  {
    "objectID": "slides/09-nonlinear.html#polynomial-regression-in-stan",
    "href": "slides/09-nonlinear.html#polynomial-regression-in-stan",
    "title": "Nonlinear Regression",
    "section": "Polynomial regression in Stan",
    "text": "Polynomial regression in Stan\n\ndat_poly &lt;- data.frame(\n  Y = scale(dat_pat$Y),\n  X = scale(dat_pat$X)\n)\ndat_poly$X2 &lt;- dat_poly$X^2\nstan_data &lt;- list(\n  n = nrow(dat_pat),\n  p = 2,\n  Y = dat_poly$Y,\n  X = cbind(dat_poly$X, dat_poly$X2),\n)\ncompile_model &lt;- stan_model(file = \"nonlinear_linear.stan\")\nfit_quadratic &lt;- sampling(compile_model, data = stan_data)"
  },
  {
    "objectID": "slides/09-nonlinear.html#polynomial-regression-in-stan-1",
    "href": "slides/09-nonlinear.html#polynomial-regression-in-stan-1",
    "title": "Nonlinear Regression",
    "section": "Polynomial regression in Stan",
    "text": "Polynomial regression in Stan\n\n// saved in nonlinear_linear.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates\n  vector[n] Y; // outcome vector\n  matrix[n, p] X; // covariate vector\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, sigma); // likelihood\n  target += normal_lpdf(alpha | 0, 1); // prior for alpha\n  target += normal_lpdf(beta | 0, 1); // prior for beta\n  target += inv_gamma_lpdf(sigma | 3, 1); // prior for sigma\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n] log_lik;\n  vector[n] mu;\n  for (i in 1:n) {\n    mu[i] = alpha + X[i, ] * beta;\n    in_sample[i] = normal_rng(mu[i], sigma);\n    log_lik[i] = normal_lpdf(Y[i] |  mu[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/09-nonlinear.html#extract-posterior-mean-for-mu",
    "href": "slides/09-nonlinear.html#extract-posterior-mean-for-mu",
    "title": "Nonlinear Regression",
    "section": "Extract posterior mean for \\(\\mu\\)",
    "text": "Extract posterior mean for \\(\\mu\\)\n\nmu &lt;- rstan::extract(fit_quadratic, pars = \"mu\")$mu\nmu &lt;- mu * sd(dat_pat$Y) + mean(dat_pat$Y) # transform to original unstandardized Y_i\nmu_mean &lt;- apply(mu, 2, mean)\nmu_lower &lt;- apply(mu, 2, function(x) quantile(x, probs = 0.025))\nmu_upper &lt;- apply(mu, 2, function(x) quantile(x, probs = 0.975))"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression",
    "href": "slides/09-nonlinear.html#b-spline-regression",
    "title": "Nonlinear Regression",
    "section": "B-spline regression",
    "text": "B-spline regression\n\nlibrary(splines)\nnum_knots &lt;- 5\nknot_list &lt;- quantile(dat_pat$X, probs = seq(from = 0, to = 1, length.out = num_knots))\nB &lt;- bs(dat_pat$X,\n        knots = knot_list[-c(1, num_knots)], \n        degree = 3, \n        intercept = TRUE)\nB\n\n                 1            2            3           4            5\n [1,] 1.0000000000 0.0000000000 0.0000000000 0.000000000 0.0000000000\n [2,] 0.3932160225 0.5205689817 0.0833170637 0.002897932 0.0000000000\n [3,] 0.1303337386 0.6136422453 0.2378607351 0.018163281 0.0000000000\n [4,] 0.0220025742 0.5116331668 0.4098320335 0.056532225 0.0000000000\n [5,] 0.0001737807 0.3325401051 0.5396793792 0.127606735 0.0000000000\n [6,] 0.0000000000 0.1814804087 0.5792969517 0.238573820 0.0006488199\n [7,] 0.0000000000 0.0883598392 0.5363295039 0.367729756 0.0075809005\n [8,] 0.0000000000 0.0097143260 0.3289042281 0.593688910 0.0676925363\n [9,] 0.0000000000 0.0003772242 0.1863761908 0.659307134 0.1539394508\n[10,] 0.0000000000 0.0000000000 0.0788259578 0.624941237 0.2955691681\n[11,] 0.0000000000 0.0000000000 0.0357900241 0.542804472 0.4124556582\n[12,] 0.0000000000 0.0000000000 0.0107764123 0.421496598 0.5286019082\n[13,] 0.0000000000 0.0000000000 0.0006952375 0.263269190 0.6110273290\n[14,] 0.0000000000 0.0000000000 0.0000000000 0.145095180 0.5888020313\n[15,] 0.0000000000 0.0000000000 0.0000000000 0.060977201 0.4468755310\n[16,] 0.0000000000 0.0000000000 0.0000000000 0.016061781 0.2349278937\n[17,] 0.0000000000 0.0000000000 0.0000000000 0.002189635 0.0740048857\n[18,] 0.0000000000 0.0000000000 0.0000000000 0.000000000 0.0000000000\n                 6            7\n [1,] 0.0000000000 0.0000000000\n [2,] 0.0000000000 0.0000000000\n [3,] 0.0000000000 0.0000000000\n [4,] 0.0000000000 0.0000000000\n [5,] 0.0000000000 0.0000000000\n [6,] 0.0000000000 0.0000000000\n [7,] 0.0000000000 0.0000000000\n [8,] 0.0000000000 0.0000000000\n [9,] 0.0000000000 0.0000000000\n[10,] 0.0006636368 0.0000000000\n[11,] 0.0089498455 0.0000000000\n[12,] 0.0391250811 0.0000000000\n[13,] 0.1250082431 0.0000000000\n[14,] 0.2659535203 0.0001492682\n[15,] 0.4675827148 0.0245645535\n[16,] 0.5868492773 0.1621610484\n[17,] 0.4743685242 0.4494369547\n[18,] 0.0000000000 1.0000000000\nattr(,\"degree\")\n[1] 3\nattr(,\"knots\")\n     25%      50%      75% \n2.295205 4.965753 6.997945 \nattr(,\"Boundary.knots\")\n[1] 0.000000 9.257534\nattr(,\"intercept\")\n[1] TRUE\nattr(,\"class\")\n[1] \"bs\"     \"basis\"  \"matrix\""
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-with-10-knots",
    "href": "slides/09-nonlinear.html#b-spline-regression-with-10-knots",
    "title": "Nonlinear Regression",
    "section": "B-spline regression with 10 knots",
    "text": "B-spline regression with 10 knots"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-with-10-knots-1",
    "href": "slides/09-nonlinear.html#b-spline-regression-with-10-knots-1",
    "title": "Nonlinear Regression",
    "section": "B-spline regression with 10 knots",
    "text": "B-spline regression with 10 knots"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-with-5-knots",
    "href": "slides/09-nonlinear.html#b-spline-regression-with-5-knots",
    "title": "Nonlinear Regression",
    "section": "B-spline regression with 5 knots",
    "text": "B-spline regression with 5 knots"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-1",
    "href": "slides/09-nonlinear.html#b-spline-regression-1",
    "title": "Nonlinear Regression",
    "section": "B-spline regression",
    "text": "B-spline regression\n\nstan_data &lt;- list(\n  n = nrow(dat_pat),\n  p = ncol(B),\n  Y = dat_poly$Y,\n  X = B\n)\nfit_bspline &lt;- sampling(compile_model, data = stan_data)"
  },
  {
    "objectID": "slides/09-nonlinear.html#change-point-motivation",
    "href": "slides/09-nonlinear.html#change-point-motivation",
    "title": "Nonlinear Regression",
    "section": "Change point motivation",
    "text": "Change point motivation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgression is defined by slow (or stable) deterioration, followed by a rapid decrease.\nFlexible modeling of MD across time.\nBiological representation of progression through the change point.\nChange points are a framework for inherently parameterizing progression."
  },
  {
    "objectID": "slides/09-nonlinear.html#writing-down-a-model",
    "href": "slides/09-nonlinear.html#writing-down-a-model",
    "title": "Nonlinear Regression",
    "section": "Writing down a model",
    "text": "Writing down a model\n\nModel for the observed data:\n\n\\[Y_i = \\mu_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0,\\sigma^2).\\]\n. . .\n\nModel for the mean process:\n\n\\[\\mu_i =\\left\\{ \\begin{array}{ll}\n        {\\beta}_0 + \\beta_1 X_i & \\text{ } \\mbox{$X_i \\leq \\theta$},\\\\\n        {\\beta}_0 + \\beta_1 \\theta + {\\beta}_2(X_i - \\theta)& \\text{ } \\mbox{$X_i &gt; \\theta.$}\\end{array} \\right.\\]\n\n\\(\\theta \\in (\\min X_i, \\max X_i)\\) represents a change point."
  },
  {
    "objectID": "slides/09-nonlinear.html#change-point-model-in-stan",
    "href": "slides/09-nonlinear.html#change-point-model-in-stan",
    "title": "Nonlinear Regression",
    "section": "Change point model in Stan",
    "text": "Change point model in Stan\n\n// saved in change_points.stan\nfunctions {\n  vector compute_mean(vector X, real beta0, real beta1, real beta2, real theta) {\n    int n = size(X);\n    vector[n] mu;\n    for (t in 1:n) {\n      if (X[t] &lt;= theta) mu[t] = beta0 + beta1 * X[t];\n      if (X[t] &gt; theta) mu[t] = beta0 + beta1 * theta + beta2 * (X[t] - theta);\n  }\n  return mu;\n  }\n}\ndata {\n  int&lt;lower=1&gt; n;\n  vector[n] Y;\n  vector[n] X;\n  int n_pred;\n  vector[n_pred] X_pred;\n}\ntransformed data {\n  real min_X = min(X);\n  real max_X = max(X);\n}\nparameters {\n  real beta0;\n  real beta1;\n  real beta2;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = min_X, upper = max_X&gt; theta;\n}\nmodel {\n  vector[n] mu = compute_mean(X, beta0, beta1, beta2, theta);\n  target += normal_lpdf(Y | mu, sigma);\n  target += normal_lpdf(sigma | 0, 1);\n  target += normal_lpdf(beta0 | 0, 1);\n  target += normal_lpdf(beta1 | 0, 1);\n  target += normal_lpdf(beta2 | 0, 1);\n}\ngenerated quantities {\n  vector[n_pred] mu_pred = compute_mean(X_pred, beta0, beta1, beta2, theta);\n  array[n_pred] real Y_pred_out = normal_rng(mu_pred, sigma);\n  vector[n] mu = compute_mean(X, beta0, beta1, beta2, theta);\n  array[n] real Y_pred_in = normal_rng(mu, sigma);\n}"
  },
  {
    "objectID": "slides/09-nonlinear.html#change-point-regression",
    "href": "slides/09-nonlinear.html#change-point-regression",
    "title": "Nonlinear Regression",
    "section": "Change point regression",
    "text": "Change point regression\n\nstan_model &lt;- stan_model(file = \"change_points.stan\")\nn_pred &lt;- 1000\nstan_data &lt;- list(Y = dat_pat$Y, \n                  X = dat_pat$X,\n                  n = nrow(dat_pat),\n                  n_pred = n_pred,\n                  X_pred = seq(0, max(dat_pat$X) + 2, length.out = n_pred))\nfit_cp &lt;- sampling(stan_model, data = stan_data)\nprint(fit_cp, probs = c(0.025, 0.5, 0.0975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n       mean se_mean   sd   25%   50%   75% n_eff Rhat\nbeta0 -1.82    0.02 0.66 -2.29 -1.85 -1.40   749 1.00\nbeta1 -0.05    0.01 0.29 -0.19 -0.04  0.10   560 1.01\nbeta2 -0.84    0.02 0.45 -1.12 -0.87 -0.58   889 1.00\nsigma  1.27    0.01 0.26  1.08  1.23  1.41   792 1.00\ntheta  5.36    0.07 1.79  4.69  5.57  6.34   749 1.00\n\nSamples were drawn using NUTS(diag_e) at Wed Jan  1 17:29:30 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/09-nonlinear.html#diagnostics",
    "href": "slides/09-nonlinear.html#diagnostics",
    "title": "Nonlinear Regression",
    "section": "Diagnostics",
    "text": "Diagnostics"
  },
  {
    "objectID": "slides/09-nonlinear.html#posterior-fit",
    "href": "slides/09-nonlinear.html#posterior-fit",
    "title": "Nonlinear Regression",
    "section": "Posterior fit",
    "text": "Posterior fit"
  },
  {
    "objectID": "slides/09-nonlinear.html#posterior-fit-1",
    "href": "slides/09-nonlinear.html#posterior-fit-1",
    "title": "Nonlinear Regression",
    "section": "Posterior fit",
    "text": "Posterior fit"
  },
  {
    "objectID": "slides/09-nonlinear.html#prediction",
    "href": "slides/09-nonlinear.html#prediction",
    "title": "Nonlinear Regression",
    "section": "Prediction",
    "text": "Prediction"
  },
  {
    "objectID": "slides/11-regularization.html#review-of-last-lecture",
    "href": "slides/11-regularization.html#review-of-last-lecture",
    "title": "Regularization",
    "section": "",
    "text": "On Tuesday, we learned about robust regression.\n\nHeteroskedasticity\nHeavy-tailed distributions\nMedian regression\n\nThese were all models for the observed data \\(Y_i\\).\nToday, we will focus on prior specifications for \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/11-regularization.html#sparsity-in-regression-problems",
    "href": "slides/11-regularization.html#sparsity-in-regression-problems",
    "title": "Regularization",
    "section": "Sparsity in regression problems",
    "text": "Sparsity in regression problems\n\nSupervised learning can be cast as the problem of estimating a set of coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_j\\}_{j=1}^{p}\\) that determines some functional relationship between a set of \\(\\{x_{ij}\\}_{j = 1}^p\\) and a target variable \\(Y_i\\).\nThis is a central focus of statistics and machine learning.\nChallenges arise in “large-\\(p\\)” problems where, in order to avoid overly complex models that predict poorly, some form of dimension reduction is needed.\nFinding a sparse solution, where some \\(\\beta_j\\) are zero, is desirable."
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-sparse-estimation",
    "href": "slides/11-regularization.html#bayesian-sparse-estimation",
    "title": "Regularization",
    "section": "Bayesian sparse estimation",
    "text": "Bayesian sparse estimation\n\nFrom a Bayesian-learning perspective, there are two main sparse-estimation alternatives: discrete mixtures and shrinkage priors.\nDiscrete mixtures have been very popular, with the spike-and-slab prior being the gold standard.\n\nEasy to force \\(\\beta_j\\) to exactly zero, but require discrete parameter specification.\n\nShrinkage priors force \\(\\beta_j\\) to zero using regularization, but struggle to get exact zeros.\n\nIn recent years, shrinkage priors have become dominant in Bayesian sparsity priors."
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-prior",
    "href": "slides/11-regularization.html#horseshoe-prior",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\n\nThe horseshoe prior is specified as,\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\mathcal C^+(0, 1),\n\\end{aligned}\\] where \\(\\mathcal C^+(0, 1)\\) is a half-Cauchy distribution for the local parameter \\(\\lambda_j\\).\n\n\\(\\lambda_j\\)’s are the local shrinkage parameters.\n\\(\\tau\\) is the global shrinkage parameter."
  },
  {
    "objectID": "slides/11-regularization.html#half-cauchy-distribution",
    "href": "slides/11-regularization.html#half-cauchy-distribution",
    "title": "Regularization",
    "section": "Half-Cauchy distribution",
    "text": "Half-Cauchy distribution\nA random variable \\(X \\sim \\mathcal C^+(\\mu,\\sigma)\\) follows a half-Cauchy distribution with location \\(\\mu\\) and scale \\(\\sigma &gt; 0\\) and has the following density,\n\\[f(X | \\mu, \\sigma) = \\frac{2}{\\pi \\sigma}\\frac{1}{1 + (X - \\mu)^2 / \\sigma^2},\\quad X \\geq \\mu\\]\n\nThe Half-Cauchy distribution with \\(\\mu = 0\\) is a useful prior for non-negative parameters that may be very large, as allowed by the very heavy tails of the Cauchy distribution."
  },
  {
    "objectID": "slides/11-regularization.html#half-cauchy-distribution-in-stan",
    "href": "slides/11-regularization.html#half-cauchy-distribution-in-stan",
    "title": "Regularization",
    "section": "Half-Cauchy distribution in Stan",
    "text": "Half-Cauchy distribution in Stan\nIn Stan, the half-Cauchy distribution can be specified by putting a constraint on the parameter definition.\n\nparameters {\n  real&lt;lower = 0&gt; lambda;\n}\nmodel {\n  target += cauchy_lpdf(lambda | 0, 1);\n}"
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-prior-1",
    "href": "slides/11-regularization.html#horseshoe-prior-1",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\nThe horseshoe prior has two interesting features that make it particularly useful as a shrinkage prior for sparse problems.\n\nIt has flat, Cauchy-like tails that allow strong signals to remain large (that is, un-shrunk) a posteriori.\nIt has an infinitely tall spike at the origin that provides severe shrinkage for the zero elements of \\(\\boldsymbol{\\beta}\\).\n\nAs we will see, these are key elements that make the horseshoe an attractive choice for handling sparse vectors."
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-prior-2",
    "href": "slides/11-regularization.html#horseshoe-prior-2",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\nThe horseshoe prior has two interesting features that make it particularly useful as a shrinkage prior for sparse problems.\n\nIt has flat, Cauchy-like tails that allow strong signals to remain large (that is, un-shrunk) a posteriori.\nIt has an infinitely tall spike at the origin that provides severe shrinkage for the zero elements of \\(\\boldsymbol{\\beta}\\).\n\nAs we will see, these are key elements that make the horseshoe an attractive choice for handling sparse vectors."
  },
  {
    "objectID": "slides/11-regularization.html#relation-to-other-shrinkage-priors",
    "href": "slides/11-regularization.html#relation-to-other-shrinkage-priors",
    "title": "Regularization",
    "section": "Relation to other shrinkage priors",
    "text": "Relation to other shrinkage priors\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\sim f(\\lambda_j)\n\\end{aligned}\\]\n\n\\(\\lambda_j = 1 / \\lambda\\), implies ridge regression.\n\\(f(\\lambda_j) = \\text{Exponential}(0.5)\\), implies lasso.\n\\(f(\\lambda_j) = \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\\), implies relevance vector machine.\n\\(f(\\lambda_j) = \\mathcal C^+(0,1)\\), implies horseshoe."
  },
  {
    "objectID": "slides/11-regularization.html#horsehoe-density",
    "href": "slides/11-regularization.html#horsehoe-density",
    "title": "Regularization",
    "section": "Horsehoe density",
    "text": "Horsehoe density\n\n\n\n\n\nFrom Carvalho 2009"
  },
  {
    "objectID": "slides/11-regularization.html#shrinkage-of-each-prior",
    "href": "slides/11-regularization.html#shrinkage-of-each-prior",
    "title": "Regularization",
    "section": "Shrinkage of each prior",
    "text": "Shrinkage of each prior\n\nDefine the posterior mean of \\(\\beta_j\\) as \\(\\bar{\\beta}_j\\) and the maximum likelihood estimator for \\(\\beta_j\\) as \\(\\hat{\\beta}_j\\).\nThe following relationship holds: \\(\\bar{\\beta}_j = (1 - \\kappa_j) \\hat{\\beta}_j\\),\n\n\\[\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}s_j^2\\lambda_j^2}.\\]\n\n\\(\\kappa_j\\) is called the shrinkage factor for \\(\\beta_j\\).\n\\(s_j^2 = \\mathbb{V}(x_j)\\) is the variance for each predictor."
  },
  {
    "objectID": "slides/11-regularization.html#standardization-of-predictors",
    "href": "slides/11-regularization.html#standardization-of-predictors",
    "title": "Regularization",
    "section": "Standardization of predictors",
    "text": "Standardization of predictors\n\nIn regularization problems, predictors are standardized (to mean zero and standard deviation one).\nThis means that so that \\(s_j = 1\\).\nShrinkage parameter:\n\n\\[\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}\\lambda_j^2}.\\]\n\n\\(\\kappa_j = 1\\), implies complete shrinkage.\n\\(\\kappa_j = 0\\), implies no shrinkage."
  },
  {
    "objectID": "slides/11-regularization.html#shrinkage-parameter",
    "href": "slides/11-regularization.html#shrinkage-parameter",
    "title": "Regularization",
    "section": "Shrinkage parameter",
    "text": "Shrinkage parameter\n\n\n\n\n\nFrom Carvalho 2009"
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-shrinkage-parameter",
    "href": "slides/11-regularization.html#horseshoe-shrinkage-parameter",
    "title": "Regularization",
    "section": "Horseshoe shrinkage parameter",
    "text": "Horseshoe shrinkage parameter\n\nChoosing \\(\\lambda_j ∼ \\mathcal C^+(0, 1)\\) implies \\(\\kappa_j ∼ \\text{Beta}(0.5, 0.5)\\), a density that is symmetric and unbounded at both 0 and 1.\nThis horseshoe-shaped shrinkage profile expects to see two things a priori:\n\nStrong signals (\\(\\kappa \\approx 0\\), no shrinkage), and\nZeros (\\(\\kappa \\approx 1\\), total shrinkage)."
  },
  {
    "objectID": "slides/11-regularization.html#spike-and-slab-prior",
    "href": "slides/11-regularization.html#spike-and-slab-prior",
    "title": "Regularization",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nDiscrete parameter specification,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\text{Bernoulli}(\\pi).\n\\end{aligned}\\]\n\n\\(\\lambda_j \\in \\{0,1\\}\\), thus this model permits exact zeros.\nThe number of zeros is dictated by \\(\\pi\\), which can either be pre-specified or given a prior.\nDiscrete parameters can not be specified in Stan!"
  },
  {
    "objectID": "slides/11-regularization.html#spike-and-slab-prior-1",
    "href": "slides/11-regularization.html#spike-and-slab-prior-1",
    "title": "Regularization",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nSpike-and-slab can be written generally as a two-component mixture of Gaussians,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau, \\omega &\\stackrel{ind}{\\sim} \\lambda_j N(0, \\tau^2) + (1-\\lambda_j) N(0,\\omega^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\text{Bernoulli}(\\pi).\n\\end{aligned}\\]\n\n\\(\\omega \\ll \\tau\\) and the indicator variable \\(\\lambda_j \\in \\{0, 1\\}\\) denotes whether \\(\\beta_j\\) is close to zero (comes from the “spike”, \\(\\lambda_j = 0\\)) or non-zero (comes from the “slab”, \\(\\lambda_j = 1\\)).\nOften \\(\\omega = 0\\) (the spike is a true spike)."
  },
  {
    "objectID": "slides/11-regularization.html#similarity-to-horseshoe",
    "href": "slides/11-regularization.html#similarity-to-horseshoe",
    "title": "Regularization",
    "section": "Similarity to horseshoe",
    "text": "Similarity to horseshoe\n\nLetting \\(c \\rightarrow \\infty\\), all the mass is concentrated at the extremes \\(\\kappa_j = 0\\) and \\(\\kappa_j = 1\\) (this resembles the horseshoe).\n\n\n\n\n\n\nFrom Piironena and Vehtari 2017\n\nThe horseshoe can be seen as a continuous approximation to the spike-and-slab prior as \\(c \\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/11-regularization.html#regularized-horseshoe-prior",
    "href": "slides/11-regularization.html#regularized-horseshoe-prior",
    "title": "Regularization",
    "section": "Regularized horseshoe prior",
    "text": "Regularized horseshoe prior\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau, c &\\sim N\\left(0, \\tau^2 \\tilde{\\lambda}_j^2\\right),\\quad \\tilde{\\lambda}_j^2 = \\frac{c^2 \\lambda_j^2}{c^2 + \\tau^2 \\lambda_j^2},\\\\\n\\lambda_j &\\sim \\mathcal C^+(0,1).\n\\end{aligned}\\]\n\nWhen \\(\\tau^2 \\lambda_j^2 \\ll c^2\\) (i.e., \\(\\beta_j\\) close to zero), \\(\\beta_j \\sim  N\\left(0, \\tau^2\\lambda_j^2\\right)\\)\nWhen \\(\\tau^2 \\lambda_j^2 \\gg c^2\\), (i.e., \\(\\beta_j\\) far from zero), \\(\\beta_j \\sim  N\\left(0, c^2\\right)\\)\n\\(c \\rightarrow \\infty\\) recovers the original horseshoe.\n\nWhy is this an appealing extension?"
  },
  {
    "objectID": "slides/11-regularization.html#regularized-horseshoe-compared-to-spike-and-slab",
    "href": "slides/11-regularization.html#regularized-horseshoe-compared-to-spike-and-slab",
    "title": "Regularization",
    "section": "Regularized horseshoe compared to spike-and-slab",
    "text": "Regularized horseshoe compared to spike-and-slab\n\nThe regularized horseshoe prior is comparable to the spike-and-slab with finite \\(c\\).\n\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/11-regularization.html#choosing-a-prior-for-c2",
    "href": "slides/11-regularization.html#choosing-a-prior-for-c2",
    "title": "Regularization",
    "section": "Choosing a prior for \\(c^2\\)",
    "text": "Choosing a prior for \\(c^2\\)\n\nUnless substantial knowledge about the scale of the relevant coefficients exists, it is recommended to place a prior for \\(c\\) instead of fixing it.\nOften a reasonable choice is, \\[c^2 \\sim \\text{Inv-Gamma}(\\alpha, \\beta), \\quad \\alpha = \\nu/2, \\beta = \\nu s^2 / 2,\\]\nThis translates to a \\(t_{\\nu}(0,s^2)\\) slab for the coefficients far from 0.\nAnother motivation for using inverse-Gamma is that it has a heavy right tail accompanied by a light left tail thereby preventing much mass from accumulating near zero."
  },
  {
    "objectID": "slides/11-regularization.html#choosing-a-prior-for-tau",
    "href": "slides/11-regularization.html#choosing-a-prior-for-tau",
    "title": "Regularization",
    "section": "Choosing a prior for \\(\\tau\\)",
    "text": "Choosing a prior for \\(\\tau\\)\n\nCarvalho et al. 2009 suggest \\(\\tau \\sim \\mathcal C^+(0,1)\\).\nPolson and Scott 2011 recommend \\(\\tau | \\sigma \\sim \\mathcal C^+(0, \\sigma^2)\\).\nAnother prior comes from a quantity called the effective number of nonzero coefficients,\n\n\\[m_{eff} = \\sum_{j=1}^p (1 - \\kappa_j).\\]"
  },
  {
    "objectID": "slides/11-regularization.html#global-shrinkage-parameter-tau",
    "href": "slides/11-regularization.html#global-shrinkage-parameter-tau",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\nThe prior mean can be shown to be,\n\n\\[\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = \\frac{\\tau \\sigma^{-1} \\sqrt{n}}{1 + \\tau \\sigma^{-1} \\sqrt{n}}p.\\]\n\nSetting \\(\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = p_0\\) (prior guess for the number of non-zero coefficients) yields for \\(\\tau\\),\n\n\\[\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}.\\]"
  },
  {
    "objectID": "slides/11-regularization.html#global-shrinkage-parameter-tau-1",
    "href": "slides/11-regularization.html#global-shrinkage-parameter-tau-1",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/11-regularization.html#prepare-for-next-class",
    "href": "slides/11-regularization.html#prepare-for-next-class",
    "title": "Regularization",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03, which was just assigned.\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Classification"
  },
  {
    "objectID": "slides/13-multiclass.html#review-of-last-lecture",
    "href": "slides/13-multiclass.html#review-of-last-lecture",
    "title": "Multiclass Classification",
    "section": "",
    "text": "On Tuesday, we learned about classification using logistic regression.\nToday, we will focus on multiclass classification: multinomial regression, ordinal regression."
  },
  {
    "objectID": "slides/13-multiclass.html#generalized-linear-models",
    "href": "slides/13-multiclass.html#generalized-linear-models",
    "title": "Multiclass Classification",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nPreviously, we have focused on linear regression. Other forms of regression follow naturally from linear regression."
  },
  {
    "objectID": "slides/13-multiclass.html#models-for-binary-outcomes",
    "href": "slides/13-multiclass.html#models-for-binary-outcomes",
    "title": "Multiclass Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nSuppose we have a binary outcome (e.g., \\(Y = 1\\) if a condition is satisfied and \\(Y = 0\\) if not) and predictors on a variety of scales.\nIf the predictors are discrete and the binary outcomes are independent, we can use the Bernoulli distribution for individual 0-1 data or the binomial distribution for grouped data that are counts of successes in each group."
  },
  {
    "objectID": "slides/13-multiclass.html#models-for-binary-outcomes-1",
    "href": "slides/13-multiclass.html#models-for-binary-outcomes-1",
    "title": "Multiclass Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nLet’s suppose we want to model \\(P(Y = 1)\\).\nOne strategy might be to simply fit a linear regression model to the probabilities.\nFor example,\n\n\\[\\begin{aligned}\nP(Y_i = 1) &= \\alpha + \\beta_1x_{i1} + \\cdots + \\beta_px_{ip} + \\epsilon_i\\\\\n&= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#primary-biliary-cirrhosis",
    "href": "slides/13-multiclass.html#primary-biliary-cirrhosis",
    "title": "Multiclass Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nThe Mayo Clinic conducted a trial for primary biliary cirrhosis, comparing the drug D-penicillamine vs. placebo. Patients were followed for a specified duration, and their status at the end of follow-up (whether they died) was recorded.\nResearchers are interested in predicting whether a patient died based on the following variables:\n\nascites: whether the patient had ascites (1 = yes, 0 = no)\nbilirubin: serum bilirubin in mg/dL\nstage: histologic stage of disease (ordinal categorical variable with stages 1, 2, 3, and 4)"
  },
  {
    "objectID": "slides/13-multiclass.html#what-can-go-wrong",
    "href": "slides/13-multiclass.html#what-can-go-wrong",
    "title": "Multiclass Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nSuppose we fit the following model:\n\n\\[\\begin{aligned}\nP(Y_i = 1) &= \\alpha + \\beta_1(ascites)_i + \\beta_2(bilirubin)_i\\\\\n&\\quad+\\beta_3(stage = 2)_i + \\beta_4(stage = 3)_i\\\\\n&\\quad+\\beta_5(stage = 4)_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\n\\end{aligned}\\]\n\nWhat can go wrong?"
  },
  {
    "objectID": "slides/13-multiclass.html#what-can-go-wrong-1",
    "href": "slides/13-multiclass.html#what-can-go-wrong-1",
    "title": "Multiclass Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "slides/13-multiclass.html#what-can-go-wrong-2",
    "href": "slides/13-multiclass.html#what-can-go-wrong-2",
    "title": "Multiclass Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nAdditionally, as a probability, \\(P(Y_i = 1)\\) must be in the interval [0, 1], but there is nothing in the model that enforces this constraint, so that you could be estimating probabilities that are negative or that are greater than 1!"
  },
  {
    "objectID": "slides/13-multiclass.html#from-probabilities-to-log-odds",
    "href": "slides/13-multiclass.html#from-probabilities-to-log-odds",
    "title": "Multiclass Classification",
    "section": "From probabilities to log-odds",
    "text": "From probabilities to log-odds\n\nSuppose the probability of an event is \\(\\pi\\).\nThen the odds that the event occurs is \\(\\frac{\\pi}{1 - \\pi}\\).\nTaking the (natural) log of the odds, we have the logit of \\(\\pi\\): the log-odds:\n\n\\[\\text{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).\\]\n\nNote that although \\(\\pi\\) is constrained to lie between 0 and 1, the logit of \\(\\pi\\) is unconstrained - it can be anything from \\(-\\infty\\) to \\(\\infty\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#logistic-regression-model",
    "href": "slides/13-multiclass.html#logistic-regression-model",
    "title": "Multiclass Classification",
    "section": "Logistic regression model",
    "text": "Logistic regression model\n\nLet’s create a model for the logit of \\(\\pi\\): \\(\\text{logit}(\\pi_i)= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\\)\nThis is a linear model for a transformation of the outcome of interest, and is also equivalent to,\n\n\\[\\pi_i = \\frac{\\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})}{1 + \\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})} = \\text{expit}(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}).\\]\n\nThe expression on the right is called a logistic function and cannot yield a value that is negative or a value that is &gt;1. Fitting a model of this form is known as logistic regression."
  },
  {
    "objectID": "slides/13-multiclass.html#logistic-regression",
    "href": "slides/13-multiclass.html#logistic-regression",
    "title": "Multiclass Classification",
    "section": "Logistic regression",
    "text": "Logistic regression\n\\[\\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nNegative logits represent probabilities less than one-half, and positive logits represent probabilities above one-half."
  },
  {
    "objectID": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression",
    "href": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression",
    "title": "Multiclass Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nTypically we interpret functions of parameters in logistic regression rather than the parameters themselves.\n\nFor the simple model: \\(\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\beta x_{i},\\) we note that the probability that \\(Y_i = 1\\) when \\(X_i = 0\\) is\n\\[P(Y_i = 1 | X_{i} = 0) = \\frac{\\exp(\\alpha)}{1 + \\exp(\\alpha)}.\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression-1",
    "href": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression-1",
    "title": "Multiclass Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nSuppose that \\(X\\) is a binary (0/1) variable (e.g., \\(X\\) = 1 for males and 0 for non-males).\n\nIn this case, we interpret \\(\\exp(\\beta)\\) as the odds ratio (OR) of the response for the two possible levels of \\(X\\).\nFor \\(X\\) on other scales, \\(\\exp(\\beta)\\) is interpreted as the odds ratio of the response comparing two values of \\(X\\) one unit apart.\n\nWhy?"
  },
  {
    "objectID": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression-2",
    "href": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression-2",
    "title": "Multiclass Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nThe log odds of response for \\(X = 1\\) is given by \\(\\alpha + \\beta\\), and the log odds of response for \\(X = 0\\) is \\(\\alpha\\).\nSo the odds ratio of response comparing \\(X = 1\\) to \\(X = 0\\) is given by \\(\\frac{\\exp(\\alpha + \\beta)}{\\exp(\\alpha)} = \\exp(\\beta)\\).\nIn a multivariable logistic regression model with more than one predictor, this OR is interpreted conditionally on values of other variables (i.e., controlling for them)."
  },
  {
    "objectID": "slides/13-multiclass.html#bayesian-logistic-regression",
    "href": "slides/13-multiclass.html#bayesian-logistic-regression",
    "title": "Multiclass Classification",
    "section": "Bayesian logistic regression",
    "text": "Bayesian logistic regression\n\nWe start with observations \\(Y_i \\in \\{0,1\\}\\) for \\(i = 1,\\ldots,n\\), where \\(Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\), \\(\\pi_i = P(Y_i = 1)\\).\nThe log-odds are modeled as \\(\\text{logit}(\\pi_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nTo complete the Bayesian model specification, we must place priors on \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\).\n\nAll priors we have discussed up-to-this point apply!\n\nHistorically, this was a difficult model to fit, but can be easily implemented in Stan."
  },
  {
    "objectID": "slides/13-multiclass.html#logistic-regression-in-stan",
    "href": "slides/13-multiclass.html#logistic-regression-in-stan",
    "title": "Multiclass Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\n// Saved in logistic_regression.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n}\nmodel {\n  target += bernoulli_logit_lpmf(Y | alpha + X * beta);\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = bernoulli_logit_rng(alpha + height_c[i] * beta);\n    log_lik[i] = bernoulli_logit_lmpf(Y[i] | alpha + X[i, ] * beta);\n  }\n}"
  },
  {
    "objectID": "slides/13-multiclass.html#primary-biliary-cirrhosis-1",
    "href": "slides/13-multiclass.html#primary-biliary-cirrhosis-1",
    "title": "Multiclass Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nhead(pbc)\n\n  id trt      age sex ascites hepato spiders edema bili chol albumin copper\n1  1   1 58.76523   f       1      1       1   1.0 14.5  261    2.60    156\n2  2   1 56.44627   f       0      1       1   0.0  1.1  302    4.14     54\n3  3   1 70.07255   m       0      0       0   0.5  1.4  176    3.48    210\n4  4   1 54.74059   f       0      1       1   0.5  1.8  244    2.54     64\n5  5   2 38.10541   f       0      1       1   0.0  3.4  279    3.53    143\n6  7   2 55.53457   f       0      1       0   0.0  1.0  322    4.09     52\n  alk.phos    ast trig platelet protime stage outcome\n1   1718.0 137.95  172      190    12.2     4       1\n2   7394.8 113.52   88      221    10.6     3       0\n3    516.0  96.10   55      151    12.0     4       1\n4   6121.8  60.63   92      183    10.3     4       1\n5    671.0 113.15   72      136    10.9     3       1\n6    824.0  60.45  213      204     9.7     3       0"
  },
  {
    "objectID": "slides/13-multiclass.html#prepare-data-for-stan",
    "href": "slides/13-multiclass.html#prepare-data-for-stan",
    "title": "Multiclass Classification",
    "section": "Prepare data for Stan",
    "text": "Prepare data for Stan\n\nX &lt;- model.matrix(object = ~ ascites + bili + as.factor(stage), data = pbc)[, -1]\nY &lt;- pbc$outcome\nstan_data &lt;- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X)\nhead(X)\n\n  ascites bili as.factor(stage)2 as.factor(stage)3 as.factor(stage)4\n1       1 14.5                 0                 0                 1\n2       0  1.1                 0                 1                 0\n3       0  1.4                 0                 0                 1\n4       0  1.8                 0                 0                 1\n5       0  3.4                 0                 1                 0\n6       0  1.0                 0                 1                 0"
  },
  {
    "objectID": "slides/13-multiclass.html#logistic-regression-in-stan-1",
    "href": "slides/13-multiclass.html#logistic-regression-in-stan-1",
    "title": "Multiclass Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\nlibrary(rstan)\ncompiled_model &lt;- stan_model(file = \"logistic_regression.stan\")\nfit &lt;- sampling(compiled_model, data = stan_data)\nprint(fit, pars = c(\"alpha\", \"beta\"), probs = c(0.025, 0.5, 0.975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n         mean se_mean   sd  2.5%   50% 97.5% n_eff Rhat\nalpha   -3.35    0.04 1.21 -6.13 -3.24 -1.41   791    1\nbeta[1]  2.24    0.04 1.32  0.18  2.05  5.48  1252    1\nbeta[2]  0.38    0.00 0.08  0.24  0.38  0.54  2005    1\nbeta[3]  1.71    0.04 1.25 -0.34  1.59  4.55   783    1\nbeta[4]  2.19    0.04 1.22  0.20  2.07  4.96   806    1\nbeta[5]  2.61    0.04 1.23  0.59  2.48  5.46   798    1\n\nSamples were drawn using NUTS(diag_e) at Mon Dec 30 14:36:11 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/13-multiclass.html#convergence-diagnostics",
    "href": "slides/13-multiclass.html#convergence-diagnostics",
    "title": "Multiclass Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/13-multiclass.html#convergence-diagnostics-1",
    "href": "slides/13-multiclass.html#convergence-diagnostics-1",
    "title": "Multiclass Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/13-multiclass.html#back-to-the-pbc-data",
    "href": "slides/13-multiclass.html#back-to-the-pbc-data",
    "title": "Multiclass Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nFitting a logistic regression model, we obtain\n\n\n\n\n\n\n\nvariable\nmean\nsd\n2.5%\n97.5%\n\n\n\n\nalpha\nintercept\n-3.35\n0.04\n-6.13\n-1.41\n\n\nbeta[1]\nascites\n2.24\n0.04\n0.18\n5.48\n\n\nbeta[2]\nbilirubin\n0.38\n0.00\n0.24\n0.54\n\n\nbeta[3]\nstage == 2\n1.71\n0.04\n-0.34\n4.55\n\n\nbeta[4]\nstage == 3\n2.19\n0.04\n0.20\n4.96\n\n\nbeta[5]\nstage == 4\n2.61\n0.04\n0.59\n5.46\n\n\n\n\n\n\nHow might we interpret these coefficients as odds ratios?"
  },
  {
    "objectID": "slides/13-multiclass.html#back-to-the-pbc-data-1",
    "href": "slides/13-multiclass.html#back-to-the-pbc-data-1",
    "title": "Multiclass Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nRemember, we are interested in the probability that a patient died during follow-up (a “success”). We are predicting the log-odds of this event happening.\n\nThe posterior mean for ascites was 2.24. Thus, the odds ratio for dying is \\(\\exp(2.24) \\approx 9.4\\). That is, patients with ascites have 9.4 times the odds of dying compared to patients that do not, holding all other variables constant.\nThe posterior mean for bilirubin was 0.38. Thus, the odds ratio for dying for a patient with 1 additional mg/dL serum bilirubin compared to another is \\(\\exp(0.38) \\approx 1.46\\), holding all other variables constant.\nThe baseline stage was 1. The posterior mean for stage 3 was 2.19. Thus, patients in stage 3 have approximately 8.93 times the odds of dying compared to patients that do not, holding all other variables constant."
  },
  {
    "objectID": "slides/13-multiclass.html#predicted-probabilities",
    "href": "slides/13-multiclass.html#predicted-probabilities",
    "title": "Multiclass Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nThere is a one-to-one relationship between \\(\\pi\\) and \\(\\text{logit}(\\pi)\\). So, if we predict \\(\\text{logit}(\\pi)\\), we can “back-transform” to get back to a predicted probability.\nFor instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\nx_i &lt;- matrix(c(0, 5, 1, 0, 0), ncol = 1)\npars &lt;- rstan::extract(fit, pars = c(\"alpha\", \"beta\"))\nlog_odds &lt;- pars$alpha + as.numeric(pars$beta %*% x_i)\npi &lt;- exp(log_odds) / (1 + exp(log_odds))"
  },
  {
    "objectID": "slides/13-multiclass.html#predicted-probabilities-1",
    "href": "slides/13-multiclass.html#predicted-probabilities-1",
    "title": "Multiclass Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior mean of the predicted probabilities is 0.56."
  },
  {
    "objectID": "slides/13-multiclass.html#posterior-predictive-checks",
    "href": "slides/13-multiclass.html#posterior-predictive-checks",
    "title": "Multiclass Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/13-multiclass.html#posterior-predictive-checks-1",
    "href": "slides/13-multiclass.html#posterior-predictive-checks-1",
    "title": "Multiclass Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_bars(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/13-multiclass.html#posterior-predictive-checks-2",
    "href": "slides/13-multiclass.html#posterior-predictive-checks-2",
    "title": "Multiclass Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_stat(stan_data$Y, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(stan_data$Y, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(stan_data$Y, y_pred, stat = \"q025\")\nppc_stat(stan_data$Y, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/13-multiclass.html#model-comparison",
    "href": "slides/13-multiclass.html#model-comparison",
    "title": "Multiclass Classification",
    "section": "Model comparison",
    "text": "Model comparison\n\nComparing our model to a baseline that removed stage.\n\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_baseline &lt;- loo::extract_log_lik(fit_baseline, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model &lt;- loo::waic(log_lik)\nwaic_model_baseline &lt;- loo::waic(log_lik_baseline)\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"full\" = waic_model, \"baseline\" = waic_model_baseline))\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nfull        0.00      0.00 -155.94      9.51         7.69    2.09    311.88\nbaseline   -1.69      3.09 -157.63      9.36         4.86    1.98    315.25\n         se_waic\nfull       19.01\nbaseline   18.73"
  },
  {
    "objectID": "slides/13-multiclass.html#other-models-for-binary-data",
    "href": "slides/13-multiclass.html#other-models-for-binary-data",
    "title": "Multiclass Classification",
    "section": "Other models for binary data",
    "text": "Other models for binary data\n\nOther transformations (also called link functions) can be used to ensure the probabilities lie in [0, 1], including the Probit (popular in Bayesian statistics)."
  },
  {
    "objectID": "slides/13-multiclass.html#steps-to-selecting-a-bayesian-glm",
    "href": "slides/13-multiclass.html#steps-to-selecting-a-bayesian-glm",
    "title": "Multiclass Classification",
    "section": "Steps to selecting a Bayesian GLM",
    "text": "Steps to selecting a Bayesian GLM\n\nIdentify the support of the response distribution.\nSelect the likelihood by picking a parametric family of distributions with this support.\nChoose a link function \\(g\\) that transforms the range of parameters to the whole real line.\nSpecify a linear model on the transformed parameters.\nSelect priors for the regression coefficients."
  },
  {
    "objectID": "slides/13-multiclass.html#example-of-selecting-a-bayesian-glm",
    "href": "slides/13-multiclass.html#example-of-selecting-a-bayesian-glm",
    "title": "Multiclass Classification",
    "section": "Example of selecting a Bayesian GLM",
    "text": "Example of selecting a Bayesian GLM\n\nSupport: \\(Y_i \\in \\{0, 1, 2, \\ldots\\}\\).\nLikelihood family: \\(Y_i \\stackrel{ind}{\\sim} \\text{Poisson}(\\lambda_i)\\).\nLink: \\(g(\\lambda_i) = \\log(\\lambda_i) \\in (−\\infty, \\infty)\\).\nRegression model: \\(\\log(\\lambda_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nPriors: \\(\\alpha, \\beta_j \\sim N(0, 10^2)\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression",
    "href": "slides/13-multiclass.html#multinomial-regression",
    "title": "Multiclass Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\n\nUnder this formulation,\n\n\\[P(Y_i = k) = P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\]\n\nThen it follows that (since probabilities must sum to 1),\n\n\\[\\begin{aligned}\nP(Y_i = K) &= 1 - \\sum_{j=1}^{K-1} P(Y_i = j)\\\\\n&= 1 - \\sum_{j=1}^{K-1} P(Y_i = K)\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}.\\\\\n\\implies P(Y_i = K) &= \\frac{1}{1 + \\sum_{j=1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}.\n\\end{aligned}\\]\n\nThe individual probabilities are given by, \\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-1",
    "href": "slides/13-multiclass.html#multinomial-regression-1",
    "title": "Multiclass Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\n\nUnder this formulation,\n\n\\[P(Y_i = k) = P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\]\n\nThen it follows that (since probabilities must sum to 1),\n\n\\[\\begin{aligned}\nP(Y_i = K) &= 1 - \\sum_{j=1}^{K-1} P(Y_i = j)\\\\\n&= 1 - \\sum_{j=1}^{K-1} P(Y_i = K)\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}.\\\\\n\\implies P(Y_i = K) &= \\frac{1}{1 + \\sum_{j=1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}.\n\\end{aligned}\\]\n\nThe individual probabilities are given by, \\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#ordinal-regression-using-stan",
    "href": "slides/13-multiclass.html#ordinal-regression-using-stan",
    "title": "Multiclass Classification",
    "section": "Ordinal regression using Stan",
    "text": "Ordinal regression using Stan\n\n// ordinal.stan\ndata {\n  int&lt;lower = 2&gt; K;\n  int&lt;lower = 0&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1, upper = K&gt; Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  ordered[K - 1] alpha;\n}\nmodel {\n  target += ordered_logistic_glm_lpmf(Y | X, beta, alpha);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(alpha | 0, 10);\n}\ngenerated quantities {\n  vector[p] ors = exp(beta);\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = ordered_logistic_rng(X[i, ] * beta, alpha);\n    log_lik[i] = ordered_logistic_glm_lpmf(Y[i] | X[i, ], beta, alpha);\n  }\n}\n\nordered_logistic_glm_lpmf"
  },
  {
    "objectID": "slides/13-multiclass.html#prepare-for-next-class",
    "href": "slides/13-multiclass.html#prepare-for-next-class",
    "title": "Multiclass Classification",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03.\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Missing data"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-2",
    "href": "slides/13-multiclass.html#multinomial-regression-2",
    "title": "Multiclass Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nLet \\(Y_i \\in \\{1,\\ldots,K\\}\\) be an ordinal outcome with \\(K\\) categories. Then \\(P(Y \\leq k)\\) is the cumulative probability of \\(Y\\) less than or equal to a specific category \\(k=1,\\ldots,K-1\\). The odds of being less than or equal a particular category can be defined as, \\[\\frac{P(Y\\leq k)}{P(Y &gt; k)}\\] for \\(k=1,\\ldots,K-1\\), since \\(P(Y &gt; K) = 0\\) and dividing by zero is undefined. The log odds is also known as the logit, so that \\[\\log \\frac{P(Y\\leq k)}{P(Y &gt; k)} = \\text{logit}P(Y\\leq k) = \\alpha_k + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nProportional odds regression."
  },
  {
    "objectID": "slides/13-multiclass.html#ordinal-regression",
    "href": "slides/13-multiclass.html#ordinal-regression",
    "title": "Multiclass Classification",
    "section": "Ordinal regression",
    "text": "Ordinal regression\nLet \\(Y_i \\in \\{1,\\ldots,K\\}\\) be an ordinal outcome with \\(K\\) categories.\n\nThe likelihood in ordinal regression is identical to the one from multinomial regression,\n\n\\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}}.\\]\n\nWe need to add additional constraints that guarantee ordinality."
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-likelihood",
    "href": "slides/13-multiclass.html#multinomial-likelihood",
    "title": "Multiclass Classification",
    "section": "Multinomial likelihood",
    "text": "Multinomial likelihood"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-in-stan",
    "href": "slides/13-multiclass.html#multinomial-regression-in-stan",
    "title": "Multiclass Classification",
    "section": "Multinomial regression in Stan",
    "text": "Multinomial regression in Stan\n\nHard coding the likelihood.\n\n\n// additive_log_ratio.stan\nfunctions {\n  matrix compute_alr_probs(int n, int K, int p, matrix X, matrix beta) {\n    matrix[n, K] probs;\n    matrix[n, K - 1] expXbeta = exp(X * beta);\n    for (i in 1:n) {\n      real sum_i = sum(expXbeta[i, ]);\n      for (j in 1:K) {\n        if (j &lt; K) {\n          probs[i, j] = expXbeta[i, j] / (1 + sum_i);\n        }\n        if (j == K) probs[i, j] = 1 - sum(probs[i, 1:(K - 1)]);\n      }\n    }\n    return probs\n  }\n}\ndata {\n  int&lt;lower = 1&gt; K;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  array[n] int&lt;lower = 1, upper = K&gt; Y;\n  matrix[n, p] X;\n  matrix[n, K] delta;\n}\nparameters {\n  matrix[p, K - 1] beta;\n}\nmodel {\n  matrix[n, K] probs = compute_alr_probs(n, K, p, X, beta);\n  for (i in 1:n) {\n    for (j in 1:K) {\n      target += delta[i, j] * log(probs[i, j]);\n    }\n  }\n  target += normal_lpdf(to_vector(beta) | 0, 10);\n}"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-in-stan-2",
    "href": "slides/13-multiclass.html#multinomial-regression-in-stan-2",
    "title": "Multiclass Classification",
    "section": "Multinomial regression in Stan",
    "text": "Multinomial regression in Stan\n\nZero identifiability constraint.\n\n\n// multi_logit.stan\ndata {\n  int&lt;lower = 1&gt; K;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  array[n] int&lt;lower = 1, upper = K&gt; Y;\n  matrix[n, p] X;\n}\ntransformed data {\n  vector[p] zeros = rep_vector(0, p);\n}\nparameters {\n  matrix[p, K - 1] beta_raw;\n}\ntransformed parameters {\n  matrix[p, K] beta = append_col(beta_raw, zeros);\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    target += categorical_logit_lpmf(Y[i] | Xbeta[i]')\n  }\n  target += normal_lpdf(to_vector(beta) | 0, 10);\n}\ngenerated quantities {\n  matrix[p, K - 1] ors = exp(beta_raw);\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = categorical_logit_rng(Xbeta[i]');\n    log_lik[i] = categorical_logit_lpmf(Y[i] | Xbeta[i]');\n  }\n}"
  },
  {
    "objectID": "slides/13-multiclass.html#m",
    "href": "slides/13-multiclass.html#m",
    "title": "Multiclass Classification",
    "section": "M",
    "text": "M\n\nThe most common form of multinomial regression is motivated through \\(K - 1\\) independent binary logistic regression models, where one outcome is chosen as the reference.\nIf outcome \\(K\\) is chosen as reference, the \\(K − 1\\) regression equations are:\n\n\\[\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}\\]\n\nThis formulation is called additive log ratio."
  },
  {
    "objectID": "slides/13-multiclass.html#a",
    "href": "slides/13-multiclass.html#a",
    "title": "Multiclass Classification",
    "section": "a",
    "text": "a\n\nThe log-likelihood can be written as,\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\sum_{j=1}^K \\delta_{ij} \\log P(Y_i = j).\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-random-variable",
    "href": "slides/13-multiclass.html#multinomial-random-variable",
    "title": "Multiclass Classification",
    "section": "Multinomial random variable",
    "text": "Multinomial random variable\n\nAssume an outcome \\(Y_i \\in \\{1,\\ldots,K\\}\\) for \\(i = 1,\\ldots,n\\).\nThe likelihood in multinomial regression can be written as the following categorical likelihood,\n\n\\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}},\\]\n\n\\(\\delta_{ij} = 1(Y_i = j)\\) is the Kronecker delta.\nSince \\(Y_i\\) is discrete, we only need to specify \\(P(Y_i = j)\\) for all \\(i\\) and \\(j\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#log-linear-regression",
    "href": "slides/13-multiclass.html#log-linear-regression",
    "title": "Multiclass Classification",
    "section": "Log-linear regression",
    "text": "Log-linear regression\n\nOne way to motivate multinomial regression is through a log-linear specification:\n\n\\[\\log P(Y_i = j) = \\mathbf{x}_i\\boldsymbol{\\beta}_j - \\log Z.\\]\n\n\\(\\boldsymbol{\\beta}_j\\) is a \\(j\\) specific set of regression parameters.\n\\(P(Y_i = j) = \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}/Z\\).\n\\(Z\\) is a normalizing constant that guarentees that \\(\\sum_{j=1}^K P(Y_i = j) = 1\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#adsf",
    "href": "slides/13-multiclass.html#adsf",
    "title": "Multiclass Classification",
    "section": "adsf",
    "text": "adsf\n\nThe log-likelihood can be written as,\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\sum_{j=1}^K \\delta_{ij} \\log P(Y_i = j).\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#finding-the-normalizing-constant",
    "href": "slides/13-multiclass.html#finding-the-normalizing-constant",
    "title": "Multiclass Classification",
    "section": "Finding the normalizing constant",
    "text": "Finding the normalizing constant\n\nWe know that,\n\n\\[\\begin{aligned}\n1 &= \\sum_{j=1}^K P(Y_i = j) = \\frac{1}{Z}\\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\\\\\n&\\implies Z = \\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-probabilities",
    "href": "slides/13-multiclass.html#multinomial-probabilities",
    "title": "Multiclass Classification",
    "section": "Multinomial probabilities",
    "text": "Multinomial probabilities\n\nThus, we have the following,\n\n\\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\\]\n\nThis function is called the softmax function.\nUnfortunately, this specification is not identifiable."
  },
  {
    "objectID": "slides/13-multiclass.html#identifiability-issue",
    "href": "slides/13-multiclass.html#identifiability-issue",
    "title": "Multiclass Classification",
    "section": "Identifiability issue",
    "text": "Identifiability issue\n\nWe can add a vector \\(\\mathbf{c}\\) to all parameters and get the same result,\n\n\\[\\begin{aligned}\n\\frac{\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_k + \\mathbf{c})\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_j + \\mathbf{c})\\}} &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n\\end{aligned}\\]\n\nA common solution is to set: \\(\\boldsymbol{\\beta}_K = \\mathbf{0}\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#updating-the-probabilities",
    "href": "slides/13-multiclass.html#updating-the-probabilities",
    "title": "Multiclass Classification",
    "section": "Updating the probabilities",
    "text": "Updating the probabilities\n\nUsing the identifiability constraint of \\(\\boldsymbol{\\beta}_K = \\mathbf{0}\\), the probabilities become, \\[\\begin{aligned}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\},\\\\\nP(Y_i = K) &= \\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n\\end{aligned}\\]\nHow to interpret the \\(\\boldsymbol{\\beta}_k\\)?"
  },
  {
    "objectID": "slides/13-multiclass.html#deriving-the-log-additive-ratio-model",
    "href": "slides/13-multiclass.html#deriving-the-log-additive-ratio-model",
    "title": "Multiclass Classification",
    "section": "Deriving the log-additive ratio model",
    "text": "Deriving the log-additive ratio model\n\nUsing our specification of the probabilities, it can be seen that,\n\n\\[\\begin{aligned}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\\\\n&= \\left[\\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\right]\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}\\\\\n&= P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}.\n\\end{aligned}\\]\n\\(\\implies \\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots,K-1\\}\\)"
  },
  {
    "objectID": "slides/13-multiclass.html#deriving-the-additive-log-ratio-model",
    "href": "slides/13-multiclass.html#deriving-the-additive-log-ratio-model",
    "title": "Multiclass Classification",
    "section": "Deriving the additive log ratio model",
    "text": "Deriving the additive log ratio model\n\nUsing our specification of the probabilities, it can be seen that,\n\n\\[\\begin{aligned}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\\\\n&= \\left[\\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\right]\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}\\\\\n&= P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}.\n\\end{aligned}\\]\n\\(\\implies \\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots,K-1\\}\\)"
  },
  {
    "objectID": "slides/13-multiclass.html#additive-log-ratio-model",
    "href": "slides/13-multiclass.html#additive-log-ratio-model",
    "title": "Multiclass Classification",
    "section": "Additive log ratio model",
    "text": "Additive log ratio model\n\nIf outcome \\(K\\) is chosen as reference, the \\(K − 1\\) regression equations are:\n\n\\[\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}\\]\n\nThis formulation is called additive log ratio.\n\\(\\beta_{jk}\\) represent the log-odds of being in category \\(k\\) relative to the baseline category \\(K\\) with a one-unit change in \\(X_{ij}\\).\n\n\\(\\exp (\\beta_{jk})\\) is an odds ratio."
  },
  {
    "objectID": "slides/13-multiclass.html#getting-back-to-the-likelihood",
    "href": "slides/13-multiclass.html#getting-back-to-the-likelihood",
    "title": "Multiclass Classification",
    "section": "Getting back to the likelihood",
    "text": "Getting back to the likelihood\n\nThe log-likelihood can be written as,\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\sum_{j=1}^K \\delta_{ij} \\log P(Y_i = j).\\]\n\nThe \\(P(Y_i = j)\\) are given by the additive log ratio model.\nAs Bayesians, we only need to specify priors for \\(\\boldsymbol{\\beta}_k, k \\in \\{1,\\ldots,K-1\\}\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-in-stan-1",
    "href": "slides/13-multiclass.html#multinomial-regression-in-stan-1",
    "title": "Multiclass Classification",
    "section": "Multinomial regression in Stan",
    "text": "Multinomial regression in Stan\n\nNon-identifiable version.\n\n\n// multi_logit_bad.stan\ndata {\n  int&lt;lower = 1&gt; K;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  array[n] int&lt;lower = 1, upper = K&gt; Y;\n  matrix[n, p] X;\n}\nparameters {\n  matrix[p, K] beta;\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    target += categorical_logit_lpmf(Y[i] | Xbeta[i]')\n  }\n  target += normal_lpdf(to_vector(beta) | 0, 10);\n}\n\ncategorical_logit"
  },
  {
    "objectID": "slides/13-multiclass.html#multiclass-regression",
    "href": "slides/13-multiclass.html#multiclass-regression",
    "title": "Multiclass Classification",
    "section": "Multiclass regression",
    "text": "Multiclass regression\n\nOften times one encounters an outcome variable that is nominal and has more than two categories.\nIf there is no inherent rank or order to the variable, we can use multinomial regression. Examples include:\n\ngender (male, female, non-binary),\nblood type (A, B, AB, O).\n\nIf there is an order to the variable, we can use ordinal regression. Examples include:\n\nstages of cancer (stage I, II, III, IV),\npain level (mild, moderate, severe)."
  },
  {
    "objectID": "slides/13-multiclass.html#ordinal-regression-1",
    "href": "slides/13-multiclass.html#ordinal-regression-1",
    "title": "Multiclass Classification",
    "section": "Ordinal regression",
    "text": "Ordinal regression\nThe log odds is also known as the logit, so that \\[\\log \\frac{P(Y\\leq k)}{P(Y &gt; k)} = \\text{logit}P(Y\\leq k) = \\alpha_k + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nProportional odds regression."
  },
  {
    "objectID": "slides/13-multiclass.html#proportional-odds-assumption",
    "href": "slides/13-multiclass.html#proportional-odds-assumption",
    "title": "Multiclass Classification",
    "section": "Proportional odds assumption",
    "text": "Proportional odds assumption\n\n\\(P(Y_i \\leq k)\\) is the cumulative probability of \\(Y_i\\) less than or equal to a specific category \\(k=1,\\ldots,K-1\\).\nThe odds of being less than or equal to a particular category can be defined as, \\[\\frac{P(Y\\leq k)}{P(Y &gt; k)} \\text { for } k=1,\\ldots,K-1.\\]\nNot defined for \\(k = K\\), since division by zero is not defined."
  },
  {
    "objectID": "slides/13-multiclass.html#proportional-odds-assumption-1",
    "href": "slides/13-multiclass.html#proportional-odds-assumption-1",
    "title": "Multiclass Classification",
    "section": "Proportional odds assumption",
    "text": "Proportional odds assumption\nThe log odds can then be modeled as follows, \\[\\log \\frac{P(Y\\leq k)}{P(Y &gt; k)} = \\text{logit}P(Y\\leq k) = \\alpha_k + \\mathbf{x}_i \\boldsymbol{\\beta}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#proportional-odds-regression",
    "href": "slides/13-multiclass.html#proportional-odds-regression",
    "title": "Multiclass Classification",
    "section": "Proportional odds regression",
    "text": "Proportional odds regression\nThe log odds can then be modeled as follows, \\[\\log \\frac{P(Y_i\\leq k)}{P(Y_i &gt; k)} = \\text{logit}P(Y_i\\leq k) = \\alpha_k - \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nWhy \\(-\\boldsymbol{\\beta}\\)?\n\\(\\boldsymbol{\\beta}\\) is a common regression parameter. For a one-unit increase in \\(x_{ij}\\), \\(\\beta_j\\) is the change in log odds of moving to a more severe level of the outcome \\(Y_i\\).\n\\(\\alpha_k\\) for \\(k = 1,\\ldots,K-1\\) are \\(k\\)-specific intercepts that corresponds to the log odds of moving from level \\(k\\) to \\(k+1\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#a-latent-variable-representation",
    "href": "slides/13-multiclass.html#a-latent-variable-representation",
    "title": "Multiclass Classification",
    "section": "A latent variable representation",
    "text": "A latent variable representation\n\nDefine a latent variable,\n\n\\[Y_i^* = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim \\text{Logistic}(0, 1).\\]\n\n\\(\\mathbb{E}[\\epsilon_i] = 0\\).\n\\(\\mathbb{V}(\\epsilon_i) = \\pi^2/3\\).\nCDF: \\(P(\\epsilon_i \\leq x) = \\frac{1}{1 + \\exp\\{-x\\}} = \\frac{\\exp\\{x\\}}{1 + \\exp\\{x\\}} = \\text{expit}(x).\\)"
  },
  {
    "objectID": "slides/13-multiclass.html#a-latent-variable-representation-1",
    "href": "slides/13-multiclass.html#a-latent-variable-representation-1",
    "title": "Multiclass Classification",
    "section": "A latent variable representation",
    "text": "A latent variable representation\n\nDefine a set of \\(K-1\\) cut-points, \\((c_1,\\ldots,c_{K-1}) \\in \\mathbb{R}^{K-1}\\). We also define \\(c_0 = -\\infty, c_K = \\infty\\).\nOur ordinal random variable can be generated as,\n\n\\[Y_i = \\left\\{\n\\begin{matrix*}[l]\n1 & c_0 &lt; Y_i^* \\leq c_1\\\\\n2 & c_1 &lt; Y_i^* \\leq c_2\\\\\n\\vdots & \\\\\nK & c_{K-1} &lt; Y_i^* \\leq c_K\\\\\n\\end{matrix*}\n\\right.\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#induced-pmf-on-y_i",
    "href": "slides/13-multiclass.html#induced-pmf-on-y_i",
    "title": "Multiclass Classification",
    "section": "Induced pmf on \\(Y_i\\)",
    "text": "Induced pmf on \\(Y_i\\)\n\\[\\begin{aligned}\nP(Y_i = k) &= P(c_{k-1} &lt; Y_i^* \\leq c_k)\\\\\n&= P(c_{k-1} &lt; \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k)\\\\\n&= P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k) - P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i &lt; c_{k-1})\\\\\n&= P(\\epsilon_i \\leq c_k - \\mathbf{x}_i \\boldsymbol{\\beta}) - P(\\epsilon_i &lt; c_{k-1} - \\mathbf{x}_i \\boldsymbol{\\beta})\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#understanding-the-probabilities",
    "href": "slides/13-multiclass.html#understanding-the-probabilities",
    "title": "Multiclass Classification",
    "section": "Understanding the probabilities",
    "text": "Understanding the probabilities\n\nOne can solve for \\(P(Y_i \\leq k), \\quad k = 1,\\ldots,K-1\\)),\n\n\\[P(Y_i \\leq k) = \\frac{\\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}}{1 + \\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}} = \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}).\\]\n\n\\(P(Y_i \\leq K) = 1\\).\n\nThe individual probabilities are then given by,\n\\[\\begin{aligned}\nP(Y_i = k) &= P(Y_i \\leq k) - P(Y_i \\leq k-1)\\\\\n&= \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(\\alpha_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#equivalence-of-the-two-specifications",
    "href": "slides/13-multiclass.html#equivalence-of-the-two-specifications",
    "title": "Multiclass Classification",
    "section": "Equivalence of the two specifications",
    "text": "Equivalence of the two specifications\n\nProbabilities under the latent specification: \\[\\begin{aligned}\nP(Y_i = k) &= P(c_{k-1} &lt; Y_i^* \\leq c_k)\\\\\n&= P(c_{k-1} &lt; \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k)\\\\\n&= P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k) - P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i &lt; c_{k-1})\\\\\n&= P(\\epsilon_i \\leq c_k - \\mathbf{x}_i \\boldsymbol{\\beta}) - P(\\epsilon_i &lt; c_{k-1} - \\mathbf{x}_i \\boldsymbol{\\beta})\\\\\n&= \\text{expit}(c_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(c_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n\\end{aligned}\\]\nEquivalency:\n\n\\(\\alpha_k = c_k, \\quad k = 1,\\ldots, K-1\\), assuming that \\(\\alpha_k &lt; \\alpha_{k+1}\\)."
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html",
    "href": "ae/ae-01-monte-carlo.html",
    "title": "AE 01: Monte Carlo sampling",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE is a demonstration and you do not have to turn anything in!\n\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#learning-goals",
    "href": "ae/ae-01-monte-carlo.html#learning-goals",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nPerform some Monte Carlo estimation"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-01-monte-carlo.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-01-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone using the web URL.). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick AE 01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#r-and-r-studio",
    "href": "ae/ae-01-monte-carlo.html#r-and-r-studio",
    "title": "AE 01: Monte Carlo sampling",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we may tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#exercise-1",
    "href": "ae/ae-01-monte-carlo.html#exercise-1",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Exercise 1",
    "text": "Exercise 1\nCompute a one-sided Bayesian p-value for \\(\\mu\\): \\(P(\\mu &lt; 14 | \\mathbf{Y})\\) using Monte Carlo sampling. Interpret the results in plain English.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#exercise-2",
    "href": "ae/ae-01-monte-carlo.html#exercise-2",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe can also compute \\(P(\\mu &lt; 14 | \\mathbf{Y})\\) using pnorm, since we have the posterior in closed form. Compute the exact probability and compare it to the Monte Carlo estimate given in Exercise 1.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#exercise-3",
    "href": "ae/ae-01-monte-carlo.html#exercise-3",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute a 95% confidence interval for \\(\\mu\\). Provide an interpretation for this confidence interval.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉\n\nRecall: This AE is a demonstration and nothing needs to be turned in!"
  },
  {
    "objectID": "ae/ae-01-mcmc.html",
    "href": "ae/ae-01-mcmc.html",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE is due on Sunday, January 19 at 11:59pm. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-01-mcmc.html#learning-goals",
    "href": "ae/ae-01-mcmc.html#learning-goals",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nPerform Gibbs sampling for Bayesian linear regression and compute some basic summaries"
  },
  {
    "objectID": "ae/ae-01-mcmc.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-01-mcmc.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-01-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-01-mcmc.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-01-mcmc.html#r-and-r-studio",
    "href": "ae/ae-01-mcmc.html#r-and-r-studio",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we may tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!"
  },
  {
    "objectID": "ae/ae-01-mcmc.html#exercise-1",
    "href": "ae/ae-01-mcmc.html#exercise-1",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "Exercise 1",
    "text": "Exercise 1\nCompute the posterior mean and standard deviation for the intercept, slope, and measurement error. Provide an interpretation for each of these parameter estimates.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-mcmc.html#exercise-2",
    "href": "ae/ae-01-mcmc.html#exercise-2",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "Exercise 2",
    "text": "Exercise 2\nCompute a 95% confidence interval for the regression slope. Provide an interpretation for this confidence interval.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-mcmc.html#exercise-3",
    "href": "ae/ae-01-mcmc.html#exercise-3",
    "title": "AE 01: Posterior estimation using Gibbs sampling",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute a one-sided Bayesian p-value for the regression slope: \\(P(\\beta_1 &lt; 0)\\). Interpret the results in plain English. Is intraocular pressure associated with disease progression?\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "hw/hw-01-creation.html#exercises-1-4",
    "href": "hw/hw-01-creation.html#exercises-1-4",
    "title": "HW 01: Bayesian principles",
    "section": "Exercises 1-4",
    "text": "Exercises 1-4\nDefine a random variable \\(Y_i\\) that represents the number of hospital visits during pregnancy for each woman \\(i\\). Assume that this random variable follows a Poisson distribution with rate \\(\\lambda\\), \\(Y_i \\stackrel{iid}{\\sim} \\text{Poisson}(\\lambda)\\). For a Poisson distribution, the mean and variance of the are equal to \\(\\lambda\\). We are interested in performing statistical inference on \\(\\lambda\\) using a Bayesian approach. A frequently used prior for \\(\\lambda\\) is \\(\\text{Gamma}(\\text{shape = }a, \\text{rate = }b)\\), where \\(\\mathbb{E}[\\lambda] = a/b\\) and \\(\\mathbb{V}(\\lambda) = a/b^2\\).\n\nExercise 1\nThe researchers have prior knowledge that leads them to believe that \\(\\lambda\\) should have mean 8 and variance 4. What values of \\(a\\) and \\(b\\) should they specify?\n\nAnswer: If \\(\\mu\\) is the mean of \\(\\lambda\\) and \\(\\sigma^2\\) is the variance, it is easy to see that \\(a = \\mu^2/\\sigma^2\\) and \\(b=\\mu/\\sigma^2\\). To obtain a prior distribution with mean 8 (i.e., \\(\\mu=8\\)) and variance 4 (i.e., \\(\\sigma^2=4\\)), they would need to specify \\(a=16\\) and \\(b=2\\).\n\n\n\nExercise 2\nUsing the prior specified in Exercise 1, what is the a priori probability that \\(\\lambda\\) is greater than 11? This is equivalent to computing \\(P(\\lambda &gt; 11)\\). To find your answer use Monte Carlo sampling. Report your Monte Carlo standard error and make sure it is less than 0.01.\n\nAnswer:\n\nWe can use Monte Carlo sampling for this. Below I take \\(S=50,000\\) samples from the prior distribution and compute the \\(P(\\lambda &gt; 11)\\).\n\nprior_samples &lt;- rgamma(50000, shape = 16, rate = 2)\nmean(prior_samples &gt; 11)\n\n[1] 0.07714\n\n\nI also verify that the Monte Carlo standard error is smaller than 0.01.\n\nsqrt(var(prior_samples) / length(prior_samples))\n\n[1] 0.008941763\n\n\n\n\nExercise 3\nUsing the prior specified in Exercise 1, compute the posterior distribution for \\(\\lambda\\), \\(f(\\lambda | \\mathbf{Y})\\), where \\(\\mathbf{Y}_i = (Y_1,\\ldots,Y_n).\\) Recall that the Gamma prior for \\(\\lambda\\) is a conjugate prior, so that the posterior is given by: \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). For this analysis, remove missing values of visits. Visualize the posterior distribution and report the posterior mean and a 95% credible interval. Provide an interpretation of the posterior distribution within the context of the US births data.\n\nAnswer:\n\nI will start by obtaining \\(S=50,000\\) samples from the posterior distribution and plotting the samples using a histogram with a density plot (both are not necessary).\n\nvisits &lt;- births14$visits[!is.na(births14$visits)] # remove missing values\nn &lt;- length(visits)\nsum_y &lt;- sum(visits)\nposterior_samples &lt;- rgamma(50000, shape = 16 + sum_y, rate = 2 + n)\ndata.frame(samples = posterior_samples) |&gt; ggplot(aes(x = samples)) + \n  geom_histogram(aes(y = ..density..),\n                 colour = 1, fill = \"white\") +\n  geom_density(lwd = 1.2,\n               linetype = 1,\n               colour = \"black\") + \n  labs(x = expression(lambda),\n       y = \"Density\",\n       title = expression(paste(\"Posterior distribution of \", lambda)),\n       subtitle = \"Rate for number of hospital visits in 2014 for each woman\")\n\n\n\n\n\n\n\n\nThe posterior mean and 95% credible interval can be computed as follows.\n\nsummaries &lt;- data.frame(mean = mean(posterior_samples),\n                        lower = quantile(posterior_samples, probs = 0.025),\n                        upper = quantile(posterior_samples, probs = 0.975))\nrownames(summaries) &lt;- \"Posterior Estimates\"\nsummaries |&gt; kable(row.names = TRUE,\n                   col.names = c(\"Mean\", \"2.5%\", \"97.5%\"),\n                   digits = 2,\n                   escape = FALSE)\n\n\n\n\n\nMean\n2.5%\n97.5%\n\n\n\n\nPosterior Estimates\n11.34\n11.13\n11.56\n\n\n\n\n\nThrough this analysis, we estimated the posterior mean number of hospital visits for women during their pregnancies in the United States in 2014 to be 11.34. The 95% credible interval was (11.13, 11.56), indicating that the posterior probability that \\(\\lambda\\) falls between 11.13 and 11.56 is 95%.\n\n\nExercise 4\nWhat is the posterior probability that \\(\\lambda\\) is greater than 11? This is equivalent to computing \\(P(\\lambda &gt; 11 | \\mathbf{Y})\\). Again, use Monte Carlo sampling. Provide an interpretation for this probability in the context of hospital visits.\n\nAnswer: The posterior probability can be computed using the posterior samples computed in Exercise 3.\n\n\nmean(posterior_samples &gt; 11)\n\n[1] 0.99932\n\n\nThe posterior probability that \\(\\lambda\\) is greater than 11 is equal to 1. This indicates that in 2014, the mean number of hospital visits per pregnancy was almost certainly greater than 11, with a probability of approximately 1.\n\n\nExercise 5\nCreate a figure that includes both the prior and posterior distributions for \\(\\lambda\\). Also, include a figure of the observed data. Use these figures to make a comparison of the prior and posterior probabilities found in Exercise 2 and Exercise 4, respectively. Describe any changes in these two probabilities within the context of the observed data.\n\nAnswer: We can plot the prior and posterior distributions, along with a figure of the observed data.\n\nn_seq &lt;- 1000\nx &lt;- seq(0, 30, length.out = n_seq)\nprior_dens &lt;- dgamma(x, shape = 16, rate = 2)\nposterior_dens &lt;- dgamma(x, shape = 16 + sum_y, rate = 2 + n)\ndat.fig &lt;- data.frame(x = rep(x, 2), \n                      y = c(prior_dens, posterior_dens),\n                      Type = rep(c(\"Prior\", \"Posterior\"), each = n_seq)\n)\nggplot(dat.fig, aes(x = x, y = y, color = Type)) + \n  geom_line(linewidth = 1.25) + \n  labs(x = expression(pi), \n       y = \"Density\", \n       subtitle = \"Prior and posterior distributions with the following prior: Gamma(a = 16, b = 2)\") + \n  theme(legend.position = \"right\")\nggplot(births14, aes(x = visits)) + \n  geom_histogram(aes(y = ..density..),\n                 colour = 1, fill = \"white\") +\n  geom_density(lwd = 1.2,\n               linetype = 1,\n               colour = \"black\") + \n  labs(x = \"Number of hospital visits per pregnancy\",\n       y = \"Density\",\n       subtitle = \"Visualization of number of visits per pregnancy in 2014\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prior probability of \\(\\lambda\\) being greater than 11 was equal to 0.08. The posterior probability is 1. The posterior value is nearly 1, as opposed to a prior probability much closer to 0. This makes sense in light of the above figures. In particular, since the observed data has a large sample size (n = 944) the posterior is going to be pulled towards the likelihood. Furthermore, since the mean of the observered data is 11.35, combined with the large sample size, it makes sense that the posterior distribution is pulled above 11."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-2-1",
    "href": "hw/hw-01-creation.html#exercise-2-1",
    "title": "HW 01: Bayesian principles",
    "section": "Exercise 2",
    "text": "Exercise 2\nBased on the slides from the Monte Carlo sampling lecture,\nWhat is the posterior mean and the 95% credible interval for the \\(\\lambda\\)?\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "hw/hw-02-creation.html",
    "href": "hw/hw-02-creation.html",
    "title": "HW 02: Bayesian linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis assignment is due on Thursday, February 13 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-1",
    "href": "hw/hw-02-creation.html#exercise-1",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nSetup a multivariable linear regression to estimate the association between access to recreational facilities and exercise, making sure to allow for this relationship to change based on crime. Be sure to control for the following confounders: age, marital status, and race. Define the random variable \\(Y_i\\) as the exercise in MET minutes per week for women \\(i\\) and assume that \\(Y_i \\stackrel{ind}{\\sim} N(\\mu_i, \\sigma^2)\\) for \\(i = 1,\\ldots,n\\) where \\[\\begin{align*}\n\\mu_i &= \\alpha + recreation_i \\beta_1 + crime_i \\beta_2+(recreation_i \\times crime_i) \\beta_3\\\\\n&\\quad+ age_i \\beta_4 + black_i \\beta_5 + asian_i \\beta_6 + married_i \\beta_7\\\\\n&= \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta}.\n\\end{align*}\\]\nFit this regression using a Bayesian framework in Stan to estimate \\((\\alpha, \\boldsymbol{\\beta},\\sigma)\\). For all model parameters, choose weakly-informative priors, \\(\\alpha \\sim N(0,100)\\), \\(\\beta_j \\sim N(0,100)\\) for \\(j=1,\\ldots,p\\), and \\(\\sigma \\sim \\text{Half-Normal}(0,100)\\).\nEvaluate model convergence and present posterior predictive checks. Provide an argument for whether the model fits the data well."
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-2",
    "href": "hw/hw-02-creation.html#exercise-2",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nRefit the model in Exercise 1, this time using centered outcome and predictor variables, \\(Y_i^* \\stackrel{ind}{\\sim} N(\\mu_i, \\sigma^2)\\), where \\(\\mu_i^* = \\alpha + \\mathbf{x}_i^*\\boldsymbol{\\beta}\\). The centered data are defined as, \\(Y_i^* = Y_i - \\overline{Y}\\), where \\(\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\) and \\(\\mathbf{x}_i^* = \\mathbf{x}_i-\\bar{\\mathbf{x}}\\), where \\(\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\\). Use the same priors as before. Once again, evaluate model convergence and present posterior predictive checks. Provide an argument for whether the model fits the data well and make a comparison to the model from Exercise 1. Make an argument for why this model may have improved model fit."
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-3",
    "href": "hw/hw-02-creation.html#exercise-3",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nFor the model from Exercise 2, present the posterior means, standard deviations, and 95% credible intervals for all model parameters. What is interpretation of the slope main effect corresponding to recreation in your model? Within the context of the association between access to recreational facilities and exercise, is this main effect parameter useful to interpret?"
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-4",
    "href": "hw/hw-02-creation.html#exercise-4",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the association between access to recreational facilities and exercise, for a pregnant women living in an area with 5 annual crimes/1,000 people? What about for 15 annual crimes/1,000 people? Provide posterior mean and standard deviations for both quantities."
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-5",
    "href": "hw/hw-02-creation.html#exercise-5",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret the posterior mean and standard deviations from Exercise 4 and compare and contrast them. What do these posterior slopes say about the impact of crime on the relationship between access to recreational facilities and exercise."
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-6",
    "href": "hw/hw-02-creation.html#exercise-6",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nResearchers are interested in the level of crime where the association between recreational facilities and exercise disappears. Present the posterior median and interquartile range (i.e., 25% and 75% percentiles) for this quantity."
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-7",
    "href": "hw/hw-02-creation.html#exercise-7",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nCompute the posterior predictive distribution for a patient with 10 recreational facilities within a one-mile radius, 5 crimes within a one-mile buffer per 1,000 people, is 40 years old, married, and white race. Report the posterior mean and 95% credible intervals."
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-8",
    "href": "hw/hw-02-creation.html#exercise-8",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nResearchers are interested in comparing their original model (i.e., the one from Exercise 2) with a model that does not contain an interaction term between recreational facility access and crime. Fit the model without the interaction term and perform a model comparison between the two models using an information criteria. Which model would you suggest as more scientifically plausible?"
  },
  {
    "objectID": "hw/hw-01-creation.html#exercises-1-5",
    "href": "hw/hw-01-creation.html#exercises-1-5",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercises 1-5",
    "text": "Exercises 1-5\nDefine a random variable \\(Y_i\\) that represents the number of hospital visits during pregnancy for each woman \\(i\\), for \\(i = 1,\\ldots,n\\). Assume that this random variable follows a Poisson distribution with rate \\(\\lambda\\), such that \\(Y_i \\stackrel{iid}{\\sim} \\text{Poisson}(\\lambda)\\). For a Poisson distribution, the mean and variance are equal to \\(\\lambda\\). We are interested in performing statistical inference on \\(\\lambda\\) using a Bayesian approach. A frequently used prior for \\(\\lambda\\) is \\(\\text{Gamma}(\\text{shape = }a, \\text{rate = }b)\\), where \\(\\mathbb{E}[\\lambda] = a/b\\) and \\(\\mathbb{V}(\\lambda) = a/b^2\\).\n\nExercise 1\nThe researchers have prior knowledge that leads them to believe that \\(\\lambda\\) should have mean 8 and variance 4. What values of \\(a\\) and \\(b\\) should they specify?\n\n\nExercise 2\nUsing the prior specified in Exercise 1, compute the probability that \\(\\lambda\\) is greater than 11? For this computation compute the exact probability using the pgamma function in R. This is equivalent to computing \\(P(\\lambda &gt; 11)\\).\n\n\nExercise 3\nCompute the same probability as in Exercise 2, this time using Monte Carlo sampling. Report your Monte Carlo standard error and make sure it is less than 0.01.\n\n\nExercise 4\nSuppose the researchers are interested in the quantity, \\(\\alpha = \\sqrt{\\lambda}\\). Compute the probability that \\(\\alpha\\) is greater than 2.5. Use the same number of Monte Carlo samples as in Exercise 3 and describe why Monte Carlo sampling is needed to compute this probability.\n\n\nExercise 5\nUsing the prior specified in Exercise 1, compute the posterior distribution for \\(\\lambda\\), \\(f(\\lambda | \\mathbf{Y})\\), where \\(\\mathbf{Y}_i = (Y_1,\\ldots,Y_n).\\) Recall that the Gamma prior for \\(\\lambda\\) is a conjugate prior, so that the posterior is given by: \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). Visualize the posterior distribution and report the posterior mean and a 95% credible interval. Provide an interpretation of the posterior summaries within the context of the US births data.\n\n\nExercise 6\nWhat is the posterior probability that \\(\\lambda\\) is greater than 11? This is equivalent to computing \\(P(\\lambda &gt; 11 | \\mathbf{Y})\\). Again, use Monte Carlo sampling. Provide an interpretation for this probability in the context of hospital visits.\n\n\nExercise 7\nCreate a figure that includes both the prior and posterior distributions for \\(\\lambda\\). Also, include a figure of the observed data. Use these figures to make a comparison of the prior and posterior probabilities found in Exercise 3 and Exercise 6, respectively. Describe any changes in these two probabilities and how they relate to the observed data."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-6-10",
    "href": "hw/hw-01-creation.html#exercise-6-10",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercise 6-10",
    "text": "Exercise 6-10\nDefine a random variable \\(Y_i\\) that represents the weight of the baby at birth in ounces \\(i\\) for pregnancy \\(i\\). We are interested in \\[weight_i = \\beta_0 + \\]\nAssume that this random variable follows a Poisson distribution with rate \\(\\lambda\\), \\(Y_i \\stackrel{iid}{\\sim} \\text{Poisson}(\\lambda)\\). For a Poisson distribution, the mean and variance of the are equal to \\(\\lambda\\). We are interested in performing statistical inference on \\(\\lambda\\) using a Bayesian approach. A frequently used prior for \\(\\lambda\\) is \\(\\text{Gamma}(\\text{shape = }a, \\text{rate = }b)\\), where \\(\\mathbb{E}[\\lambda] = a/b\\) and \\(\\mathbb{V}(\\lambda) = a/b^2\\).\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercises-1-7",
    "href": "hw/hw-01-creation.html#exercises-1-7",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercises 1-7",
    "text": "Exercises 1-7\nDefine a random variable \\(Y_i\\) that represents the number of hospital visits during pregnancy for each woman \\(i\\), for \\(i = 1,\\ldots,n\\). Assume that this random variable follows a Poisson distribution with rate \\(\\lambda\\), such that \\(Y_i \\stackrel{iid}{\\sim} \\text{Poisson}(\\lambda)\\). For a Poisson distribution, the mean and variance are equal to \\(\\lambda\\). We are interested in performing statistical inference on \\(\\lambda\\) using a Bayesian approach. A frequently used prior for \\(\\lambda\\) is \\(\\text{Gamma}(\\text{shape = }a, \\text{rate = }b)\\), where \\(\\mathbb{E}[\\lambda] = a/b\\) and \\(\\mathbb{V}(\\lambda) = a/b^2\\).\n\nExercise 1\nThe researchers have prior knowledge that leads them to believe that \\(\\lambda\\) should have mean 8 and variance 4. What values of \\(a\\) and \\(b\\) should they specify?\n\n\nExercise 2\nUsing the prior specified in Exercise 1, compute the probability that \\(\\lambda\\) is greater than 11? For this computation compute the exact probability using the pgamma function in R. This is equivalent to computing \\(P(\\lambda &gt; 11)\\).\n\n\nExercise 3\nCompute the same probability as in Exercise 2, this time using Monte Carlo sampling. Report your Monte Carlo standard error and make sure it is less than 0.01.\n\n\nExercise 4\nSuppose the researchers are interested in the quantity, \\(\\alpha = \\sqrt{\\lambda}\\). Compute the probability that \\(\\alpha\\) is greater than 2.5. Use the same number of Monte Carlo samples as in Exercise 3 and describe why Monte Carlo sampling makes this computation much more efficient than computing the exact probability.\n\n\nExercise 5\nUsing the prior specified in Exercise 1, compute the posterior distribution for \\(\\lambda\\), \\(f(\\lambda | \\mathbf{Y})\\), where \\(\\mathbf{Y}_i = (Y_1,\\ldots,Y_n).\\) Recall that the Gamma prior for \\(\\lambda\\) is a conjugate prior, so that the posterior is given by: \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). Visualize the posterior distribution and report the posterior mean and a 95% credible interval. Provide an interpretation of the posterior summaries within the context of the US births data.\n\n\nExercise 6\nWhat is the posterior probability that \\(\\lambda\\) is greater than 11? This is equivalent to computing \\(P(\\lambda &gt; 11 | \\mathbf{Y})\\). Again, use Monte Carlo sampling. Provide an interpretation for this probability in the context of hospital visits.\n\n\nExercise 7\nCreate a figure that includes both the prior and posterior distributions for \\(\\lambda\\). Also, include a figure of the observed data. Use these figures to make a comparison of the prior and posterior probabilities found in Exercise 3 and Exercise 6, respectively. Describe any changes in these two probabilities and how they relate to the observed data."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-8-",
    "href": "hw/hw-01-creation.html#exercise-8-",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercise 8-",
    "text": "Exercise 8-\nDefine a random variable \\(weight_i\\) that represents the weight of the baby at birth in ounces for pregnancy \\(i\\). We are interested in learning the association between birth weight and the smoking habit, \\(habit_i\\), of the mother. Fit the following Bayesian linear regression model using Gibbs sampling,\n\\[\\begin{align*}\nweight_i &= \\beta_0 + \\beta_1 \\times 1(habit_i = \\text{smoker}) + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2),\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\\\\n\\boldsymbol{\\beta} &\\sim N(\\boldsymbol{\\beta}_0, \\sigma_{\\beta}^2\\mathbf{I})\\\\\n\\sigma^2 &\\sim \\text{Inv-Gamma}(a,b).\n\\end{align*}\\]\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-8-10",
    "href": "hw/hw-01-creation.html#exercise-8-10",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercise 8-10",
    "text": "Exercise 8-10\nDefine a random variable \\(weight_i\\) that represents the weight of the baby at birth in ounces for pregnancy \\(i\\). We are interested in learning the association between birth weight and the smoking habit, \\(habit_i\\), of the mother. Fit the following Bayesian linear regression model using Gibbs sampling,\n\\[\\begin{align*}\nweight_i &= \\beta_0 + \\beta_1 \\times 1(habit_i = \\text{smoker}) + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2),\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\\\\n\\boldsymbol{\\beta} &\\sim N(\\mathbf{0}, 100 \\mathbf{I})\\\\\n\\sigma^2 &\\sim \\text{Inv-Gamma}(3,1).\n\\end{align*}\\]\n\nExercise 8\nObtain samples from the posterior distribution of \\((\\boldsymbol{\\beta},\\sigma^2)\\) given the observed data. Visualize the posterior distributions and provide justification that the Gibbs sampler has converged.\n\n\nExercise 9\nReport the posterior mean, standard deviation, and 95% credible intervals for each parameter.\n\n\nExercise 10\nIf someone were to fit the same regression using a frequentist approach the resulting model would look like the following.\n\nmod &lt;- lm(weight ~ habit, data = births14)\nres &lt;- summary(mod)\nprint(res)\n\n\nCall:\nlm(formula = weight ~ habit, data = births14)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4965 -0.6865  0.0635  0.8150  3.1135 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.30654    0.04586 159.317  &lt; 2e-16 ***\nhabitsmoker -0.75203    0.16412  -4.582 5.34e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.241 on 792 degrees of freedom\nMultiple R-squared:  0.02583,   Adjusted R-squared:  0.0246 \nF-statistic:    21 on 1 and 792 DF,  p-value: 5.345e-06\n\n\nSuppose researchers are interested in testing the following hypothesis test: \\(H_0: \\beta_1 = 0, H_1: \\beta_1 &lt; 0\\). We can compute this p-value from the frequentist model.\n\npvalue &lt;- pt(coef(res)[, 3], mod$df, lower = TRUE)[2]\n\nThe resulting p-value is &lt;0.001. Compute the Bayesian p-value that corresponds to the same hypothesis test, \\(P(\\beta_1 &lt; 0 | \\mathbf{Y})\\). Interpret both p-values at a Type-I error rate of 0.05 and compare and contrast their interpretations in the context of the association between smoking and low birth weight.\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "ae/ae-02-mcmc.html",
    "href": "ae/ae-02-mcmc.html",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE is a demonstration and you do not have to turn anything in!\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#learning-goals",
    "href": "ae/ae-02-mcmc.html#learning-goals",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nPerform Gibbs sampling for Bayesian linear regression and compute some basic summaries"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-02-mcmc.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-02-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick AE 02.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-02-mcmc.html#r-and-r-studio",
    "href": "ae/ae-02-mcmc.html#r-and-r-studio",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we may tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#exercise-1",
    "href": "ae/ae-02-mcmc.html#exercise-1",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Exercise 1",
    "text": "Exercise 1\nCompute the posterior mean and standard deviation for the intercept, slope, and measurement error. Provide an interpretation for each of these parameter estimates.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#exercise-2",
    "href": "ae/ae-02-mcmc.html#exercise-2",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Exercise 2",
    "text": "Exercise 2\nCompute a 95% confidence interval for the regression slope. Provide an interpretation for this confidence interval.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#exercise-3",
    "href": "ae/ae-02-mcmc.html#exercise-3",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute a one-sided Bayesian p-value for the regression slope: \\(P(\\beta_1 &lt; 0)\\). Interpret the results in plain English. Is intraocular pressure associated with disease progression?\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉\n\nThis AE is a demonstration and you do not have to turn anything in!"
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-2-1",
    "href": "hw/hw-02-creation.html#exercise-2-1",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nAssess the assumptions required for validity of the regression in Exercise 1."
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-3-1",
    "href": "hw/hw-02-creation.html#exercise-3-1",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nAssess the assumptions required for validity of the regression in Exercise 1."
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-x",
    "href": "hw/hw-02-creation.html#exercise-x",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise X",
    "text": "Exercise X\nShould I include this? Assess the assumptions required for validity of the regression in Exercise 1."
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-assumptions",
    "href": "slides/03-mcmc.html#linear-regression-assumptions",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression assumptions",
    "text": "Linear regression assumptions\n\nLinearity between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\): \\(\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\)\nIndependence of errors: \\(\\epsilon_i\\)’s are independent Gaussian\nHomoskedasticity: constant \\(\\sigma^2\\) across \\(\\mathbf{x}_i\\)\nNormality of errors: \\(\\epsilon_i \\sim N(0,\\sigma^2)\\)\nNo multicollinearity: \\(\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1}\\) exists"
  },
  {
    "objectID": "hw/hw-02-creation.html#exercise-9",
    "href": "hw/hw-02-creation.html#exercise-9",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nPerform a sensitivity analysis to the choice of prior for \\((\\alpha,\\boldsymbol{\\beta},\\sigma)\\). Make sure to change the family of priors for each parameter. Are your results robust to the choice of prior?\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 2!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "hw/hw-01-blank.html",
    "href": "hw/hw-01-blank.html",
    "title": "HW 01",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "hw/hw-01.html#exercises-1-7",
    "href": "hw/hw-01.html#exercises-1-7",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercises 1-7",
    "text": "Exercises 1-7\nDefine a random variable \\(Y_i\\) that represents the number of hospital visits during pregnancy for each woman \\(i\\), for \\(i = 1,\\ldots,n\\). Assume that this random variable follows a Poisson distribution with rate \\(\\lambda\\), such that \\(Y_i \\stackrel{iid}{\\sim} \\text{Poisson}(\\lambda)\\). For a Poisson distribution, the mean and variance are equal to \\(\\lambda\\). We are interested in performing statistical inference on \\(\\lambda\\) using a Bayesian approach. A frequently used prior for \\(\\lambda\\) is \\(\\text{Gamma}(\\text{shape = }a, \\text{rate = }b)\\), where \\(\\mathbb{E}[\\lambda] = a/b\\) and \\(\\mathbb{V}(\\lambda) = a/b^2\\).\n\nExercise 1\nThe researchers have prior knowledge that leads them to believe that \\(\\lambda\\) should have mean 8 and variance 4. What values of \\(a\\) and \\(b\\) should they specify?\n\n\nExercise 2\nUsing the prior specified in Exercise 1, compute the probability that \\(\\lambda\\) is greater than 11? For this computation compute the exact probability using the pgamma function in R. This is equivalent to computing \\(P(\\lambda &gt; 11)\\).\n\n\nExercise 3\nCompute the same probability as in Exercise 2, this time using Monte Carlo sampling. Report your Monte Carlo standard error and make sure it is less than 0.01.\n\n\nExercise 4\nSuppose the researchers are interested in the quantity, \\(\\alpha = \\sqrt{\\lambda}\\). Compute the probability that \\(\\alpha\\) is greater than 2.5. Use the same number of Monte Carlo samples as in Exercise 3 and describe why Monte Carlo sampling makes this computation much more efficient than computing the exact probability.\n\n\nExercise 5\nUsing the prior specified in Exercise 1, compute the posterior distribution for \\(\\lambda\\), \\(f(\\lambda | \\mathbf{Y})\\), where \\(\\mathbf{Y}_i = (Y_1,\\ldots,Y_n).\\) Recall that the Gamma prior for \\(\\lambda\\) is a conjugate prior, so that the posterior is given by: \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). Visualize the posterior distribution and report the posterior mean and a 95% credible interval. Provide an interpretation of the posterior summaries within the context of the US births data.\n\n\nExercise 6\nWhat is the posterior probability that \\(\\lambda\\) is greater than 11? This is equivalent to computing \\(P(\\lambda &gt; 11 | \\mathbf{Y})\\). Again, use Monte Carlo sampling. Provide an interpretation for this probability in the context of hospital visits.\n\n\nExercise 7\nCreate a figure that includes both the prior and posterior distributions for \\(\\lambda\\). Also, include a figure of the observed data. Use these figures to make a comparison of the prior and posterior probabilities found in Exercise 3 and Exercise 6, respectively. Describe any changes in these two probabilities and how they relate to the observed data.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-8-10",
    "href": "hw/hw-01.html#exercise-8-10",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercise 8-10",
    "text": "Exercise 8-10\nDefine a random variable \\(weight_i\\) that represents the weight of the baby at birth in ounces for pregnancy \\(i\\). We are interested in learning the association between birth weight and the smoking habit, \\(habit_i\\), of the mother. Fit the following Bayesian linear regression model using Gibbs sampling,\n\\[\\begin{align*}\nweight_i &= \\beta_0 + \\beta_1 \\times 1(habit_i = \\text{smoker}) + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2),\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\\\\n\\boldsymbol{\\beta} &\\sim N(\\mathbf{0}, 100 \\mathbf{I})\\\\\n\\sigma^2 &\\sim \\text{Inv-Gamma}(3,1).\n\\end{align*}\\]\n\nExercise 8\nObtain samples from the posterior distribution of \\((\\boldsymbol{\\beta},\\sigma^2)\\) given the observed data. Visualize the posterior distributions and provide justification that the Gibbs sampler has converged.\n\n\nExercise 9\nReport the posterior mean, standard deviation, and 95% credible intervals for each parameter.\n\n\nExercise 10\nIf someone were to fit the same regression using a frequentist approach the resulting model would look like the following.\n\nmod &lt;- lm(weight ~ habit, data = births14)\nres &lt;- summary(mod)\nprint(res)\n\n\nCall:\nlm(formula = weight ~ habit, data = births14)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4965 -0.6865  0.0635  0.8150  3.1135 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.30654    0.04586 159.317  &lt; 2e-16 ***\nhabitsmoker -0.75203    0.16412  -4.582 5.34e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.241 on 792 degrees of freedom\nMultiple R-squared:  0.02583,   Adjusted R-squared:  0.0246 \nF-statistic:    21 on 1 and 792 DF,  p-value: 5.345e-06\n\n\nSuppose researchers are interested in testing the following hypothesis test: \\(H_0: \\beta_1 = 0, H_1: \\beta_1 &lt; 0\\). We can compute this p-value from the frequentist model.\n\npvalue &lt;- pt(coef(res)[, 3], mod$df, lower = TRUE)[2]\n\nThe resulting p-value is &lt;0.001. Compute the Bayesian p-value that corresponds to the same hypothesis test, \\(P(\\beta_1 &lt; 0 | \\mathbf{Y})\\). Interpret both p-values at a Type-I error rate of 0.05 and compare and contrast their interpretations in the context of the association between smoking and low birth weight.\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "slides/14-missing.html#review-of-last-lecture",
    "href": "slides/14-missing.html#review-of-last-lecture",
    "title": "Missing Data",
    "section": "",
    "text": "Last, we learned about classification for binary and multiclass problems."
  },
  {
    "objectID": "slides/14-missing.html#missing-data-framework",
    "href": "slides/14-missing.html#missing-data-framework",
    "title": "Missing Data",
    "section": "Missing data framework",
    "text": "Missing data framework\n\nWe are interested in modeling a random variable \\(Y_{i}\\), for \\(i \\in \\{1,\\ldots,n\\}\\).\nIn a missing data setting, we only observe the outcome in subset of observations, \\(\\mathbf{Y}_{obs} = \\{Y_{i}:i \\in \\mathcal N_{obs}\\}\\).\n\n\\(\\mathcal N_{obs}\\) is the set of indeces in the observed set, such that \\(|\\mathcal N_{obs}|= n_{obs}\\) is the number of observed data points.\n\nThe remaining observations are assumed to be missing and are contained in \\(\\mathbf{Y}_{mis} = \\{Y_{i}:i \\in \\mathcal N_{mis}\\}\\).\n\n\\(\\mathcal N_{mis}\\) is the set of indeces of the missing data and \\(|\\mathcal N_{mis}|= n_{mis}\\) is the number of missing data points.\n\nThe full set of data is given by \\(\\mathbf{Y}=(\\mathbf{Y}_{obs},\\mathbf{Y}_{mis})\\)."
  },
  {
    "objectID": "slides/14-missing.html#prepare-for-next-class",
    "href": "slides/14-missing.html#prepare-for-next-class",
    "title": "Missing Data",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03.\nComplete reading to prepare for Thursday’s lecture\nThursday’s lecture: Hierarchical Models"
  },
  {
    "objectID": "slides/14-missing.html#defining-the-full-data-likelihood",
    "href": "slides/14-missing.html#defining-the-full-data-likelihood",
    "title": "Multiclass Classification",
    "section": "Defining the full data likelihood",
    "text": "Defining the full data likelihood\n\nDefine \\(O_{i}\\) as a binary indicator of observation \\(Y_{i}\\) being present, where \\(O_{i} = 1\\) indicates that \\(Y_{i}\\) was observed.\nThe collection of missingness indicators is given by \\(\\mathbf{O} = \\{O_{i}:i = 1,\\ldots,n\\}\\).\nThe missing data mechanism is then defined by the form of \\(f(\\mathbf{O} | \\mathbf{Y},\\boldsymbol{\\Omega},\\boldsymbol{\\Phi})\\).\n\nThe parameter block, \\((\\boldsymbol{\\Omega},\\boldsymbol{\\Phi})\\), consists of \\(\\boldsymbol{\\Omega}\\), the target parameters of interest (e.g., feature effects on gold standard response), and \\(\\boldsymbol{\\Phi}\\), the nuisance parameters for inference on \\(\\boldsymbol{\\Omega}\\).\n\nIn particular, this will include the parameters that affect the possible dependence of missing data mechanism on the data."
  },
  {
    "objectID": "slides/14-missing.html#missing-data",
    "href": "slides/14-missing.html#missing-data",
    "title": "Missing Data",
    "section": "Missing data",
    "text": "Missing data\n\nMissing data can appear in the outcome and/or predictors.\nToday, we will write down some math for missing data occuring in the outcome space, however this is generalizable to missingness in the predictor space."
  },
  {
    "objectID": "slides/14-missing.html#missing-data-notation",
    "href": "slides/14-missing.html#missing-data-notation",
    "title": "Missing Data",
    "section": "Missing data notation",
    "text": "Missing data notation\n\nDefine \\(O_{i}\\) as a binary indicator of observation \\(Y_{i}\\) being present, where \\(O_{i} = 1\\) indicates that \\(Y_{i}\\) was observed.\nThe collection of missingness indicators is given by \\(\\mathbf{O} = \\{O_{i}:i = 1,\\ldots,n\\}\\).\nOur observed data then consists of \\((\\mathbf{Y}_{obs}, \\mathbf{O})\\)."
  },
  {
    "objectID": "slides/14-missing.html#observed-data-likelihood",
    "href": "slides/14-missing.html#observed-data-likelihood",
    "title": "Missing Data",
    "section": "Observed data likelihood",
    "text": "Observed data likelihood\n\nThe likelihood for the observed data must be written by marginalizing over the unobserved outcome variables.\n\n\\[\\begin{aligned}\nf(\\mathbf{Y}_{obs}, \\mathbf{O} | \\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) &= \\int f(\\mathbf{Y}_{obs}, \\mathbf{Y}_{mis}, \\mathbf{O} | \\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&= \\int f(\\mathbf{Y}, \\mathbf{O} |\\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&= \\int \\underbrace{f(\\mathbf{Y} |\\mathbf{X}, \\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{X},\\mathbf{Y}, \\boldsymbol{\\phi})}_{\\text{complete data likelihood}} d\\mathbf{Y}_{mis}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14-missing.html#next",
    "href": "slides/14-missing.html#next",
    "title": "Missing Data",
    "section": "Next",
    "text": "Next\nThere are many ways to handle missing data. The simplest approach is to perform a complete case analysis, where we delete entries with a missing value for at least one variable. If the data are missing completely at random (MCAR), then we get unbiased parameter estimate. However:\nCredible intervals will usually be too wide, since the sample size has effectively been reduced (drastically). If missing values occur for other reasons, this yields biased estimates. The next step up is instead of dropping all the missing entries, we replace missing values of a feature with the mean or median of available values for the feature. This is called single mean/median imputation, and is often recommended in many machine learning tasks if the data are MCAR as it can still lead to unbiased parameter estimates.\nThere are also some drawbacks with this method, especially if you’re interested in studying the variability. Single mean/median imputation artificially reduces the variance of features, which results in credible intervals which are too narrow. As shown in Figure 2, the peak at the mean is much higher relative to the surrounding values. The mean of the distribution can still be estimated appropriately, but the amount of spread is reduced.\nAnother drawback is it doesn’t account for relationship between variables, thus reduces correlation. For example, the correlation between the two variables is 0.665 if we only look at the complete cases. If we use single mean imputation, the correlation decreases to 0.589.\nNote that if you don’t care about assessing variability in your estimates, which is often the case for classification tasks, then mean imputation can work quite well."
  },
  {
    "objectID": "slides/14-missing.html#complete-data-likelihood",
    "href": "slides/14-missing.html#complete-data-likelihood",
    "title": "Missing Data",
    "section": "Complete data likelihood",
    "text": "Complete data likelihood\n\nThe joint distribution of \\((\\mathbf{Y}, \\mathbf{O})\\) can be written as,\n\n\\[f(\\mathbf{Y}, \\mathbf{O} | \\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) = \\underbrace{f(\\mathbf{Y} | \\mathbf{X}, \\boldsymbol{\\theta})}_{\\text{likelihood}} \\times  \\underbrace{f(\\mathbf{O} | \\mathbf{Y}, \\mathbf{X}, \\boldsymbol{\\phi})}_{\\text{missing model}}.\\]\n\nThe parameter block, \\((\\boldsymbol{\\theta},\\boldsymbol{\\phi})\\), consists of:\n\n\\(\\boldsymbol{\\theta}\\), the target parameters of interest (e.g., feature effects on outcome), and\n\\(\\boldsymbol{\\phi}\\), the nuisance parameters.\n\n\n. . .\nCan we perform inference using this likelihood?"
  },
  {
    "objectID": "slides/14-missing.html#missing-data-models-mcar",
    "href": "slides/14-missing.html#missing-data-models-mcar",
    "title": "Missing Data",
    "section": "Missing data models: MCAR",
    "text": "Missing data models: MCAR\nThe data are missing completely at random (MCAR) if the missing mechanism is defined as, \\[\\begin{aligned}\nf(\\mathbf{O} | \\mathbf{Y},\\mathbf{X},\\boldsymbol{\\phi}) &= f(\\mathbf{O} | \\mathbf{Y}_{obs}, \\mathbf{Y}_{mis},\\mathbf{X},\\boldsymbol{\\phi})\\\\\n&= f(\\mathbf{O} | \\boldsymbol{\\phi}).\n\\end{aligned}\\]\n\nThe missingness does not depend on any data."
  },
  {
    "objectID": "slides/14-missing.html#missing-data-models-mar",
    "href": "slides/14-missing.html#missing-data-models-mar",
    "title": "Missing Data",
    "section": "Missing data models: MAR",
    "text": "Missing data models: MAR\nThe data are missing at random (MAR) if the missing mechanism is defined as, \\[\\begin{aligned}\nf(\\mathbf{O} | \\mathbf{Y},\\mathbf{X},\\boldsymbol{\\phi}) &= f(\\mathbf{O} | \\mathbf{Y}_{obs}, \\mathbf{Y}_{mis},\\mathbf{X},\\boldsymbol{\\phi})\\\\\n&= f(\\mathbf{O} | \\mathbf{Y}_{obs},\\mathbf{X},\\boldsymbol{\\phi}).\n\\end{aligned}\\]\n\nThe missingness depends on the observed data only."
  },
  {
    "objectID": "slides/14-missing.html#missing-data-models-mnar",
    "href": "slides/14-missing.html#missing-data-models-mnar",
    "title": "Missing Data",
    "section": "Missing data models: MNAR",
    "text": "Missing data models: MNAR\nThe data are missing not at random (MNAR) if the missing mechanism is defined as, \\[\\begin{aligned}\nf(\\mathbf{O} | \\mathbf{Y},\\mathbf{X},\\boldsymbol{\\phi}) &= f(\\mathbf{O} | \\mathbf{Y}_{obs}, \\mathbf{Y}_{mis},\\mathbf{X},\\boldsymbol{\\phi}).\n\\end{aligned}\\]\n\nThe missingness depends on the observed and missing data."
  },
  {
    "objectID": "slides/14-missing.html#implications-of-the-missing-model",
    "href": "slides/14-missing.html#implications-of-the-missing-model",
    "title": "Multiclass Classification",
    "section": "Implications of the missing model",
    "text": "Implications of the missing model\n\\[\\begin{aligned}\nf(\\mathbf{Y}_{obs}, \\mathbf{O} | \\boldsymbol{\\theta},\\boldsymbol{\\phi}) &= \\int f(\\mathbf{Y} | \\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y}, \\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&= \\int f(\\mathbf{Y} | \\boldsymbol{\\theta}) f(\\mathbf{O} | \\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&=  f(\\mathbf{O} | \\boldsymbol{\\phi}) \\int f(\\mathbf{Y}_{obs} | \\boldsymbol{\\theta}) f(\\mathbf{Y}_{mis} | \\boldsymbol{\\theta})d\\mathbf{Y}_{mis}\\\\\n&=  f(\\mathbf{Y}_{obs} | \\boldsymbol{\\theta})f(\\mathbf{O} | \\boldsymbol{\\phi})\\int  f(\\mathbf{Y}_{mis} | \\boldsymbol{\\theta}) d\\mathbf{Y}_{mis}\\\\\n&=  f(\\mathbf{Y}_{obs} | \\boldsymbol{\\theta})f(\\mathbf{O} | \\boldsymbol{\\phi}).\n\\end{aligned}\\]\n\nUnder the MCAR assumption, we are allowed to fit the complete case analysis."
  },
  {
    "objectID": "slides/14-missing.html#implications-of-the-missing-model-mcar",
    "href": "slides/14-missing.html#implications-of-the-missing-model-mcar",
    "title": "Missing Data",
    "section": "Implications of the missing model: MCAR",
    "text": "Implications of the missing model: MCAR\n\\[\\begin{aligned}\nf(\\mathbf{Y}_{obs}, \\mathbf{O} |\\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) &= \\int f(\\mathbf{Y} | \\mathbf{X},\\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y}, \\mathbf{X},\\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&= \\int f(\\mathbf{Y} | \\mathbf{X},\\boldsymbol{\\theta}) f(\\mathbf{O} | \\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&=  f(\\mathbf{O} | \\boldsymbol{\\phi}) \\int f(\\mathbf{Y}_{obs} |\\mathbf{X}_{obs}, \\boldsymbol{\\theta}) f(\\mathbf{Y}_{mis} |\\mathbf{X}_{mis}, \\boldsymbol{\\theta})d\\mathbf{Y}_{mis}\\\\\n&=  f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs},\\boldsymbol{\\theta})f(\\mathbf{O} | \\boldsymbol{\\phi})\\int  f(\\mathbf{Y}_{mis} |\\mathbf{X}_{mis}, \\boldsymbol{\\theta}) d\\mathbf{Y}_{mis}\\\\\n&=  f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs},\\boldsymbol{\\theta})f(\\mathbf{O} | \\boldsymbol{\\phi}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14-missing.html#implications-of-the-missing-model-mar",
    "href": "slides/14-missing.html#implications-of-the-missing-model-mar",
    "title": "Missing Data",
    "section": "Implications of the missing model: MAR",
    "text": "Implications of the missing model: MAR\n\\[\\begin{aligned}\nf(\\mathbf{Y}_{obs}, \\mathbf{O} |\\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) &= \\int f(\\mathbf{Y} |\\mathbf{X}, \\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y},\\mathbf{X}, \\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}= \\int f(\\mathbf{Y} | \\mathbf{X},\\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y}_{obs},\\mathbf{X}, \\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}=  f(\\mathbf{O} | \\mathbf{Y}_{obs},\\mathbf{X},\\boldsymbol{\\phi}) \\int f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs},\\boldsymbol{\\theta}) f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis},\\boldsymbol{\\theta})d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}=  f(\\mathbf{Y}_{obs} |\\mathbf{X}_{obs}, \\boldsymbol{\\theta})f(\\mathbf{O} |\\mathbf{Y}_{obs},\\mathbf{X}, \\boldsymbol{\\phi})\\int  f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis},\\boldsymbol{\\theta}) d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}=  f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs},\\boldsymbol{\\theta})f(\\mathbf{O} |\\mathbf{Y}_{obs}, \\mathbf{X},\\boldsymbol{\\phi}).\n\\end{aligned}\\]\n\nUnbiased Parameter Estimates: Similar to MCAR, we can perform a complete case analysis and can ignore the missing data model! This is never done, however, because it leads to incorrect inference."
  },
  {
    "objectID": "slides/14-missing.html#implications-of-the-missing-model-mnar",
    "href": "slides/14-missing.html#implications-of-the-missing-model-mnar",
    "title": "Missing Data",
    "section": "Implications of the missing model: MNAR",
    "text": "Implications of the missing model: MNAR\n\\[\\begin{aligned}\nf(\\mathbf{Y}_{obs}, \\mathbf{O} | \\mathbf{X},\\boldsymbol{\\theta},\\boldsymbol{\\phi}) &= \\int f(\\mathbf{Y} | \\mathbf{X},\\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y},\\mathbf{X}, \\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}= \\int f(\\mathbf{Y} | \\mathbf{X},\\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y}_{obs},\\mathbf{Y}_{mis}, \\mathbf{X},\\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}= \\int f(\\mathbf{Y}_{obs} |\\mathbf{X}_{obs}, \\boldsymbol{\\theta}) f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis},\\boldsymbol{\\theta})f(\\mathbf{O} | \\mathbf{Y},\\mathbf{X},\\boldsymbol{\\phi})d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}= f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs},\\boldsymbol{\\theta}) \\int f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis},\\boldsymbol{\\theta})f(\\mathbf{O} | \\mathbf{Y},\\mathbf{X},\\boldsymbol{\\phi})d\\mathbf{Y}_{mis}\n\\end{aligned}\\]\n\nUnder the MNAR assumption, we are NOT allowed to ignore the missing data. We must specify a model for the missing data.\nThis is really hard! We can ignore this in our class."
  },
  {
    "objectID": "slides/14-missing.html#summary-of-missing-mechanisms",
    "href": "slides/14-missing.html#summary-of-missing-mechanisms",
    "title": "Missing Data",
    "section": "Summary of missing mechanisms",
    "text": "Summary of missing mechanisms\n\nUnder MCAR and MAR, we are allowed to fit our model to the observed data (i.e., a complete case analysis/listwise deletion). Under these settings the missingness is considered ignorable.\nUnder MAR, fitting the complete case analysis is not efficient and advanced techniques are needed to guarentee proper statistical inference.\nUnder MNAR, we must model the missing data mechanism. This data is considered non-ignorable."
  },
  {
    "objectID": "slides/14-missing.html#generating-data",
    "href": "slides/14-missing.html#generating-data",
    "title": "Missing Data",
    "section": "Generating data",
    "text": "Generating data\n\nWe want to have data under the following criteria:\n\nFull data\nMCAR data\nMAR data\nMNAR data\n\nWe then want to fit the following models: Complete case analysis, full Bayesian model, mice version."
  },
  {
    "objectID": "slides/14-missing.html#lets-look-at-some-data",
    "href": "slides/14-missing.html#lets-look-at-some-data",
    "title": "Missing Data",
    "section": "Let’s look at some data",
    "text": "Let’s look at some data\n\nlibrary(openintro)\nfulldata &lt;- data.frame(y = bdims$wgt,\n                       x = bdims$hgt)"
  },
  {
    "objectID": "slides/14-missing.html#simulate-mcar-data",
    "href": "slides/14-missing.html#simulate-mcar-data",
    "title": "Multiclass Classification",
    "section": "Simulate MCAR data",
    "text": "Simulate MCAR data\n\nset.seed(1)\nmissing_prop &lt;- 0.25\nmcardata &lt;- ampute(fulldata, \n                   prop = missing_prop, \n                   pattern = data.frame(y = c(0),\n                                        x = c(1)), \n                   freq = c(1), \n                   mech = \"MCAR\")\nmardata &lt;- ampute(fulldata, \n                   prop = missing_prop, \n                   pattern = data.frame(y = c(0),\n                                        x = c(1)), \n                   freq = c(1), \n                   mech = \"MAR\")\nmnardata &lt;- ampute(fulldata, \n                  prop = missing_prop, \n                  pattern = data.frame(y = c(0),\n                                       x = c(1)), \n                  freq = c(1), \n                  mech = \"MNAR\")"
  },
  {
    "objectID": "slides/14-missing.html#visualize-data",
    "href": "slides/14-missing.html#visualize-data",
    "title": "Missing Data",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/14-missing.html#model-fits",
    "href": "slides/14-missing.html#model-fits",
    "title": "Missing Data",
    "section": "Model fits",
    "text": "Model fits"
  },
  {
    "objectID": "slides/14-missing.html#simulate-missing-data",
    "href": "slides/14-missing.html#simulate-missing-data",
    "title": "Missing Data",
    "section": "Simulate missing data",
    "text": "Simulate missing data\n\nset.seed(54)\nn &lt;- nrow(fulldata)\nexpit &lt;- function(x) exp(x) / (1 + exp(x))\nfulldata$o &lt;- rbinom(n, 1, expit(1.5 * scale(fulldata$x1) - 3 * fulldata$x2))\nhead(fulldata)\n\n     y    x1 x2  Sex o\n1 65.6 174.0  1 Male 1\n2 71.8 175.3  1 Male 0\n3 80.7 193.5  1 Male 0\n4 72.6 186.5  1 Male 0\n5 78.8 187.2  1 Male 0\n6 74.8 181.5  1 Male 0"
  },
  {
    "objectID": "slides/14-missing.html#missing-pattern",
    "href": "slides/14-missing.html#missing-pattern",
    "title": "Missing Data",
    "section": "Missing pattern",
    "text": "Missing pattern\n\n# md.pattern(mardata, rotate.names = TRUE)"
  },
  {
    "objectID": "slides/14-missing.html#model",
    "href": "slides/14-missing.html#model",
    "title": "Multiclass Classification",
    "section": "Model",
    "text": "Model\n\\[\\begin{aligned}\nY_i &= \\alpha + X_i \\beta + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0, \\sigma^2)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14-missing.html#complete-case-model",
    "href": "slides/14-missing.html#complete-case-model",
    "title": "Missing Data",
    "section": "Complete-case model",
    "text": "Complete-case model\n\\[\\begin{aligned}\nY_i &\\stackrel{ind}{\\sim} N(\\alpha + X_i \\beta, \\sigma^2), \\quad i \\in \\mathcal N_{obs}\\\\\n\\alpha &\\sim N(0,10)\\\\\n\\beta &\\sim N(0,10)\\\\\n\\sigma &\\sim \\text{Half-Normal}(0, 10)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14-missing.html#complete-case-model-1",
    "href": "slides/14-missing.html#complete-case-model-1",
    "title": "Multiclass Classification",
    "section": "Complete-case model",
    "text": "Complete-case model\n\\[\\begin{aligned}\nY_i &\\stackrel{ind}{\\sim} N(\\alpha + X_i \\beta, \\sigma^2), \\quad i \\in \\mathcal N_{obs}\\\\\nY_i &\\stackrel{ind}{\\sim} N(\\alpha + X_i \\beta, \\sigma^2), \\quad i \\in \\mathcal N_{mis}\\\\\n\\alpha &\\sim N(0,10)\\\\\n\\beta &\\sim N(0,10)\\\\\n\\sigma &\\sim \\text{Half-Normal}(0, 10)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14-missing.html#full-bayesian-model",
    "href": "slides/14-missing.html#full-bayesian-model",
    "title": "Missing Data",
    "section": "Full Bayesian model",
    "text": "Full Bayesian model\n\nSince we are Bayesians, we can treat the unobserved \\(\\mathbf{Y}_{mis}\\) as parameters and work with the complete data likelihood.\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\mathbf{Y}_{mis} | \\mathbf{Y}_{obs},\\mathbf{O},\\mathbf{X}) &\\propto f(\\mathbf{Y}, \\mathbf{O}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi} | \\mathbf{X})\\\\\n&\\hspace{-4in}=f(\\mathbf{Y}, \\mathbf{O} | \\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi})f(\\boldsymbol{\\theta},\\boldsymbol{\\phi})\\\\\n&\\hspace{-4in}= f(\\mathbf{Y} | \\mathbf{X}, \\boldsymbol{\\theta})f(\\mathbf{O} | \\mathbf{Y}, \\mathbf{X}, \\boldsymbol{\\phi})f(\\boldsymbol{\\theta},\\boldsymbol{\\phi})\\\\\n&\\hspace{-4in}=f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs}, \\boldsymbol{\\theta})f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis}, \\boldsymbol{\\theta})f(\\mathbf{O} | \\mathbf{Y}, \\mathbf{X}, \\boldsymbol{\\phi})f(\\boldsymbol{\\theta},\\boldsymbol{\\phi})\\\\\n\\end{aligned}\\]\n\nWe can simplify this posterior by dropping the missing data mechanism."
  },
  {
    "objectID": "prepare/prepare-jan21.html",
    "href": "prepare/prepare-jan21.html",
    "title": "Prepare for January 21 lecture",
    "section": "",
    "text": "📖 Read Getting started with Stan\n📖 Read Linear regression in Stan\n✅ Work on HW 01"
  },
  {
    "objectID": "prepare/prepare-jan23.html",
    "href": "prepare/prepare-jan23.html",
    "title": "Prepare for January 23 lecture",
    "section": "",
    "text": "📖 Read about the posterior predictive distribution in Hoff Chapter 4, Section 4.3\n📖 Review prior choice recommendations\n✅ Work on HW 01"
  },
  {
    "objectID": "slides/14-missing.html#missing-data-in-research",
    "href": "slides/14-missing.html#missing-data-in-research",
    "title": "Missing Data",
    "section": "Missing data in research",
    "text": "Missing data in research\n\nIn any real-world dataset, missing values are nearly always going to be present.\n\nMissing data can be totally innocuous or a source of bias.\n\nCannot determine which because there are no values to inspect.\nHandling missing data is extremely difficult!"
  },
  {
    "objectID": "slides/14-missing.html#examples",
    "href": "slides/14-missing.html#examples",
    "title": "Missing Data",
    "section": "Examples",
    "text": "Examples\n\nSuppose an individual’s depression scores are missing in dataset of patients with colon cancer.\nIt could be missing because:\n\nA data entry error where some values did not make it into the dataset.\nThe patient is a man, and men are less likely to complete the depression score in general (i.e., it is not related to the depression itself).\nThe patient has depression, leading to them not completing the depression survey."
  },
  {
    "objectID": "slides/14-missing.html#classifactions-of-missing-data",
    "href": "slides/14-missing.html#classifactions-of-missing-data",
    "title": "Missing Data",
    "section": "Classifactions of missing data",
    "text": "Classifactions of missing data\n\nMissing completely at random (MCAR)\n\nThis is the ideal case but rarely seen in practice. Usually a data entry problem.\n\nMissing at random (MAR)\n\nThe missing value is related to some other variable that has been collected.\n\nMissing not at random (MNAR)\n\nThe missing value is related to a variable that was not collected or not observed."
  },
  {
    "objectID": "slides/14-missing.html#key-points-about-mcar-assumption",
    "href": "slides/14-missing.html#key-points-about-mcar-assumption",
    "title": "Missing Data",
    "section": "Key points about MCAR assumption",
    "text": "Key points about MCAR assumption\n\nNo bias: The analysis based on the observed data will not be biased, as the missingness does not systematically favor any particular pattern in the data.\nReduced power: While unbiased, MCAR still reduces the statistical power of the analysis due to the smaller sample size resulting from missing data.\nSimple handling methods: Because of its random nature, MCAR allows for straightforward handling methods like listwise deletion (i.e., complete-case analysis) or simple imputation techniques (e.g., mean imputation) without introducing bias."
  },
  {
    "objectID": "slides/14-missing.html#key-points-about-mar-assumption",
    "href": "slides/14-missing.html#key-points-about-mar-assumption",
    "title": "Missing Data",
    "section": "Key points about MAR assumption",
    "text": "Key points about MAR assumption\n\nComplete-case analysis is not acceptable:\n\nParameter estimation remains unbiased, but, in general, estimation of variances and intervals is biased.\nAlso, smaller sample size leads to less power and worse prediction.\nUnder certain missingness settings, parameter estimation may not be unbiased.\n\nSimple handling approaches fail: Methods like mean imputation will also result in small estimated standard errors.\nMore advanced methods are needed: Multiple imputation, Bayes."
  },
  {
    "objectID": "slides/14-missing.html#why-bayesian-missing-data",
    "href": "slides/14-missing.html#why-bayesian-missing-data",
    "title": "Missing Data",
    "section": "Why Bayesian missing data",
    "text": "Why Bayesian missing data\nUnbiased Parameter Estimates: Multiple imputation helps to ensure that the parameter estimates (e.g., regression coefficients, group means) are unbiased. By creating multiple imputed datasets and combining the results, we can obtain more accurate estimates that reflect the uncertainty due to missing data1.\nAccurate Standard Errors: Multiple imputation provides accurate standard errors, which leads to reliable p-values and appropriate statistical inferences. This is crucial for making valid conclusions from the data1.\nFlexibility: Multiple imputation is flexible and can handle various types of missing data patterns. It allows for the inclusion of covariates that explain the missingness, which is important for MAR data2.\nHandling Complex Data Structures: Multiple imputation can be used with complex data structures, such as hierarchical or longitudinal data, where the missingness mechanism may vary across different levels or time points2.\nRobustness to Model Misspecification: Multiple imputation is robust to certain types of model misspecification. For example, even if the imputation model is not perfectly specified, the combined results from multiple imputed datasets can still provide valid inferences3."
  },
  {
    "objectID": "ae/ae-03-stan.html",
    "href": "ae/ae-03-stan.html",
    "title": "AE 03: Introduction to Stan",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-03-stan.html#learning-goals",
    "href": "ae/ae-03-stan.html#learning-goals",
    "title": "AE 03: Introduction to Stan",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nPerform Bayesian linear regression using HMC\nPrepare data for a regression task in Stan\nPrint posterior results from Stan"
  },
  {
    "objectID": "ae/ae-03-stan.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-03-stan.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 03: Introduction to Stan",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-03-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick AE 03.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-03-stan.html#exercise-1",
    "href": "ae/ae-03-stan.html#exercise-1",
    "title": "AE 03: Introduction to Stan",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit the following model in Stan and present posterior summaries.\n\\[\nY_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2),\n\\] where \\(\\mathbf{x}_i = (Age_i, 1(Sex_i = Male), BMI_i, BP_i)\\) and flat priors for all parameters: \\(f(\\alpha,\\boldsymbol{\\beta},\\sigma) \\propto c.\\) Flat priors are specified by default, so we can omit any prior specification.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-03-stan.html#exercise-2",
    "href": "ae/ae-03-stan.html#exercise-2",
    "title": "AE 03: Introduction to Stan",
    "section": "Exercise 2",
    "text": "Exercise 2\nFit the same model using the lm function in R. Compare the Bayesian and OLS/MLE results.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-03-stan.html#exercise-3",
    "href": "ae/ae-03-stan.html#exercise-3",
    "title": "AE 03: Introduction to Stan",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit the same regression model as in Exercise 1, but with the following priors:\n\\[\\begin{align*}\n\\alpha &\\sim N(0,10)\\\\\n\\beta_j &\\sim N(0,10),\\quad j=1,\\ldots,p\\\\\n\\sigma^2 &\\sim \\text{Inv-Gamma}(3,1).\n\\end{align*}\\]\nCompare the results from this model to the previous two models.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "prepare/prepare-jan28.html",
    "href": "prepare/prepare-jan28.html",
    "title": "Prepare for January 28 lecture",
    "section": "",
    "text": "Occasionally during the semester we will reference material from the text book Bayesian Data Analysis Third Edition by Andrew Gelman et al. A PDF of the textbook is available for download at Andrew Gelman’s website. We will refer to this textbook as BDA3.\n📖 Read BDA3 Chapter 6 to learn about posterior predictive checking\n📖 Read BDA3 Chapter 11.4 and 11.5 to learn about model convergence metrics\n✅ Work on HW 01 which is due Thursday before class"
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html",
    "href": "ae/ae-04-priors-ppd.html",
    "title": "AE 04: Priors in Stan",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html#learning-goals",
    "href": "ae/ae-04-priors-ppd.html#learning-goals",
    "title": "AE 04: Priors in Stan",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will…\n\nUnderstand the idea of centering data for stabilizing inference\nGain knowledge on prior specification and its sometimes unintended impact\nBe able to compute and interpret a posterior predictive distribution"
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-04-priors-ppd.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 04: Priors in Stan",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-04-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick AE 04.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html#exercise-1",
    "href": "ae/ae-04-priors-ppd.html#exercise-1",
    "title": "AE 04: Priors in Stan",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit the centered regression model detailed above with the following priors: \\(\\alpha \\sim N(0,10^2)\\), \\(\\beta_j \\sim N(0,10^2)\\), and \\(\\sigma^2 \\sim \\text{Inv-Gamma}(3,1).\\) Obtain posterior samples for this model using Stan. Be sure to present posterior samples for \\(\\alpha^*\\).\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html#exercise-2",
    "href": "ae/ae-04-priors-ppd.html#exercise-2",
    "title": "AE 04: Priors in Stan",
    "section": "Exercise 2",
    "text": "Exercise 2\nStan is extremely flexible in terms of the priors that can be used for parameters. Using the centered data specification, change the priors for all three parameters. Report how sensitive the results are.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-priors-ppd.html#exercise-3",
    "href": "ae/ae-04-priors-ppd.html#exercise-3",
    "title": "AE 04: Priors in Stan",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute the posterior predictive distribution for a 60 year old male with a BMI of 25 and average blood pressure of 85.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "slides/14-missing.html#example",
    "href": "slides/14-missing.html#example",
    "title": "Missing Data",
    "section": "Example",
    "text": "Example\n\nSuppose an individual’s depression scores are missing in dataset of patients with colon cancer.\nIt could be missing because:\n\nA data entry error where some values did not make it into the dataset.\nThe patient is a man, and men are less likely to complete the depression score in general (i.e., it is not related to the unobserved depression).\nThe patient has depression and as a result did not complete the depression survey."
  },
  {
    "objectID": "slides/14-missing.html#full-bayesian-model-1",
    "href": "slides/14-missing.html#full-bayesian-model-1",
    "title": "Missing Data",
    "section": "Full Bayesian model",
    "text": "Full Bayesian model\n\nAssuming that \\(f(\\boldsymbol{\\theta},\\boldsymbol{\\phi}) = f(\\boldsymbol{\\theta})f(\\boldsymbol{\\phi})\\), the missingness process does not need to be explicitly modeled when we are interested in inference for \\(\\boldsymbol{\\theta}\\).\n\n\\[\\begin{aligned}\n&f(\\boldsymbol{\\theta}, \\mathbf{Y}_{mis} | \\mathbf{Y}_{obs},\\mathbf{O},\\mathbf{X})\\\\ &\\hspace{2in}\\propto f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs}, \\boldsymbol{\\theta})f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis}, \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14-missing.html#full-bayesian-model-2",
    "href": "slides/14-missing.html#full-bayesian-model-2",
    "title": "Missing Data",
    "section": "Full Bayesian model",
    "text": "Full Bayesian model"
  },
  {
    "objectID": "slides/14-missing.html#full-bayesian-model-for-linear-regression",
    "href": "slides/14-missing.html#full-bayesian-model-for-linear-regression",
    "title": "Missing Data",
    "section": "Full Bayesian model for linear regression",
    "text": "Full Bayesian model for linear regression\n\\[\\begin{aligned}\nY_i | \\alpha, \\beta, \\sigma^2 &\\stackrel{ind}{\\sim} N(\\alpha + \\mathbf{x}_i\\boldsymbol{\\beta}, \\sigma^2), \\quad i \\in \\mathcal N_{obs}\\\\\nY_i | \\alpha, \\beta, \\sigma^2&\\stackrel{ind}{\\sim} N(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2), \\quad i \\in \\mathcal N_{mis}\\\\\n\\alpha &\\sim f(\\alpha)\\\\\n\\beta &\\sim f(\\boldsymbol{\\beta})\\\\\n\\sigma &\\sim f(\\sigma),\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (height_i, 1(sex_i = male), height_i \\times 1(sex_i = male))\\)."
  },
  {
    "objectID": "slides/14-missing.html#lets-motivate-with-some-data",
    "href": "slides/14-missing.html#lets-motivate-with-some-data",
    "title": "Missing Data",
    "section": "Let’s motivate with some data",
    "text": "Let’s motivate with some data"
  },
  {
    "objectID": "slides/14-missing.html#full-bayesian-missing-data-model-in-stan",
    "href": "slides/14-missing.html#full-bayesian-missing-data-model-in-stan",
    "title": "Missing Data",
    "section": "Full Bayesian missing data model in Stan",
    "text": "Full Bayesian missing data model in Stan\n\n// saved in missing-full-bayes.stan\ndata {\n  int&lt;lower = 1&gt; n_obs;\n  int&lt;lower = 1&gt; n_mis;\n  int&lt;lower = 1&gt; p;\n  vector[n_obs] Y_obs;\n  matrix[n_obs, p] X_obs;\n  matrix[n_mis, p] X_mis;\n}\ntransformed data {\n  vector[n_obs] Y_obs_centered;\n  real Y_bar;\n  matrix[n_obs, p] X_obs_centered;\n  matrix[n_mis, p] X_mis_centered;\n  row_vector[p] X_bar;\n  Y_bar = mean(Y_obs);\n  Y_obs_centered = Y_obs - Y_bar;\n  matrix[n_obs + n_mis, p] X = append_row(X_obs, X_mis);\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_obs_centered[, i] = X_obs[, i] - X_bar[i];\n    X_mis_centered[, i] = X_mis[, i] - X_bar[i];\n  }\n}\nparameters {\n  real alpha_centered;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n_mis] Y_mis_centered;\n}\nmodel {\n  target += normal_lpdf(Y_obs_centered | alpha_centered + X_obs_centered * beta, sigma);\n  target += normal_lpdf(Y_mis_centered | alpha_centered + X_mis_centered * beta, sigma);\n  target += normal_lpdf(alpha_centered | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 10);\n}\ngenerated quantities {\n  real alpha;\n  vector[n_mis] Y_mis;\n  alpha = Y_bar + alpha_centered - X_bar * beta;\n  Y_mis = Y_mis_centered + Y_bar;\n}"
  },
  {
    "objectID": "slides/14-missing.html#fit-missing-data-model",
    "href": "slides/14-missing.html#fit-missing-data-model",
    "title": "Missing Data",
    "section": "Fit missing data model",
    "text": "Fit missing data model\n\nstan_missing_data_model &lt;- stan_model(file = \"missing-full-bayes.stan\")\nX &lt;- model.matrix(~ x1 * x2, data = fulldata)[, -1]\nstan_data_missing_data &lt;- list(\n  n_obs = sum(fulldata$o == 1),\n  n_mis = sum(fulldata$o == 0),\n  p = ncol(X),\n  Y_obs = array(fulldata$y[fulldata$o == 1]),\n  X_obs = X[fulldata$o == 1, ],\n  X_mis = X[fulldata$o == 0, ]\n)\nfit_full_bayes_joint &lt;- sampling(stan_missing_data_model, stan_data_missing_data)\nprint(fit_full_bayes_joint, pars = c(\"alpha\", \"beta\", \"sigma\"), probs = c(0.025, 0.975))"
  },
  {
    "objectID": "slides/14-missing.html#explore-latent-missing-variable",
    "href": "slides/14-missing.html#explore-latent-missing-variable",
    "title": "Missing Data",
    "section": "Explore latent missing variable",
    "text": "Explore latent missing variable\n\nlibrary(bayesplot)\nY_mis &lt;- rstan::extract(fit_full_bayes_joint, pars = \"Y_mis\")$Y_mis\ncolnames(Y_mis) &lt;- paste0(\"Y_mis[\", 1:ncol(Y_mis), \"]\")\nmcmc_areas_ridges(Y_mis[, 1:10])"
  },
  {
    "objectID": "slides/14-missing.html#explore-model-fit",
    "href": "slides/14-missing.html#explore-model-fit",
    "title": "Missing Data",
    "section": "Explore model fit",
    "text": "Explore model fit\n\nprint(fit_full_bayes_joint, pars = c(\"alpha\", \"beta\", \"sigma\"), probs = c(0.025, 0.975))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean    sd   2.5% 97.5% n_eff Rhat\nalpha   -48.38    0.83 21.10 -90.15 -7.03   653 1.00\nbeta[1]   0.65    0.00  0.12   0.41  0.90   673 1.00\nbeta[2]  -3.19    0.22  9.75 -22.23 16.56  2047 1.00\nbeta[3]   0.08    0.00  0.06  -0.03  0.19  2487 1.00\nsigma     8.21    0.02  0.58   7.17  9.45   600 1.01\n\nSamples were drawn using NUTS(diag_e) at Wed Jan 22 19:43:45 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/14-missing.html#bayesian-multiple-imputation",
    "href": "slides/14-missing.html#bayesian-multiple-imputation",
    "title": "Missing Data",
    "section": "Bayesian multiple imputation",
    "text": "Bayesian multiple imputation"
  },
  {
    "objectID": "slides/14-missing.html#fit-a-complete-case-analysis",
    "href": "slides/14-missing.html#fit-a-complete-case-analysis",
    "title": "Missing Data",
    "section": "Fit a complete case analysis",
    "text": "Fit a complete case analysis\n\n// missing-complete-case.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\ntransformed data {\n  vector[n] Y_centered;\n  real Y_bar;\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  Y_bar = mean(Y);\n  Y_centered = Y - Y_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n}\nparameters {\n  real alpha_centered;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(Y_centered | alpha_centered + X_centered * beta, sigma);\n  target += normal_lpdf(alpha_centered | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 10);\n}\ngenerated quantities {\n  real alpha;\n  alpha = Y_bar + alpha_centered - X_bar * beta;\n}"
  },
  {
    "objectID": "slides/14-missing.html#create-m-imputed-datasets",
    "href": "slides/14-missing.html#create-m-imputed-datasets",
    "title": "Missing Data",
    "section": "Create \\(m\\) imputed datasets",
    "text": "Create \\(m\\) imputed datasets\n\nm &lt;- 100\nn_chains &lt;- 2\nmardata &lt;- data.frame(\n  y = fulldata$y,\n  x1 = fulldata$x1,\n  x2 = fulldata$x2\n)\nmardata$y[fulldata$o == 0] &lt;- NA\nimp &lt;- mice(mardata, m = m, print = FALSE)"
  },
  {
    "objectID": "slides/14-missing.html#fit-the-model-to-the-imputed-data",
    "href": "slides/14-missing.html#fit-the-model-to-the-imputed-data",
    "title": "Missing Data",
    "section": "Fit the model to the imputed data",
    "text": "Fit the model to the imputed data\n\nstan_bayesian_mi &lt;- stan_model(file = \"bayesian-mi.stan\")\nalpha &lt;- beta &lt;- sigma &lt;- r_hat &lt;- n_eff &lt;- NULL\nn_chains &lt;- 2\nfor (i in 1:m) {\n  \n  ###Load each imputed dataset and fit the Stan complete case model\n  data &lt;- complete(imp, i)\n  X &lt;- model.matrix(~ x1 * x2, data = data)[, -1, drop = FALSE]\n  stan_data_bayesian_mi &lt;- list(\n    n = nrow(data),\n    p = ncol(X),\n    Y = data$y,\n    X = X\n  )\n  fit_mi &lt;- sampling(stan_bayesian_mi, stan_data_bayesian_mi, chains = n_chains)\n  \n  ###Save convergence diagnostics from each imputed dataset\n  r_hat &lt;- cbind(r_hat, summary(fit_mi)$summary[, \"Rhat\"])\n  n_eff &lt;- cbind(n_eff, summary(fit_mi)$summary[, \"n_eff\"])\n  pars &lt;- rstan::extract(fit_mi, pars = c(\"alpha\", \"beta\", \"sigma\"))\n  \n  ### Save the parameters from each imputed dataset\n  n_sims_chain &lt;- length(pars$alpha) / n_chains\n  alpha &lt;- rbind(alpha, cbind(i, rep(1:n_chains, each = n_sims_chain), pars$alpha))\n  beta &lt;- rbind(beta, cbind(i, rep(1:n_chains, each = n_sims_chain), pars$beta))\n  sigma &lt;- rbind(sigma, cbind(i, rep(1:n_chains, each = n_sims_chain), pars$sigma))\n}"
  },
  {
    "objectID": "slides/14-missing.html#inspect-traceplots",
    "href": "slides/14-missing.html#inspect-traceplots",
    "title": "Missing Data",
    "section": "Inspect traceplots",
    "text": "Inspect traceplots"
  },
  {
    "objectID": "slides/14-missing.html#inspect-traceplots-1",
    "href": "slides/14-missing.html#inspect-traceplots-1",
    "title": "Missing Data",
    "section": "Inspect traceplots",
    "text": "Inspect traceplots"
  },
  {
    "objectID": "slides/14-missing.html#inspect-traceplots-2",
    "href": "slides/14-missing.html#inspect-traceplots-2",
    "title": "Missing Data",
    "section": "Inspect traceplots",
    "text": "Inspect traceplots"
  },
  {
    "objectID": "slides/14-missing.html#inspect-traceplots-3",
    "href": "slides/14-missing.html#inspect-traceplots-3",
    "title": "Missing Data",
    "section": "Inspect traceplots",
    "text": "Inspect traceplots"
  },
  {
    "objectID": "slides/14-missing.html#inspect-traceplots-4",
    "href": "slides/14-missing.html#inspect-traceplots-4",
    "title": "Missing Data",
    "section": "Inspect traceplots",
    "text": "Inspect traceplots"
  },
  {
    "objectID": "slides/14-missing.html#comparison-of-methods",
    "href": "slides/14-missing.html#comparison-of-methods",
    "title": "Missing Data",
    "section": "Comparison of methods",
    "text": "Comparison of methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData\nModel\nMean\nSD\n2.5%\n97.5%\nn_eff\nRhat\nSetting\n\n\n\n\nFull\nOLS\n-43.82\n13.78\n-70.89\n-16.75\nNA\nNA\n5\n\n\nMAR\nBayes Joint\n-48.38\n21.10\n-90.15\n-7.03\n652.73\n1\n3\n\n\nMAR\nBaye MI\n-43.45\n23.72\n-85.03\n3.97\n119974.91\n1\n4\n\n\nMAR\nOLS\n-30.56\n23.70\n-77.55\n16.43\nNA\nNA\n6"
  },
  {
    "objectID": "slides/14-missing.html#comparison-of-methods-alpha",
    "href": "slides/14-missing.html#comparison-of-methods-alpha",
    "title": "Missing Data",
    "section": "Comparison of methods: \\(\\alpha\\)",
    "text": "Comparison of methods: \\(\\alpha\\)\n\n\n\n\n\nData\nModel\nMean\nSD\n2.5%\n97.5%\nn_eff\nRhat\n\n\n\n\nFull\nOLS\n-43.82\n13.78\n-70.89\n-16.75\nNA\nNA\n\n\nMAR\nBayes Joint\n-48.38\n21.10\n-90.15\n-7.03\n652.73\n1\n\n\nMAR\nBayes MI\n-43.45\n23.72\n-85.03\n3.97\n119974.91\n1\n\n\nMAR\nOLS\n-30.56\n23.70\n-77.55\n16.43\nNA\nNA"
  },
  {
    "objectID": "slides/14-missing.html#comparison-of-methods-beta_1",
    "href": "slides/14-missing.html#comparison-of-methods-beta_1",
    "title": "Missing Data",
    "section": "Comparison of methods: \\(\\beta_1\\)",
    "text": "Comparison of methods: \\(\\beta_1\\)\n\n\n\n\n\nData\nModel\nMean\nSD\n2.5%\n97.5%\nn_eff\nRhat\n\n\n\n\nFull\nOLS\n0.63\n0.08\n0.47\n0.80\nNA\nNA\n\n\nMAR\nBayes Joint\n0.65\n0.12\n0.41\n0.90\n672.77\n1\n\n\nMAR\nBayes MI\n0.63\n0.14\n0.35\n0.87\n119778.47\n1\n\n\nMAR\nOLS\n0.55\n0.14\n0.27\n0.83\nNA\nNA"
  },
  {
    "objectID": "slides/14-missing.html#comparison-of-methods-beta_2",
    "href": "slides/14-missing.html#comparison-of-methods-beta_2",
    "title": "Missing Data",
    "section": "Comparison of methods: \\(\\beta_2\\)",
    "text": "Comparison of methods: \\(\\beta_2\\)\n\n\n\n\n\nData\nModel\nMean\nSD\n2.5%\n97.5%\nn_eff\nRhat\n\n\n\n\nFull\nOLS\n-17.13\n19.56\n-55.57\n21.30\nNA\nNA\n\n\nMAR\nBayes Joint\n-3.19\n9.75\n-22.23\n16.56\n2047.43\n1\n\n\nMAR\nBayes MI\n-5.52\n10.24\n-25.29\n14.89\n85499.19\n1\n\n\nMAR\nOLS\n-82.60\n49.54\n-180.82\n15.62\nNA\nNA"
  },
  {
    "objectID": "slides/14-missing.html#comparison-of-methods-beta_3",
    "href": "slides/14-missing.html#comparison-of-methods-beta_3",
    "title": "Missing Data",
    "section": "Comparison of methods: \\(\\beta_3\\)",
    "text": "Comparison of methods: \\(\\beta_3\\)\n\n\n\n\n\nData\nModel\nMean\nSD\n2.5%\n97.5%\nn_eff\nRhat\n\n\n\n\nFull\nOLS\n0.15\n0.11\n-0.08\n0.37\nNA\nNA\n\n\nMAR\nBayes Joint\n0.08\n0.06\n-0.03\n0.19\n2487.34\n1\n\n\nMAR\nBayes MI\n0.09\n0.06\n-0.03\n0.20\n83444.33\n1\n\n\nMAR\nOLS\n0.52\n0.27\n-0.02\n1.06\nNA\nNA"
  },
  {
    "objectID": "slides/14-missing.html#comparison-of-methods-sigma",
    "href": "slides/14-missing.html#comparison-of-methods-sigma",
    "title": "Missing Data",
    "section": "Comparison of methods: \\(\\sigma\\)",
    "text": "Comparison of methods: \\(\\sigma\\)\n\n\n\n\n\nData\nModel\nMean\nSD\n2.5%\n97.5%\nn_eff\nRhat\n\n\n\n\nFull\nOLS\n8.73\n0.39\n8.07\n9.49\nNA\nNA\n\n\nMAR\nBayes Joint\n8.21\n0.58\n7.17\n9.45\n599.95\n1.01\n\n\nMAR\nBayes MI\n8.36\n0.35\n7.71\n9.09\n151152.94\n1.00\n\n\nMAR\nOLS\n7.87\n0.52\n6.84\n8.90\nNA\nNA"
  },
  {
    "objectID": "slides/14-missing.html#summary-of-bayesian-joint-model",
    "href": "slides/14-missing.html#summary-of-bayesian-joint-model",
    "title": "Missing Data",
    "section": "Summary of Bayesian joint model",
    "text": "Summary of Bayesian joint model\n\nThe joint model treats the missing data as parameters (i.e., latent variables in the model).\nPlacing a prior on the missing data allows us to jointly learn the model parameters and the missing data.\nEquivalent to multiple imputation at every step of the HMC. Can be slow!\nIn Stan, we can only treat continuous missing data as parameters, so this method is somewhat limited (what do we do if the missing data is a binary outcome?)"
  },
  {
    "objectID": "slides/14-missing.html#multiple-imputation",
    "href": "slides/14-missing.html#multiple-imputation",
    "title": "Missing Data",
    "section": "Multiple imputation",
    "text": "Multiple imputation\n\nAs an alternative to fitting a joint model, there are many approaches that allow us to impute missing data before the actual model fitting takes place.\nEach missing value is not imputed once but \\(m\\) times leading to a total of \\(m\\) fully imputed data sets.\nThe model can then be fitted to each of those data sets separately and results are pooled across models, afterwards.\nOne widely applied package for multiple imputation is mice (Buuren & Groothuis-Oudshoorn, 2010) and we will use it in combination with Stan."
  },
  {
    "objectID": "slides/14-missing.html#mice",
    "href": "slides/14-missing.html#mice",
    "title": "Missing Data",
    "section": "Mice",
    "text": "Mice\n\nHere, we apply the default settings of mice, which means that all variables will be used to impute missing values in all other variables and imputation functions automatically chosen based on the variables’ characteristics.\n\n\nlibrary(mice)\nm &lt;- 100\nmardata &lt;- data.frame(\n  y = fulldata$y,\n  x1 = fulldata$x1,\n  x2 = fulldata$x2\n)\nmardata$y[fulldata$o == 0] &lt;- NA\nimp &lt;- mice(mardata, m = m, print = FALSE)"
  },
  {
    "objectID": "slides/14-missing.html#mice-1",
    "href": "slides/14-missing.html#mice-1",
    "title": "Missing Data",
    "section": "Mice",
    "text": "Mice\n\nNow, we have \\(m = 5\\) imputed data sets stored within the imp object.\n\nIn practice, we will likely need more than 5 of those to accurately account for the uncertainty induced by the missingness, perhaps even in the area of 100 imputed data sets (Zhou & Reiter, 2010).\n\nWe can extract the first imputed dataset.\n\n\ndata &lt;- complete(imp, 1)\n\n\nWe can now fit our model \\(m\\) times for each imputed data sets and combine the posterior samples from all chains for inference."
  },
  {
    "objectID": "slides/14-missing.html#model-for-the-imputed-datasets",
    "href": "slides/14-missing.html#model-for-the-imputed-datasets",
    "title": "Missing Data",
    "section": "Model for the imputed datasets",
    "text": "Model for the imputed datasets\n\n// bayesian-mi.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\ntransformed data {\n  vector[n] Y_centered;\n  real Y_bar;\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  Y_bar = mean(Y);\n  Y_centered = Y - Y_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n}\nparameters {\n  real alpha_centered;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(Y_centered | alpha_centered + X_centered * beta, sigma);\n  target += normal_lpdf(alpha_centered | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 10);\n}\ngenerated quantities {\n  real alpha;\n  alpha = Y_bar + alpha_centered - X_bar * beta;\n}"
  },
  {
    "objectID": "prepare/prepare-jan30.html",
    "href": "prepare/prepare-jan30.html",
    "title": "Prepare for January 30 lecture",
    "section": "",
    "text": "📖 Read sections 9.1-9.4 of Mark Lai’s course notes for an introduction to model comparison\n📖 Review BDA3 Chapter 7.1-7.3 to learn more about model comparison metrics\n✅ Work on HW 01 which is due Thursday before class"
  },
  {
    "objectID": "slides/10-robust.html",
    "href": "slides/10-robust.html",
    "title": "Robust Regression",
    "section": "",
    "text": "On Thursday, we started to branch out from linear regression.\nWe learned about approaches for nonlinear regression.\nToday we will address approaches for robust regression, which will generalize the assumption of homoskedasticity (and also the normality assumption)."
  },
  {
    "objectID": "slides/11-regularization.html",
    "href": "slides/11-regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "On Tuesday, we learned about robust regression.\n\nHeteroskedasticity\nHeavy-tailed distributions\nMedian regression\n\nThese were all models for the observed data \\(Y_i\\).\nToday, we will focus on prior specifications for \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/13-multiclass.html",
    "href": "slides/13-multiclass.html",
    "title": "Multiclass Classification",
    "section": "",
    "text": "On Tuesday, we learned about classification using logistic regression.\nToday, we will focus on multiclass classification: multinomial regression, ordinal regression."
  },
  {
    "objectID": "hw/hw-02.html#exercise-9",
    "href": "hw/hw-02.html#exercise-9",
    "title": "HW 02: Bayesian linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nPerform a sensitivity analysis to the choice of prior for \\((\\alpha,\\boldsymbol{\\beta},\\sigma)\\). Make sure to change the family of priors for each parameter. Are your results robust to the choice of prior?\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 2!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02-blank.html",
    "href": "hw/hw-02-blank.html",
    "title": "HW 02",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-5",
    "href": "slides/08-workflow.html#specify-likelihood-priors-5",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[Y_i] = \\alpha^+ + (\\mathbf{x}_i - \\bar{\\mathbf{x}}) \\boldsymbol{\\beta},\\quad\\bar{\\mathbf{x}}=\\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha^+\\) is the intercept, or average weight for someone who is an average height\n\n\nmodel {\n  target += normal_lpdf(Y | alpha_plus + X_centered * beta, sigma);\n  target += normal_lpdf(alpha_plus | 150, 10);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 10);\n}\n\n\nHard to put a weakly informative prior on \\(\\alpha^+\\)."
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-6",
    "href": "slides/08-workflow.html#specify-likelihood-priors-6",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[Y_i - \\bar{Y}] = \\alpha^* + (\\mathbf{x}_i - \\bar{\\mathbf{x}}) \\boldsymbol{\\beta},\\quad\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^n Y_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha^*\\) is the intercept for the centered data, should be zero.\n\n\ntransformed data {\n  vector[n] Y_centered;\n  real Y_bar;\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n  Y_bar = mean(Y);\n  Y_centered = Y - Y_bar;\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-7",
    "href": "slides/08-workflow.html#specify-likelihood-priors-7",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[Y_i - \\bar{Y}] = \\alpha^* + (\\mathbf{x}_i - \\bar{\\mathbf{x}}) \\boldsymbol{\\beta},\\quad\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^n Y_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha^*\\) is the intercept for the centered data, should be zero.\n\n\nmodel {\n  target += normal_lpdf(Y_centered | alpha_star + X_centered * beta, sigma);\n  target += normal_lpdf(alpha_star | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 10);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-2",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-2",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\n\n###Compile the Stan code\nprior_check &lt;- stan_model(file = \"workflow_prior_pred_check.stan\")\n\n###Define the Stan data object\nY &lt;- dat$weight\nX &lt;- matrix(dat$height)\nstan_data &lt;- list(\n  n = nrow(dat), \n  p = ncol(X),\n  Y_bar = mean(Y),\n  X = X,\n  sigma_alpha = 10,\n  sigma_beta = 10,\n  sigma_sigma = 10)\n\n###Simulate data from the prior\nprior_check1 &lt;- sampling(prior_check, data = stan_data, \n                         algorithm = \"Fixed_param\", chains = 1, iter = 1000)"
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-3",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-3",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:"
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-4",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-4",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\n\n###Compile the Stan code\nprior_check &lt;- stan_model(file = \"workflow_prior_pred_check.stan\")\n\n###Define the Stan data object\nY &lt;- dat$weight\nX &lt;- matrix(dat$height)\nstan_data &lt;- list(\n  n = nrow(dat), \n  p = ncol(X),\n  Y_bar = mean(Y),\n  X = X,\n  sigma_alpha = 10,\n  sigma_beta = 5,\n  sigma_sigma = 4)\n\n###Simulate data from the prior\nprior_check2 &lt;- sampling(prior_check, data = stan_data, \n                         algorithm = \"Fixed_param\", chains = 1, iter = 1000)"
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-5",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-5",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:"
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-6",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-6",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:"
  },
  {
    "objectID": "slides/08-workflow.html#fit-the-model-to-real-data-2",
    "href": "slides/08-workflow.html#fit-the-model-to-real-data-2",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\n###Fit the model\nfit_workflow &lt;- sampling(regression_model, data = stan_data)\nprint(fit_workflow)\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n           mean se_mean    sd    2.5%     50%   97.5% n_eff Rhat\nalpha   -230.32    0.25 16.71 -263.60 -230.48 -198.47  4349    1\nbeta[1]    5.68    0.00  0.25    5.20    5.68    6.17  4367    1\nsigma     20.09    0.01  0.61   18.93   20.07   21.32  4148    1\n\nSamples were drawn using NUTS(diag_e) at Mon Feb  3 13:39:30 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "prepare/prepare-feb04.html",
    "href": "prepare/prepare-feb04.html",
    "title": "Prepare for February 4 lecture",
    "section": "",
    "text": "📖 Review Bayesian Workflow by Gelman et al.\n✅ Work on HW 02"
  },
  {
    "objectID": "prepare/prepare-feb06.html",
    "href": "prepare/prepare-feb06.html",
    "title": "Prepare for February 6 lecture",
    "section": "",
    "text": "📖 Review Curve fitting with B-splines\n✅ Work on HW 02"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html",
    "href": "ae/ae-05-nonlinear.html",
    "title": "AE 05: Change point regression",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#learning-goals",
    "href": "ae/ae-05-nonlinear.html#learning-goals",
    "title": "AE 05: Change point regression",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nPerform Gibbs sampling for Bayesian linear regression and compute some basic summaries"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-05-nonlinear.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 05: Change point regression",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-05-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-05.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#exercise-1",
    "href": "ae/ae-05-nonlinear.html#exercise-1",
    "title": "AE 05: Change point regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nFor eye 4, fit a change point regression model. Present MCMC convergence diagnostics. Did the model converge?\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#exercise-2",
    "href": "ae/ae-05-nonlinear.html#exercise-2",
    "title": "AE 05: Change point regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nUsing the model from Exercise 1, present posterior estimates for model parameters. Provide an point and interval estimate for when this eye progressed.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#exercise-3",
    "href": "ae/ae-05-nonlinear.html#exercise-3",
    "title": "AE 05: Change point regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nCreate a figure that plots the time since first visual field visit versus the mean deviation. Overlay the posterior mean process with a 95% credible band (similar to the figure on slide 32 from today). Also include a vertical line for the posterior mean estimate of the change point.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "slides/09-nonlinear.html",
    "href": "slides/09-nonlinear.html",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "On Tuesday, we put all of our skills together and learned about the Bayesian workflow.\nWe have now learned all the skills needed to perform Bayesian inference.\nThe rest of the course we will introduce new models and data types that are useful for performing biomedical data science."
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#exercise-4",
    "href": "ae/ae-05-nonlinear.html#exercise-4",
    "title": "AE 05: Change point regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nNow fit the change point model to eye 30. Did the MCMC sampler converge? To help answer this question, visualize the observed data for eye 30.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-05-nonlinear.html#optional-exercise-5",
    "href": "ae/ae-05-nonlinear.html#optional-exercise-5",
    "title": "AE 05: Change point regression",
    "section": "(Optional) Exercise 5",
    "text": "(Optional) Exercise 5\nGoing back to eye 4, fit a new change point model where both the mean and variance process are modeled as having a change point, so that \\(Y_i \\stackrel{ind}{\\sim} N(\\mu(X_i),\\sigma(X_i)^2)\\), where\n\\[\\log \\sigma (X_i) =\\left\\{ \\begin{array}{ll}\n        {\\gamma}_0 + \\gamma_1 X_i & \\text{ } \\mbox{$X_i \\leq \\theta$},\\\\\n        {\\gamma}_0 + \\gamma_1 \\theta + {\\gamma}_2(X_i - \\theta)& \\text{ } \\mbox{$X_i &gt; \\theta.$}\\end{array} \\right.\\]\nPresent posterior summaries for each parameter and create a visualization of the posterior standard deviations across time.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "slides/10-robust.html#modeling-the-association-between-age-and-igg",
    "href": "slides/10-robust.html#modeling-the-association-between-age-and-igg",
    "title": "Robust Regression",
    "section": "Modeling the association between age and IgG",
    "text": "Modeling the association between age and IgG\n\nLinear regression can be written as follows for \\(i = 1,\\ldots,n\\),\n\n\\[Y_i = \\alpha + \\beta X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2).\\]\n\n\\(\\beta\\) represent the change in IgG serum concentration with a one year increase in age."
  },
  {
    "objectID": "slides/10-robust.html#linear-regression-assumptions",
    "href": "slides/10-robust.html#linear-regression-assumptions",
    "title": "Robust Regression",
    "section": "Linear regression assumptions",
    "text": "Linear regression assumptions\n\\[\\begin{aligned}\nY_i &= \\alpha + \\beta X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(Y_i\\) are independent observations (independence).\n\\(Y_i\\) is linearly related to \\(X_i\\) (linearity).\n\\(\\epsilon_i = Y_i - \\mu_i\\) is normally distributed (normality).\n\\(\\epsilon_i\\) has constant variance across \\(X_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/10-robust.html#weighted-regression-1",
    "href": "slides/10-robust.html#weighted-regression-1",
    "title": "Robust Regression",
    "section": "Weighted regression",
    "text": "Weighted regression\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  int&lt;lower = 1&gt; n_i[n];\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma2;\n}\ntransformed parameters {\n  vector[n] tau2 = sigma2 / n_i;\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, sqrt(tau2));\n}"
  },
  {
    "objectID": "slides/10-robust.html#returning-to-igg",
    "href": "slides/10-robust.html#returning-to-igg",
    "title": "Robust Regression",
    "section": "Returning to IgG",
    "text": "Returning to IgG"
  },
  {
    "objectID": "slides/09-nonlinear.html#posterior-histograms",
    "href": "slides/09-nonlinear.html#posterior-histograms",
    "title": "Nonlinear Regression",
    "section": "Posterior histograms",
    "text": "Posterior histograms"
  },
  {
    "objectID": "slides/10-robust.html#a-prior-to-induce-a-heavy-tail-1",
    "href": "slides/10-robust.html#a-prior-to-induce-a-heavy-tail-1",
    "title": "Robust Regression",
    "section": "A prior to induce a heavy-tail",
    "text": "A prior to induce a heavy-tail\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\\]\n\nUnder this prior, the marginal likelihood for \\(Y_i\\) is equivalent to a Student-t distribution,\n\n\\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} t_{\\nu}\\left(0, \\sigma\\right).\\]"
  },
  {
    "objectID": "slides/10-robust.html#vizualizing-heavy-tail-distributions-2",
    "href": "slides/10-robust.html#vizualizing-heavy-tail-distributions-2",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/10-robust.html#another-example-of-robust-regression-1",
    "href": "slides/10-robust.html#another-example-of-robust-regression-1",
    "title": "Robust Regression",
    "section": "Another example of robust regression",
    "text": "Another example of robust regression\n\nLet’s revisit our general heteroskedastic regression, \\[Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\sigma^2 \\lambda_i).\\]\nWe can induce another form of robust regression using the following prior for \\(\\lambda_i\\), \\(\\lambda_i \\sim \\text{Exponential}(1/2)\\).\nUnder this prior, the induced marginal model is, \\[Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\stackrel{iid}{\\sim} \\text{Laplace}(\\mu = 0, \\sigma).\\]\nThis has a really nice interpretation!"
  },
  {
    "objectID": "slides/10-robust.html#laplace-distribution",
    "href": "slides/10-robust.html#laplace-distribution",
    "title": "Robust Regression",
    "section": "Laplace distribution",
    "text": "Laplace distribution\nSuppse a variable \\(Y_i\\) follows a Laplace (or double exponential) distribution, then the pdf is given by,\n\\[f(Y_i | \\mu, \\sigma) = \\frac{1}{2\\sigma} \\exp\\left\\{-\\frac{|Y_i - \\mu|}{\\sigma}\\right\\}\\]\n\n\\(\\mathbb{E}[Y_i] = \\mu\\)\n\\(\\mathbb{V}(Y_i) = 2 \\sigma^2\\)\nUnder the Laplace likelihood, estimation of \\(\\mu\\) is equivalent to estimating the population median of \\(Y_i\\)."
  },
  {
    "objectID": "slides/10-robust.html#posterior-of-beta",
    "href": "slides/10-robust.html#posterior-of-beta",
    "title": "Robust Regression",
    "section": "Posterior of \\(\\beta\\)",
    "text": "Posterior of \\(\\beta\\)\n\n\n\n\n\nModel\nMean\nLower\nUpper\n\n\n\n\nLaplace\n0.73\n0.56\n0.89\n\n\nStudent-t\n0.69\n0.55\n0.82\n\n\nGaussian\n0.69\n0.56\n0.83\n\n\nGaussian with Covariates in Variance\n0.76\n0.62\n0.89"
  },
  {
    "objectID": "slides/10-robust.html#model-comparison",
    "href": "slides/10-robust.html#model-comparison",
    "title": "Robust Regression",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\n\n\n\n\nelpd_diff\nelpd_loo\nlooic\n\n\n\n\nStudent-t\n0.00\n-624.42\n1248.84\n\n\nGaussian\n-2.01\n-626.43\n1252.86\n\n\nGaussian with Covariates in Variance\n-6.09\n-630.51\n1261.02\n\n\nLaplace\n-13.10\n-637.52\n1275.05"
  },
  {
    "objectID": "slides/10-robust.html#examining-the-student-t-model-fit",
    "href": "slides/10-robust.html#examining-the-student-t-model-fit",
    "title": "Robust Regression",
    "section": "Examining the Student-t model fit",
    "text": "Examining the Student-t model fit\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd 2.5% 97.5% n_eff Rhat\nalpha   3.32    0.00 0.21 2.91  3.74  2084    1\nbeta[1] 0.69    0.00 0.07 0.55  0.82  2122    1\nsigma   1.72    0.00 0.10 1.53  1.91  2744    1\nnu      7.80    0.05 2.35 4.40 13.29  2617    1\n\nSamples were drawn using NUTS(diag_e) at Sun Feb  9 14:54:53 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/10-robust.html#examining-the-student-t-model-fit-1",
    "href": "slides/10-robust.html#examining-the-student-t-model-fit-1",
    "title": "Robust Regression",
    "section": "Examining the Student-t model fit",
    "text": "Examining the Student-t model fit"
  },
  {
    "objectID": "slides/10-robust.html#local-variance-parameter",
    "href": "slides/10-robust.html#local-variance-parameter",
    "title": "Robust Regression",
    "section": "Local variance parameter",
    "text": "Local variance parameter"
  },
  {
    "objectID": "slides/10-robust.html#examining-the-student-t-model-fit-2",
    "href": "slides/10-robust.html#examining-the-student-t-model-fit-2",
    "title": "Robust Regression",
    "section": "Examining the Student-t model fit",
    "text": "Examining the Student-t model fit"
  },
  {
    "objectID": "slides/10-robust.html#summary-of-robust-regression",
    "href": "slides/10-robust.html#summary-of-robust-regression",
    "title": "Robust Regression",
    "section": "Summary of robust regression",
    "text": "Summary of robust regression\n\nRobust regression techniques can be used when the assumptions of constant variance and/or normality of the residuals do not hold.\nHeteroskedastic variance can viewed as inducing extreme value distributions.\nExtreme value regression using Student-t and Laplace distributions are robust to outliers.\nLaplace regression is equivalent to median regression."
  },
  {
    "objectID": "slides/10-robust.html#why-is-robust-regression-not-more-comomon",
    "href": "slides/10-robust.html#why-is-robust-regression-not-more-comomon",
    "title": "Robust Regression",
    "section": "Why is robust regression not more comomon?",
    "text": "Why is robust regression not more comomon?\n\nDespite their desirable properties, robust methods are not widely used. Why?\n\nHistorically computationally complex.\nNot available in statistical software packages.\n\nBayesian modeling using Stan alleviates these bottlenecks!"
  },
  {
    "objectID": "prepare/prepare-feb11.html",
    "href": "prepare/prepare-feb11.html",
    "title": "Prepare for February 11 lecture",
    "section": "",
    "text": "📖 Read BDA3 Chapter 17.1-17.2 to be introduced to robust regression\n📖 Review Robust regression\n✅ Work on HW 02"
  },
  {
    "objectID": "slides/11-regularization.html#global-local-shrinkage",
    "href": "slides/11-regularization.html#global-local-shrinkage",
    "title": "Regularization",
    "section": "Global-local shrinkage",
    "text": "Global-local shrinkage\n\nLet’s assume \\(\\mathbf{Y} \\stackrel{}{\\sim}N\\left(\\alpha + \\mathbf{X}\\boldsymbol{\\beta},\\sigma^2\\mathbf{I}_n\\right)\\).\nSparsity can be induced into \\(\\boldsymbol{\\beta}\\) using a global-local prior,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} f(\\lambda_j).\n\\end{aligned}\\]\n\n\\(\\tau^2\\) is the global variance term.\n\\(\\lambda_j\\) is the local term.\nThe degree of sparsity depends on the choice of \\(f(\\lambda_j)\\)."
  },
  {
    "objectID": "slides/11-regularization.html#similarity-to-spike-and-slab",
    "href": "slides/11-regularization.html#similarity-to-spike-and-slab",
    "title": "Regularization",
    "section": "Similarity to spike-and-slab",
    "text": "Similarity to spike-and-slab\n\nA horseshoe prior can be considered as a continuous approximation to the spike-and-slab prior.\n\nThe spike-and-slab places a discrete probability mass at exactly zero (the “spike”) and a separate distribution around non-zero values (the “slab”).\nThe horseshoe prior smoothly approximates this behavior with a very concentrated distribution near zero.\n\n\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/11-regularization.html#ridge-regression",
    "href": "slides/11-regularization.html#ridge-regression",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nRidge regression is motivated by extending linear regression to the setting where:\n\nthere are too many predictors (sparsity is desired) and/or,\n\\(\\mathbf{X}^\\top \\mathbf{X}\\) is ill-conditioned were singular or nearly singular (multicollinearity).\n\nThe OLS estimate becomes unstable: \\[\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1}\\mathbf{X}^\\top \\mathbf{Y}.\\]"
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-ridge-prior",
    "href": "slides/11-regularization.html#bayesian-ridge-prior",
    "title": "Regularization",
    "section": "Bayesian ridge prior",
    "text": "Bayesian ridge prior\nRidge regression can be obtained using the following global-local shrinkage prior,\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &= 1 / \\lambda\\\\\n\\tau^2 &= \\sigma^2.\n\\end{aligned}\\]\n\nThis is equivalent to: \\(f(\\beta_j | \\lambda, \\sigma) \\stackrel{iid}{\\sim} N\\left(0,\\frac{\\sigma^2}{\\lambda}\\right)\\).\nHow is this equivalent to ridge regression?"
  },
  {
    "objectID": "slides/11-regularization.html#ridge-regression-1",
    "href": "slides/11-regularization.html#ridge-regression-1",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\nThe ridge estimator minimizes the penalized sum of squares,\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{RIDGE}} = \\arg \\min_{\\boldsymbol{\\beta}}\\left||\\mathbf{Y} - \\boldsymbol{\\mu}\\right||_2^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\\]\n\n\\(\\boldsymbol{\\mu} = \\alpha + \\mathbf{X}\\boldsymbol{\\beta}\\).\n\\(||\\mathbf{v}||_2 = \\sqrt{\\mathbf{v}^\\top \\mathbf{v}}\\) is the L2 norm.\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{RIDGE}} = \\left(\\mathbf{X}^\\top\\mathbf{X} + \\lambda \\mathbf{I}_p\\right)^{-1}\\mathbf{X}^\\top\\mathbf{Y}\\)\n\nAdding the \\(\\lambda\\) to diagonals of \\(\\mathbf{X}^\\top\\mathbf{X}\\) stabilizes the inverse, which becomes unstable with multicollinearity."
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-ridge-prior-1",
    "href": "slides/11-regularization.html#bayesian-ridge-prior-1",
    "title": "Regularization",
    "section": "Bayesian ridge prior",
    "text": "Bayesian ridge prior\n\nThe negative log-posterior is proportional to,\n\n\\[\\frac{||\\mathbf{Y} - \\boldsymbol{\\mu}||_2^2}{2\\sigma^2} + \\frac{\\lambda \\sum_{j=1}^p \\beta_j^2}{2\\sigma^2}.\\]\n\nThe posterior mean and mode are \\(\\hat{\\boldsymbol{\\beta}}_{\\text{RIDGE}}\\).\nSince \\(\\lambda\\) is applied to the squared norm of the \\(\\boldsymbol{\\beta}\\), people often standardize all of the covariates to make them have a similar scale.\nBayesian statistics is inherently performing regularization!"
  },
  {
    "objectID": "slides/11-regularization.html#lasso-regression",
    "href": "slides/11-regularization.html#lasso-regression",
    "title": "Regularization",
    "section": "Lasso regression",
    "text": "Lasso regression\nThe least absolute shrinkage and selection operator (lasso) estimator minimizes the penalized sum of squares,\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{LASSO}} = \\arg \\min_{\\boldsymbol{\\beta}}\\left||\\mathbf{Y} - \\boldsymbol{\\mu}\\right||_2^2 + \\lambda \\sum_{j=1}^p |\\beta_j|\\]\n\n\\(\\lambda = 0\\) reduces to OLS etimator.\n\\(\\lambda = \\infty\\) leads to \\(\\hat{\\boldsymbol{\\beta}}_{\\text{LASSO}} = 0\\).\nLasso is desirable because it can set some \\(\\beta_j\\) exactly to zero."
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-lasso-prior",
    "href": "slides/11-regularization.html#bayesian-lasso-prior",
    "title": "Regularization",
    "section": "Bayesian lasso prior",
    "text": "Bayesian lasso prior\nLasso regression can be obtained using the following global-local shrinkage prior,\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\stackrel{iid}{\\sim} \\text{Exponential}(0.5).\n\\end{aligned}\\]\n\nThis is equivalent to: \\(f(\\beta_j | \\tau) \\stackrel{iid}{\\sim} \\text{Laplace}\\left(0,\\tau\\right)\\).\nHow is this equivalent to lasso regression?"
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-ridge-prior-2",
    "href": "slides/11-regularization.html#bayesian-ridge-prior-2",
    "title": "Regularization",
    "section": "Bayesian ridge prior",
    "text": "Bayesian ridge prior\n\nThe negative log-posterior is proportional to,\n\n\\[\\frac{||\\mathbf{Y} - \\boldsymbol{\\mu}||_2^2}{2\\sigma^2} + \\frac{\\lambda \\sum_{j=1}^p \\beta_j^2}{2\\sigma^2}.\\]\n\nThe posterior mean and mode are \\(\\hat{\\boldsymbol{\\beta}}_{\\text{RIDGE}}\\).\nSince \\(\\lambda\\) is applied to the squared norm of the \\(\\boldsymbol{\\beta}\\), people often standardize all of the covariates to make them have a similar scale.\nBayesian statistics is inherently performing regularization!"
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-lasso-prior-1",
    "href": "slides/11-regularization.html#bayesian-lasso-prior-1",
    "title": "Regularization",
    "section": "Bayesian lasso prior",
    "text": "Bayesian lasso prior\n\nThe negative log-posterior is proportional to,\n\n\\[\\frac{||\\mathbf{Y} - \\boldsymbol{\\mu}||_2^2}{2\\sigma^2} + \\frac{\\sum_{j=1}^p |\\beta_j|}{\\tau}.\\]\n\nLasso is recovered by specifying: \\(\\lambda = 1/\\tau\\).\nThe posterior mode is \\(\\hat{\\boldsymbol{\\beta}}_{\\text{LASSO}}\\).\nAs \\(\\lambda\\) increases, more coefficients are set to zero (less variables are selected), and among the non-zero coefficients, more shrinkage is employed."
  },
  {
    "objectID": "slides/11-regularization.html#relevance-vector-machine",
    "href": "slides/11-regularization.html#relevance-vector-machine",
    "title": "Regularization",
    "section": "Relevance vector machine",
    "text": "Relevance vector machine\n\nBefore we get to the horseshoe, one more global-local prior, called the relevance vector machine.\nThis model can be obtained using the following prior,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\n\\end{aligned}\\]\n\nThis is equivalent to: \\(f(\\beta_j | \\tau) \\stackrel{iid}{\\sim} {t}_{\\nu}\\left(0,\\tau\\right)\\)."
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-lasso-does-not-work",
    "href": "slides/11-regularization.html#bayesian-lasso-does-not-work",
    "title": "Regularization",
    "section": "Bayesian lasso does not work",
    "text": "Bayesian lasso does not work\n\nThere is a consensus that the Bayesian lasso does not work well.\nIt does not yield \\(\\beta_j\\) that are exactly zero and it can overly shrink non-zero \\(\\beta_j\\).\nThe gold-standard sparsity-inducing prior in Bayesian statistics is the horseshoe prior."
  },
  {
    "objectID": "slides/11-regularization.html#half-cauchy-distribution-1",
    "href": "slides/11-regularization.html#half-cauchy-distribution-1",
    "title": "Regularization",
    "section": "Half-Cauchy distribution",
    "text": "Half-Cauchy distribution"
  },
  {
    "objectID": "prepare/prepare-feb13.html",
    "href": "prepare/prepare-feb13.html",
    "title": "Prepare for February 13 lecture",
    "section": "",
    "text": "📖 Read BDA3 Chapter 14.6 about regularization in Bayesian models\n📖 Read Handling Sparsity via the Horseshoe by Carvalho, Polson, and Scott.\n✅ Work on HW 02"
  },
  {
    "objectID": "slides/11-regularization.html#non-gaussian-observation-models",
    "href": "slides/11-regularization.html#non-gaussian-observation-models",
    "title": "Regularization",
    "section": "Non-Gaussian observation models",
    "text": "Non-Gaussian observation models\n\nThe reference value:\n\n\\[\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}.\\]\n\nThis framework can be applied to non-Gaussian observation data models using plug-in estimates values for \\(\\sigma\\).\n\nGaussian approximations to the likelihood.\nFor example: For logistic regression \\(\\sigma = 2\\)."
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-in-stan",
    "href": "slides/11-regularization.html#horseshoe-in-stan",
    "title": "Regularization",
    "section": "Horseshoe in Stan",
    "text": "Horseshoe in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0&gt; tau0;\n}\nparameters {\n  real alpha;     \n  real&lt;lower = 0&gt; sigma;\n  vector[p] z;\n  vector&lt;lower = 0&gt;[p] lambda;\n  real&lt;lower = 0&gt; tau;\n}\ntransformed parameters {\n  vector[p] beta;\n  beta = tau * lambda .* z;\n}\nmodel {\n  // likelihood\n  target += normal_lpdf(Y | alpha + X * beta, sigma);\n  // population parameters\n  target += normal_lpdf(alpha | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  // horseshoe prior\n  target += std_normal_lpdf(z);\n  target += cauchy_lpdf(lambda | 0, 1);\n  target += cauchy_lpdf(tau | 0, tau0);\n}"
  },
  {
    "objectID": "slides/11-regularization.html#coding-up-the-model-in-stan",
    "href": "slides/11-regularization.html#coding-up-the-model-in-stan",
    "title": "Regularization",
    "section": "Coding up the model in Stan",
    "text": "Coding up the model in Stan\nHorseshoe model has the following form,\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\stackrel{ind}{\\sim} N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\stackrel{iid}{\\sim} \\mathcal C^+(0, 1),\\\\\n\\tau &\\sim \\mathcal C^+(0, \\tau_0^2).\n\\end{aligned}\\]\nEfficient parameter transformation, \\[\\beta_j = \\tau \\lambda_j z_j, \\quad z_j \\stackrel{iid}{\\sim} N(0,1).\\]"
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-in-stan-1",
    "href": "slides/11-regularization.html#horseshoe-in-stan-1",
    "title": "Regularization",
    "section": "Horseshoe in Stan",
    "text": "Horseshoe in Stan\n\ndata {\n  int&lt;lower = 0&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0&gt; tau0;\n}\ntransformed data {\n  real Y_bar;\n  vector[n] Y_centered;\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n  Y_bar = mean(Y);\n  Y_centered = Y - Y_bar;\n}\nparameters {\n  real alpha;     \n  real&lt;lower = 0&gt; sigma;\n  vector[p] z;                  // unscaled horseshoe regression pars\n  vector&lt;lower = 0&gt;[p] lambda;  // local shrinkage parameter\n  real&lt;lower = 0&gt; tau;          // global shrinkage parameter\n}\ntransformed parameters {\n  vector[p] beta;                                   // scaled coefficients\n  beta = tau * lambda .* z;                         // horseshoe pars\n}\nmodel {\n  // ordinal likelihood\n  target += normal_lpdf(Y_centered | alpha + X_centered * beta, sigma);\n    // horseshoe prior\n  target += std_normal_lpdf(z);\n  target += cauchy_lpdf(lambda | 0, 1);\n  target += cauchy_lpdf(tau | 0, tau0);\n  // population parameters\n  target += normal_lpdf(alpha | 0, 3);\n}\ngenerated quantities {\n  vector[n] Y_pred;\n  vector[n] mu;\n  vector[n] log_lik;\n  real alpha_star = Y_bar + alpha - X_bar * beta;\n  for (i in 1:n) {\n    mu[i] = alpha_star + X[i, ] * beta;\n    Y_pred[i] = normal_rng(mu[i], sigma);\n    log_lik[i] = normal_lpdf(Y_centered[i] | alpha + X_centered[i, ] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "hw/hw-03-creation.html",
    "href": "hw/hw-03-creation.html",
    "title": "HW 03: Going beyond linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis assignment is due on Thursday, February 27 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-1",
    "href": "hw/hw-03-creation.html#exercise-1",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nFormulate this regression problem within the framework of a Bayesian model (hint: Laplace distribution!). Fit this regression using Stan to estimate \\((\\alpha, \\boldsymbol{\\beta})\\) and any other parameters that arise in the model. For all model parameters, choose weakly-informative priors. Evaluate model convergence."
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-2",
    "href": "hw/hw-03-creation.html#exercise-2",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nPerform a posterior predictive check using the median as a test statistic. Be sure to present a posterior predictive p-value and use it to describe the model fit."
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-3",
    "href": "hw/hw-03-creation.html#exercise-3",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nPerform a posterior predictive check for the 2.5th and 97.5th quantiles. Comment on the difference between the result from Exercise 2."
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-4",
    "href": "hw/hw-03-creation.html#exercise-4",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nPresent posterior summaries for all population parameters. For all predictors with a significant association (i.e., 95% credible interval does not include zero), provide an interpretation within the context of the problem."
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-5",
    "href": "hw/hw-03-creation.html#exercise-5",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nFit the same model as in Exercise 1 using linear regression. Compare the posterior mean estimates of \\(\\boldsymbol{\\beta}\\) between the two models. Do they correspond?"
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-6",
    "href": "hw/hw-03-creation.html#exercise-6",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nPerform a model comparison between the models in Exercise 1 and Exercise 5. Which model is preferred? Provide intuition for why one model may be preferred over the other."
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-7",
    "href": "hw/hw-03-creation.html#exercise-7",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nBefore fitting the horseshoe regression, a realistic value of \\(\\tau_0\\) must be determined. Compute a realistic value for \\(\\tau_0\\) based on the effective number of non-zero coefficients. Researchers have a prior belief that the number of non-zero coefficients will be equal to 1 (i.e., \\(q_0 = 1\\)). When computing \\(\\tau_0\\) be sure to provide a visual justification by plotting the effective number of non-zero coefficients for your choice of \\(\\tau_0\\)."
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-8",
    "href": "hw/hw-03-creation.html#exercise-8",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit the logistic regression model using the horseshoe prior above. Present model convergence diagnostics and make a statement about whether the MCMC sampler has converged."
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-9",
    "href": "hw/hw-03-creation.html#exercise-9",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nVisualize the posterior distributions for the population parameters \\((\\alpha, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma})\\). \\(\\alpha\\) is the intercept on the scale of the original data. Make a statement about the impact of the horseshoe prior on the posterior shape for \\(\\boldsymbol{\\gamma}\\)."
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-10",
    "href": "hw/hw-03-creation.html#exercise-10",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nPresent posterior summaries for \\((\\alpha, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma})\\). Choose one predictor that is significant and provide an interpretation of the posterior mean."
  },
  {
    "objectID": "hw/hw-03-creation.html#exercise-11",
    "href": "hw/hw-03-creation.html#exercise-11",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 11",
    "text": "Exercise 11\nVisualize the posterior distribution of \\(\\pi_2\\) and \\(\\pi_4\\), which are the probability of having a length of stay greater than 5 days for observation \\(Y_2\\) and \\(Y_4\\), respectively (i.e., the second and fourth rows of the hdp dataset). Compute \\(P(\\pi_4 &gt; \\pi_2 | \\mathbf{Y})\\) and make a statement about which patient is more likely to have a longer length of stay."
  },
  {
    "objectID": "hw/hw-03.html#exercise-1",
    "href": "hw/hw-03.html#exercise-1",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nFormulate this regression problem within the framework of a Bayesian model (hint: Laplace distribution!). Fit this regression using Stan to estimate \\((\\alpha, \\boldsymbol{\\beta})\\) and any other parameters that arise in the model. For all model parameters, choose weakly-informative priors. Evaluate model convergence.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-2",
    "href": "hw/hw-03.html#exercise-2",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nPerform a posterior predictive check using the median as a test statistic. Be sure to present a posterior predictive p-value and use it to describe the model fit.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-3",
    "href": "hw/hw-03.html#exercise-3",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nPerform a posterior predictive check for the 2.5th and 97.5th quantiles. Comment on the difference between the result from Exercise 2.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-4",
    "href": "hw/hw-03.html#exercise-4",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nPresent posterior summaries for all population parameters. For all predictors with a significant association (i.e., 95% credible interval does not include zero), provide an interpretation within the context of the problem.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-5",
    "href": "hw/hw-03.html#exercise-5",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nFit the same model as in Exercise 1 using linear regression. Compare the posterior mean estimates of \\(\\boldsymbol{\\beta}\\) between the two models. Do they correspond?",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-6",
    "href": "hw/hw-03.html#exercise-6",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nPerform a model comparison between the models in Exercise 1 and Exercise 5. Which model is preferred? Provide intuition for why one model may be preferred over the other.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-7",
    "href": "hw/hw-03.html#exercise-7",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nBefore fitting the horseshoe regression, a realistic value of \\(\\tau_0\\) must be determined. Compute a realistic value for \\(\\tau_0\\) based on the effective number of non-zero coefficients. Researchers have a prior belief that the number of non-zero coefficients will be equal to 1 (i.e., \\(q_0 = 1\\)). When computing \\(\\tau_0\\) be sure to provide a visual justification by plotting the effective number of non-zero coefficients for your choice of \\(\\tau_0\\).",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-8",
    "href": "hw/hw-03.html#exercise-8",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit the logistic regression model using the horseshoe prior above. Present model convergence diagnostics and make a statement about whether the MCMC sampler has converged.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-9",
    "href": "hw/hw-03.html#exercise-9",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nVisualize the posterior distributions for the population parameters \\((\\alpha, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma})\\). \\(\\alpha\\) is the intercept on the scale of the original data. Make a statement about the impact of the horseshoe prior on the posterior shape for \\(\\boldsymbol{\\gamma}\\).",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-10",
    "href": "hw/hw-03.html#exercise-10",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nPresent posterior summaries for \\((\\alpha, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma})\\). Choose one predictor that is significant and provide an interpretation of the posterior mean.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03.html#exercise-11",
    "href": "hw/hw-03.html#exercise-11",
    "title": "HW 03: Going beyond linear regression",
    "section": "Exercise 11",
    "text": "Exercise 11\nVisualize the posterior distribution of \\(\\pi_2\\) and \\(\\pi_4\\), which are the probability of having a length of stay greater than 5 days for observation \\(Y_2\\) and \\(Y_4\\), respectively (i.e., the second and fourth rows of the hdp dataset). Compute \\(P(\\pi_4 &gt; \\pi_2 | \\mathbf{Y})\\) and make a statement about which patient is more likely to have a longer length of stay.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-03-blank.html",
    "href": "hw/hw-03-blank.html",
    "title": "HW 03",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "slides/12-classification.html#bernoulli-random-variable-example",
    "href": "slides/12-classification.html#bernoulli-random-variable-example",
    "title": "Classification",
    "section": "Bernoulli random variable example",
    "text": "Bernoulli random variable example\nA Bernoulli random variable represents a random variable with two possible outcomes: 0 or 1.\nScenario:\nImagine a medical study on a new drug for hypertension (high blood pressure). You want to model whether a patient responds positively to the treatment.\n\nSuccess (1): The patient’s blood pressure decreases significantly (e.g., more than 10% reduction).\nFailure (0): The patient does not experience a significant decrease in blood pressure."
  },
  {
    "objectID": "slides/12-classification.html#binomial-random-variable-example",
    "href": "slides/12-classification.html#binomial-random-variable-example",
    "title": "Classification",
    "section": "Binomial random variable example",
    "text": "Binomial random variable example\nA Binomial random variable represents the number of successes in a fixed number of independent Bernoulli trials.\nScenario:\nA clinical trial is conducted where 10 patients are given a new drug for diabetes. You want to model how many of these 10 patients experience a significant reduction in their blood sugar levels (e.g., a decrease by at least 20%).\n\nEach patient’s outcome is a Bernoulli random variable: success (1) if their blood sugar level decreases, failure (0) if it does not.\nThe total number of successes (patients who experience a reduction) is modeled as a Binomial random variable."
  },
  {
    "objectID": "slides/12-classification.html#predicted-probabilities-2",
    "href": "slides/12-classification.html#predicted-probabilities-2",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nFor instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd  2.5%  50% 97.5% n_eff Rhat\neta_new 0.28    0.01 0.40 -0.48 0.27  1.07  2754    1\npi_new  0.57    0.00 0.09  0.38 0.57  0.75  2780    1\nY_new   0.57    0.01 0.49  0.00 1.00  1.00  4013    1\n\nSamples were drawn using NUTS(diag_e) at Mon Feb 17 09:17:32 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/12-classification.html#predicted-probabilities-3",
    "href": "slides/12-classification.html#predicted-probabilities-3",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior mean of the predicted probabilities is 0.57."
  },
  {
    "objectID": "ae/ae-06-classification.html",
    "href": "ae/ae-06-classification.html",
    "title": "AE 06: Classification",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE spans the lectures on February 18 and 20, so this is considered a Thursday AE!\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-06-classification.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-06-classification.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 06: Classification",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-06-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-06.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-06-classification.html#mimic-iv-emergency-department-ed-data",
    "href": "ae/ae-06-classification.html#mimic-iv-emergency-department-ed-data",
    "title": "AE 06: Classification",
    "section": "MIMIC-IV Emergency Department (ED) Data",
    "text": "MIMIC-IV Emergency Department (ED) Data\nThe MIMIC-IV-ED database, which includes over 400,000 emergency department (ED) admissions to the Beth Israel Deaconess Medical Center between 2011 and 2019. We will use the MIMIC-IV-ED demo dataset, which is an educational version containing data for only 100 patients, making it a perfect tool for exploring electronic health records (EHR) and learning how to analyze real-world clinical data.\n\nAbout MIMIC-IV-ED\nThe MIMIC-IV-ED dataset is a large, publicly accessible resource intended to facilitate data analysis and research in emergency care. The demo dataset maintains the same structure as the original MIMIC-IV-ED, but with the protected health information (PHI) removed to ensure deidentification and privacy.\n\n\nDataset Access and Ethics\nAccess to the full MIMIC-IV-ED dataset requires registration, identity verification, completion of human participant training, and a signed data use agreement via PhysioNet. However, this demo dataset is publicly available, so you can start exploring it right away!\nThe project has been approved by the Institutional Review Boards (IRBs) of Beth Israel Deaconess Medical Center and the Massachusetts Institute of Technology (MIT), and all patient data is deidentified in compliance with HIPAA regulations."
  },
  {
    "objectID": "ae/ae-06-classification.html#structure-of-the-mimic-iv-ed-demo-dataset",
    "href": "ae/ae-06-classification.html#structure-of-the-mimic-iv-ed-demo-dataset",
    "title": "AE 06: Classification",
    "section": "Structure of the MIMIC-IV-ED Demo Dataset",
    "text": "Structure of the MIMIC-IV-ED Demo Dataset\nEHR data is a relational database, which means the data is organized into tables that are linked by common identifiers (in this case, subject_id). Here’s an overview of the key tables in the dataset:\n\nedstays: A patient tracking table that includes the unique ID for each patient visit.\ndiagnosis: Contains diagnostic codes and descriptions for each ED visit.\nmedrecon: Records the medications administered during the patient’s ED visit.\npyxis: Provides information on the medications dispensed from the hospital’s automated medication dispensing system.\ntriage: Contains triage data, which includes the assessment of patients when they first arrive in the ED.\nvitalsign: Includes measurements of vital signs taken during the visit, such as heart rate, blood pressure, and temperature.\n\nEach table is designed to capture different aspects of the patient’s visit to the ED, giving you the ability to explore patient demographics, diagnoses, treatments, and outcomes."
  },
  {
    "objectID": "ae/ae-06-classification.html#what-is-ehr-data",
    "href": "ae/ae-06-classification.html#what-is-ehr-data",
    "title": "AE 06: Classification",
    "section": "What is EHR Data?",
    "text": "What is EHR Data?\n\nElectronic Health Records (EHR) Overview\nEHR data refers to digital records of a patient’s medical history, typically collected during healthcare encounters such as doctor visits, hospital stays, and emergency department admissions. This data is real-world data that is collected during routine medical care, and it is used by healthcare providers to manage patient care, track diagnoses, and prescribe treatments.\nEHRs are made up of a variety of information, such as:\n\nDemographics: Information about the patient, including age, gender, and race.\nClinical Data: Diagnoses, medications, lab test results, and other medical conditions.\nVitals: Measurements of important bodily functions, such as heart rate, blood pressure, and temperature.\nProcedures: Records of surgeries, imaging tests, and other medical interventions.\nBilling and Insurance Information: Data related to how healthcare services are paid for.\n\nEHR data is crucial in modern healthcare, enabling both providers and researchers to track health outcomes, improve care quality, and conduct studies on medical practices and disease progression.\n\n\nEHR Data in the Context of MIMIC-IV-ED\nThe MIMIC-IV-ED dataset is an example of EHR data, specifically from emergency department (ED) visits. It contains a wide variety of patient-related information, from demographics to vital signs, to medications and diagnostic codes. By analyzing this type of data, researchers can identify patterns, predict outcomes, and improve healthcare delivery.\nHowever, it’s important to understand that EHR data is far from perfect. Here are a few challenges and considerations when working with this type of data:"
  },
  {
    "objectID": "ae/ae-06-classification.html#challenges-of-ehr-data",
    "href": "ae/ae-06-classification.html#challenges-of-ehr-data",
    "title": "AE 06: Classification",
    "section": "Challenges of EHR Data",
    "text": "Challenges of EHR Data\n\n1. Biases in Data Collection\nEHR data reflects the way care is actually provided in the real world, meaning it can carry systematic biases. For example:\n\nSociodemographic Bias: Certain groups of people (e.g., those from lower socioeconomic backgrounds) may experience different levels of access to healthcare, leading to underrepresentation of these groups in the data.\nProvider Bias: Clinicians may make diagnostic or treatment decisions that are influenced by their experiences, leading to biases in how certain conditions are treated or recorded.\nSelection Bias: Only patients who visit the ED are included in the dataset. This excludes people who might have similar health conditions but do not seek emergency care.\n\n\n\n2. Missing Data\nSince EHR data is collected during routine medical care, it often contains missing data. Some reasons for missing data include:\n\nIncomplete records: Not every patient will have all the data points recorded. For example, certain tests or treatments may not be administered to every patient.\nVariable documentation: Different providers may record information inconsistently or leave fields blank.\nPatient noncompliance: Some patients might not provide full information during visits, leading to gaps in the data.\n\nHandling missing data is a key challenge when working with EHR data, and it can impact the accuracy of any analysis performed.\n\n\n3. Data Quality and Inconsistencies\nEHR data is typically entered manually by clinicians or extracted from various systems. As a result, there may be data quality issues such as:\n\nTypos or errors in data entry (e.g., incorrect medication dosages).\nInconsistencies across systems, especially when data is pulled from multiple healthcare organizations or devices.\nCoding errors: Diagnoses and procedures are often coded using standard systems like ICD codes, which can sometimes be misapplied.\n\n\n\n4. Billing-Related Data\nSome elements of EHR data are heavily influenced by billing processes. For example, certain procedures or diagnoses might be recorded in the system primarily for reimbursement purposes, rather than being a true reflection of the patient’s condition. This can introduce confounding factors in analysis, as billing codes may not always align with clinical realities."
  },
  {
    "objectID": "ae/ae-06-classification.html#analyzing-ehr-data",
    "href": "ae/ae-06-classification.html#analyzing-ehr-data",
    "title": "AE 06: Classification",
    "section": "Analyzing EHR Data",
    "text": "Analyzing EHR Data\nWhile these challenges can make EHR data more complex to work with, they also present opportunities for learning and improvement. By using appropriate methods, researchers can account for biases and missingness, and still gain valuable insights from the data.\nIn the case of the MIMIC-IV-ED dataset, these challenges are present, but the data is carefully deidentified and has been cleaned for use in research and education. It’s important to acknowledge these limitations and use statistical techniques to account for them when analyzing the data."
  },
  {
    "objectID": "ae/ae-06-classification.html#mimic-iv-ed-demo-exploring-key-datasets",
    "href": "ae/ae-06-classification.html#mimic-iv-ed-demo-exploring-key-datasets",
    "title": "AE 06: Classification",
    "section": "MIMIC-IV-ED Demo: Exploring Key Datasets",
    "text": "MIMIC-IV-ED Demo: Exploring Key Datasets\nIn this course, we will work most with the edstays and triage datasets. The edstays dataset contains important information, including the start and end time of the ED encounter, demographics (including gender and race), the mode of arrival, and importantly the discharge disposition. Discharge disposition is a critical outcome for ED encounters that indicates how a patient left the encounter and can include, among others, going home, being admitted to the hospital.\n\nedstays &lt;- read_csv(\"mimic_ed/edstays.csv\")\n\n\n\n\n\n\n\nWhen a patient enters the ED, they are triaged, meaning a medical professional will quickly assess their condition to determine the severity of their illness or injury and prioritize them in line for treatment based on how urgently they need care, essentially sorting patients based on their need for immediate attention; the most critical cases will be seen first. The data in the triage dataset comes from this process and includes vital measurements, including, among others, temperature, oxygen saturation, and pain; an assessment of acuity, and also a chief complaint variable. Vitals in the triage dataset are different from those in the vitals dataset, which are collected during the ED stay and are time stamped.\n\ntriage &lt;- read_csv(\"mimic_ed/triage.csv\")\n\n\n\n\n\n\n\n\nData is Messy: Race\nWorking with any real-world data is messy and requires substantial data processing. To illustrate this, let’s consider the variable race from the edstays dataset.\n\n\n\n\n\nCategory\nn\n\n\n\n\nWHITE\n138\n\n\nBLACK/AFRICAN AMERICAN\n46\n\n\nHISPANIC/LATINO - CUBAN\n11\n\n\nPORTUGUESE\n9\n\n\nUNKNOWN\n6\n\n\nWHITE - BRAZILIAN\n3\n\n\nHISPANIC/LATINO - SALVADORAN\n3\n\n\nOTHER\n2\n\n\nWHITE - OTHER EUROPEAN\n1\n\n\nUNABLE TO OBTAIN\n1\n\n\nPATIENT DECLINED TO ANSWER\n1\n\n\nMULTIPLE RACE/ETHNICITY\n1\n\n\n\n\n\nThere are 12 categories of race in only 100 patients. Imagine what this would look like in 400,000 patients and with racial categories changing over time! When working with EHR data, variables should be processed into forms that are appropriate for the research question. Typically this means grouping categorical variables like race into meaningful categories. The following is an example.\n\nedstays &lt;- edstays %&gt;%\n  mutate(race2 = case_when(\n    race == \"BLACK/AFRICAN AMERICAN\" ~ \"black\",\n    race == \"WHITE - BRAZILIAN\" ~ \"white\",\n    race == \"WHITE - OTHER EUROPEAN\" ~ \"white\",\n    race == \"WHITE\" ~ \"white\",\n    race == \"UNKNOWN\" ~ \"other\",\n    race == \"UNABLE TO OBTAIN\" ~ \"other\",\n    race == \"PATIENT DECLINED TO ANSWER\" ~ \"other\",\n    race == \"OTHER\" ~ \"other\",\n    race == \"PORTUGUESE\" ~ \"other\",\n    race == \"MULTIPLE RACE/ETHNICITY\" ~ \"other\",\n    race == \"HISPANIC/LATINO - SALVADORAN\" ~ \"other\",\n    race == \"HISPANIC/LATINO - CUBAN\" ~ \"other\",\n    TRUE ~ NA_character_  # handles any other cases that don't match\n  )) %&gt;%\n  mutate(race2 = relevel(factor(race2), ref = \"white\"))\n\nThis yields the following categories.\n\n\n\n\n\nCategory\nn\n\n\n\n\nwhite\n142\n\n\nblack\n46\n\n\nother\n34"
  },
  {
    "objectID": "ae/ae-06-classification.html#data-is-messy-race",
    "href": "ae/ae-06-classification.html#data-is-messy-race",
    "title": "Exploring MIMIC-IV-ED Demo Datasets",
    "section": "Data is Messy: Race",
    "text": "Data is Messy: Race\nWorking with any real-world data is messy and requires substantial data processing. To illustrate this, let’s consider the variable race from the edstays dataset.\n\ntable(edstays$race)\n\n\n      BLACK/AFRICAN AMERICAN      HISPANIC/LATINO - CUBAN \n                          46                           11 \nHISPANIC/LATINO - SALVADORAN      MULTIPLE RACE/ETHNICITY \n                           3                            1 \n                       OTHER   PATIENT DECLINED TO ANSWER \n                           2                            1 \n                  PORTUGUESE             UNABLE TO OBTAIN \n                           9                            1 \n                     UNKNOWN                        WHITE \n                           6                          138 \n           WHITE - BRAZILIAN       WHITE - OTHER EUROPEAN \n                           3                            1 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "ae/ae-06-classification.html#data-is-messy-ed-length-of-stay",
    "href": "ae/ae-06-classification.html#data-is-messy-ed-length-of-stay",
    "title": "AE 06: Classification",
    "section": "Data is Messy: ED Length of Stay",
    "text": "Data is Messy: ED Length of Stay\nAnother example of data processing in EHR data is computing ED length of stay. Length of stay is a critical outcome in healthcare applications and can be computed as the time of admission to discharge. In real-world datasets this is not pre-computed, but must be derived from date-time variables. We first check to see if the admission and discharge times are date-time data objects.\n\nclass(edstays$outtime)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nclass(edstays$intime)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nIf they are not, they can be converted.\n\nedstays$outtime &lt;- as.POSIXct(edstays$outtime)\nedstays$intime &lt;- as.POSIXct(edstays$intime)\n\nWe can then compute the length of stay by computing the time differences. It is important to specify the units of time for consistency. We then convert to a numeric and visualize.\n\nedstays$los &lt;- difftime(as.POSIXct(edstays$outtime), as.POSIXct(edstays$intime), units = \"hours\")\nedstays$los &lt;- as.numeric(edstays$los)\nggplot(edstays, aes(x = los)) +\n  geom_histogram() +\n  labs(x = \"ED Length of Stay (hours)\",\n       y = \"Count\")"
  },
  {
    "objectID": "ae/ae-06-classification.html#data-for-todays-ae",
    "href": "ae/ae-06-classification.html#data-for-todays-ae",
    "title": "Exploring MIMIC-IV-ED Demo Datasets",
    "section": "Data for Today’s AE",
    "text": "Data for Today’s AE\nWe will focus on the following variables:\n\nlos: ED length of stay in hours\nrace2: categorical race variable with three groups white, black, other.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "ae/ae-06-classification.html#exercise-1",
    "href": "ae/ae-06-classification.html#exercise-1",
    "title": "AE 06: Classification",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit the logistic regression model: \\[\\begin{align*}\nY_i &\\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\\\\n\\text{logit}(\\pi_i) &= \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta},\n\\end{align*}\\] where \\(Y_i\\) is the binary indicator for a length of stay being greater than 6 hours and \\(\\mathbf{x}_i = (black_i, other_i)\\) contains the covariates for race, with white as the reference category. Place weakly-informative priors. Evaluate model convergence, check model fit with posterior predictive checks. Present the posterior mean and 95% credible interval for the odds ratio for black race.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-06-classification.html#exercise-2",
    "href": "ae/ae-06-classification.html#exercise-2",
    "title": "AE 06: Classification",
    "section": "Exercise 2",
    "text": "Exercise 2\nFit the additive log ratio regression model for \\(Y_i \\in \\{1,\\ldots,K\\}\\) for \\(i = 1,\\ldots,n\\), \\[\\log \\left(\\frac{P(Y_i = k)}{P(Y_i = K)} \\right)= \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\},\\] where \\(K\\) is chosen as reference. Place weakly-informative priors. Evaluate model convergence, check model fit with posterior predictive checks. Present the posterior mean and 95% credible interval for the odds ratio for black race.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-06-classification.html#exercise-3",
    "href": "ae/ae-06-classification.html#exercise-3",
    "title": "AE 06: Classification",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit the proportional odds regression model for ordinal \\(Y_i \\in \\{1,\\ldots,K\\}\\) for \\(i = 1,\\ldots,n\\), \\[\\log \\frac{P(Y_i\\leq k)}{P(Y_i &gt; k)} = \\text{logit}P(Y_i\\leq k) = \\alpha_k - \\mathbf{x}_i \\boldsymbol{\\beta}.\\] Place weakly-informative priors. Evaluate model convergence, check model fit with posterior predictive checks. Present the posterior mean and 95% credible interval for the odds ratio for black race.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "prepare/prepare-feb18.html",
    "href": "prepare/prepare-feb18.html",
    "title": "Prepare for February 18 lecture",
    "section": "",
    "text": "📖 Read AE 06: Classification. This AE contains an introduction to an EHR dataset we will use for a few assignments in the course. Please read the introduction of the data through the exercises and be ready to work with the data on Tuesday.\n📖 Review Stan documentation on logistic and probit regression.\n✅ Work on HW 03"
  },
  {
    "objectID": "slides/13-multiclass.html#visualizing-the-latent-process",
    "href": "slides/13-multiclass.html#visualizing-the-latent-process",
    "title": "Multiclass Classification",
    "section": "Visualizing the latent process",
    "text": "Visualizing the latent process"
  },
  {
    "objectID": "slides/13-multiclass.html#adding-thresholds-alpha_k",
    "href": "slides/13-multiclass.html#adding-thresholds-alpha_k",
    "title": "Multiclass Classification",
    "section": "Adding thresholds (\\(\\alpha_k\\))",
    "text": "Adding thresholds (\\(\\alpha_k\\))"
  },
  {
    "objectID": "slides/13-multiclass.html#getting-category-probabilities",
    "href": "slides/13-multiclass.html#getting-category-probabilities",
    "title": "Multiclass Classification",
    "section": "Getting category probabilities",
    "text": "Getting category probabilities"
  },
  {
    "objectID": "slides/13-multiclass.html#adding-thresholds-c_k",
    "href": "slides/13-multiclass.html#adding-thresholds-c_k",
    "title": "Multiclass Classification",
    "section": "Adding thresholds (\\(c_k\\))",
    "text": "Adding thresholds (\\(c_k\\))"
  },
  {
    "objectID": "slides/13-multiclass.html#enforcing-order-in-the-cutoffs",
    "href": "slides/13-multiclass.html#enforcing-order-in-the-cutoffs",
    "title": "Multiclass Classification",
    "section": "Enforcing order in the cutoffs",
    "text": "Enforcing order in the cutoffs\n\nIn Stan, when you define a parameter as ordered[K-1] alpha;, the values of alpha are automatically constrained to be strictly increasing.\nThis transformation ensures that the alpha values follow the required order, i.e., alpha[1] &lt; alpha[2] &lt; ... &lt; alpha[K-1].\nStan doesn’t sample alpha directly but instead works with an unconstrained parameter vector, which we will call gamma."
  },
  {
    "objectID": "slides/13-multiclass.html#enforcing-order-in-the-cutoffs-1",
    "href": "slides/13-multiclass.html#enforcing-order-in-the-cutoffs-1",
    "title": "Multiclass Classification",
    "section": "Enforcing order in the cutoffs",
    "text": "Enforcing order in the cutoffs\n\nparameters {\n  vector[K - 1] gamma;\n}\ntransformed parameters {\n  vector[K - 1] alpha;\n  alpha[1] = gamma[1];\n  for (j in 2:K) {\n    alpha[j] = alpha[j - 1] + exp(gamma[j]);\n  }\n}\n\n\nHere, gamma represents a vector of independent, unconstrained variables, and the transformation ensures that alpha is strictly increasing by construction.\nLuckily we can use ordered, since Stan takes care of this (including the Jacobian) in the background."
  },
  {
    "objectID": "slides/13-multiclass.html#log-jacobian-adjustment",
    "href": "slides/13-multiclass.html#log-jacobian-adjustment",
    "title": "Multiclass Classification",
    "section": "Log-Jacobian Adjustment",
    "text": "Log-Jacobian Adjustment\nSince Stan samples from the unconstrained space (via gamma), it needs to account for the transformation when calculating the log-posterior. Specifically, the Jacobian of the transformation from the unconstrained space (gamma) to the ordered space (alpha) must be included in the log-posterior.\nThe Jacobian adjustment is computed as the log of the absolute determinant of the transformation matrix. In this case, the matrix represents the cumulative sum operation, and the adjustment ensures that the correct density is used for the reparameterized alpha values.\nThus, Stan adds the following adjustment to the log-posterior: target += log_jacobian_adjustment\nThis ensures that the log-posterior reflects the reparameterization and that the probability density for alpha is correctly adjusted for the ordering constraint."
  },
  {
    "objectID": "slides/12-classification.html",
    "href": "slides/12-classification.html",
    "title": "Classification",
    "section": "",
    "text": "Last week, we learned about Bayesian approaches to robust regression and regularization.\n\nGlobal-local shrinkage priors.\n\nThis week, we will focus on classification models.\n\nToday: Binary classification (logistic regression).\nThursday: Multiclass classification (multinomial, ordinal regression)."
  },
  {
    "objectID": "prepare/prepare-feb20.html",
    "href": "prepare/prepare-feb20.html",
    "title": "Prepare for February 20 lecture",
    "section": "",
    "text": "📖 If you have not already, familizarize yourself with the data from AE 06: Classification.\n📖 Read about multinomial regression.\n📖 Read about ordinal regression.\n✅ Work on HW 03"
  },
  {
    "objectID": "slides/15-hierarchical.html#review-of-last-lecture",
    "href": "slides/15-hierarchical.html#review-of-last-lecture",
    "title": "Hierarchical Models",
    "section": "",
    "text": "On Tuesday, we learned about missing data, including various types: MCAR, MAR, and MNAR.\nWe learned about two Bayesian approaches to missing data under the MAR assumption:\n\nJoint model that assumes the missing data are parameters.\nMultiple imputation.\n\nMoving forward: Up until today, we have dealt with independent data. Today, we will look at our first example of dependent data!"
  },
  {
    "objectID": "slides/15-hierarchical.html#linear-regression-assumptions",
    "href": "slides/15-hierarchical.html#linear-regression-assumptions",
    "title": "Hierarchical Models",
    "section": "Linear regression assumptions",
    "text": "Linear regression assumptions\n\\[\\begin{aligned}\nY_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(Y_i\\) are independent observations (independence).\n\\(Y_i\\) is linearly related to \\(\\mathbf{x}_i\\) (linearity).\n\\(\\epsilon_i = Y_i - \\mu_i\\) is normally distributed (normality).\n\\(\\epsilon_i\\) has constant variance across \\(\\mathbf{x}_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/15-hierarchical.html#prepare-for-next-class",
    "href": "slides/15-hierarchical.html#prepare-for-next-class",
    "title": "Hierarchical Models",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on Exam 01, which is due before next Thursday’s class!\nNext Tuesday’s class will be office hours. I will be available in the lecture room during the meeting time.\nNext Thursday’s lecture: Longitudinal Data"
  },
  {
    "objectID": "slides/15-hierarchical.html#recap-linear-regression-assumptions",
    "href": "slides/15-hierarchical.html#recap-linear-regression-assumptions",
    "title": "Hierarchical Models",
    "section": "Recap: Linear Regression Assumptions",
    "text": "Recap: Linear Regression Assumptions\nIn linear regression, we typically assume:\n[ Y_i = _0 + _1 X_i + _i ]\nWhere: - (Y_i) is the response variable for observation (i), - (X_i) is the predictor for observation (i), - (_0, _1) are model parameters, - (_i) is the error term."
  },
  {
    "objectID": "slides/15-hierarchical.html#independence-assumption-in-linear-regression",
    "href": "slides/15-hierarchical.html#independence-assumption-in-linear-regression",
    "title": "Hierarchical Models",
    "section": "Independence Assumption in Linear Regression",
    "text": "Independence Assumption in Linear Regression\nWe assume that the residuals \\(\\epsilon_i\\) are independent:\n\\[\\mathbb{C}(\\epsilon_i, \\epsilon_j) = 0, \\quad \\text{for} \\quad i \\neq j,\\] where \\(\\mathbb{C}(X, Y)\\) is the covariance between two random variables \\(X\\) and \\(Y\\). As a note: \\(\\mathbb{C}(X, X) = \\mathbb{V}(X)\\).\n\nThis implies that the observations \\(Y_i\\) and \\(Y_j\\) are independent, and their correlation is zero.\n\nCorrelation: \\(\\rho(X,Y) = \\frac{\\mathbb{C}(X, Y)}{\\sqrt{\\mathbb{V}(X)\\mathbb{V}(Y)}}\\)."
  },
  {
    "objectID": "slides/15-hierarchical.html#real-world-data-dependent-observations",
    "href": "slides/15-hierarchical.html#real-world-data-dependent-observations",
    "title": "Hierarchical Models",
    "section": "Real-World Data: Dependent Observations",
    "text": "Real-World Data: Dependent Observations\nHowever, in real-world data, the independence assumption often does not hold:\n\nRepeated measures data (e.g., same individual over time).\nClustered data (e.g., students within a school, patients within a hospital).\nLongitudinal data (e.g., disease severity measures over time).\nSpatial data (e.g., disease counts observed across zip codes)."
  },
  {
    "objectID": "slides/15-hierarchical.html#the-challenge",
    "href": "slides/15-hierarchical.html#the-challenge",
    "title": "Hierarchical Models",
    "section": "The Challenge",
    "text": "The Challenge\n\nIf we assume independence in the presence of correlation:\n\nBiased parameter estimates: Parameter estimation will be biased due to group-level dependencies that effect the outcome.\nUnderestimated uncertainty: The model will not account for the true variability, leading to narrower confidence intervals.\nInaccurate Predictions: Predictions for new groups may be biased because the model doesn’t properly account for group-level variability.\n\nThus, we need a way to account for dependencies between observations, especially when data are grouped or clustered."
  },
  {
    "objectID": "slides/15-hierarchical.html#extending-linear-regression-the-random-intercept",
    "href": "slides/15-hierarchical.html#extending-linear-regression-the-random-intercept",
    "title": "Hierarchical Models",
    "section": "Extending Linear Regression: The Random Intercept",
    "text": "Extending Linear Regression: The Random Intercept\nOne way to extend linear regression is by adding a random intercept.\nConsider data grouped by some grouping variable \\(G_i\\). For example, students within schools."
  },
  {
    "objectID": "slides/15-hierarchical.html#random-intercept-model",
    "href": "slides/15-hierarchical.html#random-intercept-model",
    "title": "Hierarchical Models",
    "section": "Random Intercept Model",
    "text": "Random Intercept Model\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\n\nFrom a frequentist perspective, this model may be called a random intercept model, but in the Bayesian framework all parameters are random variables, so the terms fixed and random effects don’t apply.\n\n\\(\\theta_i\\): group-specific parameters (random effect).\n\\(\\alpha, \\boldsymbol{\\beta}, \\sigma\\): population parameters (common across all groups, \\(\\boldsymbol{\\beta}\\) are the fixed effects)."
  },
  {
    "objectID": "slides/15-hierarchical.html#understanding-the-random-intercept",
    "href": "slides/15-hierarchical.html#understanding-the-random-intercept",
    "title": "Hierarchical Models",
    "section": "Understanding the Random Intercept",
    "text": "Understanding the Random Intercept\n\n\\(\\theta_i\\): group-specific parameter captures group-level differences (e.g., hospital level).\nThe intercept \\(\\theta_i\\) allows for each group to have its own baseline value.\nThis model introduces dependence within groups because observations from the same group share the same intercept \\(\\theta_i\\).\n\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{ij} | \\boldsymbol{\\Omega},\\theta_i] &= \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i\\\\\n&= (\\alpha + \\theta_i) + \\mathbf{x}_{ij} \\boldsymbol{\\beta}\\\\\n&= \\alpha_i + \\mathbf{x}_{ij} \\boldsymbol{\\beta}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#why-does-this-model-work",
    "href": "slides/15-hierarchical.html#why-does-this-model-work",
    "title": "Hierarchical Models",
    "section": "Why Does This Model Work?",
    "text": "Why Does This Model Work?\n\nWithin-group correlation: Observations within the same hospital are more similar due to the shared intercept.\nBetween-group differences: Groups have different intercepts \\(\\alpha_i^*\\), reflecting different baseline effects."
  },
  {
    "objectID": "slides/15-hierarchical.html#modeling-correlation-with-a-random-intercept",
    "href": "slides/15-hierarchical.html#modeling-correlation-with-a-random-intercept",
    "title": "Hierarchical Models",
    "section": "Modeling Correlation with a Random Intercept",
    "text": "Modeling Correlation with a Random Intercept\n\nThe random intercept \\(\\theta_i\\) introduces correlation between observations within the same school.\nFor two observations \\(Y_{ij}\\) and \\(Y_{ik}\\) from the same group \\(i\\), we have:\n\n\\[\\begin{aligned}\n\\mathbb{C}(Y_{ij}, Y_{ik}) &=  \\mathbb{C}(\\alpha + \\beta X_{ij} + \\theta_i + \\epsilon_{ij}, \\alpha + \\beta X_{ik} + \\theta_i + \\epsilon_{ik})\\\\\n&= \\mathbb{C}(\\theta_i, \\theta_i)\\\\\n&= \\mathbb{V}(\\theta_i).\n\\end{aligned}\\]\n\nThis non-zero covariance reflects the correlation between observations in the same group."
  },
  {
    "objectID": "slides/15-hierarchical.html#example-students-within-schools",
    "href": "slides/15-hierarchical.html#example-students-within-schools",
    "title": "Hierarchical Models",
    "section": "Example: Students Within Schools",
    "text": "Example: Students Within Schools\nImagine data from students in multiple schools. A random intercept model allows us to account for:\n\nBetween-school differences: Each school has its own baseline performance.\nWithin-school correlation: Students in the same school are more likely to have similar outcomes due to shared school-level effects."
  },
  {
    "objectID": "slides/15-hierarchical.html#conclusion",
    "href": "slides/15-hierarchical.html#conclusion",
    "title": "Hierarchical Models",
    "section": "Conclusion",
    "text": "Conclusion\n\nBy introducing a group-specific intercept, we allow for dependencies between observations within groups, making the model more realistic for real-world clustered or repeated measures data.\nFor the remainder of the class, we will expand upon this hierarchical modeling framework to account for complext data types that are frequently encountered in research, including longitudinal and spatial data."
  },
  {
    "objectID": "slides/15-hierarchical.html#what-is-hierarchical-data",
    "href": "slides/15-hierarchical.html#what-is-hierarchical-data",
    "title": "Hierarchical Models",
    "section": "What is Hierarchical Data?",
    "text": "What is Hierarchical Data?\nHierarchical data refers to data that is organized into groups or clusters, where each group contains multiple observations."
  },
  {
    "objectID": "slides/15-hierarchical.html#example-of-hierarchical-data",
    "href": "slides/15-hierarchical.html#example-of-hierarchical-data",
    "title": "Hierarchical Models",
    "section": "Example of Hierarchical Data",
    "text": "Example of Hierarchical Data\n\nHierarchical data refers to data that is organized into groups or clusters, where each group contains multiple observations.\nConsider data from patients within hospitals. Each patient is being treated in a hospital, with multiple patients belonging to each hospital.\nIn this case, the observation for a patient is indexed by two variables:\n\n\\(i\\): hospital index.\n\\(j\\): patient index, nested within hospital.\n\nSo, for patient \\(j\\) within hospital \\(i\\), we write the response as \\(Y_{ij}\\)."
  },
  {
    "objectID": "slides/15-hierarchical.html#observations-with-two-indices-y_ij",
    "href": "slides/15-hierarchical.html#observations-with-two-indices-y_ij",
    "title": "Hierarchical Models",
    "section": "Observations with Two Indices: \\(Y_{ij}\\)",
    "text": "Observations with Two Indices: \\(Y_{ij}\\)\n\n\\(Y_{ij}\\) represents the response for patient \\(j\\) in hospital \\(i\\).\nThe first index \\(i\\) represents group-level effects (e.g., hospital-level).\nThe second index \\(j\\) represents individual-level observations (e.g., patient).\nWe typically say that \\(i = 1,\\ldots,n.\\) and \\(j = 1,\\ldots,n_i\\).\nThe total number of observations is \\(N = \\sum_{i = 1}^{n}n_i\\)."
  },
  {
    "objectID": "slides/15-hierarchical.html#why-two-indices",
    "href": "slides/15-hierarchical.html#why-two-indices",
    "title": "Hierarchical Models",
    "section": "Why Two Indices?",
    "text": "Why Two Indices?\nHaving two indices allows us to model both:\n\nWithin-group variation (differences between patients within the same hospital).\nBetween-group variation (differences between hospitals).\n\nThe hierarchical structure captures both types of variation."
  },
  {
    "objectID": "slides/15-hierarchical.html#visualizing-hierarchical-data",
    "href": "slides/15-hierarchical.html#visualizing-hierarchical-data",
    "title": "Hierarchical Models",
    "section": "Visualizing Hierarchical Data",
    "text": "Visualizing Hierarchical Data\nConsider this visualization of students within schools:\n\nEach point represents a student’s data.\nThe points are grouped by school, indicating that students from the same school are likely to have similar performance due to shared school-level factors.\n\nThis is a typical example of hierarchical data."
  },
  {
    "objectID": "slides/15-hierarchical.html#hierarchical-data-in-a-random-intercept-model",
    "href": "slides/15-hierarchical.html#hierarchical-data-in-a-random-intercept-model",
    "title": "Hierarchical Models",
    "section": "Hierarchical Data in a Random Intercept Model",
    "text": "Hierarchical Data in a Random Intercept Model\nNow, we can see how hierarchical data appears in our random intercept model:\n\\[\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + u_j + \\epsilon_{ij}\n\\]\n\n\\(Y_{ij}\\): response for student \\(i\\) in school \\(j\\),\n\\(u_j\\): random intercept for school \\(j\\), accounting for school-level variation,\n\\(\\epsilon_{ij}\\): residual error for student \\(i\\) within school \\(j\\).\n\nThis model allows us to account for the correlation within schools."
  },
  {
    "objectID": "slides/15-hierarchical.html#next-steps",
    "href": "slides/15-hierarchical.html#next-steps",
    "title": "Hierarchical Models",
    "section": "Next Steps",
    "text": "Next Steps\nWith this foundation, we’ll move forward and see how hierarchical models work in Bayesian analysis to account for such dependencies."
  },
  {
    "objectID": "slides/15-hierarchical.html#real-world-dependent-observations",
    "href": "slides/15-hierarchical.html#real-world-dependent-observations",
    "title": "Hierarchical Models",
    "section": "Real-World: Dependent Observations",
    "text": "Real-World: Dependent Observations\nHowever, in real-world data, the independence assumption often does not hold:\n\nRepeated measures data (e.g., same individual over time).\nClustered data (e.g., patients within a hospital).\nLongitudinal data (e.g., disease severity measures over time).\nSpatial data (e.g., disease counts observed across zip codes)."
  },
  {
    "objectID": "slides/15-hierarchical.html#hierarchical-model",
    "href": "slides/15-hierarchical.html#hierarchical-model",
    "title": "Hierarchical Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nNow, we can see how hierarchical data appears in the model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\nSubject-specific objects:\n\n\\(Y_{ij}\\): response for patient \\(j\\) in hospital \\(i\\).\n\\(\\mathbf{x}_{ij}\\) are the predictors for patient \\(j\\) in hospital \\(i\\).\n\\(\\epsilon_{ij}\\): residual error for patient \\(j\\) in hospital \\(i\\)."
  },
  {
    "objectID": "slides/15-hierarchical.html#understanding-the-correlation-structure",
    "href": "slides/15-hierarchical.html#understanding-the-correlation-structure",
    "title": "Hierarchical Models",
    "section": "Understanding the Correlation Structure",
    "text": "Understanding the Correlation Structure\n\nThe random intercept \\(\\theta_i\\) introduces correlation between observations within the same group.\nFor two observations \\(Y_{ij}\\) and \\(Y_{ik}\\) from the same group \\(i\\), we have:\n\n\\[\\begin{aligned}\n\\mathbb{C}(Y_{ij}, Y_{ik} | \\alpha,\\boldsymbol{\\beta},\\sigma) &=  \\mathbb{C}(\\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij}, \\alpha + \\mathbf{x}_{ik} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ik})\\\\\n&= \\mathbb{C}(\\theta_i, \\theta_i)\\\\\n&= \\mathbb{V}(\\theta_i).\n\\end{aligned}\\]\n\nThis non-zero covariance reflects the correlation between observations in the same group.\nNote: \\(\\mathbb{C}(Y_{ij}, Y_{i'k}) = 0\\) for \\(i \\neq i'\\).\nTo model the correlation, we need to specify the moments of \\(\\theta_i\\)."
  },
  {
    "objectID": "slides/15-hierarchical.html#introducing-the-school-specific-effects",
    "href": "slides/15-hierarchical.html#introducing-the-school-specific-effects",
    "title": "Hierarchical Models",
    "section": "Introducing the School-Specific Effects",
    "text": "Introducing the School-Specific Effects\nNow that we have the basic structure of the random intercept model, let’s talk about how we model the school-specific intercepts.\nIn our hierarchical model, we introduced a random intercept \\(u_j\\) for each school \\(j\\)."
  },
  {
    "objectID": "slides/15-hierarchical.html#school-specific-intercepts-u_j",
    "href": "slides/15-hierarchical.html#school-specific-intercepts-u_j",
    "title": "Hierarchical Models",
    "section": "School-Specific Intercepts: \\(u_j\\)",
    "text": "School-Specific Intercepts: \\(u_j\\)\nEach school \\(j\\) has a random intercept \\(u_j\\), which represents how that school’s baseline (e.g., student performance) deviates from the population average.\nWe model \\(u_j\\) as a random effect drawn from a normal distribution centered at zero, with some variance \\(\\sigma_u^2\\):\n\\[\nu_j \\sim \\mathcal{N}(0, \\sigma_u^2)\n\\]\n\nCentering at zero: This assumption reflects that, on average, schools don’t deviate from the population mean.\nVariance \\(\\sigma_u^2\\): This represents the variability in school-level intercepts. A larger \\(\\sigma_u^2\\) implies greater variability between schools."
  },
  {
    "objectID": "slides/15-hierarchical.html#why-normal-distribution",
    "href": "slides/15-hierarchical.html#why-normal-distribution",
    "title": "Hierarchical Models",
    "section": "Why Normal Distribution?",
    "text": "Why Normal Distribution?\n\nNatural Assumption: We assume that the school-specific parameters (e.g., school intercepts) are normally distributed with a mean of zero because there’s no reason to expect systematic deviations from the population average.\nFlexibility: The normal distribution allows us to model a wide range of variation in school effects, with the variance \\(\\tau^2\\) capturing how much schools differ from each other."
  },
  {
    "objectID": "slides/15-hierarchical.html#covariance-structure",
    "href": "slides/15-hierarchical.html#covariance-structure",
    "title": "Hierarchical Models",
    "section": "Covariance Structure",
    "text": "Covariance Structure\n\nThe variance \\(\\tau^2\\) for \\(\\theta_i\\) can be interpreted as the covariance between two observations from the same hospital.\nThis reflects how much two observations from the same group are expected to be similar in terms of their outcomes.\n\n\\[\\begin{aligned}\n\\mathbb{C}(Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega}) &= \\mathbb{V}(\\theta_i)\\\\\n&= \\tau^2.\n\\end{aligned}\\]\n\nThus, \\(\\tau^2\\) dictates the within-group correlation in our model.\nNote: \\(\\mathbb{C}(Y_{ij}, Y_{i'k} | \\boldsymbol{\\Omega}) = 0\\) for \\(i \\neq i'\\)."
  },
  {
    "objectID": "slides/15-hierarchical.html#identifiability-issues",
    "href": "slides/15-hierarchical.html#identifiability-issues",
    "title": "Hierarchical Models",
    "section": "Identifiability Issues",
    "text": "Identifiability Issues\n\nPopulation Intercept (\\(\\alpha\\)): This is the average intercept for the entire population, i.e., the baseline outcome across all hospitals.\nGroup-Specific Intercept (\\(\\alpha + \\theta_i\\)): The group-specific intercept, where \\(\\theta_i\\) represents the deviation from the population intercept for group \\(i\\).\n\nWe face an identifiability issue when estimating the population intercept and group-specific intercepts. We could add the same constant to all \\(\\theta_i\\)’s and subtract that constant from \\(\\alpha\\).\n\nThis is solved by setting \\(\\theta_i\\) to be mean zero apriori."
  },
  {
    "objectID": "slides/15-hierarchical.html#the-identifiability-problem",
    "href": "slides/15-hierarchical.html#the-identifiability-problem",
    "title": "Hierarchical Models",
    "section": "The Identifiability Problem",
    "text": "The Identifiability Problem\nResolution: Centering the School Intercepts\nTo resolve this, we center the school-specific intercepts by subtracting the overall population mean from each school’s intercept. This ensures that the population-level intercept \\(\\beta_0\\) represents the average outcome when \\(u_j = 0\\).\nThe model then becomes:\n\\[\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + (u_j - \\bar{u}) + \\epsilon_{ij}\n\\]\nWhere \\(\\bar{u}\\) is the average of the random intercepts across all schools, and this centering resolves the identifiability issue."
  },
  {
    "objectID": "slides/15-hierarchical.html#summary-of-key-points",
    "href": "slides/15-hierarchical.html#summary-of-key-points",
    "title": "Hierarchical Models",
    "section": "Summary of Key Points",
    "text": "Summary of Key Points\n\nSchool-specific intercepts \\(u_j\\) are modeled as random effects, assumed to come from a normal distribution centered at zero with variance \\(\\sigma_u^2\\).\nThe variance \\(\\sigma_u^2\\) captures the variability in school-level effects and the within-group correlation.\nThe identifiability issue arises from the redundancy between the population intercept and the school-specific intercepts.\nCentering the school-specific intercepts ensures that the population intercept \\(\\beta_0\\) is well-defined and identifiable."
  },
  {
    "objectID": "slides/15-hierarchical.html#school-specific-parameters-theta_i",
    "href": "slides/15-hierarchical.html#school-specific-parameters-theta_i",
    "title": "Hierarchical Models",
    "section": "School-Specific Parameters: \\(\\theta_i\\)",
    "text": "School-Specific Parameters: \\(\\theta_i\\)\nEach school \\(i\\) has a school-specific parameter \\(\\theta_i\\), which represents how that school’s baseline (e.g., student performance) deviates from the population average.\nWe model \\(\\theta_i\\) as a parameter drawn from a normal distribution centered at zero, with some variance \\(\\tau^2\\):\n\\[\\theta_i \\stackrel{iid}{\\sim} N(0, \\tau^2).\\]\n\nMean at zero: This assumption reflects that, on average, schools don’t deviate from the population mean.\nVariance \\(\\tau^2\\): This represents the variability in school-level intercepts. A larger \\(\\tau^2\\) implies greater variability between schools."
  },
  {
    "objectID": "slides/15-hierarchical.html#fitting-the-marginal-model-in-stan",
    "href": "slides/15-hierarchical.html#fitting-the-marginal-model-in-stan",
    "title": "Hierarchical Models",
    "section": "Fitting the Marginal Model in Stan",
    "text": "Fitting the Marginal Model in Stan\nNeed ragged data structure.\n\n// marginal-model.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; N;\n  int&lt;lower = 1&gt; p;\n  matrix[N, p] X;\n  vector[N] Y;\n  int n_is[n];\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; tau;\n}\ntransformed parameters {\n  real sigma2 = sigma * sigma;\n  real tau2 = tau * tau;\n}\nmodel {\n  // compute the mean process for the marginal model\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta;\n  }\n  // evaluate the likelihood for the marginal model using ragged data structure\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    matrix[n_i, n_i] Upsilon_i = sigma2 * diag_matrix(rep_vector(1.0, n_i)) + tau2 * rep_matrix(1, n_i, n_i);\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    vector[n_i] mu_i = segment(mu, pos, n_i);\n    target += multi_normal_lpdf(Y_i | mu_i, Upsilon_i);\n    pos = pos + n_i;\n  }\n  // priors\n  target += normal_lpdf(alpha | 0, 3);\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n}\ngenerated quantities {\n  // compute the mean process for the marginal model\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta;\n  }\n  // compute theta using the ragged data structure\n  int pos;\n  pos = 1;\n  vector[n] theta;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    vector[n_i] ones_i = rep_vector(1.0, n_i);\n    matrix[n_i, n_i] Upsilon_i = sigma2 * diag_matrix(ones_i) + tau2 * rep_matrix(1, n_i, n_i);\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    vector[n_i] mu_i = segment(mu, pos, n_i);\n    real mean_theta_i = tau2 * ones_i' * inverse_spd(Upsilon_i) * (Y_i - mu_i);\n    real var_theta_i = tau2 - tau2 * tau2 * ones_i' * inverse_spd(Upsilon_i) * ones_i;\n    theta[i] = normal_rng(mean_theta_i, sqrt(var_theta_i));\n    pos = pos + n_i;\n  }\n}"
  },
  {
    "objectID": "slides/15-hierarchical.html#fitting-the-conditional-model-in-stan",
    "href": "slides/15-hierarchical.html#fitting-the-conditional-model-in-stan",
    "title": "Hierarchical Models",
    "section": "Fitting the Conditional Model in Stan",
    "text": "Fitting the Conditional Model in Stan\n\n// conditional-model.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; N;\n  int&lt;lower = 1&gt; p;\n  matrix[N, p] X;\n  vector[N] Y;\n  int&lt;lower = 1, upper = n&gt; Ids[N];\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; tau;\n  vector[n] theta;\n}\nmodel {\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta + theta[Ids[i]];\n  }\n  target += normal_lpdf(Y | mu, sigma);\n  target += normal_lpdf(theta | 0, tau);\n  target += normal_lpdf(alpha | 0, 3);\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n}\ngenerated quantities {\n  real Intercept_East = alpha + theta[1];\n  real Intercept_North = alpha + theta[2];\n  real Intercept_South = alpha + theta[3];\n  real Intercept_West = alpha + theta[4];\n  real rho = 1 / (1 + ((sigma * sigma) / (tau * tau)));\n  vector[N] Y_pred;\n  vector[N] log_lik;\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta + theta[Ids[i]];\n    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);\n    Y_pred[i] = normal_rng(mu[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/15-hierarchical.html#conceptualizing-hierarchical-data",
    "href": "slides/15-hierarchical.html#conceptualizing-hierarchical-data",
    "title": "Hierarchical Models",
    "section": "Conceptualizing Hierarchical Data",
    "text": "Conceptualizing Hierarchical Data\nConsider the example of patients within hospitals:\n\nEach data point (i.e., observed data \\(Y_{ij}\\)) represents an outcome measured on a patient.\nThe data points are grouped by hospital, indicating that patients from the same hospital are likely to have similar outcomes due to shared hospital-level factors.\n\nThis is a typical example of hierarchical data."
  },
  {
    "objectID": "slides/15-hierarchical.html#likelihood",
    "href": "slides/15-hierarchical.html#likelihood",
    "title": "Hierarchical Models",
    "section": "Likelihood",
    "text": "Likelihood\nDefine \\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) and \\(\\mathbf{Y} = (\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n)\\). The likelihood can be written as,\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\Omega}) &= \\prod_{i=1}^n \\prod_{j = 1}^{n_i} f(Y_{ij} | \\alpha, \\boldsymbol{\\beta}, \\theta_i)\\\\\n&=\\prod_{i=1}^n f(\\mathbf{Y}_i | \\alpha, \\boldsymbol{\\beta}, \\theta_i),\n\\end{aligned}\\] where \\(\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta}, \\boldsymbol{\\theta},\\sigma,\\tau)\\)."
  },
  {
    "objectID": "slides/15-hierarchical.html#group-level-likelihood",
    "href": "slides/15-hierarchical.html#group-level-likelihood",
    "title": "Hierarchical Models",
    "section": "Group-level Likelihood",
    "text": "Group-level Likelihood\n\\[\\mathbf{Y}_i = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i\\boldsymbol{\\beta} + \\theta_i \\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}_i,\\quad \\boldsymbol{\\epsilon}_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#moments-for-the-marginal-model",
    "href": "slides/15-hierarchical.html#moments-for-the-marginal-model",
    "title": "Hierarchical Models",
    "section": "Moments for the Marginal Model",
    "text": "Moments for the Marginal Model\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{ij} | \\boldsymbol{\\Omega}] &= \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta}\\\\\n\\mathbb{V}(Y_{ij} | \\boldsymbol{\\Omega}) &= \\tau^2 + \\sigma^2\\\\\n\\mathbb{C}(Y_{ij}, Y_{lk} | \\boldsymbol{\\Omega}) &= 0,\\quad i \\neq l\\\\\n\\mathbb{C}(Y_{ij}, Y_{lk} | \\boldsymbol{\\Omega}) &= \\tau^2,\\quad i = l,\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#induced-within-correlation",
    "href": "slides/15-hierarchical.html#induced-within-correlation",
    "title": "Hierarchical Models",
    "section": "Induced Within Correlation",
    "text": "Induced Within Correlation\n\\[\\begin{aligned}\n\\rho (Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega}) &= \\frac{\\mathbb{C}(Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega})}{\\sqrt{\\mathbb{V}(Y_{ij} |  \\boldsymbol{\\Omega}) \\mathbb{V}(Y_{ik} |  \\boldsymbol{\\Omega})}}\\\\\n&=\\frac{\\tau^2}{\\tau^2 + \\sigma^2}\\\\\n&= \\frac{1}{1 + \\frac{\\sigma^2}{\\tau^2}}.\n\\end{aligned}\\]\nThis model induces positive correlation within group observations."
  },
  {
    "objectID": "slides/15-hierarchical.html#hospital-specific-parameters-theta_i",
    "href": "slides/15-hierarchical.html#hospital-specific-parameters-theta_i",
    "title": "Hierarchical Models",
    "section": "Hospital-Specific Parameters: \\(\\theta_i\\)",
    "text": "Hospital-Specific Parameters: \\(\\theta_i\\)\nEach hospital \\(i\\) has a hospital-specific parameter \\(\\theta_i\\), which represents how that hospital’s baseline (e.g., health outcomes) deviates from the population average.\nWe model \\(\\theta_i\\) as a parameter drawn from a normal distribution centered at zero, with some variance \\(\\tau^2\\):\n\\[\\theta_i \\stackrel{iid}{\\sim} N(0, \\tau^2).\\]\n\nMean at zero: This assumption reflects that, on average, hospitals don’t deviate from the population mean (helps with identifiability).\nVariance \\(\\tau^2\\): This represents the variability in hospital-level intercepts. A larger \\(\\tau^2\\) implies greater variability between hospitals."
  },
  {
    "objectID": "slides/15-hierarchical.html#induced-within-correlation-1",
    "href": "slides/15-hierarchical.html#induced-within-correlation-1",
    "title": "Hierarchical Models",
    "section": "Induced Within Correlation",
    "text": "Induced Within Correlation\n\\[\\rho (Y_{ij}, Y_{ik} | \\alpha,\\boldsymbol{\\beta},\\sigma) = \\frac{1}{1 + \\frac{\\sigma^2}{\\tau^2}}\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#group-specific-intercept-model-conditional-specification",
    "href": "slides/15-hierarchical.html#group-specific-intercept-model-conditional-specification",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\nFor \\(i = 1,\\ldots,n\\) and \\(j = 1,\\ldots,n_i\\), \\[\\begin{aligned}\nY_{ij} | \\boldsymbol{\\Omega},\\theta_i &\\stackrel{ind}{\\sim} N(\\alpha + \\mathbf{x}_{ij}\\boldsymbol{\\beta} + \\theta_i,\\sigma^2)\\\\\n\\theta_i | \\tau^2 &\\stackrel{iid}{\\sim} N(0,\\tau^2)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta},\\sigma,\\tau)\\) are the population parameters."
  },
  {
    "objectID": "slides/15-hierarchical.html#group-specific-intercept-model-conditional-specification-1",
    "href": "slides/15-hierarchical.html#group-specific-intercept-model-conditional-specification-1",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\n\nMoments for the Conditional Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{ij} | \\boldsymbol{\\Omega},\\theta_i] &= \\alpha + \\mathbf{x}_{ij}\\boldsymbol{\\beta} + \\theta_i\\\\\n\\mathbb{V}(Y_{ij} | \\boldsymbol{\\Omega},\\theta_i) &= \\sigma^2\\\\\n\\mathbb{C}(Y_{ij}, Y_{lk} | \\boldsymbol{\\Omega},\\theta_i,\\theta_l) &= 0,\\quad \\forall i,j,l,k.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#moments-for-the-conditional-model",
    "href": "slides/15-hierarchical.html#moments-for-the-conditional-model",
    "title": "Hierarchical Models",
    "section": "Moments for the Conditional Model",
    "text": "Moments for the Conditional Model\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{ij} | \\boldsymbol{\\Omega},\\theta_i] &= \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\theta_i\\\\\n\\mathbb{V}(Y_{ij} | \\boldsymbol{\\Omega},\\theta_i) &= \\sigma^2\\\\\n\\mathbb{C}(Y_{ij}, Y_{lk} | \\boldsymbol{\\Omega}) &= 0,\\quad \\forall i,j,l,k.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#group-specific-intercept-model-conditional-specification-2",
    "href": "slides/15-hierarchical.html#group-specific-intercept-model-conditional-specification-2",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\n\nDefine \\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) and \\(\\mathbf{Y} = (\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n)\\).\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n \\prod_{j = 1}^{n_i} f(Y_{ij} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})  \\prod_{i=1}^n f(\\theta_i | \\tau^2) f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_n)\\)."
  },
  {
    "objectID": "slides/15-hierarchical.html#group-specific-intercept-model-marginal-specification",
    "href": "slides/15-hierarchical.html#group-specific-intercept-model-marginal-specification",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\nTo derive a marginal model it is useful to write the model at the level of the independent observations, \\(\\mathbf{Y}_i\\).\n\\[\\mathbf{Y}_i = \\begin{bmatrix}\n    Y_{i1}\\\\\n    Y_{i2}\\\\\n    \\vdots\\\\\n    Y_{in_i}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    \\alpha + \\mathbf{x}_{i1} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{i1}\\\\\n    \\alpha + \\mathbf{x}_{i2} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{i2}\\\\\n    \\vdots \\\\\n    \\alpha + \\mathbf{x}_{in_i} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{in_i}\n  \\end{bmatrix} = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i \\boldsymbol{\\beta} + \\theta_i \\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}_i,\\] where \\(\\mathbf{1}_{n_i}\\) is an \\(n_i \\times 1\\) dimensional vector of ones, \\(\\mathbf{X}_i\\) is an \\(n_i \\times p\\) dimensional matrix with rows \\(\\mathbf{x}_{ij}\\).\n\n\\(\\boldsymbol{\\epsilon}_i = (\\epsilon_{i1},\\ldots,\\epsilon_{in_i}) \\stackrel{ind}{\\sim} N(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i})\\), with \\(\\mathbf{0}_{n_i}\\) an \\(n_i \\times 1\\) dimensional vector of zeros."
  },
  {
    "objectID": "slides/15-hierarchical.html#group-specific-intercept-model-conditional-specification-3",
    "href": "slides/15-hierarchical.html#group-specific-intercept-model-conditional-specification-3",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\n\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n f(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#group-specific-intercept-model-conditional-specification-4",
    "href": "slides/15-hierarchical.html#group-specific-intercept-model-conditional-specification-4",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\n\nDefine \\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) and \\(\\mathbf{Y} = (\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n)\\).\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n \\prod_{j = 1}^{n_i} f(Y_{ij} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})  \\prod_{i=1}^n f(\\theta_i | \\tau^2) f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_n)\\)."
  },
  {
    "objectID": "slides/15-hierarchical.html#group-specific-intercept-model-marginal-specification-1",
    "href": "slides/15-hierarchical.html#group-specific-intercept-model-marginal-specification-1",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\n\nMoments for the Marginal Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}] &= \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i\\boldsymbol{\\beta}\\\\\n\\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) &= \\tau^2 \\mathbf{1}_{n_i} \\mathbf{1}_{n_i}^\\top + \\sigma^2 \\mathbf{I}_{n_i} = \\boldsymbol{\\Upsilon}_i\\\\\n\\mathbb{C}(\\mathbf{Y}_{i}, \\mathbf{Y}_{i'} | \\boldsymbol{\\Omega}) &= \\mathbf{0}_{n_i \\times n_i},\\quad i \\neq i'.\n\\end{aligned}\\]\n\\[\\implies \\boldsymbol{\\Upsilon}_i = \\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) = \\begin{bmatrix}\n    \\tau^2 + \\sigma^2 & \\tau^2 & \\cdots & \\tau^2\\\\\n    \\tau^2 & \\tau^2 + \\sigma^2 & \\cdots & \\tau^2\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    \\tau^2 & \\tau^2 & \\cdots &\\tau^2 + \\sigma^2\n  \\end{bmatrix}.\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#group-specific-intercept-model-marginal-specification-2",
    "href": "slides/15-hierarchical.html#group-specific-intercept-model-marginal-specification-2",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\nFor \\(i = 1,\\ldots,n\\), \\[\\begin{aligned}\n\\mathbf{Y}_{i} | \\boldsymbol{\\Omega} &\\stackrel{ind}{\\sim} N(\\alpha \\mathbf{1}_{n_i}+ \\mathbf{X}_i\\boldsymbol{\\beta},\\boldsymbol{\\Upsilon}_i)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta},\\sigma,\\tau)\\) are the population parameters."
  },
  {
    "objectID": "slides/15-hierarchical.html#group-specific-intercept-model-marginal-specification-3",
    "href": "slides/15-hierarchical.html#group-specific-intercept-model-marginal-specification-3",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\n\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n f(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega}).\n\\end{aligned}\\]\nWhy might we be interested in fitting the marginal model?"
  },
  {
    "objectID": "slides/15-hierarchical.html#recovering-the-group-specific-parameters",
    "href": "slides/15-hierarchical.html#recovering-the-group-specific-parameters",
    "title": "Hierarchical Models",
    "section": "Recovering the Group-Specific Parameters",
    "text": "Recovering the Group-Specific Parameters\n\nWe can still recover the \\(\\theta_i\\) when we fit the marginal model, we only need to compute \\(f(\\theta_i | \\mathbf{Y}_i,\\boldsymbol{\\Omega})\\) for all \\(i\\).\nWe can obtain this full conditional by specifying the joint distribution,\n\n\\[f\\left(\\begin{bmatrix}\n    \\mathbf{Y}_i\\\\\n    \\theta_i\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega}\\right) = N\\left(\\begin{bmatrix}\n    \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i \\boldsymbol{\\beta} + \\theta_i \\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}_i\\\\\n    0\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol{\\Upsilon}_i & \\tau^2 \\mathbf{1}_{n_i}\\\\\n    \\tau^2 \\mathbf{1}_{n_i}^\\top & \\tau^2\n  \\end{bmatrix}\\right).\\]\nWe can then use the conditional specification of a multivariate normal to find, \\(f(\\theta_i | \\mathbf{Y}_i, \\boldsymbol{\\Omega}) = N(\\mathbb{E}_{\\theta_i},\\mathbb{V}_{\\theta_i})\\), where\n\\[\\begin{aligned}\n\\mathbb{E}_{\\theta_i} &= \\mathbf{0}_{n_i} + \\tau^2 \\mathbf{1}_{n_i}^\\top \\boldsymbol{\\Upsilon}_i^{-1} (\\mathbf{Y}_i - \\alpha \\mathbf{1}_{n_i} - \\mathbf{X}_i \\boldsymbol{\\beta})\\\\\n\\mathbb{V}_{\\theta_i} &= \\tau^2 - \\tau^4 \\mathbf{1}_{n_i}^\\top \\boldsymbol{\\Upsilon}_i^{-1} \\mathbf{1}_{n_i}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#resolution-centering-the-group-intercepts",
    "href": "slides/15-hierarchical.html#resolution-centering-the-group-intercepts",
    "title": "Hierarchical Models",
    "section": "Resolution: Centering the Group Intercepts",
    "text": "Resolution: Centering the Group Intercepts\nTo resolve this, we center the hospital-specific intercepts by subtracting the overall population mean from each hospital’s intercept. This ensures that the population-level intercept \\(\\alpha\\) represents the average outcome when \\(\\theta_i = 0\\)."
  },
  {
    "objectID": "prepare/prepare-feb25.html",
    "href": "prepare/prepare-feb25.html",
    "title": "Prepare for February 25 lecture",
    "section": "",
    "text": "📖 Read BDA3 Chapter 18.1-18.2 about missing data.\n📖 Read A Note on Bayesian Inference After Multiple Imputation by Zhou and Reiter 2010.\n✅ Finish HW 03, which is due before class on Thursday."
  },
  {
    "objectID": "slides/15-hierarchical.html#hierarchical-model-1",
    "href": "slides/15-hierarchical.html#hierarchical-model-1",
    "title": "Hierarchical Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nNow, we can see how hierarchical data appears in the model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\nGroup-specific objects:\n\n\\(\\theta_i\\): group-specific parameter for hospital \\(i\\), accounting for hospital-level variation (group-specific, random effect).\n\nThe group-specific parameters are responsible for inducing correlation into the model."
  },
  {
    "objectID": "slides/15-hierarchical.html#hierarchical-model-2",
    "href": "slides/15-hierarchical.html#hierarchical-model-2",
    "title": "Hierarchical Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nNow, we can see how hierarchical data appears in the model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\nPopulation parameters:\n\n\\(\\alpha\\): intercept for the entire population.\n\\(\\boldsymbol{\\beta}\\): regression parameters for the entire population.\n\\(\\sigma\\): residual error parameter for the entire population."
  },
  {
    "objectID": "slides/15-hierarchical.html#prior-for-theta_i",
    "href": "slides/15-hierarchical.html#prior-for-theta_i",
    "title": "Hierarchical Models",
    "section": "Prior for \\(\\theta_i\\)",
    "text": "Prior for \\(\\theta_i\\)\nWe model \\(\\theta_i\\) as a parameter drawn from a normal distribution centered at zero, with some variance \\(\\tau^2\\):\n\\[\\theta_i \\stackrel{iid}{\\sim} N(0, \\tau^2).\\]\n\nMean at zero: This assumption reflects that, on average, hospitals don’t deviate from the population mean (helps with identifiability).\nVariance \\(\\tau^2\\): This represents the variability in hospital-level intercepts. A larger \\(\\tau^2\\) implies greater variability between hospitals.\n\nEach hospital \\(i\\) has a hospital-specific parameter \\(\\theta_i\\), which represents how that hospital’s baseline (e.g., health outcomes) deviates from the population average."
  },
  {
    "objectID": "slides/15-hierarchical.html#lets-see-this-model-in-action",
    "href": "slides/15-hierarchical.html#lets-see-this-model-in-action",
    "title": "Hierarchical Models",
    "section": "Lets see this model in action!",
    "text": "Lets see this model in action!"
  },
  {
    "objectID": "slides/15-hierarchical.html#example-data-glucose-measurement-in-4-primary-care-clinics",
    "href": "slides/15-hierarchical.html#example-data-glucose-measurement-in-4-primary-care-clinics",
    "title": "Hierarchical Models",
    "section": "Example data: Glucose Measurement in 4 Primary Care Clinics",
    "text": "Example data: Glucose Measurement in 4 Primary Care Clinics\n\nWe will study glucose values for patients being seen at 4 primary care clinics across the city. The clinics each represent a geographical region: east, west, north, and south.\nThe dataset consists of glucose measurements (mg/dl) from patients, and also risk factors:\n\nAge (years).\nBMI (\\(kg/m^2\\)).\nSedx (0 = male, 1 = female).\nSmoking status (0 = non-smoker, 1 = smoker).\nPhysical activity level (0 = low, 1 = moderate, 2 = high).\nGlucose lowering medication (0 = none, 1 = yes)."
  },
  {
    "objectID": "slides/15-hierarchical.html#goals",
    "href": "slides/15-hierarchical.html#goals",
    "title": "Hierarchical Models",
    "section": "Goals:",
    "text": "Goals:\n\nTo explore the distribution of glucose levels in different hospital settings.\nTo compare glucose levels across the four hospitals.\nTo identify any potential patterns or variations in glucose levels by hospital.\n\nThe simulation provides a basis for understanding potential differences in glucose values that might arise in different healthcare environments."
  },
  {
    "objectID": "slides/15-hierarchical.html#preview-the-data",
    "href": "slides/15-hierarchical.html#preview-the-data",
    "title": "Hierarchical Models",
    "section": "Preview the Data",
    "text": "Preview the Data"
  },
  {
    "objectID": "slides/15-hierarchical.html#fitting-a-model",
    "href": "slides/15-hierarchical.html#fitting-a-model",
    "title": "Hierarchical Models",
    "section": "Fitting a Model",
    "text": "Fitting a Model\nWe would like to fit the following model: \\(Y_{ij} \\stackrel{ind}{\\sim}N(\\mu_{ij},\\sigma^2)\\), where \\(Y_{ij}\\) is the glucose value for patient \\(i\\) in clinic \\(j\\) and\n\\[\\begin{aligned}\n\\mu_{ij} &= \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i\\\\\n&= (\\alpha + \\theta_i) + \\beta_1 Age_{ij} + \\beta_2 BMI_i + \\beta_3 Female_i \\\\\n&\\quad + \\beta_4 Smoker_i + \\beta_5 Moderate\\_Activity_i\\\\\n&\\quad + \\beta_6 High\\_Activity_i + \\beta_7 On\\_Meds_i.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#writing-down-a-model",
    "href": "slides/15-hierarchical.html#writing-down-a-model",
    "title": "Hierarchical Models",
    "section": "Writing down a model",
    "text": "Writing down a model\nWe would like to fit the following model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\n\n\\(Y_{ij}\\) is the glucose value for patient \\(i\\) in clinic \\(j\\)\n\\(\\theta_i\\) for \\(i = 1,\\ldots,4\\) is the clinic-specific intercept deviation. \\[\\begin{aligned}\n\\mathbf{x}_{ij} &= (Age_{ij}, BMI_{ij}, Female_{ij},Smoker_{ij}, \\\\\n&\\quad Moderate\\_Activity_{ij}, High\\_Activity_{ij}, On\\_Meds_{ij}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-hierarchical.html#fitting-the-model-in-stan",
    "href": "slides/15-hierarchical.html#fitting-the-model-in-stan",
    "title": "Hierarchical Models",
    "section": "Fitting the Model in Stan",
    "text": "Fitting the Model in Stan\n\nX &lt;- model.matrix(~ age + bmi + gender + smoking + as.factor(activity) + medication, data = data)[, -1]\nstan_data &lt;- list(\n  N = nrow(data),\n  n = length(unique(data$region)),\n  p = ncol(X),\n  X = X,\n  Y = data$glucose,\n  Ids = as.numeric(as.factor(data$region))\n)\nconditional_model &lt;- stan_model(model_code = \"conditional-model.stan\")\nfit_conditional &lt;- sampling(conditional_model, stan_data)"
  },
  {
    "objectID": "slides/15-hierarchical.html#assessing-convergence",
    "href": "slides/15-hierarchical.html#assessing-convergence",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(fit_conditional, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/15-hierarchical.html#posterior-summaries",
    "href": "slides/15-hierarchical.html#posterior-summaries",
    "title": "Hierarchical Models",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nprint(fit_conditional, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\", \"rho\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nalpha    99.03    0.10 3.77  91.46  96.59  99.06 101.61 106.29  1533    1\nbeta[1]   0.26    0.00 0.03   0.20   0.23   0.26   0.28   0.32  4245    1\nbeta[2]   0.48    0.00 0.07   0.35   0.44   0.48   0.53   0.62  4989    1\nbeta[3]  10.50    0.01 0.65   9.18  10.06  10.51  10.93  11.75  5966    1\nbeta[4]  -4.75    0.01 0.66  -6.03  -5.20  -4.76  -4.30  -3.46  7324    1\nbeta[5]   0.28    0.01 0.78  -1.21  -0.26   0.29   0.80   1.82  4746    1\nbeta[6]  -4.53    0.01 0.77  -6.03  -5.06  -4.53  -4.00  -3.02  4869    1\nbeta[7] -17.89    0.01 0.64 -19.15 -18.31 -17.89 -17.47 -16.64  5387    1\nsigma     7.43    0.00 0.23   6.99   7.28   7.43   7.59   7.90  5900    1\ntau       4.58    0.02 1.33   2.59   3.60   4.35   5.34   7.65  2941    1\nrho       0.27    0.00 0.11   0.11   0.19   0.26   0.34   0.51  3136    1\n\nSamples were drawn using NUTS(diag_e) at Wed Feb 26 10:48:54 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/15-hierarchical.html#assessing-convergence-1",
    "href": "slides/15-hierarchical.html#assessing-convergence-1",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_conditional, regex_pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/15-hierarchical.html#explore-the-clinic-specific-variation",
    "href": "slides/15-hierarchical.html#explore-the-clinic-specific-variation",
    "title": "Hierarchical Models",
    "section": "Explore the Clinic-Specific Variation",
    "text": "Explore the Clinic-Specific Variation\n\nprint(fit_conditional, pars = c(\"Intercept_East\", \"Intercept_South\", \"Intercept_North\", \"Intercept_West\"), probs = c(0.025, 0.975))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n                  mean se_mean   sd  2.5%  97.5% n_eff Rhat\nIntercept_East  102.08    0.05 2.88 96.55 107.62  3461    1\nIntercept_South  98.46    0.05 2.88 92.89 103.99  3498    1\nIntercept_North  92.35    0.05 2.87 86.78  97.90  3398    1\nIntercept_West  103.55    0.05 2.90 97.85 109.10  3502    1\n\nSamples were drawn using NUTS(diag_e) at Wed Feb 26 10:48:54 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/15-hierarchical.html#explore-the-clinic-specific-variation-1",
    "href": "slides/15-hierarchical.html#explore-the-clinic-specific-variation-1",
    "title": "Hierarchical Models",
    "section": "Explore the Clinic-Specific Variation",
    "text": "Explore the Clinic-Specific Variation"
  },
  {
    "objectID": "slides/15-hierarchical.html#compare-to-linear-regression",
    "href": "slides/15-hierarchical.html#compare-to-linear-regression",
    "title": "Hierarchical Models",
    "section": "Compare to Linear Regression",
    "text": "Compare to Linear Regression"
  },
  {
    "objectID": "slides/15-hierarchical.html#comparison-to-linear-regression-boldsymbolbeta",
    "href": "slides/15-hierarchical.html#comparison-to-linear-regression-boldsymbolbeta",
    "title": "Hierarchical Models",
    "section": "Comparison to Linear Regression: \\(\\boldsymbol{\\beta}\\)",
    "text": "Comparison to Linear Regression: \\(\\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "slides/15-hierarchical.html#model-comparison",
    "href": "slides/15-hierarchical.html#model-comparison",
    "title": "Hierarchical Models",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nlibrary(loo)\nwaic_conditional &lt;- waic(extract_log_lik(fit_conditional))\nwaic_lin_reg &lt;- waic(extract_log_lik(fit_lin_reg))\ncomparison &lt;- loo_compare(list(\"Hierarchical Model\" = waic_conditional, \"Linear Regression\" = waic_lin_reg))\nprint(comparison, simplify = FALSE)\n\n                   elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic\nHierarchical Model     0.0       0.0 -1720.6      15.2         11.5     0.7  \nLinear Regression    -71.6      10.7 -1792.2      15.6          8.6     0.6  \n                   waic    se_waic\nHierarchical Model  3441.2    30.4\nLinear Regression   3584.3    31.1"
  },
  {
    "objectID": "slides/15-hierarchical.html#fitting-the-model-in-stan-1",
    "href": "slides/15-hierarchical.html#fitting-the-model-in-stan-1",
    "title": "Hierarchical Models",
    "section": "Fitting the Model in Stan",
    "text": "Fitting the Model in Stan\n\nstan_data &lt;- list(\n  N = nrow(data),\n  n = length(unique(data$region)),\n  p = ncol(X),\n  X = X,\n  Y = data$glucose,\n  n_is = as.numeric(table(data$region))\n)\nmarginal_model &lt;- stan_model(model_code = \"marginal-model.stan\")\nfit_marginal &lt;- sampling(marginal_model, stan_data)"
  },
  {
    "objectID": "slides/15-hierarchical.html#assessing-convergence-2",
    "href": "slides/15-hierarchical.html#assessing-convergence-2",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(fit_marginal, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\"))"
  },
  {
    "objectID": "slides/15-hierarchical.html#assessing-convergence-3",
    "href": "slides/15-hierarchical.html#assessing-convergence-3",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_marginal, regex_pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\"))"
  },
  {
    "objectID": "slides/15-hierarchical.html#posterior-summaries-1",
    "href": "slides/15-hierarchical.html#posterior-summaries-1",
    "title": "Hierarchical Models",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nprint(fit_marginal, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nalpha    99.07    0.06 3.66  91.97  96.66  99.05 101.50 106.36  3285    1\nbeta[1]   0.26    0.00 0.03   0.20   0.24   0.26   0.28   0.32  3957    1\nbeta[2]   0.48    0.00 0.07   0.35   0.44   0.48   0.53   0.61  3389    1\nbeta[3]  10.48    0.01 0.67   9.18  10.01  10.49  10.95  11.76  4532    1\nbeta[4]  -4.75    0.01 0.65  -6.01  -5.18  -4.75  -4.34  -3.45  4611    1\nbeta[5]   0.29    0.01 0.77  -1.26  -0.23   0.28   0.79   1.77  3834    1\nbeta[6]  -4.51    0.01 0.79  -6.04  -5.04  -4.50  -4.00  -3.01  3855    1\nbeta[7] -17.88    0.01 0.66 -19.14 -18.33 -17.88 -17.44 -16.56  4370    1\nsigma     7.43    0.00 0.24   6.99   7.27   7.43   7.60   7.92  4223    1\ntau       4.55    0.02 1.27   2.61   3.62   4.38   5.27   7.48  3714    1\n\nSamples were drawn using NUTS(diag_e) at Wed Feb 26 11:25:39 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "prepare/prepare-feb27.html",
    "href": "prepare/prepare-feb27.html",
    "title": "Prepare for February 27 lecture",
    "section": "",
    "text": "📖 Read pages 175-182 from Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists.\n✅ Finish HW 03, which is due before class on Thursday."
  },
  {
    "objectID": "slides/16-longitudinal.html#review-of-last-lecture",
    "href": "slides/16-longitudinal.html#review-of-last-lecture",
    "title": "Longitudinal Data",
    "section": "",
    "text": "During our last lecture, we introduced correlated (or dependent) data sources.\nWe discussed the idea of accounting for dependencies within a group using group-specific parameters.\nWe introduced the random intercept model and studied the induced correlation (forced to be positive) in the marginal model.\nToday we will look at longitudinal data and introduce a simple model that accounts for group-level changes."
  },
  {
    "objectID": "slides/16-longitudinal.html#prepare-for-next-class",
    "href": "slides/16-longitudinal.html#prepare-for-next-class",
    "title": "Longitudinal Data",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 04, which will be assigned soon. It’s not due until March 25.\nEnjoy your spring break!"
  },
  {
    "objectID": "slides/16-longitudinal.html#longitudinal-data",
    "href": "slides/16-longitudinal.html#longitudinal-data",
    "title": "Longitudinal Data",
    "section": "Longitudinal Data",
    "text": "Longitudinal Data\nRepeated measurements taken over time from the same subjects. Examples include:\n\nMonitor Disease Progression: Track how diseases evolve, such as diabetes or glaucoma.\nEvaluate Treatments: Understand how interventions work over time.\nPersonalized Health Insights: Capture individual health trajectories for personalized care.\nStudy Long-Term Effects: Evaluate the long-term outcomes of medical treatments or behaviors."
  },
  {
    "objectID": "slides/16-longitudinal.html#linear-mixed-model",
    "href": "slides/16-longitudinal.html#linear-mixed-model",
    "title": "Longitudinal Data",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nThe subject-specific intercepts and slope model can be seen as a special case of the linear mixed model (LMM). For \\(i = 1,\\ldots,n\\), LMM is defined as:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\n\\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) are subject-level observations.\n\\(Y_{it}\\) is the \\(t\\)th observation in subject \\(i\\).\n\\(\\boldsymbol{\\epsilon}_i = (\\epsilon_{i1},\\ldots,\\epsilon_{in_i})\\), such that \\(\\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#longitudinal-data-1",
    "href": "slides/16-longitudinal.html#longitudinal-data-1",
    "title": "Longitudinal Data",
    "section": "Longitudinal Data",
    "text": "Longitudinal Data"
  },
  {
    "objectID": "slides/16-longitudinal.html#what-is-longitudinal-data",
    "href": "slides/16-longitudinal.html#what-is-longitudinal-data",
    "title": "Longitudinal Data",
    "section": "What is Longitudinal Data?",
    "text": "What is Longitudinal Data?\n\nRepeated measurements taken over time from the same subjects.\nUseful for studying health outcomes, interventions, and disease progression.\nAllows for understanding individual variation over time."
  },
  {
    "objectID": "slides/16-longitudinal.html#why-is-longitudinal-data-important-in-health-studies",
    "href": "slides/16-longitudinal.html#why-is-longitudinal-data-important-in-health-studies",
    "title": "Longitudinal Data",
    "section": "Why is Longitudinal Data Important in Health Studies?",
    "text": "Why is Longitudinal Data Important in Health Studies?\n\nMonitor Disease Progression: Track how diseases evolve, such as diabetes or glaucoma.\nEvaluate Treatments: Understand how interventions work over time.\nPersonalized Health Insights: Capture individual health trajectories for personalized care.\nStudy Long-Term Effects: Evaluate the long-term outcomes of medical treatments or behaviors."
  },
  {
    "objectID": "slides/16-longitudinal.html#example-glaucoma-disease-progression",
    "href": "slides/16-longitudinal.html#example-glaucoma-disease-progression",
    "title": "Longitudinal Data",
    "section": "Example: Glaucoma Disease Progression",
    "text": "Example: Glaucoma Disease Progression\nImagine we are tracking mean deviation (MD, dB), a key measure of visual field loss in glaucoma patients, over time.\n\nMultiple measurements of MD for each patient across several years.\nWe’re interested in glaucoma progression, which is defined as the rate of change in MD over time (dB/year).\nDefine \\(Y_{it}\\) as the MD value for eye \\(i\\) (\\(i = 1,\\ldots,n\\)) at time \\(t\\) (\\(t = 1,\\ldots,n_i\\)) and the time of each observation as \\(X_{it}\\) with \\(X_{i0} = 0\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#classical-approach-ols-regression-for-each-eye",
    "href": "slides/16-longitudinal.html#classical-approach-ols-regression-for-each-eye",
    "title": "Longitudinal Data",
    "section": "Classical Approach: OLS Regression for Each Eye",
    "text": "Classical Approach: OLS Regression for Each Eye\nIn the classical OLS regression model, we fit the model separately for each eye:\n\\[MD_{it} = \\alpha_i + time_{it} \\beta_i + \\epsilon_{it}\\]\nWhere: - \\(MD_{it}\\) is the mean deviation for eye \\(i\\) at time \\(t\\). - \\(\\alpha_i\\) and \\(\\beta_i\\) are the intercept and slope for the individual eye.\nProblem:\n\nFitting OLS for each eye separately allows each eye to have a unique intercept and slope.\nThis leads to overfitting since extreme values of \\(\\alpha_i\\) and \\(\\beta_i\\) are allowed, which may not be realistic or stable across the population.\nShrinking extreme values toward the population average is desirable."
  },
  {
    "objectID": "slides/16-longitudinal.html#random-intercepts-and-slopes",
    "href": "slides/16-longitudinal.html#random-intercepts-and-slopes",
    "title": "Longitudinal Data",
    "section": "Random Intercepts and Slopes",
    "text": "Random Intercepts and Slopes\nA random intercept and slope model overcomes the limitations of OLS by allowing each individual to have:\n\nRandom intercepts: Different baseline MD for each eye.\nRandom slopes: Different rates of progression (change in MD over time) for each eye.\n\nThe model can be expressed as:\n##MD_{it} = 0 + 1 time{it} + u{0i} + u_{1i} time_{it} + _{it}$$\nWhere: - \\(\\beta_0\\) is the overall average baseline MD (fixed intercept). - \\(\\beta_1\\) is the overall rate of disease progression (fixed slope). - \\(u_{0i}\\) is the random intercept for eye \\(i\\) (individual deviation from the overall baseline). - \\(u_{1i}\\) is the random slope for eye \\(i\\) (individual deviation in rate of progression). - \\(\\epsilon_{it}\\) is the residual error term.\nKey Advantage:\n\nShrinkage: The mixed model shrinks the individual estimates of \\(\\alpha_i\\) and \\(\\beta_i\\) toward the population average, preventing overfitting and making the estimates more stable."
  },
  {
    "objectID": "slides/16-longitudinal.html#ols-regression-for-each-eye",
    "href": "slides/16-longitudinal.html#ols-regression-for-each-eye",
    "title": "Longitudinal Data",
    "section": "OLS Regression for Each Eye",
    "text": "OLS Regression for Each Eye\nIn the classical OLS regression model, we fit the model separately for each eye:\n\\[MD_{it} = \\alpha_i + time_{it} \\beta_i + \\epsilon_{it}\\]\nWhere:\n\n\\(MD_{it}\\) is the mean deviation for eye \\(i\\) at time \\(t\\).\n\\(\\alpha_i\\) and \\(\\beta_i\\) are the intercept and slope for the individual eye.\n\nProblem:\n\nFitting OLS for each eye separately allows each eye to have a unique intercept and slope.\nThis leads to overfitting since extreme values of \\(\\alpha_i\\) and \\(\\beta_i\\) are allowed, which may not be realistic or stable across the population.\nShrinking extreme values toward the population average is desirable."
  },
  {
    "objectID": "slides/16-longitudinal.html#stan-code-for-independent-intercept-and-slope",
    "href": "slides/16-longitudinal.html#stan-code-for-independent-intercept-and-slope",
    "title": "Longitudinal Data",
    "section": "Stan code for independent intercept and slope",
    "text": "Stan code for independent intercept and slope\n\n// lmm-independent.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; N;\n  vector[N] Time;\n  vector[N] MD;\n  int&lt;lower = 1, upper = n&gt; Ids[N];\n}\nparameters {\n  real beta0;\n  real beta1;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] z0;\n  vector[n] z1;\n  real&lt;lower = 0&gt; tau0;\n  real&lt;lower = 0&gt; tau1;\n}\ntransformed parameters {\n  vector[n] theta0;\n  vector[n] theta1;\n  theta0 = tau0 * z0;\n  theta1 = tau1 * z1;\n}\nmodel {\n  // likelihood\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = (beta0 + theta0[Ids[i]]) + (beta1 + theta1[Ids[i]]) * Time[i];\n  }\n  target += normal_lpdf(MD | mu, sigma);\n  // subject-specific parameters\n  target += std_normal_lpdf(z0);\n  target += std_normal_lpdf(z1);\n  // population parameters\n  target += normal_lpdf(beta0 | 0, 3);\n  target += normal_lpdf(beta1 | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau0 | 0, 3);\n  target += normal_lpdf(tau1 | 0, 3);\n}"
  },
  {
    "objectID": "slides/16-longitudinal.html#stan-code-for-dependent-intercept-and-slope",
    "href": "slides/16-longitudinal.html#stan-code-for-dependent-intercept-and-slope",
    "title": "Longitudinal Data",
    "section": "Stan code for dependent intercept and slope",
    "text": "Stan code for dependent intercept and slope\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; N;\n  vector[N] Time;\n  vector[N] MD;\n  int&lt;lower = 1, upper = n&gt; Ids[N];\n}\nparameters {\n  real beta0;\n  real beta1;\n  real&lt;lower = 0&gt; sigma;\n  matrix[2, n] theta;\n  real&lt;lower = 0&gt; tau0;\n  real&lt;lower = 0&gt; tau1;\n  real&lt;lower = -1, upper = 1&gt; rho;\n}\ntransformed parameters {\n  cov_matrix[2] Sigma;\n  Sigma[1, 1] = tau0 * tau0;\n  Sigma[2, 1] = rho * tau0 * tau1;\n  Sigma[1, 2] = rho * tau0 * tau1;\n  Sigma[2, 2] = tau1 * tau1;\n}\nmodel {\n  // likelihood\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = (beta0 + theta[1, Ids[i]]) + (beta1 + theta[2, Ids[i]]) * Time[i];\n  }\n  target += normal_lpdf(MD | mu, sigma);\n  // subject-specific parameters\n  for (i in 1:n) {\n    target += multi_normal_lpdf(theta[, i] | rep_vector(0.0, 2), Sigma);\n  }\n  // population parameters\n  target += normal_lpdf(beta0 | 0, 3);\n  target += normal_lpdf(beta1 | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau0 | 0, 3);\n  target += normal_lpdf(tau1 | 0, 3);\n  target += uniform_lpdf(rho | -1, 1);\n}"
  },
  {
    "objectID": "slides/17-gps.html#recalling-what-went-before-the-break",
    "href": "slides/17-gps.html#recalling-what-went-before-the-break",
    "title": "Gaussian Processes",
    "section": "Recalling what went before the break",
    "text": "Recalling what went before the break"
  },
  {
    "objectID": "slides/17-gps.html#a-brief-agenda",
    "href": "slides/17-gps.html#a-brief-agenda",
    "title": "Gaussian Processes",
    "section": "A brief agenda",
    "text": "A brief agenda\nBy the end of this lecture you should:\n\nUnderstand the basic concepts of Gaussian process and its usefulness in statistical modeling\nCompute posterior and predictive inference for a simple Gaussian process model\nUse GP as a building block in modeling time-correlated data"
  },
  {
    "objectID": "slides/17-gps.html#motivation",
    "href": "slides/17-gps.html#motivation",
    "title": "Gaussian Processes",
    "section": "Motivation",
    "text": "Motivation\n\\[\nY_{it} = \\underbrace{X_i\\beta}_{\\text{\"Fixed effect\"}} + \\underbrace{\\eta_{it}}_{\\text{\"Random effect\"}} + \\epsilon_{it}\\quad\\text{for the }i\\text{th patient}\n\\]\nThere are any number of models to choose for \\(\\eta_{it}\\). What are our options?\n\nMultivariate models (e.g., Gaussian with unknown covariance)\nSpecify a basis function in \\(t\\) as in Lecture 9\nGaussian process model is something of a hybrid of the two\n\n\\[\n\\text{Want a prior for }\\eta_t = \\eta(t)\\text{ as a function of time }t\n\\]"
  },
  {
    "objectID": "slides/17-gps.html#a-review-of-computation-linear-algebra-needed-to-sample",
    "href": "slides/17-gps.html#a-review-of-computation-linear-algebra-needed-to-sample",
    "title": "Gaussian Processes",
    "section": "A review of computation / linear algebra needed to sample",
    "text": "A review of computation / linear algebra needed to sample\nSampling 1D normal random variables:\n\nN &lt;- 100\ntheta &lt;- 3\nsigma &lt;- 1\nz &lt;- rnorm(N)\n\nx &lt;- theta + sigma * z\n# This is the same as\nx &lt;- rnorm(N, theta, sigma)\n\nSampling multivariate normal random variables:\n\nN &lt;- 100\nD &lt;- 3\ntheta &lt;- c(3, -1, 5)\nSigma &lt;- matrix(\n  c(\n    1, .6, .3,\n    .6, 1, .6,\n    .3, .6, 1\n  ),\n  nrow = 3, ncol = 3\n)\nz &lt;- rnorm(N)\nR &lt;- chol(covmat) ## t(R) %*% R == Sigma\nx &lt;- t(R) %*% z\n\n# This is the same as\nx &lt;- rmvnorm(N, theta, Sigma)\n\nWhat about “random functions”?"
  },
  {
    "objectID": "slides/17-gps.html#constructing-a-gaussian-random-function-in-time",
    "href": "slides/17-gps.html#constructing-a-gaussian-random-function-in-time",
    "title": "Gaussian Processes",
    "section": "Constructing a Gaussian Random Function in Time",
    "text": "Constructing a Gaussian Random Function in Time\nLet’s think of a way to build a function that is “Gaussian” from the bottom up.\n\nWe start at time \\(t=0\\) from \\(X_0 = 0\\).\nWhen time passes by amount of \\(h &gt; 0\\), we want\n\n\\[X_{t+h} - X_t \\sim No(0,h).\\]\n\nEach increment will be independent from each other.\n\nNow think of ``passing to the limit’’ in some way…\nThis works (!)"
  },
  {
    "objectID": "slides/17-gps.html#visualizing-the-brownian-motion",
    "href": "slides/17-gps.html#visualizing-the-brownian-motion",
    "title": "Gaussian Processes",
    "section": "Visualizing the Brownian Motion",
    "text": "Visualizing the Brownian Motion\nSuppose we observe this process \\(X_t\\) at time points \\((t_1,t_2,\\ldots,t_N)\\).\n\\[\n(X_1,\\ldots,X_N)^T \\sim No\\left(0,\\Sigma\\right),\\; \\Sigma = \\begin{bmatrix}\nt_1 & t_2 & \\cdots & t_1 \\\\\nt_1 & t_2 & \\cdots & t_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nt_1 & t_2 & \\cdots & t_N\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/17-gps.html#what-if-i-want-different-correlation-structures",
    "href": "slides/17-gps.html#what-if-i-want-different-correlation-structures",
    "title": "Gaussian Processes",
    "section": "What if I want different correlation structures?",
    "text": "What if I want different correlation structures?\nFor longitudinal data,\n\nWe are not interested in an indefinitely long time span.\nOften the model should match ``stabilizing’’ behavior of the data.\n\n\nHypothesis 1 : The marginal variability of \\(X_t\\) should stay the same.\nHypothesis 2 : Time correlation should decay in, and only depend on, the amount of time elapsed.\nHypothesis 3 : We have expectations about the span of correlation and ``smoothness.’’"
  },
  {
    "objectID": "slides/17-gps.html#stationary-gaussian-processes",
    "href": "slides/17-gps.html#stationary-gaussian-processes",
    "title": "Gaussian Processes",
    "section": "Stationary Gaussian processes",
    "text": "Stationary Gaussian processes\nA different process results from a stationary kernel:\n\\[\nCov(X_t, X_s) = C(|t-s|).\n\\]\nWe want \\(C(0) = \\sigma^2 &gt; 0\\) and \\(C\\to 0\\) as \\(|t-s|\\to\\infty\\). Some common choices:\n\nExponential kernel: \\(C(h) = \\sigma^2\\exp(-h/\\rho)\\)\nSquare exponential kernel: \\(C(h) = \\sigma^2\\exp\\{-h^2/(2\\rho^2)\\}\\)\n\n\nWhat has changed from before? Why is this more desirable?"
  },
  {
    "objectID": "slides/17-gps.html#interpreting-hyperparameters-and-kernels",
    "href": "slides/17-gps.html#interpreting-hyperparameters-and-kernels",
    "title": "Gaussian Processes",
    "section": "Interpreting Hyperparameters and Kernels",
    "text": "Interpreting Hyperparameters and Kernels\nThe within-time correlation structure of a GP is built based on specifying a covariance kernel and its hyperparameters.\nOften, we fix the overall kernel function while learning its “bandwidth” \\(\\rho\\) due to poor identifiability from the data."
  },
  {
    "objectID": "slides/17-gps.html#default-prior-choices",
    "href": "slides/17-gps.html#default-prior-choices",
    "title": "Gaussian Processes",
    "section": "Default Prior Choices",
    "text": "Default Prior Choices\n\nStan team often recommends \\(\\rho^{-1} \\sim Gamma(5,5)\\)\nApplicable with the caveat on units being normalized to a scale ~ 1"
  },
  {
    "objectID": "slides/17-gps.html#prior-sampling-in-stan",
    "href": "slides/17-gps.html#prior-sampling-in-stan",
    "title": "Gaussian Processes",
    "section": "Prior Sampling in Stan",
    "text": "Prior Sampling in Stan\n\nWe can only collect data, and compute quantities, at finitely many time points.\nOnce we have a model for GP, sampling prior = sampling from a multivariate normal (with a special structure).\n\n\ntransformed data {\n  // 0. A \"small nugget\" to stabilize matrix root computation\n  // This is for.better numerical stability in taking large matrix roots\n  real delta = 1e-9;\n\n  // 1. Compute the squared exponential kernel matrix\n  vector[N] mu = rep_vector(0, N);\n  matrix[N, N] L_K;\n  matrix[N, N] K = gp_exp_quad_cov(x, alpha, rho);\n  for (n in 1:N) {\n    K[n, n] = K[n, n] + delta;\n  }\n\n  // 3. Compute the root of K by Cholesky decomposition\n  L_K = cholesky_decompose(K);\n}\ngenerated quantities {\n  // 4. Sample from the prior: multivariate_normal(0, K)\n  f ~ multi_normal_cholesky(mu, L_K)\n}"
  },
  {
    "objectID": "slides/17-gps.html#posterior-inference",
    "href": "slides/17-gps.html#posterior-inference",
    "title": "Gaussian Processes",
    "section": "Posterior inference",
    "text": "Posterior inference\nNow let’s think about how to use what we’ve learned as a building block in a model with multiple patients.\nFor each \\(i\\)-th patient: random effects vector is given an independent prior based on the GP model.\n\\[\n\\boldsymbol{\\eta}_i = (\\eta_i(t_1),\\ldots, \\eta_i(t_n))^T \\stackrel{ind}{\\sim} Normal(0,\\mathbf{K}_i)\n\\] \\[\n\\mathbf{K}_i = \\begin{bmatrix}\nK(0) & K(|t_2-t_1|) & \\cdots & K(|t_n-t_1|)\\\\\nK(|t_2-t_1|) & K(0) & \\cdots & K(|t_n-t_2|)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\n\\end{bmatrix}\n\\] Remember: Different patients need not have the same number of encounters, nor need they have measurements at the same time."
  },
  {
    "objectID": "slides/17-gps.html#programming-in-stan-for-a-single-patient",
    "href": "slides/17-gps.html#programming-in-stan-for-a-single-patient",
    "title": "Gaussian Processes",
    "section": "Programming in Stan for a single patient",
    "text": "Programming in Stan for a single patient\nWe can now encode a GP programming block in our regression code for Stan.\n\ndata {\n  int&lt;lower=1&gt; N;\n  array[N] real x;\n  vector[N] y;\n}\ntransformed data {\n  // For numerical reasons\n  real delta = 1e-9;\n  \n  // A vector of zeros\n  vector[N] mu = rep_vector(0, N);\n}\nparameters {\n  // GP hyperparameters\n  vector[N] z;\n  real&lt;lower=0&gt; rho;\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  // Computes the necessary parameters for GP model\n  vector[N] f;\n  matrix[N, N] L_K;\n  matrix[N, N] K = gp_exp_quad_cov(x, alpha, rho);\n\n  // diagonal elements\n  for (n in 1:N) {\n    K[n, n] = K[n, n] + delta;\n  }\n\n  L_K = cholesky_decompose(K);\n  f = L_K * z;\n}\nmodel {\n  rho ~ inv_gamma(5, 5);\n  alpha ~ std_normal();\n  sigma ~ std_normal();\n\n  y ~ normal(f, sigma);\n}"
  },
  {
    "objectID": "slides/17-gps.html#example-analysis-longitudinal-structure",
    "href": "slides/17-gps.html#example-analysis-longitudinal-structure",
    "title": "Gaussian Processes",
    "section": "Example Analysis: Longitudinal Structure",
    "text": "Example Analysis: Longitudinal Structure\nWith longitudinal data modeling, we need to incorporate all we have learned into a ragged data structure.\n\ndata {\n  int&lt;lower=0&gt; N;       // total number of observations\n  int&lt;lower=1&gt; K;       // number of patients\n  int&lt;lower=1&gt; D;       // fixed effects dimension\n  vector[N] y;          // observation\n  array[N] real t;      // time points \n  array[K] int s;       // sizes of within-pt obs\n}\nparameters {\n  // GP hyperparameters\n  vector[N] z;\n  real&lt;lower=0&gt; rho;\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  int pos;\n  pos = 1;\n  \n  for (k in 1:K) {\n    // Latent GP random effects for the k-th patient\n    int Nk = s[k];\n    vector[Nk] f;\n    {\n      matrix[Nk, Nk] L_C;\n      // segment slices out the vector from pos to s[k]\n      // E.g., for k == 1, we extract all time points for the 1st patient\n      matrix[Nk, Nk] C = gp_exp_quad_cov(segment(t, pos, s[k]), alpha, rho);\n      for (n in 1:Nk) {\n        C[n, n] = C[n, n] + delta;\n      }\n      L_C = cholesky_decompose(C);\n      f  = L_C * segment(z, pos, s[k]);\n    }\n    // Move the position to the beginning of the next patient's measurements\n    pos = pos + s[k];\n  }\n}\n// ..."
  },
  {
    "objectID": "slides/17-gps.html#example-analysis-results",
    "href": "slides/17-gps.html#example-analysis-results",
    "title": "Gaussian Processes",
    "section": "Example Analysis Results",
    "text": "Example Analysis Results"
  },
  {
    "objectID": "slides/17-gps.html#comparing-estimates-to-least-squares-fit",
    "href": "slides/17-gps.html#comparing-estimates-to-least-squares-fit",
    "title": "Gaussian Processes",
    "section": "Comparing estimates to least squares fit",
    "text": "Comparing estimates to least squares fit\n(More to go in here)"
  },
  {
    "objectID": "slides/17-gps.html#random-effect-covariance-structure",
    "href": "slides/17-gps.html#random-effect-covariance-structure",
    "title": "Gaussian Processes",
    "section": "Random effect covariance structure",
    "text": "Random effect covariance structure\n(More to go in here)"
  },
  {
    "objectID": "slides/17-gps.html#predictive-inference-using-some-math",
    "href": "slides/17-gps.html#predictive-inference-using-some-math",
    "title": "Gaussian Processes",
    "section": "Predictive Inference using Some Math",
    "text": "Predictive Inference using Some Math\nThe model is continuous in nature. In principle, for arbitrary time points, the model allows smoothing and forecasting for unseen time points \\(t^{new}\\).\n\\[\n\\mathbf{f} \\sim Normal\\left(\\boldsymbol{\\mu},\\Sigma\\right)\n\\]\n\\[\n\\mathbf{f}^{new}|\\mathbf{f} \\sim Normal\\left(C\\Sigma^{-1}(\\mathbf{f} - \\boldsymbol{\\mu}),\\Sigma^{new}-C\\Sigma^{-1}C^T\\right)\n\\]"
  },
  {
    "objectID": "slides/17-gps.html#implementation-in-stan",
    "href": "slides/17-gps.html#implementation-in-stan",
    "title": "Gaussian Processes",
    "section": "Implementation in Stan",
    "text": "Implementation in Stan\nAn analytical formula can be implemented as a function. See the Stan Help page\n\nfunctions {\n  vector gp_pred_rng(array[] real x2,\n                     vector y1,\n                     array[] real x1,\n                     real alpha,\n                     real rho,\n                     real sigma,\n                     real delta) {\n    int N1 = rows(y1);\n    int N2 = size(x2);\n    vector[N2] f2;\n    {\n      matrix[N1, N1] L_K;\n      vector[N1] K_div_y1;\n      matrix[N1, N2] k_x1_x2;\n      matrix[N1, N2] v_pred;\n      vector[N2] f2_mu;\n      matrix[N2, N2] cov_f2;\n      matrix[N2, N2] diag_delta;\n      matrix[N1, N1] K;\n      K = gp_exp_quad_cov(x1, alpha, rho);\n      for (n in 1:N1) {\n        K[n, n] = K[n, n] + square(sigma);\n      }\n      L_K = cholesky_decompose(K);\n      K_div_y1 = mdivide_left_tri_low(L_K, y1);\n      K_div_y1 = mdivide_right_tri_low(K_div_y1', L_K)';\n      k_x1_x2 = gp_exp_quad_cov(x1, x2, alpha, rho);\n      f2_mu = (k_x1_x2' * K_div_y1);\n      v_pred = mdivide_left_tri_low(L_K, k_x1_x2);\n      cov_f2 = gp_exp_quad_cov(x2, alpha, rho) - v_pred' * v_pred;\n      diag_delta = diag_matrix(rep_vector(delta, N2));\n\n      f2 = multi_normal_rng(f2_mu, cov_f2 + diag_delta);\n    }\n    return f2;\n  }\n}\n//...\ngenerated quantities {\n  vector[N2] f2;\n  vector[N2] y2;\n\n  f2 = gp_pred_rng(x2, y1, x1, alpha, rho, sigma, delta);\n  for (n2 in 1:N2) {\n    y2[n2] = normal_rng(f2[n2], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/17-gps.html#stan-speed-concerns",
    "href": "slides/17-gps.html#stan-speed-concerns",
    "title": "Gaussian Processes",
    "section": "Stan speed concerns",
    "text": "Stan speed concerns\n\nGP is notorious for not being scalable. Tl;dr is the need for matrix root / inverse computation.\nFor datasets covered in this class, Stan works well. For research purposes, worth exploring other specialized toolkits.\n\n\nCode MCMC yourself to make it faster (e.g., using Rcpp)\nGo to https://en.wikipedia.org/wiki/Comparison_of_Gaussian_ process_software\nApproximation methods other than MCMC (not in scope of this class)"
  },
  {
    "objectID": "slides/17-gps.html#a-summary",
    "href": "slides/17-gps.html#a-summary",
    "title": "Gaussian Processes",
    "section": "A summary",
    "text": "A summary\n\nGP is a flexible, high-dimensional model for handling correlated measurements over time.\nWith some basic knowledge about conditioning Gaussian random variables, we can implement posterior computation and prediction / interpolation.\nProgramming in Stan is straightforward but can be expensive with large number of observations."
  },
  {
    "objectID": "slides/17-gps.html#prepare-for-next-class",
    "href": "slides/17-gps.html#prepare-for-next-class",
    "title": "Gaussian Processes",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 04 which is due March 25\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Using GP to model spatial data\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/16-longitudinal.html#longitudinal-data-2",
    "href": "slides/16-longitudinal.html#longitudinal-data-2",
    "title": "Longitudinal Data",
    "section": "Longitudinal Data",
    "text": "Longitudinal Data"
  },
  {
    "objectID": "slides/16-longitudinal.html#treating-eyes-as-independent",
    "href": "slides/16-longitudinal.html#treating-eyes-as-independent",
    "title": "Longitudinal Data",
    "section": "Treating Eyes as Independent",
    "text": "Treating Eyes as Independent\nWe can model each eye separately using OLS:\n\\[Y_{it} = \\beta_0 + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma_i^2).\\]\nWhere:\n\n\\(\\beta_{0i}\\) is the intercept for eye \\(i\\).\n\\(\\beta_{1i}\\) is the slope for eye \\(i\\) (i.e., disease progression).\n\\(\\sigma_i^2\\) is the residual error for eye \\(i\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#treating-eyes-as-independent-1",
    "href": "slides/16-longitudinal.html#treating-eyes-as-independent-1",
    "title": "Longitudinal Data",
    "section": "Treating Eyes as Independent",
    "text": "Treating Eyes as Independent\n\nFitting OLS for each eye separately allows each eye to have a unique intercept and slope.\nThis leads to overfitting since extreme values of \\(\\alpha_i\\) and \\(\\beta_i\\) are allowed, which may not be realistic or stable across the population.\nShrinking extreme values toward the population average is desirable."
  },
  {
    "objectID": "slides/16-longitudinal.html#rotterdam-data",
    "href": "slides/16-longitudinal.html#rotterdam-data",
    "title": "Longitudinal Data",
    "section": "Rotterdam data",
    "text": "Rotterdam data"
  },
  {
    "objectID": "slides/16-longitudinal.html#treating-eyes-separately",
    "href": "slides/16-longitudinal.html#treating-eyes-separately",
    "title": "Longitudinal Data",
    "section": "Treating Eyes Separately",
    "text": "Treating Eyes Separately\nWe can model each eye separately using OLS (this is a form of longitudinal analysis!). For \\(t = 1,\\ldots,n_i\\), the model is:\n\\[Y_{it} = \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma_i^2).\\]\nWhere:\n\n\\(\\beta_{0i}\\) is the intercept for eye \\(i\\).\n\\(\\beta_{1i}\\) is the slope for eye \\(i\\) (i.e., disease progression).\n\\(\\sigma_i^2\\) is the residual error for eye \\(i\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#treating-eyes-separately-1",
    "href": "slides/16-longitudinal.html#treating-eyes-separately-1",
    "title": "Longitudinal Data",
    "section": "Treating Eyes Separately",
    "text": "Treating Eyes Separately\n\nFitting OLS separately allows each eye to have a unique intercept and slope, which of course is consistent with the data generating process.\nHowever, this can lead to eye-specific intercepts and slopes that are not realistic (consider OLS regression with very few data points).\nEstimating eye-specific intercepts and slopes within the context of the whole study sample should shrink extreme values toward the population average."
  },
  {
    "objectID": "slides/16-longitudinal.html#ols-regression",
    "href": "slides/16-longitudinal.html#ols-regression",
    "title": "Longitudinal Data",
    "section": "OLS regression",
    "text": "OLS regression"
  },
  {
    "objectID": "slides/16-longitudinal.html#subject-specific-intercepts-and-slopes",
    "href": "slides/16-longitudinal.html#subject-specific-intercepts-and-slopes",
    "title": "Longitudinal Data",
    "section": "Subject-specific intercepts and slopes",
    "text": "Subject-specific intercepts and slopes\nFor \\(i = 1,\\ldots,n\\) and \\(t=1,\\ldots,n_i\\), we can write the model:\n\\[\\begin{aligned}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{aligned}\\]\nPopulation Parameters:\n\n\\(\\beta_0\\) is the population intercept (i.e., average MD value in the population at time zero).\n\\(\\beta_1\\) is the population slope (i.e., average disease progression).\n\\(\\sigma^2\\) is the population residual error."
  },
  {
    "objectID": "slides/16-longitudinal.html#subject-specific-intercepts-and-slopes-1",
    "href": "slides/16-longitudinal.html#subject-specific-intercepts-and-slopes-1",
    "title": "Longitudinal Data",
    "section": "Subject-specific intercepts and slopes",
    "text": "Subject-specific intercepts and slopes\nFor \\(i = 1,\\ldots,n\\) and \\(t=1,\\ldots,n_i\\), we can write the model:\n\\[\\begin{aligned}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{aligned}\\]\nSubject-Specific Parameters:\n\n\\(\\theta_{0i}\\) is the subject-specific deviation from the intercept for eye \\(i\\).\n\\(\\theta_{1i}\\) is the subject-specific deviation from the slope for eye \\(i\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#subject-specific-intercepts-and-slopes-2",
    "href": "slides/16-longitudinal.html#subject-specific-intercepts-and-slopes-2",
    "title": "Longitudinal Data",
    "section": "Subject-specific intercepts and slopes",
    "text": "Subject-specific intercepts and slopes\nFor \\(i = 1,\\ldots,n\\) and \\(t=1,\\ldots,n_i\\), we can write the model:\n\\[\\begin{aligned}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{aligned}\\]\nKey Advantage:\n\nThis model defines subject-specific estimates of \\(\\beta_{0i}\\) and \\(\\beta_{1i}\\) relative to the population average, preventing overfitting and making the estimates more stable.\nShrinks subject-specific parameters to the population average."
  },
  {
    "objectID": "slides/16-longitudinal.html#prior-for-the-suject-specific-parameters",
    "href": "slides/16-longitudinal.html#prior-for-the-suject-specific-parameters",
    "title": "Longitudinal Data",
    "section": "Prior for the Suject-specific parameters",
    "text": "Prior for the Suject-specific parameters"
  },
  {
    "objectID": "slides/16-longitudinal.html#prior-specification",
    "href": "slides/16-longitudinal.html#prior-specification",
    "title": "Longitudinal Data",
    "section": "Prior Specification",
    "text": "Prior Specification\nOne choice could be to specify independent priors for the subject-specific intercepts and slopes:\n\\[\\begin{aligned}\n\\theta_{0i} &\\stackrel{iid}{\\sim} N(0, \\tau_0^2)\\\\\n\\theta_{1i} &\\stackrel{iid}{\\sim} N(0, \\tau_1^2).\n\\end{aligned}\\]\n\nThis is the same assumption we made last lecture, where we assume a normal distribution centered at zero with some variance that reflects variability across subjects.\nOften times this assumption is oversimplified. For example in glaucoma progression, we often assume that if someone has a higher baseline MD they will a more negative slope (i.e., negative correlation)."
  },
  {
    "objectID": "slides/16-longitudinal.html#prior-specification-1",
    "href": "slides/16-longitudinal.html#prior-specification-1",
    "title": "Longitudinal Data",
    "section": "Prior Specification",
    "text": "Prior Specification\nWe can instead model the subject-specific parameters as correlated themselves using a bi-variate normal distribution. Define \\(\\boldsymbol{\\theta}_i = (\\theta_{0i},\\theta_{1i})^\\top\\) and then \\(\\boldsymbol{\\theta}_i \\stackrel{iid}{\\sim} N_2(\\mathbf{0}_2,\\boldsymbol{\\Sigma})\\).\n\\[\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n    \\tau_{0}^2 & \\tau_{01}\\\\\n    \\tau_{01} & \\tau_1^2\\\\\n  \\end{bmatrix}.\\]\n\n\\(\\tau_{01} = \\rho \\tau_0 \\tau_1\\).\n\\(\\rho\\) is the correlation between the subject-specific intercepts and slopes.\n\nLet’s talk about efficient ways to generate multivariate random variables!"
  },
  {
    "objectID": "slides/16-longitudinal.html#generating-multivariate-normal-random-variabels",
    "href": "slides/16-longitudinal.html#generating-multivariate-normal-random-variabels",
    "title": "Longitudinal Data",
    "section": "Generating Multivariate Normal Random Variabels",
    "text": "Generating Multivariate Normal Random Variabels\nSuppose we would like to generate samples of a random variable \\(\\mathbf{x}_i \\stackrel{iid}{\\sim} N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\).\nTo sample efficiently, we can decompose the covariance structure:\n\\[\\begin{aligned}\n\\boldsymbol{\\Sigma} &= \\begin{bmatrix}\n    \\tau_{0}^2 & \\rho \\tau_0 \\tau_1\\\\\n    \\rho \\tau_0 \\tau_1 & \\tau_1^2\\\\\n  \\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n    \\tau_{0} & 0\\\\\n    0 & \\tau_1\\\\\n  \\end{bmatrix}  \\begin{bmatrix}\n    1 & \\rho\\\\\n    \\rho & 1\\\\\n  \\end{bmatrix}  \\begin{bmatrix}\n    \\tau_{0} & 0\\\\\n    0 & \\tau_1\\\\\n  \\end{bmatrix}\\\\\n&=  \\mathbf{D} \\boldsymbol{\\Phi} \\mathbf{D}.\n\\end{aligned}\\]\n\n\\(\\mathbf{D}\\) is a \\(p\\)-dimensional matrix with the standard deviations on the diagonal.\n\\(\\boldsymbol{\\Phi}\\) is the correlation matrix."
  },
  {
    "objectID": "slides/16-longitudinal.html#generating-multivariate-normal-rngs",
    "href": "slides/16-longitudinal.html#generating-multivariate-normal-rngs",
    "title": "Longitudinal Data",
    "section": "Generating Multivariate Normal RNGs",
    "text": "Generating Multivariate Normal RNGs\nSuppose we would like to generate samples of a random variable \\(\\mathbf{x}_i \\stackrel{iid}{\\sim} N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\).\nTo sample efficiently, we can decompose the covariance structure:\n\\[\\begin{aligned}\n\\boldsymbol{\\Sigma} &= \\begin{bmatrix}\n    \\tau_{0}^2 & \\rho \\tau_0 \\tau_1\\\\\n    \\rho \\tau_0 \\tau_1 & \\tau_1^2\\\\\n  \\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n    \\tau_{0} & 0\\\\\n    0 & \\tau_1\\\\\n  \\end{bmatrix}  \\begin{bmatrix}\n    1 & \\rho\\\\\n    \\rho & 1\\\\\n  \\end{bmatrix}  \\begin{bmatrix}\n    \\tau_{0} & 0\\\\\n    0 & \\tau_1\\\\\n  \\end{bmatrix}\\\\\n&=  \\mathbf{D} \\boldsymbol{\\Phi} \\mathbf{D}.\n\\end{aligned}\\]\n\n\\(\\mathbf{D}\\) is a \\(p\\)-dimensional matrix with the standard deviations on the diagonal.\n\\(\\boldsymbol{\\Phi}\\) is the correlation matrix."
  },
  {
    "objectID": "slides/16-longitudinal.html#generating-multivariate-normal-rngs-1",
    "href": "slides/16-longitudinal.html#generating-multivariate-normal-rngs-1",
    "title": "Longitudinal Data",
    "section": "Generating Multivariate Normal RNGs",
    "text": "Generating Multivariate Normal RNGs\nWe can further decompose the covariance by computing the cholesky decomposition of the correlation matrix:\n\\[\\begin{aligned}\n\\boldsymbol{\\Sigma} &=  \\mathbf{D} \\boldsymbol{\\Phi} \\mathbf{D}\\\\\n&= \\mathbf{D} \\mathbf{L} \\mathbf{L}^\\top \\mathbf{D},\n\\end{aligned}\\] where \\(\\mathbf{L}\\) is the lower triangular Cholesky decomposition for \\(\\boldsymbol{\\Phi}\\), such that \\(\\boldsymbol{\\Phi} = \\mathbf{L} \\mathbf{L}^\\top\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#generating-multivariate-normal-rngs-2",
    "href": "slides/16-longitudinal.html#generating-multivariate-normal-rngs-2",
    "title": "Longitudinal Data",
    "section": "Generating Multivariate Normal RNGs",
    "text": "Generating Multivariate Normal RNGs\nWe can generate samples \\(\\mathbf{x}_i \\stackrel{iid}{\\sim} N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) using the following approach:\n\\[\\mathbf{x}_i = \\boldsymbol{\\mu} + \\mathbf{D} \\mathbf{L} \\mathbf{z}_i,\\]\nwhere \\(\\mathbf{z}_i = (z_{0i},z_{1i})\\) and \\(z_{ij} \\stackrel{iid}{\\sim} N(0,1)\\), so that \\(\\mathbb{E}[\\mathbf{z}_i] = \\mathbf{0}_2\\) and \\(\\mathbb{C}(\\mathbf{z}_i) = \\mathbf{I}_2\\).\n\\[\\begin{aligned}\n\\mathbb{E}[\\boldsymbol{\\mu} + \\mathbf{D}\\mathbf{L}\\mathbf{z}_i] &= \\boldsymbol{\\mu} +  \\mathbf{D}\\mathbf{L}\\mathbb{E}[\\mathbf{z}_i] = \\boldsymbol{\\mu}\\\\\n\\mathbb{C}(\\boldsymbol{\\mu} + \\mathbf{D}\\mathbf{L}\\mathbf{z}_i) &= \\mathbf{D}\\mathbf{L}\\mathbb{C}(\\mathbf{z}_i)\\left(\\mathbf{D}\\mathbf{L}\\right)^\\top \\\\\n&= \\mathbf{D}\\mathbf{L}\\mathbf{L}^\\top\\mathbf{D}\\\\\n&=\\boldsymbol{\\Sigma}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal.html#test",
    "href": "slides/16-longitudinal.html#test",
    "title": "Longitudinal Data",
    "section": "Test",
    "text": "Test\n\\(\\boldsymbol{\\Sigma}\\)"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#review-of-last-lecture",
    "href": "slides/16-longitudinal-test.html#review-of-last-lecture",
    "title": "Hierarchical Models",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we learned about missing data, including various types: MCAR, MAR, and MNAR.\nWe learned about two Bayesian approaches to missing data under the MAR assumption:\n\nJoint model that assumes the missing data are parameters.\nMultiple imputation.\n\nMoving forward: Up until today, we have dealt with independent data. Today, we will look at our first example of dependent data!"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#linear-regression-assumptions",
    "href": "slides/16-longitudinal-test.html#linear-regression-assumptions",
    "title": "Hierarchical Models",
    "section": "Linear regression assumptions",
    "text": "Linear regression assumptions\n\\[\\begin{aligned}\nY_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(Y_i\\) are independent observations (independence).\n\\(Y_i\\) is linearly related to \\(\\mathbf{x}_i\\) (linearity).\n\\(\\epsilon_i = Y_i - \\mu_i\\) is normally distributed (normality).\n\\(\\epsilon_i\\) has constant variance across \\(\\mathbf{x}_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#independence-assumption-in-linear-regression",
    "href": "slides/16-longitudinal-test.html#independence-assumption-in-linear-regression",
    "title": "Hierarchical Models",
    "section": "Independence Assumption in Linear Regression",
    "text": "Independence Assumption in Linear Regression\nWe assume that the residuals \\(\\epsilon_i\\) are independent:\n\\[\\mathbb{C}(\\epsilon_i, \\epsilon_j) = 0, \\quad \\text{for} \\quad i \\neq j,\\] where \\(\\mathbb{C}(X, Y)\\) is the covariance between two random variables \\(X\\) and \\(Y\\). As a note: \\(\\mathbb{C}(X, X) = \\mathbb{V}(X)\\).\n\nThis implies that the observations \\(Y_i\\) and \\(Y_j\\) are independent, and their correlation is zero.\n\nCorrelation: \\(\\rho(X,Y) = \\frac{\\mathbb{C}(X, Y)}{\\sqrt{\\mathbb{V}(X)\\mathbb{V}(Y)}}\\)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#real-world-dependent-observations",
    "href": "slides/16-longitudinal-test.html#real-world-dependent-observations",
    "title": "Hierarchical Models",
    "section": "Real-World: Dependent Observations",
    "text": "Real-World: Dependent Observations\nHowever, in real-world data, the independence assumption often does not hold:\n\nRepeated measures data (e.g., same individual over time).\nClustered data (e.g., patients within a hospital).\nLongitudinal data (e.g., disease severity measures over time).\nSpatial data (e.g., disease counts observed across zip codes)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#the-challenge",
    "href": "slides/16-longitudinal-test.html#the-challenge",
    "title": "Hierarchical Models",
    "section": "The Challenge",
    "text": "The Challenge\n\nIf we assume independence in the presence of correlation:\n\nBiased parameter estimates: Parameter estimation will be biased due to group-level dependencies that effect the outcome.\nUnderestimated uncertainty: The model will not account for the true variability, leading to narrower confidence intervals.\nInaccurate Predictions: Predictions for new groups may be biased because the model doesn’t properly account for group-level variability.\n\nThus, we need a way to account for dependencies between observations, especially when data are grouped or clustered."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#example-of-hierarchical-data",
    "href": "slides/16-longitudinal-test.html#example-of-hierarchical-data",
    "title": "Hierarchical Models",
    "section": "Example of Hierarchical Data",
    "text": "Example of Hierarchical Data\n\nHierarchical data refers to data that is organized into groups or clusters, where each group contains multiple observations.\nConsider data from patients within hospitals. Each patient is being treated in a hospital, with multiple patients belonging to each hospital.\nIn this case, the observation for a patient is indexed by two variables:\n\n\\(i\\): hospital index.\n\\(j\\): patient index, nested within hospital.\n\nSo, for patient \\(j\\) within hospital \\(i\\), we write the response as \\(Y_{ij}\\)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#observations-with-two-indices-y_ij",
    "href": "slides/16-longitudinal-test.html#observations-with-two-indices-y_ij",
    "title": "Hierarchical Models",
    "section": "Observations with Two Indices: \\(Y_{ij}\\)",
    "text": "Observations with Two Indices: \\(Y_{ij}\\)\n\n\\(Y_{ij}\\) represents the response for patient \\(j\\) in hospital \\(i\\).\nThe first index \\(i\\) represents group-level effects (e.g., hospital-level).\nThe second index \\(j\\) represents individual-level observations (e.g., patient).\nWe typically say that \\(i = 1,\\ldots,n.\\) and \\(j = 1,\\ldots,n_i\\).\nThe total number of observations is \\(N = \\sum_{i = 1}^{n}n_i\\)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#why-two-indices",
    "href": "slides/16-longitudinal-test.html#why-two-indices",
    "title": "Hierarchical Models",
    "section": "Why Two Indices?",
    "text": "Why Two Indices?\nHaving two indices allows us to model both:\n\nWithin-group variation (differences between patients within the same hospital).\nBetween-group variation (differences between hospitals).\n\nThe hierarchical structure captures both types of variation."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#conceptualizing-hierarchical-data",
    "href": "slides/16-longitudinal-test.html#conceptualizing-hierarchical-data",
    "title": "Hierarchical Models",
    "section": "Conceptualizing Hierarchical Data",
    "text": "Conceptualizing Hierarchical Data\nConsider the example of patients within hospitals:\n\nEach data point (i.e., observed data \\(Y_{ij}\\)) represents an outcome measured on a patient.\nThe data points are grouped by hospital, indicating that patients from the same hospital are likely to have similar outcomes due to shared hospital-level factors.\n\nThis is a typical example of hierarchical data."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#hierarchical-model",
    "href": "slides/16-longitudinal-test.html#hierarchical-model",
    "title": "Hierarchical Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nNow, we can see how hierarchical data appears in the model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\nSubject-specific objects:\n\n\\(Y_{ij}\\): response for patient \\(j\\) in hospital \\(i\\).\n\\(\\mathbf{x}_{ij}\\) are the predictors for patient \\(j\\) in hospital \\(i\\).\n\\(\\epsilon_{ij}\\): residual error for patient \\(j\\) in hospital \\(i\\)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#hierarchical-model-1",
    "href": "slides/16-longitudinal-test.html#hierarchical-model-1",
    "title": "Hierarchical Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nNow, we can see how hierarchical data appears in the model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\nGroup-specific objects:\n\n\\(\\theta_i\\): group-specific parameter for hospital \\(i\\), accounting for hospital-level variation (group-specific, random effect).\n\nThe group-specific parameters are responsible for inducing correlation into the model."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#hierarchical-model-2",
    "href": "slides/16-longitudinal-test.html#hierarchical-model-2",
    "title": "Hierarchical Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nNow, we can see how hierarchical data appears in the model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\nPopulation parameters:\n\n\\(\\alpha\\): intercept for the entire population.\n\\(\\boldsymbol{\\beta}\\): regression parameters for the entire population.\n\\(\\sigma\\): residual error parameter for the entire population."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#random-intercept-model",
    "href": "slides/16-longitudinal-test.html#random-intercept-model",
    "title": "Hierarchical Models",
    "section": "Random Intercept Model",
    "text": "Random Intercept Model\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\n\nFrom a frequentist perspective, this model may be called a random intercept model, but in the Bayesian framework all parameters are random variables, so the terms fixed and random effects don’t apply.\n\n\\(\\theta_i\\): group-specific parameters (random effect).\n\\(\\alpha, \\boldsymbol{\\beta}, \\sigma\\): population parameters (common across all groups, \\(\\boldsymbol{\\beta}\\) are the fixed effects)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#prior-for-theta_i",
    "href": "slides/16-longitudinal-test.html#prior-for-theta_i",
    "title": "Hierarchical Models",
    "section": "Prior for \\(\\theta_i\\)",
    "text": "Prior for \\(\\theta_i\\)\nWe model \\(\\theta_i\\) as a parameter drawn from a normal distribution centered at zero, with some variance \\(\\tau^2\\):\n\\[\\theta_i \\stackrel{iid}{\\sim} N(0, \\tau^2).\\]\n\nMean at zero: This assumption reflects that, on average, hospitals don’t deviate from the population mean (helps with identifiability).\nVariance \\(\\tau^2\\): This represents the variability in hospital-level intercepts. A larger \\(\\tau^2\\) implies greater variability between hospitals.\n\nEach hospital \\(i\\) has a hospital-specific parameter \\(\\theta_i\\), which represents how that hospital’s baseline (e.g., health outcomes) deviates from the population average."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#group-specific-intercept-model-conditional-specification",
    "href": "slides/16-longitudinal-test.html#group-specific-intercept-model-conditional-specification",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\nFor \\(i = 1,\\ldots,n\\) and \\(j = 1,\\ldots,n_i\\), \\[\\begin{aligned}\nY_{ij} | \\boldsymbol{\\Omega},\\theta_i &\\stackrel{ind}{\\sim} N(\\alpha + \\mathbf{x}_{ij}\\boldsymbol{\\beta} + \\theta_i,\\sigma^2)\\\\\n\\theta_i | \\tau^2 &\\stackrel{iid}{\\sim} N(0,\\tau^2)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta},\\sigma,\\tau)\\) are the population parameters."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#group-specific-intercept-model-conditional-specification-1",
    "href": "slides/16-longitudinal-test.html#group-specific-intercept-model-conditional-specification-1",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\n\nMoments for the Conditional Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{ij} | \\boldsymbol{\\Omega},\\theta_i] &= \\alpha + \\mathbf{x}_{ij}\\boldsymbol{\\beta} + \\theta_i\\\\\n\\mathbb{V}(Y_{ij} | \\boldsymbol{\\Omega},\\theta_i) &= \\sigma^2\\\\\n\\mathbb{C}(Y_{ij}, Y_{lk} | \\boldsymbol{\\Omega},\\theta_i,\\theta_l) &= 0,\\quad \\forall i,j,l,k.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#understanding-the-random-intercept",
    "href": "slides/16-longitudinal-test.html#understanding-the-random-intercept",
    "title": "Hierarchical Models",
    "section": "Understanding the Random Intercept",
    "text": "Understanding the Random Intercept\n\n\\(\\theta_i\\): group-specific parameter captures group-level differences (e.g., hospital level).\nThe intercept \\(\\theta_i\\) allows for each group to have its own baseline value.\nThis model introduces dependence within groups because observations from the same group share the same intercept \\(\\theta_i\\).\n\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{ij} | \\boldsymbol{\\Omega},\\theta_i] &= \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i\\\\\n&= (\\alpha + \\theta_i) + \\mathbf{x}_{ij} \\boldsymbol{\\beta}\\\\\n&= \\alpha_i + \\mathbf{x}_{ij} \\boldsymbol{\\beta}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#identifiability-issues",
    "href": "slides/16-longitudinal-test.html#identifiability-issues",
    "title": "Hierarchical Models",
    "section": "Identifiability Issues",
    "text": "Identifiability Issues\n\nPopulation Intercept (\\(\\alpha\\)): This is the average intercept for the entire population, i.e., the baseline outcome across all hospitals.\nGroup-Specific Intercept (\\(\\alpha + \\theta_i\\)): The group-specific intercept, where \\(\\theta_i\\) represents the deviation from the population intercept for group \\(i\\).\n\nWe face an identifiability issue when estimating the population intercept and group-specific intercepts. We could add the same constant to all \\(\\theta_i\\)’s and subtract that constant from \\(\\alpha\\).\n\nThis is solved by setting \\(\\theta_i\\) to be mean zero apriori."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#group-specific-intercept-model-conditional-specification-2",
    "href": "slides/16-longitudinal-test.html#group-specific-intercept-model-conditional-specification-2",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\n\nDefine \\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) and \\(\\mathbf{Y} = (\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n)\\).\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n \\prod_{j = 1}^{n_i} f(Y_{ij} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})  \\prod_{i=1}^n f(\\theta_i | \\tau^2) f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_n)\\)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#group-specific-intercept-model-marginal-specification",
    "href": "slides/16-longitudinal-test.html#group-specific-intercept-model-marginal-specification",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\nTo derive a marginal model it is useful to write the model at the level of the independent observations, \\(\\mathbf{Y}_i\\).\n\\[\\mathbf{Y}_i = \\begin{bmatrix}\n    Y_{i1}\\\\\n    Y_{i2}\\\\\n    \\vdots\\\\\n    Y_{in_i}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    \\alpha + \\mathbf{x}_{i1} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{i1}\\\\\n    \\alpha + \\mathbf{x}_{i2} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{i2}\\\\\n    \\vdots \\\\\n    \\alpha + \\mathbf{x}_{in_i} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{in_i}\n  \\end{bmatrix} = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i \\boldsymbol{\\beta} + \\theta_i \\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}_i,\\] where \\(\\mathbf{1}_{n_i}\\) is an \\(n_i \\times 1\\) dimensional vector of ones, \\(\\mathbf{X}_i\\) is an \\(n_i \\times p\\) dimensional matrix with rows \\(\\mathbf{x}_{ij}\\).\n\n\\(\\boldsymbol{\\epsilon}_i = (\\epsilon_{i1},\\ldots,\\epsilon_{in_i}) \\stackrel{ind}{\\sim} N(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i})\\), with \\(\\mathbf{0}_{n_i}\\) an \\(n_i \\times 1\\) dimensional vector of zeros."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#group-specific-intercept-model-marginal-specification-1",
    "href": "slides/16-longitudinal-test.html#group-specific-intercept-model-marginal-specification-1",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\n\nMoments for the Marginal Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}] &= \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i\\boldsymbol{\\beta}\\\\\n\\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) &= \\tau^2 \\mathbf{1}_{n_i} \\mathbf{1}_{n_i}^\\top + \\sigma^2 \\mathbf{I}_{n_i} = \\boldsymbol{\\Upsilon}_i\\\\\n\\mathbb{C}(\\mathbf{Y}_{i}, \\mathbf{Y}_{i'} | \\boldsymbol{\\Omega}) &= \\mathbf{0}_{n_i \\times n_i},\\quad i \\neq i'.\n\\end{aligned}\\]\n\\[\\implies \\boldsymbol{\\Upsilon}_i = \\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) = \\begin{bmatrix}\n    \\tau^2 + \\sigma^2 & \\tau^2 & \\cdots & \\tau^2\\\\\n    \\tau^2 & \\tau^2 + \\sigma^2 & \\cdots & \\tau^2\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    \\tau^2 & \\tau^2 & \\cdots &\\tau^2 + \\sigma^2\n  \\end{bmatrix}.\\]"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#covariance-structure",
    "href": "slides/16-longitudinal-test.html#covariance-structure",
    "title": "Hierarchical Models",
    "section": "Covariance Structure",
    "text": "Covariance Structure\n\nThe variance \\(\\tau^2\\) for \\(\\theta_i\\) can be interpreted as the covariance between two observations from the same hospital.\nThis reflects how much two observations from the same group are expected to be similar in terms of their outcomes.\n\n\\[\\begin{aligned}\n\\mathbb{C}(Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega}) &= \\mathbb{V}(\\theta_i)\\\\\n&= \\tau^2.\n\\end{aligned}\\]\n\nThus, \\(\\tau^2\\) dictates the within-group correlation in our model.\nNote: \\(\\mathbb{C}(Y_{ij}, Y_{i'k} | \\boldsymbol{\\Omega}) = 0\\) for \\(i \\neq i'\\)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#induced-within-correlation",
    "href": "slides/16-longitudinal-test.html#induced-within-correlation",
    "title": "Hierarchical Models",
    "section": "Induced Within Correlation",
    "text": "Induced Within Correlation\n\\[\\begin{aligned}\n\\rho (Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega}) &= \\frac{\\mathbb{C}(Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega})}{\\sqrt{\\mathbb{V}(Y_{ij} |  \\boldsymbol{\\Omega}) \\mathbb{V}(Y_{ik} |  \\boldsymbol{\\Omega})}}\\\\\n&=\\frac{\\tau^2}{\\tau^2 + \\sigma^2}\\\\\n&= \\frac{1}{1 + \\frac{\\sigma^2}{\\tau^2}}.\n\\end{aligned}\\]\nThis model induces positive correlation within group observations."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#induced-within-correlation-1",
    "href": "slides/16-longitudinal-test.html#induced-within-correlation-1",
    "title": "Hierarchical Models",
    "section": "Induced Within Correlation",
    "text": "Induced Within Correlation\n\\[\\rho (Y_{ij}, Y_{ik} | \\alpha,\\boldsymbol{\\beta},\\sigma) = \\frac{1}{1 + \\frac{\\sigma^2}{\\tau^2}}\\]"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#group-specific-intercept-model-marginal-specification-2",
    "href": "slides/16-longitudinal-test.html#group-specific-intercept-model-marginal-specification-2",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\nFor \\(i = 1,\\ldots,n\\), \\[\\begin{aligned}\n\\mathbf{Y}_{i} | \\boldsymbol{\\Omega} &\\stackrel{ind}{\\sim} N(\\alpha \\mathbf{1}_{n_i}+ \\mathbf{X}_i\\boldsymbol{\\beta},\\boldsymbol{\\Upsilon}_i)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta},\\sigma,\\tau)\\) are the population parameters."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#group-specific-intercept-model-marginal-specification-3",
    "href": "slides/16-longitudinal-test.html#group-specific-intercept-model-marginal-specification-3",
    "title": "Hierarchical Models",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\n\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n f(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega}).\n\\end{aligned}\\]\nWhy might we be interested in fitting the marginal model?"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#recovering-the-group-specific-parameters",
    "href": "slides/16-longitudinal-test.html#recovering-the-group-specific-parameters",
    "title": "Hierarchical Models",
    "section": "Recovering the Group-Specific Parameters",
    "text": "Recovering the Group-Specific Parameters\n\nWe can still recover the \\(\\theta_i\\) when we fit the marginal model, we only need to compute \\(f(\\theta_i | \\mathbf{Y}_i,\\boldsymbol{\\Omega})\\) for all \\(i\\).\nWe can obtain this full conditional by specifying the joint distribution,\n\n\\[f\\left(\\begin{bmatrix}\n    \\mathbf{Y}_i\\\\\n    \\theta_i\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega}\\right) = N\\left(\\begin{bmatrix}\n    \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i \\boldsymbol{\\beta} + \\theta_i \\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}_i\\\\\n    0\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol{\\Upsilon}_i & \\tau^2 \\mathbf{1}_{n_i}\\\\\n    \\tau^2 \\mathbf{1}_{n_i}^\\top & \\tau^2\n  \\end{bmatrix}\\right).\\]\nWe can then use the conditional specification of a multivariate normal to find, \\(f(\\theta_i | \\mathbf{Y}_i, \\boldsymbol{\\Omega}) = N(\\mathbb{E}_{\\theta_i},\\mathbb{V}_{\\theta_i})\\), where\n\\[\\begin{aligned}\n\\mathbb{E}_{\\theta_i} &= \\mathbf{0}_{n_i} + \\tau^2 \\mathbf{1}_{n_i}^\\top \\boldsymbol{\\Upsilon}_i^{-1} (\\mathbf{Y}_i - \\alpha \\mathbf{1}_{n_i} - \\mathbf{X}_i \\boldsymbol{\\beta})\\\\\n\\mathbb{V}_{\\theta_i} &= \\tau^2 - \\tau^4 \\mathbf{1}_{n_i}^\\top \\boldsymbol{\\Upsilon}_i^{-1} \\mathbf{1}_{n_i}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#example-data-glucose-measurement-in-4-primary-care-clinics",
    "href": "slides/16-longitudinal-test.html#example-data-glucose-measurement-in-4-primary-care-clinics",
    "title": "Hierarchical Models",
    "section": "Example data: Glucose Measurement in 4 Primary Care Clinics",
    "text": "Example data: Glucose Measurement in 4 Primary Care Clinics\n\nWe will study glucose values for patients being seen at 4 primary care clinics across the city. The clinics each represent a geographical region: east, west, north, and south.\nThe dataset consists of glucose measurements (mg/dl) from patients, and also risk factors:\n\nAge (years).\nBMI (\\(kg/m^2\\)).\nSedx (0 = male, 1 = female).\nSmoking status (0 = non-smoker, 1 = smoker).\nPhysical activity level (0 = low, 1 = moderate, 2 = high).\nGlucose lowering medication (0 = none, 1 = yes)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#preview-the-data",
    "href": "slides/16-longitudinal-test.html#preview-the-data",
    "title": "Hierarchical Models",
    "section": "Preview the Data",
    "text": "Preview the Data"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#writing-down-a-model",
    "href": "slides/16-longitudinal-test.html#writing-down-a-model",
    "title": "Hierarchical Models",
    "section": "Writing down a model",
    "text": "Writing down a model\nWe would like to fit the following model:\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).\\]\n\n\\(Y_{ij}\\) is the glucose value for patient \\(i\\) in clinic \\(j\\)\n\\(\\theta_i\\) for \\(i = 1,\\ldots,4\\) is the clinic-specific intercept deviation. \\[\\begin{aligned}\n\\mathbf{x}_{ij} &= (Age_{ij}, BMI_{ij}, Female_{ij},Smoker_{ij}, \\\\\n&\\quad Moderate\\_Activity_{ij}, High\\_Activity_{ij}, On\\_Meds_{ij}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#fitting-the-conditional-model-in-stan",
    "href": "slides/16-longitudinal-test.html#fitting-the-conditional-model-in-stan",
    "title": "Hierarchical Models",
    "section": "Fitting the Conditional Model in Stan",
    "text": "Fitting the Conditional Model in Stan\n\n// conditional-model.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; N;\n  int&lt;lower = 1&gt; p;\n  matrix[N, p] X;\n  vector[N] Y;\n  int&lt;lower = 1, upper = n&gt; Ids[N];\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; tau;\n  vector[n] theta;\n}\nmodel {\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta + theta[Ids[i]];\n  }\n  target += normal_lpdf(Y | mu, sigma);\n  target += normal_lpdf(theta | 0, tau);\n  target += normal_lpdf(alpha | 0, 3);\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n}\ngenerated quantities {\n  real Intercept_East = alpha + theta[1];\n  real Intercept_North = alpha + theta[2];\n  real Intercept_South = alpha + theta[3];\n  real Intercept_West = alpha + theta[4];\n  real rho = 1 / (1 + ((sigma * sigma) / (tau * tau)));\n  vector[N] Y_pred;\n  vector[N] log_lik;\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta + theta[Ids[i]];\n    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);\n    Y_pred[i] = normal_rng(mu[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#fitting-the-model-in-stan",
    "href": "slides/16-longitudinal-test.html#fitting-the-model-in-stan",
    "title": "Hierarchical Models",
    "section": "Fitting the Model in Stan",
    "text": "Fitting the Model in Stan\n\nX &lt;- model.matrix(~ age + bmi + gender + smoking + as.factor(activity) + medication, data = data)[, -1]\nstan_data &lt;- list(\n  N = nrow(data),\n  n = length(unique(data$region)),\n  p = ncol(X),\n  X = X,\n  Y = data$glucose,\n  Ids = as.numeric(as.factor(data$region))\n)\nconditional_model &lt;- stan_model(model_code = \"conditional-model.stan\")\nfit_conditional &lt;- sampling(conditional_model, stan_data)"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#assessing-convergence",
    "href": "slides/16-longitudinal-test.html#assessing-convergence",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(fit_conditional, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#assessing-convergence-1",
    "href": "slides/16-longitudinal-test.html#assessing-convergence-1",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_conditional, regex_pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#posterior-summaries",
    "href": "slides/16-longitudinal-test.html#posterior-summaries",
    "title": "Hierarchical Models",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nprint(fit_conditional, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\", \"rho\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nalpha    99.03    0.10 3.77  91.46  96.59  99.06 101.61 106.29  1533    1\nbeta[1]   0.26    0.00 0.03   0.20   0.23   0.26   0.28   0.32  4245    1\nbeta[2]   0.48    0.00 0.07   0.35   0.44   0.48   0.53   0.62  4989    1\nbeta[3]  10.50    0.01 0.65   9.18  10.06  10.51  10.93  11.75  5966    1\nbeta[4]  -4.75    0.01 0.66  -6.03  -5.20  -4.76  -4.30  -3.46  7324    1\nbeta[5]   0.28    0.01 0.78  -1.21  -0.26   0.29   0.80   1.82  4746    1\nbeta[6]  -4.53    0.01 0.77  -6.03  -5.06  -4.53  -4.00  -3.02  4869    1\nbeta[7] -17.89    0.01 0.64 -19.15 -18.31 -17.89 -17.47 -16.64  5387    1\nsigma     7.43    0.00 0.23   6.99   7.28   7.43   7.59   7.90  5900    1\ntau       4.58    0.02 1.33   2.59   3.60   4.35   5.34   7.65  2941    1\nrho       0.27    0.00 0.11   0.11   0.19   0.26   0.34   0.51  3136    1\n\nSamples were drawn using NUTS(diag_e) at Wed Feb 26 10:48:54 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#comparison-to-linear-regression-boldsymbolbeta",
    "href": "slides/16-longitudinal-test.html#comparison-to-linear-regression-boldsymbolbeta",
    "title": "Hierarchical Models",
    "section": "Comparison to Linear Regression: \\(\\boldsymbol{\\beta}\\)",
    "text": "Comparison to Linear Regression: \\(\\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#model-comparison",
    "href": "slides/16-longitudinal-test.html#model-comparison",
    "title": "Hierarchical Models",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nlibrary(loo)\nwaic_conditional &lt;- waic(extract_log_lik(fit_conditional))\nwaic_lin_reg &lt;- waic(extract_log_lik(fit_lin_reg))\ncomparison &lt;- loo_compare(list(\"Hierarchical Model\" = waic_conditional, \"Linear Regression\" = waic_lin_reg))\nprint(comparison, simplify = FALSE)\n\n                   elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic\nHierarchical Model     0.0       0.0 -1720.6      15.2         11.5     0.7  \nLinear Regression    -71.6      10.7 -1792.2      15.6          8.6     0.6  \n                   waic    se_waic\nHierarchical Model  3441.2    30.4\nLinear Regression   3584.3    31.1"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#explore-the-clinic-specific-variation",
    "href": "slides/16-longitudinal-test.html#explore-the-clinic-specific-variation",
    "title": "Hierarchical Models",
    "section": "Explore the Clinic-Specific Variation",
    "text": "Explore the Clinic-Specific Variation\n\nprint(fit_conditional, pars = c(\"Intercept_East\", \"Intercept_South\", \"Intercept_North\", \"Intercept_West\"), probs = c(0.025, 0.975))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n                  mean se_mean   sd  2.5%  97.5% n_eff Rhat\nIntercept_East  102.08    0.05 2.88 96.55 107.62  3461    1\nIntercept_South  98.46    0.05 2.88 92.89 103.99  3498    1\nIntercept_North  92.35    0.05 2.87 86.78  97.90  3398    1\nIntercept_West  103.55    0.05 2.90 97.85 109.10  3502    1\n\nSamples were drawn using NUTS(diag_e) at Wed Feb 26 10:48:54 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#explore-the-clinic-specific-variation-1",
    "href": "slides/16-longitudinal-test.html#explore-the-clinic-specific-variation-1",
    "title": "Hierarchical Models",
    "section": "Explore the Clinic-Specific Variation",
    "text": "Explore the Clinic-Specific Variation"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#fitting-the-marginal-model-in-stan",
    "href": "slides/16-longitudinal-test.html#fitting-the-marginal-model-in-stan",
    "title": "Hierarchical Models",
    "section": "Fitting the Marginal Model in Stan",
    "text": "Fitting the Marginal Model in Stan\nNeed ragged data structure.\n\n// marginal-model.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; N;\n  int&lt;lower = 1&gt; p;\n  matrix[N, p] X;\n  vector[N] Y;\n  int n_is[n];\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; tau;\n}\ntransformed parameters {\n  real sigma2 = sigma * sigma;\n  real tau2 = tau * tau;\n}\nmodel {\n  // compute the mean process for the marginal model\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta;\n  }\n  // evaluate the likelihood for the marginal model using ragged data structure\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    matrix[n_i, n_i] Upsilon_i = sigma2 * diag_matrix(rep_vector(1.0, n_i)) + tau2 * rep_matrix(1, n_i, n_i);\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    vector[n_i] mu_i = segment(mu, pos, n_i);\n    target += multi_normal_lpdf(Y_i | mu_i, Upsilon_i);\n    pos = pos + n_i;\n  }\n  // priors\n  target += normal_lpdf(alpha | 0, 3);\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n}\ngenerated quantities {\n  // compute the mean process for the marginal model\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta;\n  }\n  // compute theta using the ragged data structure\n  int pos;\n  pos = 1;\n  vector[n] theta;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    vector[n_i] ones_i = rep_vector(1.0, n_i);\n    matrix[n_i, n_i] Upsilon_i = sigma2 * diag_matrix(ones_i) + tau2 * rep_matrix(1, n_i, n_i);\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    vector[n_i] mu_i = segment(mu, pos, n_i);\n    real mean_theta_i = tau2 * ones_i' * inverse_spd(Upsilon_i) * (Y_i - mu_i);\n    real var_theta_i = tau2 - tau2 * tau2 * ones_i' * inverse_spd(Upsilon_i) * ones_i;\n    theta[i] = normal_rng(mean_theta_i, sqrt(var_theta_i));\n    pos = pos + n_i;\n  }\n}"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#fitting-the-model-in-stan-1",
    "href": "slides/16-longitudinal-test.html#fitting-the-model-in-stan-1",
    "title": "Hierarchical Models",
    "section": "Fitting the Model in Stan",
    "text": "Fitting the Model in Stan\n\nstan_data &lt;- list(\n  N = nrow(data),\n  n = length(unique(data$region)),\n  p = ncol(X),\n  X = X,\n  Y = data$glucose,\n  n_is = as.numeric(table(data$region))\n)\nmarginal_model &lt;- stan_model(model_code = \"marginal-model.stan\")\nfit_marginal &lt;- sampling(marginal_model, stan_data)"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#assessing-convergence-2",
    "href": "slides/16-longitudinal-test.html#assessing-convergence-2",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(fit_marginal, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\"))"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#assessing-convergence-3",
    "href": "slides/16-longitudinal-test.html#assessing-convergence-3",
    "title": "Hierarchical Models",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_marginal, regex_pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\"))"
  },
  {
    "objectID": "slides/16-longitudinal-test.html#posterior-summaries-1",
    "href": "slides/16-longitudinal-test.html#posterior-summaries-1",
    "title": "Hierarchical Models",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nprint(fit_marginal, pars = c(\"alpha\", \"beta\", \"sigma\", \"tau\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nalpha    99.07    0.06 3.66  91.97  96.66  99.05 101.50 106.36  3285    1\nbeta[1]   0.26    0.00 0.03   0.20   0.24   0.26   0.28   0.32  3957    1\nbeta[2]   0.48    0.00 0.07   0.35   0.44   0.48   0.53   0.61  3389    1\nbeta[3]  10.48    0.01 0.67   9.18  10.01  10.49  10.95  11.76  4532    1\nbeta[4]  -4.75    0.01 0.65  -6.01  -5.18  -4.75  -4.34  -3.45  4611    1\nbeta[5]   0.29    0.01 0.77  -1.26  -0.23   0.28   0.79   1.77  3834    1\nbeta[6]  -4.51    0.01 0.79  -6.04  -5.04  -4.50  -4.00  -3.01  3855    1\nbeta[7] -17.88    0.01 0.66 -19.14 -18.33 -17.88 -17.44 -16.56  4370    1\nsigma     7.43    0.00 0.24   6.99   7.27   7.43   7.60   7.92  4223    1\ntau       4.55    0.02 1.27   2.61   3.62   4.38   5.27   7.48  3714    1\n\nSamples were drawn using NUTS(diag_e) at Wed Feb 26 11:25:39 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#conclusion",
    "href": "slides/16-longitudinal-test.html#conclusion",
    "title": "Hierarchical Models",
    "section": "Conclusion",
    "text": "Conclusion\n\nBy introducing a group-specific intercept, we allow for dependencies between observations within groups, making the model more realistic for real-world clustered or repeated measures data.\nFor the remainder of the class, we will expand upon this hierarchical modeling framework to account for complext data types that are frequently encountered in research, including longitudinal and spatial data."
  },
  {
    "objectID": "slides/16-longitudinal-test.html#prepare-for-next-class",
    "href": "slides/16-longitudinal-test.html#prepare-for-next-class",
    "title": "Hierarchical Models",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on Exam 01, which is due before next Thursday’s class!\nNext Tuesday’s class will be office hours. I will be available in the lecture room during the meeting time.\nNext Thursday’s lecture: Longitudinal Data\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/16-longitudinal.html#generating-multivariate-normal-rngs-3",
    "href": "slides/16-longitudinal.html#generating-multivariate-normal-rngs-3",
    "title": "Longitudinal Data",
    "section": "Generating Multivariate Normal RNGs",
    "text": "Generating Multivariate Normal RNGs\n\nSigma &lt;- matrix(c(3, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\nmu &lt;- matrix(c(2, 5), ncol = 1)\nD &lt;- matrix(0, nrow = 2, ncol = 2)\ndiag(D) &lt;- diag(sqrt(Sigma))\nPhi &lt;- cov2cor(Sigma)\nL &lt;- t(chol(Phi))\nn_samples &lt;- 1000\nz &lt;- matrix(rnorm(n_samples * 2), nrow = 2, ncol = n_samples)\nmu_mat &lt;- matrix(rep(mu, n_samples), nrow = 2, ncol = n_samples) \nX &lt;- mu_mat + D %*% L %*% z\napply(X, 1, mean)\n\n[1] 1.946052 4.950226\n\ncov(t(X))\n\n          [,1]      [,2]\n[1,] 3.0612065 0.9283299\n[2,] 0.9283299 2.9567120"
  },
  {
    "objectID": "slides/16-longitudinal.html#back-to-the-model",
    "href": "slides/16-longitudinal.html#back-to-the-model",
    "title": "Longitudinal Data",
    "section": "Back to the model!",
    "text": "Back to the model!\n\\[\\begin{aligned}\nY_{it} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}_i &= (\\beta_{0} + \\theta_{0i}) + (\\beta_1 + \\theta_{0i}) X_{it}+ \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\boldsymbol{\\theta}_i | \\boldsymbol{\\Sigma} &\\stackrel{iid}{\\sim} N_2(\\mathbf{0}_2,\\boldsymbol{\\Sigma})\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\]\nwhere \\(\\boldsymbol{\\Omega} = (\\beta_0, \\beta_1, \\sigma^2, \\boldsymbol{\\Sigma})\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#back-to-the-model-conditional-specification",
    "href": "slides/16-longitudinal.html#back-to-the-model-conditional-specification",
    "title": "Longitudinal Data",
    "section": "Back to the model! Conditional specification",
    "text": "Back to the model! Conditional specification\nFor \\(i\\) (\\(i = 1,\\ldots,n\\)) and \\(t\\) (\\(t = 1,\\ldots, n_i\\)), the model is:\n\\[\\begin{aligned}\nY_{it} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}_i &= (\\beta_{0} + \\theta_{0i}) + (\\beta_1 + \\theta_{0i}) X_{it}+ \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\boldsymbol{\\theta}_i | \\boldsymbol{\\Sigma} &\\stackrel{iid}{\\sim} N_2(\\mathbf{0}_2,\\boldsymbol{\\Sigma})\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\]\nwhere \\(\\boldsymbol{\\Omega} = (\\beta_0, \\beta_1, \\sigma^2, \\boldsymbol{\\Sigma})\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#group-specific-intercept-model-conditional-specification",
    "href": "slides/16-longitudinal.html#group-specific-intercept-model-conditional-specification",
    "title": "Longitudinal Data",
    "section": "Group-Specific Intercept Model: Conditional Specification",
    "text": "Group-Specific Intercept Model: Conditional Specification\n\nDefine \\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) and \\(\\mathbf{Y} = (\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n)\\).\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n \\prod_{t = 1}^{n_i} f(Y_{it} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})  \\prod_{i=1}^n f(\\boldsymbol{\\theta}_i | \\boldsymbol{\\Sigma}) f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\theta}_1,\\ldots,\\boldsymbol{\\theta}_n)\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#conditional-specification",
    "href": "slides/16-longitudinal.html#conditional-specification",
    "title": "Longitudinal Data",
    "section": "Conditional specification",
    "text": "Conditional specification\nFor the conditional specification, we can write the model at the observational level, \\(Y_{it}\\). This is because conditionally on the \\(\\boldsymbol{\\theta}_i\\), \\(Y_{it}\\) and \\(Y_{it'}\\) are independent.\nFor \\(i\\) (\\(i = 1,\\ldots,n\\)) and \\(t\\) (\\(t = 1,\\ldots, n_i\\)), the model is:\n\\[\\begin{aligned}\nY_{it} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}_i &\\stackrel{iid}{\\sim} N\\left((\\beta_{0} + \\theta_{0i}) + (\\beta_1 + \\theta_{0i}) X_{it}, \\sigma^2\\right),\\\\\n\\boldsymbol{\\theta}_i | \\boldsymbol{\\Sigma} &\\stackrel{iid}{\\sim} N_2(\\mathbf{0}_2,\\boldsymbol{\\Sigma})\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\]\nwhere \\(\\boldsymbol{\\Omega} = (\\beta_0, \\beta_1, \\sigma^2, \\boldsymbol{\\Sigma})\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#conditional-specification-1",
    "href": "slides/16-longitudinal.html#conditional-specification-1",
    "title": "Longitudinal Data",
    "section": "Conditional Specification",
    "text": "Conditional Specification\n\nMoments for the Conditional Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{it} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i] &= (\\beta_0 + \\theta_{0i}) + (\\beta_1 + \\theta_{1i}) X_{it}\\\\\n\\mathbb{V}(Y_{it} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i) &= \\sigma^2\\\\\n\\mathbb{C}(Y_{it}, Y_{jt'} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i,\\boldsymbol{\\theta}_{t'}) &= 0,\\quad \\forall i,j,t,t'\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal.html#marginal-specification",
    "href": "slides/16-longitudinal.html#marginal-specification",
    "title": "Longitudinal Data",
    "section": "Marginal Specification",
    "text": "Marginal Specification\nThe LMM model is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\nMoments for the Marginal Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}] &= \\mathbf{X}_i\\boldsymbol{\\beta}\\\\\n\\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) &= \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top + \\sigma^2 \\mathbf{I}_{n_i} = \\boldsymbol{\\Upsilon}_i\\\\\n\\mathbb{C}(\\mathbf{Y}_{i}, \\mathbf{Y}_{i'} | \\boldsymbol{\\Omega}) &= \\mathbf{0}_{n_i \\times n_i},\\quad i \\neq i'.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal.html#group-specific-intercept-model-marginal-specification",
    "href": "slides/16-longitudinal.html#group-specific-intercept-model-marginal-specification",
    "title": "Longitudinal Data",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\nThe LMM model is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\nMoments for the Marginal Model:\n\n\\[\\begin{aligned}\n\\mathbb{E}[\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}] &= \\mathbf{X}_i\\boldsymbol{\\beta}\\\\\n\\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) &= \\mathbf{X}_i \\boldsymbol{\\Sigma} \\mathbf{X}_i^\\top + \\sigma^2 \\mathbf{I}_{n_i} = \\boldsymbol{\\Upsilon}_i\\\\\n\\mathbb{C}(\\mathbf{Y}_{i}, \\mathbf{Y}_{i'} | \\boldsymbol{\\Omega}) &= \\mathbf{0}_{n_i \\times n_i},\\quad i \\neq i'.\n\\end{aligned}\\]\nIt is easier to see the dependency through the scalar form:\n\\[\\begin{aligned}\n\\mathbb{V}(Y_{it}| \\boldsymbol{\\Omega}) &= \\tau_0^2 + 2 \\tau_{01} X_{it}^2 + \\tau_1^2 X_{it}^2 + \\sigma^2,\\\\\n\\mathbb{C}(Y_{it}, Y_{it'} | \\boldsymbol{\\Omega}) &= \\tau_0^2 - \\rho \\tau_0 \\tau_1 (X_{it} - X_{it'}) + \\tau_1^2 X_{it} X_{it'}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal.html#group-specific-intercept-model-marginal-specification-1",
    "href": "slides/16-longitudinal.html#group-specific-intercept-model-marginal-specification-1",
    "title": "Longitudinal Data",
    "section": "Group-Specific Intercept Model: Marginal Specification",
    "text": "Group-Specific Intercept Model: Marginal Specification\n\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n f(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal.html#visualizing-the-dependency",
    "href": "slides/16-longitudinal.html#visualizing-the-dependency",
    "title": "Longitudinal Data",
    "section": "Visualizing the dependency",
    "text": "Visualizing the dependency\n\\(\\tau_0 = 1,\\tau_1 = 1,\\sigma^2 =2, \\rho = 0.5\\)"
  },
  {
    "objectID": "slides/16-longitudinal.html#visualizing-the-dependency-1",
    "href": "slides/16-longitudinal.html#visualizing-the-dependency-1",
    "title": "Longitudinal Data",
    "section": "Visualizing the dependency",
    "text": "Visualizing the dependency\n\\(\\tau_0 = 1,\\tau_1 = 1,\\sigma^2 =2, \\rho = -0.5\\)"
  },
  {
    "objectID": "slides/16-longitudinal.html#marginal-specification-1",
    "href": "slides/16-longitudinal.html#marginal-specification-1",
    "title": "Longitudinal Data",
    "section": "Marginal Specification",
    "text": "Marginal Specification\nFor \\(i = 1,\\ldots,n\\), \\[\\begin{aligned}\n\\mathbf{Y}_{i} | \\boldsymbol{\\Omega} &\\stackrel{ind}{\\sim} N(\\mathbf{X}_i\\boldsymbol{\\beta},\\boldsymbol{\\Upsilon}_i)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\Omega} = (\\boldsymbol{\\beta},\\sigma,\\boldsymbol{\\Sigma})\\) are the population parameters."
  },
  {
    "objectID": "slides/16-longitudinal.html#marginal-specification-2",
    "href": "slides/16-longitudinal.html#marginal-specification-2",
    "title": "Longitudinal Data",
    "section": "Marginal Specification",
    "text": "Marginal Specification\nIt is not as straightforward to gain intuition behind the induced correlation structure, but we can shed some light by studying the scalar form of the covariance:\n\\[\\begin{aligned}\n\\mathbb{V}(Y_{it}| \\boldsymbol{\\Omega}) &= \\tau_0^2 + 2 \\tau_{01} X_{it}^2 + \\tau_1^2 X_{it}^2 + \\sigma^2,\\\\\n\\mathbb{C}(Y_{it}, Y_{it'} | \\boldsymbol{\\Omega}) &= \\tau_0^2 - \\rho \\tau_0 \\tau_1 (X_{it} - X_{it'}) + \\tau_1^2 X_{it} X_{it'}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal.html#what-does-the-lkj-prior-do",
    "href": "slides/16-longitudinal.html#what-does-the-lkj-prior-do",
    "title": "Longitudinal Data",
    "section": "What Does the LKJ Prior Do?",
    "text": "What Does the LKJ Prior Do?\n\nThe LKJ prior allows you to model the correlation structure in a flexible and non-informative way.\nIt is defined by a single parameter, \\(\\eta &gt; 0\\), which controls the concentration of the prior.\n\nWhen \\((\\eta = 1)\\), it is an uninformative prior (i.e., uniform on the correlations).\nWhen \\((\\eta &gt; 1)\\), the prior favors more highly correlated random effects.\nWhen \\((\\eta &lt; 1)\\), the prior favors weaker correlations.\n\nWhen \\(q=2\\), \\(\\eta = 1\\) is equivalent to \\(\\rho \\sim \\text{Uniform}(-1,1)\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#lkj-prior-formula",
    "href": "slides/16-longitudinal.html#lkj-prior-formula",
    "title": "Longitudinal Data",
    "section": "LKJ Prior Formula",
    "text": "LKJ Prior Formula\n\nThe LKJ prior on a correlation matrix \\(\\mathbf{L}\\) is given by:\n\n\\[f(\\mathbf{L} | \\eta) \\propto \\prod_{j = 2}^q L_{jj}^{q-j+2\\eta-2}.\\]\nWhere:\n\n\\(\\eta\\) is the concentration parameter.\n\\(q\\) is the size of the correlation matrix.\n\\(L_{jk}\\) is the observation in the \\(j\\)th row and \\(k\\)th column of \\(\\mathbf{L}\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#understanding-the-conditional-mean",
    "href": "slides/16-longitudinal.html#understanding-the-conditional-mean",
    "title": "Longitudinal Data",
    "section": "Understanding the conditional mean",
    "text": "Understanding the conditional mean\n\\[\\begin{aligned}\n\\mathbb{E}[Y_{it} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i] &= \\beta_{0i} + \\beta_{1i} X_{it}\\\\\n&= (\\beta_0 + \\theta_{0i}) + (\\beta_1 + \\theta_{1i}) X_{it}\\\\\n&= \\beta_0 + \\beta_1 X_{it} + \\theta_{0i} + \\theta_{1i} X_{it}\\\\\n&= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\mathbf{x}_{it} \\boldsymbol{\\theta}_i,\n\\end{aligned}\\] where \\(\\mathbf{x}_{it} = (1, X_{it})\\) and \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#conditional-specification-2",
    "href": "slides/16-longitudinal.html#conditional-specification-2",
    "title": "Longitudinal Data",
    "section": "Conditional Specification",
    "text": "Conditional Specification\n\nDefine \\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) and \\(\\mathbf{Y} = (\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n)\\).\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n \\prod_{t = 1}^{n_i} f(Y_{it} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})  \\prod_{i=1}^n f(\\boldsymbol{\\theta}_i | \\boldsymbol{\\Sigma}) f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\] where \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\theta}_1,\\ldots,\\boldsymbol{\\theta}_n)\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#linear-mixed-model-1",
    "href": "slides/16-longitudinal.html#linear-mixed-model-1",
    "title": "Longitudinal Data",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\n\\(\\mathbf{X}_i\\) is an \\((n_i \\times p)\\)-dimensional matrix with row \\(\\mathbf{x}_{it}\\) (intercept is incorporated).\n\\(\\mathbf{x}_{it}\\) contains variables that are assumed to relate to the outcome only at a population-level.\n\\(p\\) is the number of population-level variables."
  },
  {
    "objectID": "slides/16-longitudinal.html#linear-mixed-model-2",
    "href": "slides/16-longitudinal.html#linear-mixed-model-2",
    "title": "Longitudinal Data",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\n\\(\\mathbf{Z}_i\\) is an \\((n_i \\times q)\\)-dimensional matrix with row \\(\\mathbf{z}_{it}\\) (intercept is incorporated).\n\\(\\mathbf{z}_{it}\\) contains variables that are assumed to relate to the outcome with varying effects at a subject-level.\n\\(q\\) is the number of subject-level variables."
  },
  {
    "objectID": "slides/16-longitudinal.html#linear-mixed-model-3",
    "href": "slides/16-longitudinal.html#linear-mixed-model-3",
    "title": "Longitudinal Data",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\n\\(\\boldsymbol{\\beta}\\) is a \\(p\\)-dimensional vector of population-level parameters (or fixed effects).\n\\(\\boldsymbol{\\theta}_i\\) is a \\(q\\)-dimensional vector of group-level parameters (or random effects).\n\\(\\sigma^2\\) is a population-level parameter that measures residual error."
  },
  {
    "objectID": "slides/16-longitudinal.html#recovering-the-random-intercept-model",
    "href": "slides/16-longitudinal.html#recovering-the-random-intercept-model",
    "title": "Longitudinal Data",
    "section": "Recovering the Random Intercept Model",
    "text": "Recovering the Random Intercept Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\nSuppose that \\(\\mathbf{z}_{it} = 1 \\forall i,t\\). Then we get\n\\[\\begin{aligned}\nY_{it} &= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\mathbf{z}_{it}\\boldsymbol{\\theta}_{i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n&= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\theta_{i} + \\epsilon_{it}.\n\\end{aligned}\\]\n\nLMM is a general form of the random intercept model."
  },
  {
    "objectID": "slides/16-longitudinal.html#recover-the-random-intercept-model",
    "href": "slides/16-longitudinal.html#recover-the-random-intercept-model",
    "title": "Longitudinal Data",
    "section": "Recover the Random Intercept Model",
    "text": "Recover the Random Intercept Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\nSuppose that \\(\\mathbf{z}_{it} = 1 \\forall i,t\\). Then we get\n\\[\\begin{aligned}\nY_{it} &= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\mathbf{z}_{it}\\boldsymbol{\\theta}_{i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n&= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\theta_{i} + \\epsilon_{it}.\n\\end{aligned}\\]\n\nLMM is a general form of the random intercept model."
  },
  {
    "objectID": "slides/16-longitudinal.html#random-and-slope-intercept-model",
    "href": "slides/16-longitudinal.html#random-and-slope-intercept-model",
    "title": "Longitudinal Data",
    "section": "Random and Slope Intercept Model",
    "text": "Random and Slope Intercept Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\nSuppose that \\(\\mathbf{x}_{it} = \\mathbf{z}_{it} = (1, X_{it})\\), such that \\(p = q = 2\\). Then we get,\n\\[\\begin{aligned}\nY_{it} &= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\mathbf{z}_{it}\\boldsymbol{\\theta}_{i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n&= \\beta_0 + \\beta_1 X_{it} + \\theta_{0i} + \\theta_{1i} X_{it} + \\epsilon_{it}.\n\\end{aligned}\\]\n\nLMM is a general form of the random intercept model."
  },
  {
    "objectID": "slides/16-longitudinal.html#random-slope-and-intercept-model",
    "href": "slides/16-longitudinal.html#random-slope-and-intercept-model",
    "title": "Longitudinal Data",
    "section": "Random Slope and Intercept Model",
    "text": "Random Slope and Intercept Model\nFor \\(i = 1,\\ldots,n\\), the linear mixed model (LMM) is given by:\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\nSuppose that \\(\\mathbf{x}_{it} = \\mathbf{z}_{it} = (1, X_{it})\\), such that \\(p = q = 2\\). Then,\n\\[\\begin{aligned}\nY_{it} &= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\mathbf{z}_{it}\\boldsymbol{\\theta}_{i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n&= \\beta_0 + \\beta_1 X_{it} + \\theta_{0i} + \\theta_{1i} X_{it} + \\epsilon_{it}\\\\\n&= (\\beta_0 + \\theta_{0i}) + (\\beta_1 + \\theta_{1i}) X_{it} + \\epsilon_{it}.\n\\end{aligned}\\]\nwhere \\(\\boldsymbol{\\beta} = (\\beta_0,\\beta_1)\\) and \\(\\boldsymbol{\\theta}_i = (\\theta_{0i},\\theta_{1i})\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#marginal-specification-3",
    "href": "slides/16-longitudinal.html#marginal-specification-3",
    "title": "Longitudinal Data",
    "section": "Marginal Specification",
    "text": "Marginal Specification\n\nThe posterior for the conditional model can be written as:\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n f(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal.html#specifying-a-prior-distribution-for-boldsymbolomega",
    "href": "slides/16-longitudinal.html#specifying-a-prior-distribution-for-boldsymbolomega",
    "title": "Longitudinal Data",
    "section": "Specifying a Prior Distribution for \\(\\boldsymbol{\\Omega}\\)",
    "text": "Specifying a Prior Distribution for \\(\\boldsymbol{\\Omega}\\)\n\nWe must set a prior for \\(f(\\boldsymbol{\\Omega}) = f(\\boldsymbol{\\beta}) f(\\sigma) f(\\boldsymbol{\\Sigma})\\).\nWe can place standard priors on \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\).\n\\(\\boldsymbol{\\Sigma}\\) is a covariance (i.e., positive definite matrix), so we must be careful here.\nIt is natural to place a prior on the decomposition, \\(\\boldsymbol{\\Sigma} = \\mathbf{D} \\mathbf{L} \\mathbf{L}^\\top \\mathbf{D}\\).\n\nFor each of the standard deviations \\((\\tau_0,\\tau_1)\\) we can place standard priors for scales (e.g., half-normal).\nFor \\(\\mathbf{L}\\) we can place a Lewandowski-Kurowicka-Joe (LKJ) distribution, \\(\\mathbf{L} \\sim LKJ(\\eta)\\)."
  },
  {
    "objectID": "slides/16-longitudinal.html#lkj-prior-in-stan",
    "href": "slides/16-longitudinal.html#lkj-prior-in-stan",
    "title": "Longitudinal Data",
    "section": "LKJ Prior in Stan",
    "text": "LKJ Prior in Stan\n\nparameters {\n  cholesky_factor_corr[2] L;  // Cholesky factor of correlation matrix\n}\nmodel {\n  L ~ lkj_corr_cholesky(eta);\n}"
  },
  {
    "objectID": "slides/16-longitudinal.html#glaucoma-disease-progression",
    "href": "slides/16-longitudinal.html#glaucoma-disease-progression",
    "title": "Longitudinal Data",
    "section": "Glaucoma Disease Progression",
    "text": "Glaucoma Disease Progression"
  },
  {
    "objectID": "slides/16-longitudinal.html#stan-code-for-conditional-lmm",
    "href": "slides/16-longitudinal.html#stan-code-for-conditional-lmm",
    "title": "Longitudinal Data",
    "section": "Stan code for conditional LMM",
    "text": "Stan code for conditional LMM\n\n// lmm-conditional.stan\ndata {\n  int&lt;lower = 1&gt; N;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1&gt; q;\n  matrix[N, p] X;\n  matrix[N, q] Z;\n  vector[N] Y;\n  int&lt;lower = 1, upper = n&gt; Ids[N];\n  real&lt;lower = 0&gt; eta;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  matrix[q, n] z;\n  vector&lt;lower = 0&gt;[q] tau;\n  cholesky_factor_corr[q] L;\n}\ntransformed parameters {\n  matrix[q, n] theta;\n  theta = diag_pre_multiply(tau, L) * z;\n}\nmodel {\n  // likelihood\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = X[i, ] * beta + Z[i, ] * theta[, Ids[i]];\n  }\n  target += normal_lpdf(Y | mu, sigma);\n  // subject-specific parameter\n  target += std_normal_lpdf(to_vector(z));\n  // population parameters\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n  target += lkj_corr_cholesky_lpdf(L | eta);\n}\ngenerated quantities {\n  corr_matrix[q] Phi = L * transpose(L);\n  real rho = Phi[1, 2];\n  vector[n] subject_intercepts = beta[1] + to_vector(theta[1, ]);\n  vector[n] subject_slopes = beta[2] + to_vector(theta[2, ]);\n  vector[N] Y_pred;\n  vector[N] log_lik;\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = X[i, ] * beta + Z[i, ] * theta[, Ids[i]];\n    Y_pred[i] = normal_rng(mu[i], sigma);\n    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/16-longitudinal.html#stan-code-for-marginal-lmm",
    "href": "slides/16-longitudinal.html#stan-code-for-marginal-lmm",
    "title": "Longitudinal Data",
    "section": "Stan code for marginal LMM",
    "text": "Stan code for marginal LMM\nNeed ragged data structure.\n\n// lmm-marginal.stan\ndata {\n  int&lt;lower = 1&gt; N;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1&gt; q;\n  matrix[N, p] X;\n  matrix[N, q] Z;\n  vector[N] Y;\n  int&lt;lower = 1&gt; n_is[n];\n  real&lt;lower = 0&gt; eta;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector&lt;lower = 0&gt;[q] tau;\n  cholesky_factor_corr[q] L;\n}\ntransformed parameters {\n  cov_matrix[q] Sigma;\n  Sigma = diag_pre_multiply(tau, L) * transpose(diag_pre_multiply(tau, L));\n}\nmodel {\n  // evaluate the likelihood for the marginal model using ragged data structure\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    matrix[n_i, p] X_i;\n    matrix[n_i, q] Z_i;\n    for (j in 1:p) X_i[, j] = segment(X[, j], pos, n_i);\n    for (j in 1:q) Z_i[, j] = segment(Z[, j], pos, n_i);\n    vector[n_i] mu_i = X_i * beta;\n    matrix[n_i, n_i] Upsilon_i = (sigma * sigma) * diag_matrix(rep_vector(1.0, n_i)) + Z_i * Sigma * transpose(Z_i);\n    target += multi_normal_lpdf(Y_i | mu_i, Upsilon_i);\n    pos = pos + n_i;\n  }\n  // population parameters\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n  target += lkj_corr_cholesky_lpdf(L | eta);\n}\ngenerated quantities {\n  corr_matrix[q] Phi = L * transpose(L);\n  real rho = Phi[1, 2];\n  matrix[q, n] theta;\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    matrix[n_i, p] X_i;\n    matrix[n_i, q] Z_i;\n    for (j in 1:p) X_i[, j] = segment(X[, j], pos, n_i);\n    for (j in 1:q) Z_i[, j] = segment(Z[, j], pos, n_i);\n    vector[n_i] mu_i = X_i * beta;\n    matrix[q, n_i] M = Sigma * transpose(Z_i) * inverse_spd(Z_i * Sigma * transpose(Z_i) + (sigma * sigma) * diag_matrix(rep_vector(1.0, n_i)));\n    vector[q] mean_theta_i = M * (Y_i - mu_i);\n    matrix[q, q] cov_theta_i = Sigma - M * Z_i * Sigma;\n    theta[, i] = multi_normal_rng(mean_theta_i, cov_theta_i);\n    pos = pos + n_i;\n  }\n  vector[n] subject_intercepts = beta[1] + to_vector(theta[1, ]);\n  vector[n] subject_slopes = beta[2] + to_vector(theta[2, ]);\n}"
  },
  {
    "objectID": "slides/16-longitudinal.html#glaucoma-disease-progression-data",
    "href": "slides/16-longitudinal.html#glaucoma-disease-progression-data",
    "title": "Longitudinal Data",
    "section": "Glaucoma Disease Progression Data",
    "text": "Glaucoma Disease Progression Data\n\nglaucoma_longitudinal &lt;- readRDS(\"glaucoma_longitudinal.rds\")\nhead(glaucoma_longitudinal)\n\n\n\n  pat_id eye_id mean_deviation      time      age      iop\n1      1      1          -7.69 0.0000000 51.55616 10.87303\n2      1      1          -9.95 0.5753425 51.55616 10.87303\n3      1      1          -9.58 1.0547945 51.55616 10.87303\n4      1      1          -9.53 1.5726027 51.55616 10.87303\n5      1      1          -9.18 2.0136986 51.55616 10.87303\n6      1      1          -9.63 2.5671233 51.55616 10.87303\n\n\n\nlength(unique(glaucoma_longitudinal$eye_id))\n\n[1] 278\n\nnrow(glaucoma_longitudinal)\n\n[1] 4863"
  },
  {
    "objectID": "slides/16-longitudinal.html#fitting-the-model-in-stan",
    "href": "slides/16-longitudinal.html#fitting-the-model-in-stan",
    "title": "Longitudinal Data",
    "section": "Fitting the Model in Stan",
    "text": "Fitting the Model in Stan\n\nX &lt;- model.matrix(~ time, data = glaucoma_longitudinal)\nstan_data &lt;- list(\n  N = nrow(glaucoma_longitudinal),\n  n = n_eyes,\n  p = ncol(X),\n  q = ncol(X),\n  X = X,\n  Z = X,\n  Y = glaucoma_longitudinal$mean_deviation,\n  n_is = as.numeric(table(glaucoma_longitudinal$eye_id)),\n  eta = 1\n)\nlmm_marginal &lt;- stan_model(model_code = \"lmm-marginal.stan\")\nfit_lmm_marginal &lt;- sampling(lmm_marginal, stan_data)"
  },
  {
    "objectID": "slides/16-longitudinal.html#assessing-convergence",
    "href": "slides/16-longitudinal.html#assessing-convergence",
    "title": "Longitudinal Data",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(fit_lmm_conditional, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/16-longitudinal.html#assessing-convergence-1",
    "href": "slides/16-longitudinal.html#assessing-convergence-1",
    "title": "Longitudinal Data",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_lmm_conditional, regex_pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/16-longitudinal.html#posterior-summaries",
    "href": "slides/16-longitudinal.html#posterior-summaries",
    "title": "Longitudinal Data",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nprint(fit_lmm_conditional, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -8.19    0.04 0.47 -9.15 -8.50 -8.19 -7.86 -7.34   178 1.03\nbeta[2] -0.10    0.00 0.03 -0.16 -0.12 -0.10 -0.08 -0.05   778 1.00\nsigma    1.27    0.00 0.01  1.24  1.26  1.26  1.27  1.29 15569 1.00\ntau[1]   8.07    0.02 0.34  7.46  7.83  8.05  8.29  8.79   364 1.01\ntau[2]   0.44    0.00 0.02  0.40  0.42  0.44  0.45  0.48  2064 1.00\nrho     -0.27    0.00 0.06 -0.38 -0.31 -0.27 -0.23 -0.16   949 1.00\n\nSamples were drawn using NUTS(diag_e) at Tue Mar  4 15:18:54 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/16-longitudinal.html#fitting-the-conditional-model-in-stan",
    "href": "slides/16-longitudinal.html#fitting-the-conditional-model-in-stan",
    "title": "Longitudinal Data",
    "section": "Fitting the Conditional Model in Stan",
    "text": "Fitting the Conditional Model in Stan\n\nX &lt;- model.matrix(~ time, data = glaucoma_longitudinal)\nstan_data &lt;- list(\n  N = nrow(glaucoma_longitudinal),\n  n = n_eyes,\n  p = ncol(X),\n  q = ncol(X),\n  X = X,\n  Z = X,\n  Y = glaucoma_longitudinal$mean_deviation,\n  Ids = glaucoma_longitudinal$eye_id,\n  eta = 1\n)\nlmm_conditional &lt;- stan_model(model_code = \"lmm-conditional.stan\")\nfit_lmm_conditional &lt;- sampling(lmm_conditional, stan_data, iter = 5000, pars = c(\"z\", \"theta\"), include = FALSE)"
  },
  {
    "objectID": "slides/16-longitudinal.html#fitting-the-marginal-model-in-stan",
    "href": "slides/16-longitudinal.html#fitting-the-marginal-model-in-stan",
    "title": "Longitudinal Data",
    "section": "Fitting the Marginal Model in Stan",
    "text": "Fitting the Marginal Model in Stan\n\nX &lt;- model.matrix(~ time, data = glaucoma_longitudinal)\nstan_data &lt;- list(\n  N = nrow(glaucoma_longitudinal),\n  n = n_eyes,\n  p = ncol(X),\n  q = ncol(X),\n  X = X,\n  Z = X,\n  Y = glaucoma_longitudinal$mean_deviation,\n  n_is = as.numeric(table(glaucoma_longitudinal$eye_id)),\n  eta = 1\n)\nlmm_marginal &lt;- stan_model(model_code = \"lmm-marginal.stan\")\nfit_lmm_marginal &lt;- sampling(lmm_marginal, stan_data, iter = 5000)"
  },
  {
    "objectID": "slides/16-longitudinal.html#assessing-convergence-2",
    "href": "slides/16-longitudinal.html#assessing-convergence-2",
    "title": "Longitudinal Data",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(fit_lmm_marginal, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/16-longitudinal.html#assessing-convergence-3",
    "href": "slides/16-longitudinal.html#assessing-convergence-3",
    "title": "Longitudinal Data",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_lmm_marginal, regex_pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))"
  },
  {
    "objectID": "slides/16-longitudinal.html#posterior-summaries-1",
    "href": "slides/16-longitudinal.html#posterior-summaries-1",
    "title": "Longitudinal Data",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nprint(fit_lmm_marginal, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -8.23       0 0.47 -9.16 -8.55 -8.23 -7.92 -7.31 12896    1\nbeta[2] -0.10       0 0.03 -0.16 -0.12 -0.10 -0.08 -0.05 13230    1\nsigma    1.27       0 0.01  1.24  1.26  1.26  1.27  1.29 13937    1\ntau[1]   8.05       0 0.34  7.42  7.82  8.04  8.27  8.75 14264    1\ntau[2]   0.44       0 0.02  0.40  0.42  0.44  0.45  0.48 13997    1\nrho     -0.27       0 0.06 -0.38 -0.31 -0.28 -0.24 -0.16 12487    1\n\nSamples were drawn using NUTS(diag_e) at Tue Mar  4 11:31:23 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/16-longitudinal.html#recovering-the-subject-specific-parameters",
    "href": "slides/16-longitudinal.html#recovering-the-subject-specific-parameters",
    "title": "Longitudinal Data",
    "section": "Recovering the Subject-Specific Parameters",
    "text": "Recovering the Subject-Specific Parameters\n\nWe can still recover the \\(\\boldsymbol{\\theta}_i\\) when we fit the marginal model, we only need to compute \\(f(\\boldsymbol{\\theta}_i | \\mathbf{Y}_i,\\boldsymbol{\\Omega})\\) for all \\(i\\).\nWe can obtain this full conditional by specifying the joint distribution,\n\n\\[f\\left(\\begin{bmatrix}\n    \\mathbf{Y}_i\\\\\n    \\boldsymbol{\\theta}_i\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega}\\right) = N\\left(\\begin{bmatrix}\n    \\mathbf{X}_i \\boldsymbol{\\beta} \\\\\n    \\mathbf{0}_{n_1}\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol{\\Upsilon}_i & \\mathbf{Z}_i\\boldsymbol{\\Sigma}\\\\\n    \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top & \\boldsymbol{\\Sigma}\n  \\end{bmatrix}\\right).\\]\nWe can then use the conditional specification of a multivariate normal to find, \\(f(\\boldsymbol{\\theta}_i | \\mathbf{Y}_i, \\boldsymbol{\\Omega}) = N(\\mathbb{E}_{\\boldsymbol{\\theta}_i},\\mathbb{V}_{\\boldsymbol{\\theta}_i})\\), where\n\\[\\begin{aligned}\n\\mathbb{E}_{\\boldsymbol{\\theta}_i} &= \\mathbf{0}_{n_i} + \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top \\boldsymbol{\\Upsilon}_i^{-1} (\\mathbf{Y}_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\\\\n\\mathbb{V}_{\\boldsymbol{\\theta}_i} &= \\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top \\boldsymbol{\\Upsilon}_i^{-1} \\mathbf{Z}_i\\boldsymbol{\\Sigma}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/16-longitudinal.html#comparing-lmm-versus-ols",
    "href": "slides/16-longitudinal.html#comparing-lmm-versus-ols",
    "title": "Longitudinal Data",
    "section": "Comparing LMM versus OLS",
    "text": "Comparing LMM versus OLS"
  },
  {
    "objectID": "slides/16-longitudinal.html#comparing-lmm-versus-ols-1",
    "href": "slides/16-longitudinal.html#comparing-lmm-versus-ols-1",
    "title": "Longitudinal Data",
    "section": "Comparing LMM versus OLS",
    "text": "Comparing LMM versus OLS"
  },
  {
    "objectID": "prepare/prepare-mar05.html",
    "href": "prepare/prepare-mar05.html",
    "title": "Prepare for March 05 lecture",
    "section": "",
    "text": "📖 Review the linear mixed model.\n📖 Read pages 183-192 from Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists.\n✅ Turn in Exam 01, which is due before class on Thursday."
  },
  {
    "objectID": "slides/15-hierarchical.html",
    "href": "slides/15-hierarchical.html",
    "title": "Hierarchical Models",
    "section": "",
    "text": "On Tuesday, we learned about missing data, including various types: MCAR, MAR, and MNAR.\nWe learned about two Bayesian approaches to missing data under the MAR assumption:\n\nJoint model that assumes the missing data are parameters.\nMultiple imputation.\n\nMoving forward: Up until today, we have dealt with independent data. Today, we will look at our first example of dependent data!"
  },
  {
    "objectID": "hw/hw-04-blank.html",
    "href": "hw/hw-04-blank.html",
    "title": "HW 04",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "hw/hw-04.html",
    "href": "hw/hw-04.html",
    "title": "HW 04: Hierarchical models",
    "section": "",
    "text": "Due date\n\n\n\nThis assignment is due on Thursday, March 25 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercises-1-6",
    "href": "hw/hw-04.html#exercises-1-6",
    "title": "HW 04: Hierarchical models",
    "section": "Exercises 1-6",
    "text": "Exercises 1-6\nDefine \\(Y_{it}\\) as the MD value for eye \\(i\\) (\\(i = 1,\\ldots,n\\)) and visit \\(t\\) (\\(t = 1,\\ldots, n_i\\)), and \\(X_{it}\\) is the follow-up time, such that \\(X_{i0} = 0\\). Researchers are interested in estimating the eye-specific slopes for our sample to identify which patients are progressing (i.e., slope less than 0). To estimate these eye-specific slopes, researchers would like to fit the following model:\n\\[\\begin{align*}\nY_{it} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}_i &\\stackrel{iid}{\\sim} N\\left(\\beta_{0i} + \\beta_{1i}X_{it}, \\sigma^2\\right),\\\\\n\\beta_{0i} &= \\beta_{0} + \\theta_{0i}\\\\\n\\beta_{1i} &= \\beta_{1} + \\theta_{1i}\\\\\n\\boldsymbol{\\theta}_i | \\boldsymbol{\\Sigma} &\\stackrel{iid}{\\sim} N_2(\\mathbf{0}_2,\\boldsymbol{\\Sigma}),\\quad\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n    \\tau_{0}^2 & \\rho \\tau_0 \\tau_1\\\\\n    \\rho \\tau_0 \\tau_1 & \\tau_1^2\\\\\n  \\end{bmatrix}\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{align*}\\] where \\(\\boldsymbol{\\theta}_i = (\\theta_{0i},\\theta_{1i})\\) and \\(\\boldsymbol{\\Omega} = (\\beta_0, \\beta_1, \\sigma^2, \\boldsymbol{\\Sigma})\\). For priors, the researchers want to decompose the covariance structure, such that \\(\\boldsymbol{\\Sigma} = \\mathbf{D}\\mathbf{L}\\mathbf{L}^\\top\\mathbf{D}\\), where \\(\\mathbf{D}\\) is a diagonal matrix with the standard deviations \\((\\tau_0,\\tau_1)\\) on the diagonals. and \\(\\mathbf{L}\\) is the lower triangular Cholesky of the correlation matrix, \\(\\boldsymbol{\\Phi} = \\mathbf{L}\\mathbf{L}^\\top\\).",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercise-1",
    "href": "hw/hw-04.html#exercise-1",
    "title": "HW 04: Hierarchical models",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit the eye-specific intercept and slope model to the glaucoma data using the conditional model given above. For priors use \\({\\beta}_j \\sim N(0,3^2)\\) for \\(j = 0,1\\), \\(\\sigma \\sim \\text{Half-Normal}(0,3^2)\\), \\(\\tau_{j} \\sim \\text{Half-Normal}(0, 3^2)\\) for \\(j =0,1\\), and \\(\\mathbf{L} \\sim LKJ(\\eta)\\), with \\(\\eta = 1\\). Assess model convergence and perform a posterior predictive check.\nNotes: This model may take a bit longer to run, one way to speed this up is to run options(mc.cores = 4) to run the chains in parallel. You may also want to only save parameters that you will need in subsequent exercises, by specifying sampling(..., pars = c(\"parameters you don't want to save\"), include = FALSE). This will make the model fit object must smaller. Finally, it is possible that the model will need to be run a bit longer for convergence, so you may need to increase iter.",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercise-2",
    "href": "hw/hw-04.html#exercise-2",
    "title": "HW 04: Hierarchical models",
    "section": "Exercise 2",
    "text": "Exercise 2\nExplore the posterior mean estimates for the eye-specific slopes, \\(\\beta_{1i}\\). Plot the posterior mean estimates of \\(\\beta_{1i}\\) against the eye-specific slopes obtained from ordinary least squares (OLS) regression. How do the slopes compare from the two models? Hint: It may be helpful to plot a line with zero intercept and slope one.",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercise-3",
    "href": "hw/hw-04.html#exercise-3",
    "title": "HW 04: Hierarchical models",
    "section": "Exercise 3",
    "text": "Exercise 3\nSummarize the posterior distributions of the population parameters \\(\\beta_0\\) and \\(\\beta_1\\). Interpret the posterior means for each parameter in the context of the scientific problem.",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercsie-4",
    "href": "hw/hw-04.html#exercsie-4",
    "title": "HW 04: Hierarchical models",
    "section": "Exercsie 4",
    "text": "Exercsie 4\nExplore the correlation between eye-specific intercepts (\\(\\beta_{0i}\\)) and slopes (\\(\\beta_{1i}\\)). Create a scatter plot to examine their relationship. Does their appear to be a relationship? To help answer this question also create examine the posterior distribution for \\(\\rho\\) and comment on how it relates to the scatter plot.",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercise-5",
    "href": "hw/hw-04.html#exercise-5",
    "title": "HW 04: Hierarchical models",
    "section": "Exercise 5",
    "text": "Exercise 5\nCompute the posterior probability of progression for each participant, \\(p_i = P(\\beta_{1i} &lt; 0 | \\mathbf{Y})\\). Define progression as \\(prog_i = 1(p_i &gt; 0.95)\\) Plot the \\(prog_i\\) versus eye-specific baseline variables, including age, IOP, and baseline MD. Are any baseline characteristics associated with progression?",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercise-6",
    "href": "hw/hw-04.html#exercise-6",
    "title": "HW 04: Hierarchical models",
    "section": "Exercise 6",
    "text": "Exercise 6\nSuppose we have a new eye, \\(\\mathbf{Y}_{i^*} = \\left(Y_{i^*1},\\ldots,Y_{i^*n_{i^*}}\\right)\\), that was not used to train the original data. We are interested in predicting their next visual field at time \\(X_{i^*t^*}\\), where \\(t^* = (n_{i^*} + 1)\\). The posterior predictive distribution is given by:\n\\[\\begin{align*}\nf\\left(Y_{i^*t^*} | \\mathbf{Y}_{i^*}, \\mathbf{Y}\\right) &= \\int f\\left(Y_{i^*t^*},\\boldsymbol{\\Omega},\\boldsymbol{\\theta}_{i^*} | \\mathbf{Y}_{i^*}, \\mathbf{Y}\\right) d\\boldsymbol{\\Omega}d\\boldsymbol{\\theta}_{i^*}\\\\\n&= \\int f\\left(Y_{i^*t^*} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_{i^*} , \\mathbf{Y}_{i^*}, \\mathbf{Y}\\right) f\\left(\\boldsymbol{\\Omega},\\boldsymbol{\\theta}_{i^*} | \\mathbf{Y}_{i^*}, \\mathbf{Y}\\right) d\\boldsymbol{\\Omega}d\\boldsymbol{\\theta}_{i^*}\\\\\n&= \\int \\underbrace{f\\left(Y_{i^*t^*} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_{i^*}\\right)}_{\\text{Likelihood}} \\underbrace{f\\left(\\boldsymbol{\\theta}_{i^*} |\\boldsymbol{\\Omega}, \\mathbf{Y}_{i^*}\\right)}_{\\text{Eye-Specific Posterior}}\\underbrace{f\\left(\\boldsymbol{\\Omega}|\\mathbf{Y}\\right)}_{\\text{Posterior}} d\\boldsymbol{\\Omega}d\\boldsymbol{\\theta}_{i^*}\\\\\n\\end{align*}\\]\nSimilar to the posterior predictive distributions we have worked with before, we can sample from all three of these distributions to obtain samples from the posterior predictive distribution for the the future observation \\(Y_{i^*t^*}\\). The first distribution is the conditional likelihood, the second distribution is the posterior distribution for a subject-specific parameter (which we derived for the marginal likelihood in the lecture slides), and the third distribution is the posterior.\nThe new eye has five observation (\\(n_{i^*} = 5\\)) with the following MD values: \\(\\mathbf{Y}_{i^*} = (-1.44, -2.01, -1.98, -2.67, -3.01)\\) that were observed at the following time points: \\((0.00, 0.56, 1.20, 1.80, 2.40)\\). Provide the posterior predictive distribution for \\(Y_{i^*t^*}\\) at three years from baseline (i.e., \\(X_{i^*t^*} = 3\\)).",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercises-7-9",
    "href": "hw/hw-04.html#exercises-7-9",
    "title": "HW 04: Hierarchical models",
    "section": "Exercises 7-9",
    "text": "Exercises 7-9\nWhen studying the eye-specific slopes from the OLS regressions (given in the figure below), the researchers are concerned that there are some outliers that may actually be clinically realistic. For example, a slope of -3 may be possible in certain types of glaucoma.\n\n\n\n\n\n\n\n\n\nBased on this observation, the researchers would like a more robust model for the eye-specific parameters. They decide to specify a multivariate Student-t distribution: \\(\\boldsymbol{\\theta}_i \\stackrel{iid}{\\sim} t_{\\nu}(\\mathbf{0}, \\boldsymbol{\\Sigma})\\). To generate these parameters efficiently, the researchers use the following transformation of standard normal random variables and inverse-\\(\\chi^2\\) random variables, \\[\\boldsymbol{\\theta}_i = \\sqrt{\\nu v_i}\\mathbf{D}\\mathbf{L} \\mathbf{z}_i,\\] where \\(\\mathbf{z}_i = (z_{i0}, z_{i1})\\) and \\(z_{ij} \\stackrel{iid}{\\sim} N(0,1)\\), and \\(v_i \\stackrel{iid}{\\sim} \\text{Inv-}\\chi^2_{\\nu}\\). Place the following prior on \\(\\nu \\sim \\text{Gamma}(2,0.1)\\).",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercise-7",
    "href": "hw/hw-04.html#exercise-7",
    "title": "HW 04: Hierarchical models",
    "section": "Exercise 7",
    "text": "Exercise 7\nFit the same model as in Exercise 1, but replace the multivariate Gaussian prior with a multivariate Student-t prior. Assess model convergence and perform a posterior predictive check. Create a scatter plot of the posterior mean eye-specific slopes from the Gaussian and Student-t models. Describe the relationship?",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercise-8",
    "href": "hw/hw-04.html#exercise-8",
    "title": "HW 04: Hierarchical models",
    "section": "Exercise 8",
    "text": "Exercise 8\nExamine the posterior distribution of \\(\\nu\\). What does the posterior distribution of \\(\\nu\\) tell us about the appropriateness of the Student-t model, as compared to the Gaussian model?",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercise-9",
    "href": "hw/hw-04.html#exercise-9",
    "title": "HW 04: Hierarchical models",
    "section": "Exercise 9",
    "text": "Exercise 9\nCompare the Gaussian and Student-t models using LOO-IC. Which model is preferred? Is this consistent with your answer from Exercise 8?",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-04.html#exercise-4",
    "href": "hw/hw-04.html#exercise-4",
    "title": "HW 04: Hierarchical models",
    "section": "Exercise 4",
    "text": "Exercise 4\nExplore the correlation between eye-specific intercepts (\\(\\beta_{0i}\\)) and slopes (\\(\\beta_{1i}\\)). Create a scatter plot to examine their relationship. Does their appear to be a relationship? To help answer this question also create examine the posterior distribution for \\(\\rho\\) and comment on how it relates to the scatter plot.",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "slides/17-gp.html#recalling-what-went-before-the-break",
    "href": "slides/17-gp.html#recalling-what-went-before-the-break",
    "title": "Gaussian Processes",
    "section": "Recalling what went before the break",
    "text": "Recalling what went before the break"
  },
  {
    "objectID": "slides/17-gp.html#a-brief-agenda",
    "href": "slides/17-gp.html#a-brief-agenda",
    "title": "Gaussian Processes",
    "section": "A brief agenda",
    "text": "A brief agenda\nBy the end of this lecture you should:\n\nUnderstand the basic concepts of Gaussian processes (GP) and their usefulness in statistical modeling.\nCompute posterior and predictive inference for a simple GP model.\nUse GP as a building block in modeling time-correlated data."
  },
  {
    "objectID": "slides/17-gp.html#motivation",
    "href": "slides/17-gp.html#motivation",
    "title": "Gaussian Processes",
    "section": "Motivation",
    "text": "Motivation\nLet \\(i\\) index distinct eyes (\\(i=1,2,\\ldots,n\\)) and \\(t\\) index within-eye data in time (\\(t=1,2,\\ldots,n_i\\)).\nSuppose we are interested in effects of time-invariant predictors (age and iop):\n\\[Y_{it} = \\underbrace{\\mathbf{x}_i\\boldsymbol{\\beta}}_{\\text{Fixed in time}} + \\underbrace{\\eta_{it}}_{\\text{Varying in time}} + \\epsilon_{it}\\]\nIn a previous lecture, we included only time (\\(X_{it}\\)) and fit a random slope regression.\n\\[\\eta_{it} = \\theta_{0i} + \\underbrace{X_{it}}_{=Time}\\theta_{1i}.\\]\nWhat if we want a nonlinear model for time effects?"
  },
  {
    "objectID": "slides/17-gp.html#exploratory-analysis-of-population-predictors",
    "href": "slides/17-gp.html#exploratory-analysis-of-population-predictors",
    "title": "Gaussian Processes",
    "section": "Exploratory Analysis of Population Predictors",
    "text": "Exploratory Analysis of Population Predictors"
  },
  {
    "objectID": "slides/17-gp.html#exploratory-analysis-of-population-predictors-1",
    "href": "slides/17-gp.html#exploratory-analysis-of-population-predictors-1",
    "title": "Gaussian Processes",
    "section": "Exploratory Analysis of Population Predictors",
    "text": "Exploratory Analysis of Population Predictors"
  },
  {
    "objectID": "slides/17-gp.html#a-review-of-computation-linear-algebra-needed-to-sample",
    "href": "slides/17-gp.html#a-review-of-computation-linear-algebra-needed-to-sample",
    "title": "Gaussian Processes",
    "section": "A review of computation / linear algebra needed to sample",
    "text": "A review of computation / linear algebra needed to sample\nHow can we sample a 3D normal random vector?\n\np &lt;- 3\nmu &lt;- c(3, -1, 5)\nSigma &lt;- matrix(\n  c(\n    1.0, 0.6, 0.3,\n    0.6, 1.0, 0.6,\n    0.3, 0.6, 1.0\n  ),\n  nrow = 3, ncol = 3\n)\nz &lt;- rnorm(p)\nR &lt;- t(chol(Sigma)) ## R %*% t(R) == Sigma\nx &lt;- R %*% z\n\n# This is the same as\nx &lt;- rmvnorm(1, mu, Sigma)\n\nFrom previous lecture: R decomposes into D (diagonal) and L (Cholesky factor of a correlation matrix)."
  },
  {
    "objectID": "slides/17-gp.html#constructing-a-gaussian-random-function-in-time",
    "href": "slides/17-gp.html#constructing-a-gaussian-random-function-in-time",
    "title": "Gaussian Processes",
    "section": "Constructing a Gaussian Random Function in Time",
    "text": "Constructing a Gaussian Random Function in Time\nWhat about “random functions”? Can we extend this idea to arbitrarily many time points? What does that even mean?\n\nWe start at time \\(t=0\\) from \\(X_0 = 0\\).\nWhen time passes by amount of \\(h &gt; 0\\), we want\n\n\\[X_{t+h} - X_t \\sim N(0,h).\\] Note: \\(\\mathbb{E}[X_t^2] = \\mathbb{V}(X_t) = t\\) (Why?)\n\nEach increment will be independent from each other.\n\nNow think of passing to the limit: Somehow it works (!) and we have a process defined at every \\(t\\)."
  },
  {
    "objectID": "slides/17-gp.html#visualizing-the-brownian-motion",
    "href": "slides/17-gp.html#visualizing-the-brownian-motion",
    "title": "Gaussian Processes",
    "section": "Visualizing the Brownian Motion",
    "text": "Visualizing the Brownian Motion"
  },
  {
    "objectID": "slides/17-gp.html#what-if-i-want-different-correlation-structures",
    "href": "slides/17-gp.html#what-if-i-want-different-correlation-structures",
    "title": "Gaussian Processes",
    "section": "What if I want different correlation structures?",
    "text": "What if I want different correlation structures?\nFor longitudinal data,\n\nWe are not interested in an indefinitely long time span.\nWithin the time window, the data can exhibit stable behavior.\n\n\nHypothesis 1 : The marginal variability of \\(X_t\\) should stay the same.\nHypothesis 2 : Time correlation should decay in, and only depend on, the amount of time elapsed.\nHypothesis 3 : We have expectations about the span of correlation and smoothness of the process.\n\nThese are all a priori hypotheses. The data may come from something very different!"
  },
  {
    "objectID": "slides/17-gp.html#stationary-gaussian-processes",
    "href": "slides/17-gp.html#stationary-gaussian-processes",
    "title": "Gaussian Processes",
    "section": "Stationary Gaussian processes",
    "text": "Stationary Gaussian processes\nA different process results from a stationary kernel:\n\\[\n\\mathbb{C}(X_t, X_s) = C(|t-s|).\n\\]\nWe want \\(C(0) = \\sigma^2 &gt; 0\\) and \\(C\\to 0\\) as \\(h = |t-s|\\to\\infty\\). Some common choices:\n\nExponential kernel: \\(C(h) = \\sigma^2\\exp(-h/\\rho)\\)\nSquare exponential kernel: \\(C(h) = \\sigma^2\\exp\\{-h^2/(2\\rho^2)\\}\\)\nMatérn kernels"
  },
  {
    "objectID": "slides/17-gp.html#interpreting-hyperparameters-and-kernels",
    "href": "slides/17-gp.html#interpreting-hyperparameters-and-kernels",
    "title": "Gaussian Processes",
    "section": "Interpreting Hyperparameters and Kernels",
    "text": "Interpreting Hyperparameters and Kernels\nOften, we fix the overall kernel function while learning its “bandwidth” \\(\\rho\\) due to poor identifiability from the data."
  },
  {
    "objectID": "slides/17-gp.html#default-prior-choices",
    "href": "slides/17-gp.html#default-prior-choices",
    "title": "Gaussian Processes",
    "section": "Default Prior Choices",
    "text": "Default Prior Choices\n\nStan team often recommends \\(\\rho^{-1} \\sim Gamma(5,5)\\)\nA priori the prior is concentrated around 1. For a prior mean, distance increase of 1 corrresponds to a multiplicative decay of correlation by \\(e^{-1}\\sim 37\\%\\).\nBeware of the units! A year correlation is very different from that over a day."
  },
  {
    "objectID": "slides/17-gp.html#prior-sampling-in-stan",
    "href": "slides/17-gp.html#prior-sampling-in-stan",
    "title": "Gaussian Processes",
    "section": "Prior Sampling in Stan",
    "text": "Prior Sampling in Stan\nOnce we have a GP model, sampling from the prior is equivalent to sampling from a multivariate normal.\n\ndata {\n  int&lt;lower=1&gt; N;\n  array[N] real x;\n  real&lt;lower=0&gt; sigma;\n  real&lt;lower=0&gt; l;\n}\ntransformed data {\n  // 0. A \"small nugget\" to stabilize matrix root computation\n  // This is for.better numerical stability in taking large matrix roots\n  real delta = 1e-9;\n\n  // 1. Compute the squared exponential kernel matrix\n  // x is the time variable\n  vector[N] mu = rep_vector(0, N);\n  matrix[N, N] R_C;\n  matrix[N, N] C = gp_exp_quad_cov(x, sigma, l);\n  for (i in 1:N) {\n    C[i, i] = C[i, i] + delta;\n  }\n\n  // 2. Compute the root of C by Cholesky decomposition\n  R_C = cholesky_decompose(C);\n}\ngenerated quantities {\n  // 3. Sample from the prior: multivariate_normal(0, C)\n  f ~ multi_normal_cholesky(mu, R_C)\n}"
  },
  {
    "objectID": "slides/17-gp.html#posterior-inference",
    "href": "slides/17-gp.html#posterior-inference",
    "title": "Gaussian Processes",
    "section": "Posterior inference",
    "text": "Posterior inference\nFor each \\(i\\)-th eye: time effect vector is given an independent prior based on the GP model.\n\\[\\boldsymbol{\\eta}_i = (\\eta_{i1},\\ldots,\\eta_{in_i})^\\top \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0},\\mathbf{C}_i)\\]\nEach \\(\\eta_{it}\\) is a pointwise value of the process \\(\\eta_i(t) = \\eta_{it}\\).\n\\[\\mathbf{C}_i = \\begin{bmatrix}\nC(0) & C(|t_{i1}-t_{i2}|) & \\cdots & C(|t_{i1} - t_{in_i}|)\\\\\nC(|t_{i1} - t_{i2}|) & C(0) & \\cdots & C(|t_{i,2} - t_{in_i}|)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\n\\end{bmatrix}\\]\nRemember: Different eyes need not have the same number of encounters, nor need they have measurements at the same time."
  },
  {
    "objectID": "slides/17-gp.html#programming-in-stan-for-a-single-patient",
    "href": "slides/17-gp.html#programming-in-stan-for-a-single-patient",
    "title": "Gaussian Processes",
    "section": "Programming in Stan for a single patient",
    "text": "Programming in Stan for a single patient\n\ndata {\n  int&lt;lower = 1&gt; n_i;\n  array[n_i] real Time;\n  vector[n_i] Y;\n}\ntransformed data {\n  // For numerical reasons\n  real delta = 1e-9;\n  \n  // A vector of zeros\n  vector[n_i] mu = rep_vector(0, n_i);\n}\nparameters {\n  // GP hyperparameters\n  vector[n_i] z;\n  real&lt;lower = 0&gt; rho;\n  real&lt;lower = 0&gt; kappa;\n  real&lt;lower = 0&gt; sigma;\n}\ntransformed parameters {\n  // Computes the necessary parameters for GP model\n  vector[N] f;\n  matrix[N, N] L_K;\n  matrix[N, N] K = gp_exp_quad_cov(x, kappa, rho);\n\n  // diagonal elements\n  for (n in 1:N) {\n    K[n, n] = K[n, n] + delta;\n  }\n\n  L_K = cholesky_decompose(K);\n  f = L_K * z;\n}\nmodel {\n  target += normal_lpdf(Y | f, sigma);\n\n  rho ~ inv_gamma(5, 5);\n  alpha ~ std_normal();\n  sigma ~ std_normal();\n\n}"
  },
  {
    "objectID": "slides/17-gp.html#conditional-gp-model",
    "href": "slides/17-gp.html#conditional-gp-model",
    "title": "Gaussian Processes",
    "section": "Conditional GP Model",
    "text": "Conditional GP Model\n\ndata {\n  int&lt;lower=0&gt; N;       // total number of observations\n  int&lt;lower=1&gt; n;       // number of eyes\n  int&lt;lower=1&gt; p;       // fixed effects dimension\n  vector[N] Y;          // observation\n  matrix[n, p] X;        // fixed effects predictors\n  array[N] real t;      // obs time points\n  array[n] int s;       // sizes of within-pt obs\n}\ntransformed data {\n  real delta = 1e-9;\n}\nparameters {\n  // Fixed effects model\n  vector[p] beta;\n  real&lt;lower=0&gt; sigma;\n  // GP parameters\n  vector[N] z;\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; rho;\n}\ntransformed parameters {\n  vector[n] mu = X * beta;\n}\nmodel {\n  beta ~ normal(0,3);\n  z ~ std_normal();\n  alpha ~ std_normal();\n  sigma ~ std_normal();\n  rho ~ inv_gamma(5,5);\n\n  vector[N] mu_rep;\n  vector[N] eta;\n  \n  int pos;\n  pos = 1;\n  // Ragged loop computing the mean for each time obs\n  for (i in 1:n) {\n    // GP covariance for the k-th eye\n    int n_i = s[i];\n    int pos_end = pos + n_i - 1;\n    matrix[n_i, n_i] R_C;\n    matrix[n_i, n_i] C = gp_exp_quad_cov(segment(t, pos, n_i), alpha, rho);\n    for (j in 1:n_i) {\n      // adding a small term to the diagonal entries\n      C[j, j] = C[j, j] + delta;\n    }\n    R_C = cholesky_decompose(C);\n    \n    // Mean of data at each time\n    mu_rep[pos:pos_end] = rep_vector(mu[i], n_i);\n    // GP for the i-th eye\n    eta[pos:pos_end] = R_C * segment(z, pos, n_i);\n    pos = pos_end + 1;\n  }\n  // Normal observation model centered at mu_rep + eta\n  Y ~ normal(mu_rep + eta, sigma);\n}"
  },
  {
    "objectID": "slides/17-gp.html#fitting-the-model",
    "href": "slides/17-gp.html#fitting-the-model",
    "title": "Gaussian Processes",
    "section": "Fitting the Model",
    "text": "Fitting the Model\nSome data preparation code for handling the ragged structure.\n\n# Group predictors fixed across time\nfixed_df &lt;- dataset %&gt;% \n  group_by(eye_id) %&gt;% \n  reframe(age = unique(age), iop = unique(iop))\nXmat &lt;- model.matrix(~age + iop, data = fixed_df)\n# Number of measurements for each eye\ngroupsizes &lt;- dataset %&gt;% \n  group_by(eye_id) %&gt;% \n  summarise(n = n()) %&gt;% \n  pull(n)\nstan_data &lt;- list(\n  N = dim(dataset)[1],\n  Np = 21,\n  K = max(dataset$eye_id),\n  D = dim(Xmat)[2],\n  y = dataset$mean_deviation,\n  t = dataset$time,\n  tp = seq(0, 5, by = 0.25),\n  s = groupsizes,\n  x = Xmat\n)"
  },
  {
    "objectID": "slides/17-gp.html#assessing-convergence",
    "href": "slides/17-gp.html#assessing-convergence",
    "title": "Gaussian Processes",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(gp_condl_fit, pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))"
  },
  {
    "objectID": "slides/17-gp.html#assessing-convergence-1",
    "href": "slides/17-gp.html#assessing-convergence-1",
    "title": "Gaussian Processes",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nbayesplot::mcmc_acf(gp_condl_fit, regex_pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))"
  },
  {
    "objectID": "slides/17-gp.html#fitting-a-marginal-model",
    "href": "slides/17-gp.html#fitting-a-marginal-model",
    "title": "Gaussian Processes",
    "section": "Fitting a Marginal Model",
    "text": "Fitting a Marginal Model\nOnly the model segment has to be changed.\n\nmodel {\n  beta ~ normal(0,3);\n  alpha ~ std_normal();\n  sigma ~ std_normal();\n  rho ~ inv_gamma(5,5);\n\n  int pos;\n  pos = 1;\n  // Ragged loop computing joint likelihood for each eye\n  for (i in 1:n) {\n    // GP covariance for the k-th eye\n    int n_i = s[i];\n    vector[n_i] mu_rep;\n    \n    matrix[n_i, n_i] R_C;\n    matrix[n_i, n_i] C = gp_exp_quad_cov(segment(t, pos, n_i), alpha, rho);\n    for (j in 1:n_i) {\n      // Add noise variance to the diagonal entries\n      C[j, j] = C[j, j] + square(sigma);\n    }\n    R_C = cholesky_decompose(C);\n    // Marginal model for the i-th eye\n    mu_rep = rep_vector(mu[i], n_i);\n    target += multi_normal_cholesky_lpdf(to_vector(segment(Y, pos, n_i)) | mu_rep, R_C);\n    pos = pos + n_i;\n  }\n}"
  },
  {
    "objectID": "slides/17-gp.html#assessing-convergence-2",
    "href": "slides/17-gp.html#assessing-convergence-2",
    "title": "Gaussian Processes",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\ntraceplot(gp_marginal_fit, pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))"
  },
  {
    "objectID": "slides/17-gp.html#assessing-convergence-3",
    "href": "slides/17-gp.html#assessing-convergence-3",
    "title": "Gaussian Processes",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\n\nbayesplot::mcmc_acf(gp_marginal_fit, regex_pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))"
  },
  {
    "objectID": "slides/17-gp.html#comparison-against-ols-estimates",
    "href": "slides/17-gp.html#comparison-against-ols-estimates",
    "title": "Gaussian Processes",
    "section": "Comparison against OLS estimates",
    "text": "Comparison against OLS estimates"
  },
  {
    "objectID": "slides/17-gp.html#learned-gp-hyperparameter",
    "href": "slides/17-gp.html#learned-gp-hyperparameter",
    "title": "Gaussian Processes",
    "section": "Learned GP hyperparameter",
    "text": "Learned GP hyperparameter\n\nThe posterior mean of \\(\\rho\\) (in year) is very large: the pattern of mean deviation change is very gradual over time.\nA posteriori we believe mean deviations of an eye, 10 years apart and adjusted for age and iop, are still strongly correlated (\\(e^{-5/6}\\sim 43\\%\\))."
  },
  {
    "objectID": "slides/17-gp.html#predictive-inference-using-some-math",
    "href": "slides/17-gp.html#predictive-inference-using-some-math",
    "title": "Gaussian Processes",
    "section": "Predictive Inference using Some Math",
    "text": "Predictive Inference using Some Math\nThe model is continuous in nature and defined, in principle, for all times. This is very convenient for visualiztion, interpolation, and forecasting.\nIn a vector notation, the observed data for the \\(i\\)-th patient have the model:\n\\[\n\\mathbf{Y}_i = \\underbrace{\\mathbf{f}_i}_{ = \\mathbf{x}_i^T\\boldsymbol{\\beta} + \\boldsymbol{\\eta}_i } + \\boldsymbol{\\epsilon}_i\n\\]\nThe vector \\(\\mathbf{f}_i\\) may be interprted as the denoised latent process of one’s mean deviation over time.\n\\[\n\\mathbb{E}[f_{i,t}] = \\mathbf{x}_i^T\\boldsymbol{\\beta},\\; Cov[f_{i,t}, f_{i,t'}] = Cov[\\eta_{i,t},\\eta_{i,t'}]\n\\] The values of \\(f_i\\) at new time points \\(t^{new}\\) can be predicted based on an analytical formula for conditioning normal variables.\n\\[\n\\mathbf{f}^{new}|\\mathbf{f} \\sim Normal\\left(C\\Sigma^{-1}(\\mathbf{f} - \\boldsymbol{\\mu}),\\Sigma^{new}-C\\Sigma^{-1}C^T\\right)\n\\]\n\n\\(\\boldsymbol{\\mu} = \\mathbb{E}[\\mathbf{f}]\\)\n\\(\\Sigma = Cov[\\mathbf{f}]\\)\n\\(C = Cov[\\mathbf{f}^{new}, \\mathbf{f}]\\)"
  },
  {
    "objectID": "slides/17-gp.html#implementation-in-stan",
    "href": "slides/17-gp.html#implementation-in-stan",
    "title": "Gaussian Processes",
    "section": "Implementation in Stan",
    "text": "Implementation in Stan\nAn analytical formula can be implemented as a function. See the Stan Help page\n\nfunctions {\n  // Analytical formula for latent GP conditional on Gaussian observations\n  vector gp_pred_rng(array[] real x2,\n                     vector y1,\n                     array[] real x1,\n                     real ymean,\n                     real alpha,\n                     real rho,\n                     real sigma,\n                     real delta) {\n    int N1 = rows(y1);\n    int N2 = size(x2);\n    vector[N2] f2;\n    {\n      matrix[N1, N1] L_K;\n      vector[N1] K_div_y1;\n      matrix[N1, N2] k_x1_x2;\n      matrix[N1, N2] v_pred;\n      vector[N2] f2_mu;\n      matrix[N2, N2] cov_f2;\n      matrix[N2, N2] diag_delta;\n      matrix[N1, N1] K;\n      K = gp_exp_quad_cov(x1, alpha, rho);\n      for (n in 1:N1) {\n        K[n, n] = K[n, n] + square(sigma);\n      }\n      L_K = cholesky_decompose(K);\n      K_div_y1 = mdivide_left_tri_low(L_K, y1 - ymean);\n      K_div_y1 = mdivide_right_tri_low(K_div_y1', L_K)';\n      k_x1_x2 = gp_exp_quad_cov(x1, x2, alpha, rho);\n      f2_mu = (k_x1_x2' * K_div_y1);\n      v_pred = mdivide_left_tri_low(L_K, k_x1_x2);\n      cov_f2 = gp_exp_quad_cov(x2, alpha, rho) - v_pred' * v_pred;\n      diag_delta = diag_matrix(rep_vector(delta, N2));\n\n      f2 = multi_normal_rng(f2_mu, cov_f2 + diag_delta);\n    }\n    return f2;\n  }\n}\n//...\ngenerated quantities {\n  matrix[K,Np] fp;\n  matrix[Np,Np] Cp;\n  Cp = gp_exp_quad_cov(tp, alpha, rho);\n  // Posterior predictive on fixed time grid for all patients\n  int pos;\n  pos = 1;\n  for (k in 1:K) {\n    int Nk = s[k];\n    fp[k,] = mu[k] + \n      gp_pred_rng(\n          tp,\n          segment(y, pos, Nk),\n          segment(t, pos, Nk),\n          mu[k], \n          alpha,\n          rho,\n          sigma,\n          delta\n        )';\n    pos = pos + Nk;\n  }\n}"
  },
  {
    "objectID": "slides/17-gp.html#visualizing-latent-effects-using-predictive-formulae",
    "href": "slides/17-gp.html#visualizing-latent-effects-using-predictive-formulae",
    "title": "Gaussian Processes",
    "section": "Visualizing latent effects using predictive formulae",
    "text": "Visualizing latent effects using predictive formulae"
  },
  {
    "objectID": "slides/17-gp.html#random-effects-covariance-structure",
    "href": "slides/17-gp.html#random-effects-covariance-structure",
    "title": "Gaussian Processes",
    "section": "Random effects covariance structure",
    "text": "Random effects covariance structure"
  },
  {
    "objectID": "slides/17-gp.html#stan-speed-concerns",
    "href": "slides/17-gp.html#stan-speed-concerns",
    "title": "Gaussian Processes",
    "section": "Stan speed concerns",
    "text": "Stan speed concerns\n\nGP is notorious for not being scalable. Tl;dr is the need for matrix root / inverse computation.\nFor datasets covered in this class, Stan works well. For research purposes, worth exploring other specialized toolkits.\n\n\nCode MCMC yourself to make it faster (e.g., using Rcpp)\nDo a bit of software research on Wikipedia\nScalable Approximations (some of them will be coming soon!)"
  },
  {
    "objectID": "slides/17-gp.html#a-summary",
    "href": "slides/17-gp.html#a-summary",
    "title": "Gaussian Processes",
    "section": "A summary",
    "text": "A summary\n\nGP is a flexible, high-dimensional model for handling correlated measurements over time.\nWith some basic knowledge about conditioning Gaussian random variables, we can implement posterior computation and prediction / interpolation.\nProgramming in Stan is straightforward but can be expensive with large number of observations."
  },
  {
    "objectID": "slides/17-gp.html#prepare-for-next-class",
    "href": "slides/17-gp.html#prepare-for-next-class",
    "title": "Gaussian Processes",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 04 which is due March 25\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Using GP to model spatial data"
  },
  {
    "objectID": "slides/17-gp.html#visualizing-the-brownian-motion-1",
    "href": "slides/17-gp.html#visualizing-the-brownian-motion-1",
    "title": "Gaussian Processes",
    "section": "Visualizing the Brownian Motion",
    "text": "Visualizing the Brownian Motion\nSuppose we observe this process \\(X_t\\) at time points \\((t_1,t_2,\\ldots,t_N)\\)."
  },
  {
    "objectID": "slides/17-gp.html#gaussian-process-covariance-functions-in-stan",
    "href": "slides/17-gp.html#gaussian-process-covariance-functions-in-stan",
    "title": "Gaussian Processes",
    "section": "Gaussian Process Covariance Functions in Stan",
    "text": "Gaussian Process Covariance Functions in Stan\nA list of available kernels is available here.\nThe squared exponential kernel (exponentiated quadratic kernel) is given by:\n\\[C(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma^2 \\exp\\left(-\\frac{|\\mathbf{x}_i - \\mathbf{x}_j|^2}{2l^2}\\right)\\]\n\nmatrix gp_exp_quad_cov(array[] real x, real sigma, real length_scale)\n\nFor us, \\(\\mathbf{x}\\) is 1D (time)…but it does not have to be!"
  },
  {
    "objectID": "slides/17-gp.html#the-full-model",
    "href": "slides/17-gp.html#the-full-model",
    "title": "Gaussian Processes",
    "section": "The full model",
    "text": "The full model\nFor \\(i\\) (\\(i = 1,\\ldots,n\\)) and \\(t\\) (\\(t = 1,\\ldots,n_i\\)),\n\\[\\begin{align*}\nY_{it} &= \\mathbf{x}_{i}\\boldsymbol{\\beta} + \\eta_{i}(t) + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2),\\\\\n\\boldsymbol{\\eta}_i &\\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0},\\mathbf{C}_i),\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}).\n\\end{align*}\\]\n\n\\(\\boldsymbol{\\Omega}\\) represents the population parameters: \\(\\boldsymbol{\\beta}\\), \\(\\sigma^2\\), and any kernel parameters determining \\(\\mathbf{C}_i\\)\nFor concreteness: we use a squared exponential kernel \\(C\\) and model \\(\\mathbb{C}(\\eta_i(t),\\eta_i(t')) = \\alpha^2\\exp(-|t-t'|^2/2\\rho^2).\\) Thus \\(\\boldsymbol{\\Omega} = (\\boldsymbol{\\beta},\\sigma^2,\\alpha,\\rho)\\)."
  },
  {
    "objectID": "slides/17-gp.html#pointwise-observations-from-a-brownian-motion",
    "href": "slides/17-gp.html#pointwise-observations-from-a-brownian-motion",
    "title": "Gaussian Processes",
    "section": "Pointwise Observations from a Brownian Motion",
    "text": "Pointwise Observations from a Brownian Motion\nSuppose we observe this process \\(X_t\\) at time points \\((t_1,t_2,\\ldots,t_N)\\). Then the distribution of the following vector is a multivariate normal:\n\\[\n\\begin{bmatrix}\nX_1\\\\ \\vdots \\\\ X_N\n\\end{bmatrix}\\sim N\\left(\\mathbf{0},\\boldsymbol{\\Sigma}\\right),\\; \\boldsymbol{\\Sigma} = \\begin{bmatrix}\nt_1 & t_1 & \\cdots & t_1 \\\\\nt_1 & t_2 & \\cdots & t_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nt_1 & t_2 & \\cdots & t_N\n\\end{bmatrix}\n\\]\nThe covariance structure comes from solving the equation:\n\\[\nh = \\mathbb{V}(X_{t+h} - X_t) = \\underbrace{\\mathbb{E}[X_{t+h}^2]}_{=t+h} + \\underbrace{\\mathbb{E}[X_t^2]}_{=t} - 2\\mathbb{C}(X_{t+h},X_t).\n\\]"
  },
  {
    "objectID": "slides/17-gp.html#visualizing-the-process",
    "href": "slides/17-gp.html#visualizing-the-process",
    "title": "Gaussian Processes",
    "section": "Visualizing the process",
    "text": "Visualizing the process"
  },
  {
    "objectID": "slides/17-gp.html#first-look-at-the-data",
    "href": "slides/17-gp.html#first-look-at-the-data",
    "title": "Gaussian Processes",
    "section": "First Look at the Data",
    "text": "First Look at the Data\n\nhead(dataset)\n\n  pat_id eye_id mean_deviation      time      age      iop\n1      1      1          -7.69 0.0000000 51.55616 10.87303\n2      1      1          -9.95 0.5753425 51.55616 10.87303\n3      1      1          -9.58 1.0547945 51.55616 10.87303\n4      1      1          -9.53 1.5726027 51.55616 10.87303\n5      1      1          -9.18 2.0136986 51.55616 10.87303\n6      1      1          -9.63 2.5671233 51.55616 10.87303"
  },
  {
    "objectID": "slides/17-gp.html#preparing-the-data",
    "href": "slides/17-gp.html#preparing-the-data",
    "title": "Gaussian Processes",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nA few data processing steps are needed to handle the data structure with more ease.\n\n\\(N\\) will denote the number of all observations: \\(N = \\sum_{i=1}^{n} n_i\\).\n\\(n\\) will denote the total number of eyes.\n\\(p\\) will denote the number of predictors fixed across time (\\(p=3\\): intercept, slopes for age and iop).\nA separate vector of the number of observations (\\(n_i\\)) for each eye will be stored as s."
  },
  {
    "objectID": "slides/17-gp.html#preparing-the-data-1",
    "href": "slides/17-gp.html#preparing-the-data-1",
    "title": "Gaussian Processes",
    "section": "Preparing the Data",
    "text": "Preparing the Data\n\nlibrary(dplyr)\n\n# Group predictors fixed across time\nfixed_df &lt;- dataset %&gt;% \n  group_by(eye_id) %&gt;% \n  reframe(age = unique(age), iop = unique(iop))\nXmat &lt;- model.matrix(~age + iop, data = fixed_df)\n\n# Number of measurements for each eye\ngroupsizes &lt;- dataset %&gt;% \n  group_by(eye_id) %&gt;% \n  summarise(n = n()) %&gt;% \n  pull(n)\n\nstan_data &lt;- list(\n  N = dim(dataset)[1],\n  n = max(dataset$eye_id),\n  p = dim(Xmat)[2],\n  t = dataset$time,\n  Y = dataset$mean_deviation,\n  s = groupsizes,\n  X = Xmat\n)"
  },
  {
    "objectID": "slides/17-gp.html#marginal-model-specification",
    "href": "slides/17-gp.html#marginal-model-specification",
    "title": "Gaussian Processes",
    "section": "Marginal Model Specification",
    "text": "Marginal Model Specification\nThe conditional model is pretty slow (can be improved) and results in poor mixing (can be run longer). Marginalizing the random effect can stabilize the computation.\nIn vector notation, our observation model for the \\(i\\)-th eye is\n\\[\n\\mathbf{Y}_i = \\begin{bmatrix} Y_{i1}\\\\ \\vdots \\\\ Y_{in_i} \\end{bmatrix}\n= \\mathbf{x}_i\\boldsymbol{\\beta} + \\boldsymbol{\\eta}_i + \\boldsymbol{\\epsilon}_i\n\\] By marginalizing out \\(\\boldsymbol{\\eta}_i\\), we obtain\n\\[\n\\mathbf{Y}_i \\sim N_{n_i}(\\mathbf{x}_i\\boldsymbol{\\beta},\\mathbf{C}_i + \\sigma^2\\mathbf{I}_{n_i})\n\\]"
  },
  {
    "objectID": "slides/17-gp.html#estimates-of-patient-level-predictors",
    "href": "slides/17-gp.html#estimates-of-patient-level-predictors",
    "title": "Gaussian Processes",
    "section": "Estimates of Patient-level Predictors",
    "text": "Estimates of Patient-level Predictors"
  },
  {
    "objectID": "slides/17-gp.html#predictive-inference",
    "href": "slides/17-gp.html#predictive-inference",
    "title": "Gaussian Processes",
    "section": "Predictive inference",
    "text": "Predictive inference\nThe model is continuous in nature and defined, in principle, for all times. This is very convenient for visualiztion, interpolation, and forecasting.\n\\[\n\\mathbf{Y}_i = \\underbrace{\\mathbf{f}_{i}}_{ = \\mathbf{x}_{i}\\boldsymbol{\\beta} + \\boldsymbol{\\eta}_{i} } + \\boldsymbol{\\epsilon}_i\n\\]\nThe vector \\(\\mathbf{f}_{i}\\) may be interpreted as the denoised latent process of an eye-specific mean deviation over time: \\((f_{i1},\\ldots,f_{in_i})^\\top\\).\n\\[\n\\mathbb{E}[f_{it}] = \\mathbf{x}_i\\boldsymbol{\\beta},\\; \\mathbb{C}(f_{it}, f_{it'}) = \\mathbb{C}(\\eta_{it},\\eta_{it'})\n\\]"
  },
  {
    "objectID": "slides/17-gp.html#predictive-inference-using-normal-conditioning",
    "href": "slides/17-gp.html#predictive-inference-using-normal-conditioning",
    "title": "Gaussian Processes",
    "section": "Predictive Inference using Normal Conditioning",
    "text": "Predictive Inference using Normal Conditioning\nSay \\(\\mathbf{f}^{pred}_i\\) is the values of \\(f_i\\) at \\(m_i\\) new time points which we want to predict, based on our GP model:\n\\[\\begin{align*}\n\\mathbf{f}^{pred}_i &\\sim N_{m_i}(\\mathbf{x}_i\\boldsymbol{\\beta},\\mathbf{C}^{pred}_i)\\\\\n\\mathbf{Y}_i = \\mathbf{f}_i + \\boldsymbol{\\epsilon}_i &\\sim N_{n_1}(\\mathbf{x}_i\\boldsymbol{\\beta},\\mathbf{C}_i + \\sigma^2\\mathbf{I}_{n_i})\n\\end{align*}\\]\nAn analytical formula exists for the conditional distribution of \\(\\mathbf{f}^{pred}\\) given observed \\(Y_{it}\\) at \\(n_i\\) time points.\n\\[\n\\mathbf{f}^{pred}_i | \\mathbf{Y}_i \\sim N\\left(\\mathbf{x}_i\\boldsymbol{\\beta} + \\mathbf{k}_i(\\mathbf{C}_i + \\sigma^2\\mathbf{I})^{-1}(\\mathbf{Y}_i - \\mathbf{x}_i\\boldsymbol{\\beta}), \\mathbf{C}_i^{pred}-\\mathbf{k}_i(\\mathbf{C}_i + \\sigma^2\\mathbf{I})^{-1}\\mathbf{k}_i^\\top\\right)\n\\] where \\(\\mathbf{k}_i = \\mathbb{C}(\\mathbf{f}^{pred}_i, \\mathbf{f}_i)\\) (“cross-covariances”).\nThis stems from a more general fact about conditional distributions of jointly normal vectors: e.g., \\(\\mathbf{f}_i^{pred}\\) and \\(\\mathbf{f}_i\\) need not have the same mean."
  },
  {
    "objectID": "slides/17-gp.html#new-data-for-predictive-inference",
    "href": "slides/17-gp.html#new-data-for-predictive-inference",
    "title": "Gaussian Processes",
    "section": "New Data for Predictive Inference",
    "text": "New Data for Predictive Inference\nSuppose we are interested in understanding the mean deviation trend for the first 5 years from baseline (\\(t^{pred}\\in [0,5]\\)).\n\nstan_data &lt;- list(\n  N = dim(dataset)[1],\n  n = max(dataset$eye_id),\n  p = dim(Xmat)[2],\n  Y = dataset$mean_deviation,\n  t = dataset$time,\n  s = groupsizes,\n  X = Xmat,\n  # Time window for which we want predictive\n  t_pred = seq(0, 5, by = 0.25),\n  # Total length of this window\n  N_pred = 21\n)"
  },
  {
    "objectID": "slides/17-gp.html#stan-implementation",
    "href": "slides/17-gp.html#stan-implementation",
    "title": "Gaussian Processes",
    "section": "Stan Implementation",
    "text": "Stan Implementation\nSee the Stan Help page for details.\n\nfunctions {\n  // Analytical formula for latent GP conditional on Gaussian observations\n  vector gp_pred_rng(array[] real x_pred,\n                     vector Y,\n                     array[] real x,\n                     real mu,\n                     real alpha,\n                     real rho,\n                     real sigma,\n                     real delta) {\n    int N1 = rows(Y);\n    int N2 = size(x_pred);\n    vector[N2] f_pred;\n    {\n      matrix[N1, N1] L_Sigma;\n      vector[N1] Sigma_div_y;\n      matrix[N1, N2] C_x_xpred;\n      matrix[N1, N2] v_pred;\n      vector[N2] fpred_mu;\n      matrix[N2, N2] cov_fpred;\n      matrix[N2, N2] diag_delta;\n      matrix[N1, N1] Sigma;\n      Sigma = gp_exp_quad_cov(x, alpha, rho);\n      for (n in 1:N1) {\n        Sigma[n, n] = Sigma[n, n] + square(sigma);\n      }\n      L_Sigma = cholesky_decompose(Sigma);\n      Sigma_div_y = mdivide_left_tri_low(L_Sigma, Y - mu);\n      Sigma_div_y = mdivide_right_tri_low(Sigma_div_y', L_Sigma)';\n      C_x_xpred = gp_exp_quad_cov(x, x_pred, alpha, rho);\n      fpred_mu = (C_x_xpred' * Sigma_div_y);\n      v_pred = mdivide_left_tri_low(L_Sigma, C_x_xpred);\n      cov_fpred = gp_exp_quad_cov(x_pred, alpha, rho) - v_pred' * v_pred;\n      diag_delta = diag_matrix(rep_vector(delta, N2));\n\n      f_pred = multi_normal_rng(fpred_mu, cov_fpred + diag_delta);\n    }\n    return f_pred;\n  }\n}\n//...\ngenerated quantities {\n  matrix[n,Np] f_pred;\n  matrix[Np,Np] Cp;\n  Cp = gp_exp_quad_cov(t_pred, alpha, rho);\n  // Posterior predictive on fixed time grid for all eyes\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = s[i];\n    f_pred[i,] = mu[i] + \n      gp_pred_rng(\n          t_pred,\n          segment(Y, pos, n_i),\n          segment(t, pos, n_i),\n          mu[i], \n          alpha,\n          rho,\n          sigma,\n          delta\n        )';\n    pos = pos + n_i;\n  }\n}"
  },
  {
    "objectID": "slides/17-gp.html#model-specification",
    "href": "slides/17-gp.html#model-specification",
    "title": "Gaussian Processes",
    "section": "Model Specification",
    "text": "Model Specification\nWe want to fit a model estimating the effects of age and iop on mean deviation, adjusting for nonlinear time effects:\n\\[\\begin{align*}\nY_{it} &= \\underbrace{\\beta_0 + \\text{age}_i\\beta_1 + \\text{iop}_i\\beta_2}_{=\\mathbf{x}_{i}\\boldsymbol{\\beta}} + \\eta_{it} + \\epsilon_{it}\\\\\n\\boldsymbol{\\eta}_i &= \\begin{bmatrix} \\eta_{i1}\\\\ \\vdots\\\\ \\eta_{in_i} \\end{bmatrix} \\stackrel{iid}{\\sim} N(0,\\mathbf{C}_i)\\\\\n\\epsilon_{it} &\\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\\]\nOur model for \\(\\eta_{it}\\) is nonlinear in time: time enters the covariance structure of \\(\\mathbf{C}_i\\) producing a function \\(\\eta_i(t) = \\eta_{it}\\)."
  },
  {
    "objectID": "slides/17-gp.html#estimates-of-eye-level-predictors",
    "href": "slides/17-gp.html#estimates-of-eye-level-predictors",
    "title": "Gaussian Processes",
    "section": "Estimates of eye-level Predictors",
    "text": "Estimates of eye-level Predictors"
  },
  {
    "objectID": "slides/17-gp.html#random-effects-covariance-structure-for-the-first-5-years",
    "href": "slides/17-gp.html#random-effects-covariance-structure-for-the-first-5-years",
    "title": "Gaussian Processes",
    "section": "Random effects covariance structure for the first 5 years",
    "text": "Random effects covariance structure for the first 5 years"
  },
  {
    "objectID": "slides/17-gp.html#is-linear-fit-the-right-way",
    "href": "slides/17-gp.html#is-linear-fit-the-right-way",
    "title": "Gaussian Processes",
    "section": "Is Linear Fit the Right Way?",
    "text": "Is Linear Fit the Right Way?\nThere are various ways to fit a model to time-varying behavior of the curves."
  },
  {
    "objectID": "prepare/prepare-mar18.html",
    "href": "prepare/prepare-mar18.html",
    "title": "Prepare for March 18 lecture",
    "section": "",
    "text": "📖 Review the Longitudinal Data lecture slides.\n📖 Review BDA3 Chapter 21 to learn more about Gaussian process models.\n✅ Work on HW 04 which is due Tuesday March 25 before class."
  },
  {
    "objectID": "slides/18-geospatial.html#review-of-last-lecture",
    "href": "slides/18-geospatial.html#review-of-last-lecture",
    "title": "Geospatial Modeling",
    "section": "",
    "text": "During our last lecture, we learned about Gaussian processes.\nWe learned how to apply Gaussian processes to longitudinal (or time-series) data.\nThe longitudinal setting is one-dimensional (i.e., time). Today we will learn about applying Gaussian processes in two-dimensions (i.e. space)."
  },
  {
    "objectID": "slides/18-geospatial.html#prepare-for-next-class",
    "href": "slides/18-geospatial.html#prepare-for-next-class",
    "title": "Geospatial Modeling",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 04, which is due before class on Tuesday.\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Disease mapping"
  },
  {
    "objectID": "slides/18-geospatial.html#three-types-of-spatial-data",
    "href": "slides/18-geospatial.html#three-types-of-spatial-data",
    "title": "Geospatial Modeling",
    "section": "Three Types of Spatial Data:",
    "text": "Three Types of Spatial Data:\n\nGeostatistical Point Referenced Data\nLattice Data (Areal Data)\nSpatial Point Process Data\n\nAll of these data settings can be extended to space-time"
  },
  {
    "objectID": "slides/18-geospatial.html#geostatistical-point-referenced-data",
    "href": "slides/18-geospatial.html#geostatistical-point-referenced-data",
    "title": "Geospatial Modeling",
    "section": "Geostatistical Point Referenced Data",
    "text": "Geostatistical Point Referenced Data\n\nPoint observations of a continuously varying quantity over a region\n\nDaily Concentrations of Ozone Over NC"
  },
  {
    "objectID": "slides/18-geospatial.html#geostatistical-point-referenced-data-1",
    "href": "slides/18-geospatial.html#geostatistical-point-referenced-data-1",
    "title": "Geospatial Modeling",
    "section": "Geostatistical Point Referenced Data",
    "text": "Geostatistical Point Referenced Data\n\nDaily Concentrations of PM2.5 Over the US"
  },
  {
    "objectID": "slides/18-geospatial.html#lattice-data-areal-data",
    "href": "slides/18-geospatial.html#lattice-data-areal-data",
    "title": "Geospatial Modeling",
    "section": "Lattice Data (Areal Data)",
    "text": "Lattice Data (Areal Data)\n\nData observed at the level of an areal unit\n\nCounty Level Sudden Infant Death Syndrome Counts"
  },
  {
    "objectID": "slides/18-geospatial.html#lattice-data-areal-data-1",
    "href": "slides/18-geospatial.html#lattice-data-areal-data-1",
    "title": "Geospatial Modeling",
    "section": "Lattice Data (Areal Data)",
    "text": "Lattice Data (Areal Data)\n\nBirmingham Tract Level Poverty Levels"
  },
  {
    "objectID": "slides/18-geospatial.html#spatial-point-process-data",
    "href": "slides/18-geospatial.html#spatial-point-process-data",
    "title": "Geospatial Modeling",
    "section": "Spatial Point Process Data",
    "text": "Spatial Point Process Data\n\nAnalyzing the clustering of random locations\n\nLocations of a certain tree type in a forest\nEpicenter of earthquakes\n\nSometimes difficult to differentiate from point referenced geostatistical data (visually)"
  },
  {
    "objectID": "slides/18-geospatial.html#spatial-point-process-data-1",
    "href": "slides/18-geospatial.html#spatial-point-process-data-1",
    "title": "Geospatial Modeling",
    "section": "Spatial Point Process Data",
    "text": "Spatial Point Process Data\n\nMinneapolis Convenience Store Locations"
  },
  {
    "objectID": "slides/18-geospatial.html#spatial-data-analysis-when-why-and-how",
    "href": "slides/18-geospatial.html#spatial-data-analysis-when-why-and-how",
    "title": "Geospatial Modeling",
    "section": "Spatial Data Analysis: When, Why, and How?",
    "text": "Spatial Data Analysis: When, Why, and How?\n\nHow?:\n\nBayesian Hierarchical Modeling:\nFlexible framework to handle multiple levels of uncertainty.\n\nMarkov chain Monte Carlo (MCMC) offers computationally convenient solution to make inference.\n\nFrequentist methods also available through the EM algorithm\nOriginal frequentist methods ignore some of the uncertainty in estimating these spatial models."
  },
  {
    "objectID": "slides/18-geospatial.html#spatial-data-analysis-when-why-and-how-1",
    "href": "slides/18-geospatial.html#spatial-data-analysis-when-why-and-how-1",
    "title": "Geospatial Modeling",
    "section": "Spatial Data Analysis: When, Why, and How?",
    "text": "Spatial Data Analysis: When, Why, and How?\n\nHow?:\n\nBayesian Hierarchical Modeling:\nFlexible framework to handle multiple levels of uncertainty.\n\nMarkov chain Monte Carlo (MCMC) offers computationally convenient solution to make inference.\n\nFrequentist methods also available through the EM algorithm\nOriginal frequentist methods ignore some of the uncertainty in estimating these spatial models."
  },
  {
    "objectID": "slides/18-geospatial.html#spatial-data-analysis",
    "href": "slides/18-geospatial.html#spatial-data-analysis",
    "title": "Geospatial Modeling",
    "section": "Spatial Data Analysis:",
    "text": "Spatial Data Analysis:\n\nWhen?:\n\nAnytime you have spatial information collected with your data.\nIncreasing availability of some level of spatial information (latitude/longitude; county; state; etc.)\n\nWhy?:\n\nCorrect statistical inference (conditional independence may not be a valid assumption!)\n\nSpecific goals will depend on the type of spatial data you have and the objective of your analysis\n\nProducing maps with valid inference"
  },
  {
    "objectID": "slides/18-geospatial.html#spatial-data-analysis-when",
    "href": "slides/18-geospatial.html#spatial-data-analysis-when",
    "title": "Geospatial Modeling",
    "section": "Spatial Data Analysis: When?",
    "text": "Spatial Data Analysis: When?\n\nAnytime you have spatial information collected with your data.\nIncreasing availability of some level of spatial information:\n\nLatitude/longitude.\nCounty, state, etc.\nImaging data."
  },
  {
    "objectID": "slides/18-geospatial.html#spatial-data-analysis-why",
    "href": "slides/18-geospatial.html#spatial-data-analysis-why",
    "title": "Geospatial Modeling",
    "section": "Spatial Data Analysis: Why?",
    "text": "Spatial Data Analysis: Why?\n\nCorrect statistical inference (conditional independence may not be a valid assumption!).\n\nSpecific goals will depend on the type of spatial data you have and the objective of your analysis.\n\nProducing maps with valid inference."
  },
  {
    "objectID": "slides/18-geospatial.html#spatial-data-analysis-how",
    "href": "slides/18-geospatial.html#spatial-data-analysis-how",
    "title": "Geospatial Modeling",
    "section": "Spatial Data Analysis: How?",
    "text": "Spatial Data Analysis: How?\n\nBayesian Hierarchical Modeling:\n\nFlexible framework to handle multiple levels of uncertainty.\nMarkov chain Monte Carlo (MCMC) offers computationally convenient solution to make inference.\n\nFrequentist methods also available through the EM algorithm.\n\nOriginal frequentist methods ignore some of the uncertainty in estimating these spatial models."
  },
  {
    "objectID": "slides/18-geospatial.html#goals-of-a-point-referenced-analysis",
    "href": "slides/18-geospatial.html#goals-of-a-point-referenced-analysis",
    "title": "Geospatial Modeling",
    "section": "Goals of a Point-referenced Analysis",
    "text": "Goals of a Point-referenced Analysis\n\nEstimation and explanation:\n\nTypical regression parameter estimation.\n\nHow does temperature change across the domain (large-scale)?\n\n\nPrediction at unobserved locations:\n\nOriginal development of spatial methods.\nKriging named after D.G. Krige (mining applications).\n\nDesign issues:\n\nWhere to put a new air pollution monitor to optimize future prediction criteria?"
  },
  {
    "objectID": "slides/18-geospatial.html#goals-of-a-point-referenced-analysis-1",
    "href": "slides/18-geospatial.html#goals-of-a-point-referenced-analysis-1",
    "title": "Geospatial Modeling",
    "section": "Goals of a Point-referenced Analysis",
    "text": "Goals of a Point-referenced Analysis\n\nEstimation and Explanation"
  },
  {
    "objectID": "slides/18-geospatial.html#goals-of-a-point-referenced-analysis-2",
    "href": "slides/18-geospatial.html#goals-of-a-point-referenced-analysis-2",
    "title": "Geospatial Modeling",
    "section": "Goals of a Point-referenced Analysis",
    "text": "Goals of a Point-referenced Analysis\n\nSpatial Prediction\n\n\n\n\nObserved Data"
  },
  {
    "objectID": "slides/18-geospatial.html#goals-of-a-point-referenced-analysis-3",
    "href": "slides/18-geospatial.html#goals-of-a-point-referenced-analysis-3",
    "title": "Geospatial Modeling",
    "section": "Goals of a Point-referenced Analysis",
    "text": "Goals of a Point-referenced Analysis\n\nSpatial Prediction\n\n\n\n\nPrediction"
  },
  {
    "objectID": "slides/18-geospatial.html#goals-of-a-point-referenced-analysis-4",
    "href": "slides/18-geospatial.html#goals-of-a-point-referenced-analysis-4",
    "title": "Geospatial Modeling",
    "section": "Goals of a Point-referenced Analysis",
    "text": "Goals of a Point-referenced Analysis\n\nSpatial Prediction\n\n\n\n\nStandard Errors"
  },
  {
    "objectID": "slides/18-geospatial.html#point-referenced-modeling",
    "href": "slides/18-geospatial.html#point-referenced-modeling",
    "title": "Geospatial Modeling",
    "section": "Point-referenced Modeling",
    "text": "Point-referenced Modeling\n\nObservations closer in space tend to be more similar.\n\nCommon regression models assume independence among observations.\n\nNot a valid assumption here, especially at short distances.\n\n\nMultivariate normal distribution with valid spatial covariance function used in Bayesian modeling.\n\nSpatial covariance describes how observations are correlated based on their proximity to each other.\n\nAdvanced models built on similar ideas.\n\nLatent processes often used."
  },
  {
    "objectID": "slides/18-geospatial.html#point-referenced-modeling-1",
    "href": "slides/18-geospatial.html#point-referenced-modeling-1",
    "title": "Geospatial Modeling",
    "section": "Point-referenced Modeling",
    "text": "Point-referenced Modeling\n\\[  Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\stackrel{iid}{\\sim} N(0,\\sigma^2).\\]\nData Objects:\n\n\\(i \\in \\{1,\\dots,n\\}\\) indexes unique locations.\n\\(j \\in \\{1,\\dots,n_i\\}\\) indexes individuals at each location.\n\\(Y_{ij}\\) denotes the observation of individual \\(j\\) at location \\(i\\).\n\\(\\mathbf{x}_{ij} \\in \\mathbb{R}^{1 \\times p}\\), where \\(p\\) is the number of predictors (excluding intercept)."
  },
  {
    "objectID": "slides/18-geospatial.html#motivating-dataset",
    "href": "slides/18-geospatial.html#motivating-dataset",
    "title": "Geospatial Modeling",
    "section": "Motivating Dataset",
    "text": "Motivating Dataset\nWe will look at a sample of women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey.\n\nThere are ~8600 women who are nested in ~500 survey clusters.\nVariables are:\n\nloc_id: location id (i.e. survey cluster).\nhemoglobin: hemoglobin level (g/dL).\nanemia: anemia classifications.\nage: age in years.\nurban: urban vs. rural.\nLATNUM: latitude.\nLONGNUM: longitude."
  },
  {
    "objectID": "slides/18-geospatial.html#trying-to-understand-these-trends",
    "href": "slides/18-geospatial.html#trying-to-understand-these-trends",
    "title": "Geospatial Modeling",
    "section": "Trying to understand these trends",
    "text": "Trying to understand these trends"
  },
  {
    "objectID": "slides/18-geospatial.html#trying-to-understand-these-trends-1",
    "href": "slides/18-geospatial.html#trying-to-understand-these-trends-1",
    "title": "Geospatial Modeling",
    "section": "Trying to understand these trends",
    "text": "Trying to understand these trends"
  },
  {
    "objectID": "slides/18-geospatial.html#point-referenced-modeling-2",
    "href": "slides/18-geospatial.html#point-referenced-modeling-2",
    "title": "Geospatial Modeling",
    "section": "Point-referenced Modeling",
    "text": "Point-referenced Modeling\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\stackrel{iid}{\\sim} N(0,\\sigma^2).\\]\nPopulation Parameters:\n\n\\(\\alpha \\in \\mathbb{R}\\) is the intercept.\n\\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\) is the regression coefficients.\n\\(\\sigma^2 \\in \\mathbb{R}^+\\) is the overall residual error (nugget)."
  },
  {
    "objectID": "slides/18-geospatial.html#point-referenced-modeling-3",
    "href": "slides/18-geospatial.html#point-referenced-modeling-3",
    "title": "Geospatial Modeling",
    "section": "Point-referenced Modeling",
    "text": "Point-referenced Modeling\n\\[Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\stackrel{iid}{\\sim} N(0,\\sigma^2).\\]\nLocation-specific Parameters:\n\n\\(\\theta_i\\) denotes the spatial intercept at location \\(\\mathbf{u}_i\\).\n\\(\\mathbf{u}_i \\in \\mathbb{R}^d\\) denotes the spatial location of location \\(i\\). For example, \\(\\mathbf{u}_i = (\\text{latitude}_i, \\text{longitude}_i)\\), so that \\(d = 2\\).\n\nIn a spatial context, we often use the following notation:\n\\[Y_j(\\mathbf{u}_i) = \\alpha + \\mathbf{x}_j(\\mathbf{u}_i)\\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_i(\\mathbf{u}_i).\\]"
  },
  {
    "objectID": "slides/18-geospatial.html#point-referenced-modeling-4",
    "href": "slides/18-geospatial.html#point-referenced-modeling-4",
    "title": "Geospatial Modeling",
    "section": "Point-referenced Modeling",
    "text": "Point-referenced Modeling\nIn a spatial context, we often use the following notation:\n\\[Y_j(\\mathbf{u}_i) = \\alpha + \\mathbf{x}_j(\\mathbf{u}_i)\\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_i(\\mathbf{u}_i)\\]"
  },
  {
    "objectID": "slides/18-geospatial.html#location-specific-notation",
    "href": "slides/18-geospatial.html#location-specific-notation",
    "title": "Geospatial Modeling",
    "section": "Location-specific Notation",
    "text": "Location-specific Notation\n\\[\\mathbf{Y}(\\mathbf{u}_i) = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i)\\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}(\\mathbf{u}_i)\\]\n\n\\(\\mathbf{Y}(\\mathbf{u}_i) = (Y_1(\\mathbf{u}_i),\\ldots,Y_{n_i}(\\mathbf{u}_i))^\\top\\)\n\\(\\mathbf{X}(\\mathbf{u}_i)\\) is an \\(n_i \\times p\\) dimensional matrix with rows \\(\\mathbf{x}_j(\\mathbf{u}_i)\\).\n\\(\\boldsymbol{\\epsilon}(\\mathbf{u}_i) = (\\epsilon_i(\\mathbf{u}_i),\\ldots,\\epsilon_{n_i}(\\mathbf{u}_i))^\\top\\), where \\(\\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\).\n\nNote: This notation is the same as the linear mixed model we have talked about in previous lectures."
  },
  {
    "objectID": "slides/18-geospatial.html#full-data-notation",
    "href": "slides/18-geospatial.html#full-data-notation",
    "title": "Geospatial Modeling",
    "section": "Full data notation",
    "text": "Full data notation\n\\[\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}\\]\n\n\\(\\mathbf{Y} = (\\mathbf{Y}(\\mathbf{u}_1)^\\top,\\ldots,\\mathbf{Y}(\\mathbf{u}_{n})^\\top)^\\top \\in \\mathbb{R}^N\\), with \\(N = \\sum_{i=1}^n n_i\\).\n\\(\\mathbf{X} \\in \\mathbb{R}^{N \\times p}\\) that stacks \\(\\mathbf{X}(\\mathbf{u}_i)\\).\n\\(\\boldsymbol{\\theta} = (\\theta(\\mathbf{u}_1),\\ldots,\\theta(\\mathbf{u}_n))^\\top \\in \\mathbb{R}^n\\).\n\\(\\mathbf{Z}\\) is \\(N \\times n\\) dimensional binary matrix. Each row contains a single 1 in column \\(i\\) that corresponds to the location of \\(Y_j(\\mathbf{u}_i)\\).\n\nThis notation is useful because it allows us to examine the vector of location-specific parameters, \\(\\boldsymbol{\\theta}\\). Spatial dependency will be introduced through \\(\\boldsymbol{\\theta}\\)."
  },
  {
    "objectID": "slides/18-geospatial.html#motivation-for-gaussian-processes-in-spatial-analysis",
    "href": "slides/18-geospatial.html#motivation-for-gaussian-processes-in-spatial-analysis",
    "title": "Geospatial Modeling",
    "section": "Motivation for Gaussian Processes in Spatial Analysis",
    "text": "Motivation for Gaussian Processes in Spatial Analysis\nIn spatial data analysis, we are often interested in modeling spatially correlated data. One powerful way to model this correlation is through Gaussian Processes (GPs).\nConsider a vector () containing the spatial intercepts at various locations:\n[ =\n\n_1\n_2\n⋮\n_n\n\n]\nWe want to model the spatial variation in () using a prior that reflects the spatial correlation between locations.\nWhat do we want from a prior for ()?\n\nCentered at zero: We assume no prior knowledge of any systematic trends in the data, so the prior should have a mean of zero.\nCovariance dictated by a kernel: The prior covariance is governed by a kernel (or covariance function) that defines how spatially distant locations are related.\nStationary: The covariance function should depend only on the relative distance between locations, not the actual locations themselves.\nIsotropic: The spatial correlation should be the same in all directions (spherically symmetric)."
  },
  {
    "objectID": "slides/18-geospatial.html#characteristics-of-the-gp-prior-covariance",
    "href": "slides/18-geospatial.html#characteristics-of-the-gp-prior-covariance",
    "title": "Geospatial Modeling",
    "section": "Characteristics of the GP Prior Covariance",
    "text": "Characteristics of the GP Prior Covariance\nThe Matérn covariance function has a general form and is often the default choice for spatial data (squared exponential is often too smooth!). Matérn is a function of a smoothness parameter \\(\\nu &gt; 0\\) and a length scale, which we will define as \\(\\rho\\).\n\n\\(\\nu = 1/2\\): yields the exponential covariance function, gp_exponential_cov.\n\\(\\nu = 3/2\\): Matérn 3/2 covariance function, gp_matern32_cov.\n\n\\[C(||\\mathbf{h}||) = \\tau^2 \\left(1 + \\frac{\\sqrt{3} ||\\mathbf{h}||}{\\rho}\\right) \\exp\\left(-\\frac{\\sqrt{3} ||\\mathbf{h}||}{\\ell}\\right)\\]\n\n\\(\\nu = 5/2\\): Matérn 5/2 covariance function, gp_matern52_cov."
  },
  {
    "objectID": "slides/18-geospatial.html#how-to-incorporate-spatial-correlation",
    "href": "slides/18-geospatial.html#how-to-incorporate-spatial-correlation",
    "title": "Geospatial Modeling",
    "section": "How to incorporate spatial correlation?",
    "text": "How to incorporate spatial correlation?\n\nWe want to model the spatial variation in \\(\\boldsymbol{\\theta}\\) using a prior that reflects the spatial correlation between locations.\nOne powerful way to incorporate spatial correlations is through Gaussian Processes (GPs).\n\nDesirable properties:\n\nCentered at zero: We assume no prior knowledge of any systematic trends in the data, so the prior should have a mean of zero.\nSpatial covariance function: A covariance structure that defines how spatially distant locations are related.\n\nStationary: The covariance function should depend only on the relative distance between locations, not the actual locations themselves.\nIsotropic: The spatial correlation should be the same in all directions."
  },
  {
    "objectID": "slides/18-geospatial.html#accounting-for-spatial-correlation",
    "href": "slides/18-geospatial.html#accounting-for-spatial-correlation",
    "title": "Geospatial Modeling",
    "section": "Accounting for spatial correlation",
    "text": "Accounting for spatial correlation\n\nWe want to model the spatial variation in \\(\\boldsymbol{\\theta}\\) using a prior that reflects the spatial correlation between locations.\nOne powerful way to incorporate spatial correlations is through Gaussian processes (GPs).\nConsider \\(\\{\\theta(\\mathbf{u}) : \\mathbf{u} \\in \\mathcal D\\}\\), where \\(\\mathcal D\\) is a fixed subset of \\(d\\)-dimensional Euclidean space (\\(d=1\\) is time-series, \\(d=2,3\\) is spatial).\nThe process is said to be Gaussian if, for any \\(n\\geq 1\\) and observed locations \\(\\{\\mathbf{u}_1,\\ldots,\\mathbf{u}_n\\}\\), \\(\\boldsymbol{\\theta} = (\\theta(\\mathbf{u}_1),\\ldots,\\theta(\\mathbf{u}_n))^\\top\\) has a multivariate normal distribution."
  },
  {
    "objectID": "slides/18-geospatial.html#properties-of-gaussian-processes",
    "href": "slides/18-geospatial.html#properties-of-gaussian-processes",
    "title": "Geospatial Modeling",
    "section": "Properties of Gaussian Processes",
    "text": "Properties of Gaussian Processes\nWe define a GP as \\(\\theta(\\mathbf{u}) \\sim GP(\\mu(\\cdot), C(\\cdot, \\cdot))\\), where \\(\\mu(\\cdot)\\) is the mean process and \\(C(\\cdot, \\cdot)\\) is a covariance function.\n\nMean function: \\(\\mathbb{E}[\\theta(\\mathbf{u})] = \\mu(\\mathbf{u}) = 0\\).\nCovariance function: \\(\\mathbb{C}(\\theta(\\mathbf{u}_i),\\theta(\\mathbf{u}_{i'})) = C(\\mathbf{u}_i, \\mathbf{u}_{i'})\\).\n\nProperties of covariance functions:\n\nStationary: \\(\\mathbb{C}(\\theta(\\mathbf{u}),\\theta(\\mathbf{u} + \\mathbf{h})) = C(\\mathbf{h})\\), where \\(\\mathbf{h} \\in \\mathbf{R}^d\\).\nIsotropic: \\(\\mathbb{C}(\\theta(\\mathbf{u}),\\theta(\\mathbf{u} + \\mathbf{h})) = C(||\\mathbf{h}||)\\), where \\(||\\cdot||\\) is a distance length."
  },
  {
    "objectID": "slides/18-geospatial.html#choosing-a-covariance-function",
    "href": "slides/18-geospatial.html#choosing-a-covariance-function",
    "title": "Geospatial Modeling",
    "section": "Choosing a covariance function",
    "text": "Choosing a covariance function\nThe Matérn covariance function has a general form and is often the default choice for spatial data. Matérn is a function of a smoothness parameter \\(\\nu &gt; 0\\), magnitude \\(\\tau\\) and a length scale, which we will define as \\(\\rho\\). Given a fixed \\(\\nu\\), larger values of \\(\\rho\\) lead to a smoother surface.\n\n\\(\\nu = 1/2\\): exponential covariance function, gp_exponential_cov.\n\\(\\nu = 3/2\\): Matérn 3/2 covariance function, gp_matern32_cov.\n\n\\[C(||\\mathbf{h}||) = \\tau^2 \\left(1 + \\frac{\\sqrt{3} ||\\mathbf{h}||}{\\rho}\\right) \\exp\\left(-\\frac{\\sqrt{3} ||\\mathbf{h}||}{\\rho}\\right)\\]\n\n\\(\\nu = 5/2\\): Matérn 5/2 covariance function, gp_matern52_cov.\n\\(\\nu \\rightarrow \\infty\\): Squared exponential, gp_exp_quad_cov."
  },
  {
    "objectID": "slides/18-geospatial.html#point-referenced-spatial-model",
    "href": "slides/18-geospatial.html#point-referenced-spatial-model",
    "title": "Geospatial Modeling",
    "section": "Point-referenced spatial model",
    "text": "Point-referenced spatial model\n\\[\\begin{aligned}\n\\mathbf{Y} | \\alpha, \\boldsymbol{\\beta}, \\boldsymbol{\\theta},\\sigma &\\sim N_N(\\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta},\\sigma^2 \\mathbf{I}_N)\\\\\n\\boldsymbol{\\theta} &\\sim N_{n}(\\mathbf{0}_n, \\mathbf{C})\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\]\nwhere \\(\\boldsymbol{\\Omega} = (\\alpha,\\boldsymbol{\\beta},\\sigma,\\tau,\\rho)\\) and\n\\[\\mathbf{C} = \\begin{bmatrix}\nC(0) & C(||\\mathbf{u}_1 - \\mathbf{u}_2||) & \\cdots & C(||\\mathbf{u}_1 - \\mathbf{u}_{n}||)\\\\\nC(||\\mathbf{u}_1 - \\mathbf{u}_2||) & C(0) & \\cdots & C(||\\mathbf{u}_2 - \\mathbf{u}_{n}||)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nC(||\\mathbf{u}_{1} - \\mathbf{u}_{n}||) & C(||\\mathbf{u}_2 - \\mathbf{u}_{n}||) & \\cdots & C(0)\\\\\n\\end{bmatrix}.\\]"
  },
  {
    "objectID": "slides/18-geospatial.html#point-referenced-spatial-model-marginal",
    "href": "slides/18-geospatial.html#point-referenced-spatial-model-marginal",
    "title": "Geospatial Modeling",
    "section": "Point-referenced spatial model: marginal",
    "text": "Point-referenced spatial model: marginal\n\\[\\begin{aligned}\n\\mathbf{Y} | \\boldsymbol{\\Omega}  &\\sim N_N(\\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta},\\sigma^2 \\mathbf{I}_N + \\mathbf{Z} \\mathbf{C} \\mathbf{Z}^\\top)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/18-geospatial.html#point-referenced-spatial-model-1",
    "href": "slides/18-geospatial.html#point-referenced-spatial-model-1",
    "title": "Geospatial Modeling",
    "section": "Point-referenced spatial model",
    "text": "Point-referenced spatial model\nLike previous lecture, we can also specify a marginal model which is useful if we are only intereted in population parameters, speed is of concern, or the conditional specification has poor convergence.\n\\[\\begin{aligned}\n\\mathbf{Y} | \\boldsymbol{\\Omega}  &\\sim N_N(\\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta},\\sigma^2 \\mathbf{I}_N + \\mathbf{Z} \\mathbf{C} \\mathbf{Z}^\\top)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega})\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/18-geospatial.html#posterior-predictive-distribution",
    "href": "slides/18-geospatial.html#posterior-predictive-distribution",
    "title": "Geospatial Modeling",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nDefine \\(\\mathbf{Y}^* = (Y(\\mathbf{u}_{n+1}),\\ldots, Y(\\mathbf{u}_{n+q}))^\\top\\) as observations at \\(q\\) new locations. We ignore the subscript \\(j\\). The new location-specific parameters are \\(\\boldsymbol{\\theta} = (\\theta(\\mathbf{u}_{n+1}),\\ldots,\\theta(\\mathbf{u}_{n+q}))^\\top\\).\n\\[\\begin{aligned}\nf(\\mathbf{Y}^* | \\mathbf{Y}) &= \\int f(\\mathbf{Y}^*, \\boldsymbol{\\theta}^*, \\boldsymbol{\\theta}, \\boldsymbol{\\Omega} | \\mathbf{Y}) d\\boldsymbol{\\theta}^* d\\boldsymbol{\\theta} d\\boldsymbol{\\Omega}\\\\\n&= \\int \\underbrace{f(\\mathbf{Y}^* | \\boldsymbol{\\theta}^*, \\boldsymbol{\\Omega})}_{(1)} \\underbrace{f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})}_{(2)} \\underbrace{f(\\boldsymbol{\\Omega} | \\mathbf{Y})}_{(3)} d\\boldsymbol{\\theta}^* d\\boldsymbol{\\theta} d\\boldsymbol{\\Omega}\\\\\n\\end{aligned}\\]\n\nLikelihood: \\(f(\\mathbf{Y}^* | \\boldsymbol{\\theta}^*, \\boldsymbol{\\Omega}) = \\prod_{i=n+1}^{n+q} f(Y(\\mathbf{u}_i) | \\alpha, \\boldsymbol{\\beta}, \\theta(\\mathbf{u}_i),\\sigma)\\)\nKriging: \\(f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})\\)\nPosterior distribution: \\(f(\\boldsymbol{\\Omega} | \\mathbf{Y})\\)"
  },
  {
    "objectID": "slides/18-geospatial.html#kriging-distribtuion",
    "href": "slides/18-geospatial.html#kriging-distribtuion",
    "title": "Geospatial Modeling",
    "section": "Kriging Distribtuion",
    "text": "Kriging Distribtuion\nTo compute \\(f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})\\) we must specify the joint distribtuion:\n\\[f\\left(\\begin{bmatrix}\n    \\boldsymbol{\\theta}\\\\\n    \\boldsymbol{\\theta}^*\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega}\\right) = N\\left(\\begin{bmatrix}\n    \\mathbf{0}_n \\\\\n    \\mathbf{0}_{q}\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\mathbf{C} & \\mathbf{C}_{+}\\\\\n    \\mathbf{C}_{+}^\\top & \\mathbf{C}^*\n  \\end{bmatrix}\\right),\\]\nwhere \\(\\mathbf{C}\\) is the covariance of \\(\\boldsymbol{\\theta}\\),\n\\[\\mathbf{C^*} = \\begin{bmatrix}\nC(0) & C(||\\mathbf{u}_{n+1} - \\mathbf{u}_{n+2}||) & \\cdots & C(||\\mathbf{u}_{n+1} - \\mathbf{u}_{n + q + 1}||)\\\\\nC(||\\mathbf{u}_{n+1} - \\mathbf{u}_{n+2}||) & C(0) & \\cdots & C(||\\mathbf{u}_{n_2} - \\mathbf{u}_{n + q + 1}||)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nC(||\\mathbf{u}_{n + 1} - \\mathbf{u}_{n + q + 1}||) & C(||\\mathbf{u}_{n + 2} - \\mathbf{u}_{n + q + 1}||) & \\cdots & C(0)\\\\\n\\end{bmatrix} \\in \\mathbb{R}^{q \\times q},\\]\n\\[\\mathbf{C_+} = \\begin{bmatrix}\nC(||\\mathbf{u}_{1} - \\mathbf{u}_{n+1}||) & C(||\\mathbf{u}_{1} - \\mathbf{u}_{n+2}||) & \\cdots & C(||\\mathbf{u}_{1} - \\mathbf{u}_{n + q + 1}||)\\\\\nC(||\\mathbf{u}_{2} - \\mathbf{u}_{n+1}||) & C(||\\mathbf{u}_{2} - \\mathbf{u}_{n+2}||) & \\cdots & C(||\\mathbf{u}_{n} - \\mathbf{u}_{n + q + 1}||)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nC(||\\mathbf{u}_{n} - \\mathbf{u}_{n + 1}||) & C(||\\mathbf{u}_{n} - \\mathbf{u}_{n+2}||) & \\cdots & C(||\\mathbf{u}_{n} - \\mathbf{u}_{n+q+1}||)\\\\\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times q}.\\]"
  },
  {
    "objectID": "slides/18-geospatial.html#kriging-distribtuion-1",
    "href": "slides/18-geospatial.html#kriging-distribtuion-1",
    "title": "Geospatial Modeling",
    "section": "Kriging Distribtuion",
    "text": "Kriging Distribtuion\nWe can then use the conditional specification of a multivariate normal to find, \\(f(\\boldsymbol{\\theta}_i | \\mathbf{Y}_i, \\boldsymbol{\\Omega}) = N(\\mathbb{E}_{\\boldsymbol{\\theta}_i},\\mathbb{V}_{\\boldsymbol{\\theta}_i})\\), where\n\\[\\begin{aligned}\n\\mathbb{E}_{\\boldsymbol{\\theta}_i} &= \\mathbf{0}_{n_i} + \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top \\boldsymbol{\\Upsilon}_i^{-1} (\\mathbf{Y}_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\\\\n\\mathbb{V}_{\\boldsymbol{\\theta}_i} &= \\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top \\boldsymbol{\\Upsilon}_i^{-1} \\mathbf{Z}_i\\boldsymbol{\\Sigma}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/18-geospatial.html#kriging-distribution",
    "href": "slides/18-geospatial.html#kriging-distribution",
    "title": "Geospatial Modeling",
    "section": "Kriging Distribution",
    "text": "Kriging Distribution\nTo compute \\(f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})\\) we must specify the joint distribution:\n\\[f\\left(\\begin{bmatrix}\n    \\boldsymbol{\\theta}\\\\\n    \\boldsymbol{\\theta}^*\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega}\\right) = N\\left(\\begin{bmatrix}\n    \\mathbf{0}_n \\\\\n    \\mathbf{0}_{q}\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\mathbf{C} & \\mathbf{C}_{+}\\\\\n    \\mathbf{C}_{+}^\\top & \\mathbf{C}^*\n  \\end{bmatrix}\\right),\\]\nwhere \\(\\mathbf{C}\\) is the covariance of \\(\\boldsymbol{\\theta}\\),\n\\[\\mathbf{C^*} = \\begin{bmatrix}\nC(0) & C(||\\mathbf{u}_{n+1} - \\mathbf{u}_{n+2}||) & \\cdots & C(||\\mathbf{u}_{n+1} - \\mathbf{u}_{n + q}||)\\\\\nC(||\\mathbf{u}_{n+1} - \\mathbf{u}_{n+2}||) & C(0) & \\cdots & C(||\\mathbf{u}_{n_2} - \\mathbf{u}_{n + q}||)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nC(||\\mathbf{u}_{n + 1} - \\mathbf{u}_{n + q}||) & C(||\\mathbf{u}_{n + 2} - \\mathbf{u}_{n + q}||) & \\cdots & C(0)\\\\\n\\end{bmatrix} \\in \\mathbb{R}^{q \\times q},\\]\n\\[\\mathbf{C_+} = \\begin{bmatrix}\nC(||\\mathbf{u}_{1} - \\mathbf{u}_{n+1}||) & C(||\\mathbf{u}_{1} - \\mathbf{u}_{n+2}||) & \\cdots & C(||\\mathbf{u}_{1} - \\mathbf{u}_{n + q}||)\\\\\nC(||\\mathbf{u}_{2} - \\mathbf{u}_{n+1}||) & C(||\\mathbf{u}_{2} - \\mathbf{u}_{n+2}||) & \\cdots & C(||\\mathbf{u}_{n} - \\mathbf{u}_{n + q}||)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nC(||\\mathbf{u}_{n} - \\mathbf{u}_{n + 1}||) & C(||\\mathbf{u}_{n} - \\mathbf{u}_{n+2}||) & \\cdots & C(||\\mathbf{u}_{n} - \\mathbf{u}_{n+q}||)\\\\\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times q}.\\]"
  },
  {
    "objectID": "slides/18-geospatial.html#kriging-distribution-1",
    "href": "slides/18-geospatial.html#kriging-distribution-1",
    "title": "Geospatial Modeling",
    "section": "Kriging Distribution",
    "text": "Kriging Distribution\nWe can then use the conditional specification of a multivariate normal to find, \\(f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) = N(\\mathbb{E}_{\\boldsymbol{\\theta}^*},\\mathbb{V}_{\\boldsymbol{\\theta}^*})\\), where\n\\[\\begin{aligned}\n\\mathbb{E}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}_+^\\top \\mathbf{C}^{-1} \\boldsymbol{\\theta}\\\\\n\\mathbb{V}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}^* - \\mathbf{C}_+^\\top \\mathbf{C}^{-1} \\mathbf{C}_+.\n\\end{aligned}\\]\nComputationally it is efficient to compute \\(\\mathbf{L} = \\text{chol}(\\mathbf{C})\\), such that \\(\\mathbf{C} = \\mathbf{L}\\mathbf{L}^\\top\\) and write:\n\\[\\begin{aligned}\n\\mathbb{E}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}_+^\\top \\left(\\mathbf{L}\\mathbf{L}^\\top\\right)^{-1} \\boldsymbol{\\theta}\\\\\n&= \\mathbf{C}_+^\\top \\left(\\mathbf{L}^{-1}\\right)^\\top\\mathbf{L}^{-1} \\boldsymbol{\\theta}\\\\\n&= \\left(\\mathbf{L}^{-1} \\mathbf{C}_+\\right)^\\top\\mathbf{L}^{-1} \\boldsymbol{\\theta}.\n\\end{aligned}\\]\nEfficient Stan function: mdivide_left_tri_low(A, b) = inverse(tri(A)) * b.\n\\[\\begin{aligned}\n\\mathbb{V}_{\\boldsymbol{\\theta}^*} &=\\mathbf{C}^* - \\left(\\mathbf{L}^{-1} \\mathbf{C}_+\\right)^\\top \\mathbf{L}^{-1} \\mathbf{C}_+.\n\\end{aligned}\\]\nDirectly computing \\(\\mathbf{C}^{-1}\\) can lead to a matrix that is not symmetric."
  },
  {
    "objectID": "slides/18-geospatial.html#visualize-data",
    "href": "slides/18-geospatial.html#visualize-data",
    "title": "Geospatial Modeling",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/18-geospatial.html#motivating-dataset-1",
    "href": "slides/18-geospatial.html#motivating-dataset-1",
    "title": "Geospatial Modeling",
    "section": "Motivating Dataset",
    "text": "Motivating Dataset\n\n\n  loc_id hemoglobin     anemia age urban   LATNUM  LONGNUM mean_hemoglobin\n1      1       12.5 not anemic  28 rural 0.220128 21.79508        11.81053\n2      1       12.6 not anemic  42 rural 0.220128 21.79508        11.81053\n3      1       13.3 not anemic  15 rural 0.220128 21.79508        11.81053\n4      1       12.9 not anemic  28 rural 0.220128 21.79508        11.81053\n5      1       10.4       mild  32 rural 0.220128 21.79508        11.81053\n6      1       12.2 not anemic  42 rural 0.220128 21.79508        11.81053\n  community_size mean_age\n1             19       19\n2             19       19\n3             19       19\n4             19       19\n5             19       19\n6             19       19\n\n\n\n\n\n\n\n\nModeling Goals:\n\n\n\n\nLearn the associations between age and urbanality and hemoglobin, accounting for unmeasured spatial confounders.\nCreate a predicted map of hemoglobin across the spatial surface, with uncertainty quantification."
  },
  {
    "objectID": "slides/18-geospatial.html#looking-at-community-sizes",
    "href": "slides/18-geospatial.html#looking-at-community-sizes",
    "title": "Geospatial Modeling",
    "section": "Looking at community sizes",
    "text": "Looking at community sizes"
  },
  {
    "objectID": "slides/18-geospatial.html#today-we-will-focus-on-one-state",
    "href": "slides/18-geospatial.html#today-we-will-focus-on-one-state",
    "title": "Geospatial Modeling",
    "section": "Today we will focus on one state",
    "text": "Today we will focus on one state"
  },
  {
    "objectID": "slides/18-geospatial.html#today-we-will-focus-on-one-state-1",
    "href": "slides/18-geospatial.html#today-we-will-focus-on-one-state-1",
    "title": "Geospatial Modeling",
    "section": "Today we will focus on one state",
    "text": "Today we will focus on one state"
  },
  {
    "objectID": "slides/18-geospatial.html#modeling-goals",
    "href": "slides/18-geospatial.html#modeling-goals",
    "title": "Geospatial Modeling",
    "section": "Modeling goals",
    "text": "Modeling goals\n\\[Y_j(\\mathbf{u}_i) = \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\]\n\n\\(N = 490\\), \\(n = 29\\).\n\\(\\mathbf{x}_j(\\mathbf{u}_i) = (\\text{age}_{ij}/10, \\text{urban}_i)\\)\n\nGoals:\n\nLearn the associations between age and urbanality and hemoglobin, accounting for unmeasured spatial confounders.\nCreate a predicted map of hemoglobin across the spatial surface, with uncertainty quantification."
  },
  {
    "objectID": "slides/18-geospatial.html#define-the-prediction-grid",
    "href": "slides/18-geospatial.html#define-the-prediction-grid",
    "title": "Geospatial Modeling",
    "section": "Define the prediction grid",
    "text": "Define the prediction grid\nFor prediction I created a \\(20 \\times 20\\) grid, making sure the points are in the surface."
  },
  {
    "objectID": "slides/18-geospatial.html#stan-code",
    "href": "slides/18-geospatial.html#stan-code",
    "title": "Geospatial Modeling",
    "section": "Stan code",
    "text": "Stan code\n\nfunctions {\n    matrix L_matern32(array[] vector x, real tau, real rho, real delta) { \n        matrix[size(x), size(x)] cov;\n        cov = add_diag(gp_matern32_cov(x, tau, rho), delta);\n        return cholesky_decompose(cov);\n    }\n  vector theta_new_rng(array[] vector x, array[] vector x_new, real tau, \n                        real rho, vector z_obs, real delta) {\n    int n = size(x);\n    int q = size(x_new);\n    vector[q] theta_new;\n      { // everything declared inside of {} will only exist in that local environment\n      matrix[n, n] LC = L_matern32(x, tau, rho, delta);\n      vector[n] theta_obs = LC * z_obs;\n      matrix[n, q] Cplus = gp_matern32_cov(x, x_new, tau, rho);\n      matrix[n, q] LCinv_Cplus = mdivide_left_tri_low(LC, Cplus);\n      vector[n] LCinv_theta_obs = mdivide_left_tri_low(LC, theta_obs);\n      vector[q] theta_new_mu = LCinv_Cplus' * LCinv_theta_obs;\n      matrix[q, q] C_star = add_diag(gp_matern32_cov(x_new, tau, rho), delta);\n      matrix[q, q] theta_new_cov = C_star - LCinv_Cplus' * LCinv_Cplus;\n      theta_new = multi_normal_rng(theta_new_mu, theta_new_cov);\n    }\n    return theta_new;\n    }\n    matrix cov2cor(matrix V) { \n      int p = rows(V); \n      vector[p] Is = inv_sqrt(diagonal(V)); \n      return quad_form_diag(V, Is); \n     }\n}\ndata {\n  int&lt;lower = 1&gt; N;                        // number of observed data points\n  int&lt;lower = 1&gt; p;                        // number of fixed covariates\n  int&lt;lower = 1&gt; n;                        // number of unique locations\n  int&lt;lower = 1&gt; d;                        // dimension of the spatial location\n  int&lt;lower = 1&gt; q;                    // total number of points for prediction\n  vector[N] Y;                             // observed data\n  matrix[N, p] X;                          // design matrix - fixed effects\n  array[N] int&lt;lower = 1, upper = n&gt; Ids;  // location mapping indices\n  array[n] vector[d] u;                    // locations for observed data\n  array[q] vector[d] u_new;          // locations for prediction\n  matrix[q, p] X_new;                 // design matrix for the new locations\n}\ntransformed data {\n  real delta = 1e-9;\n  matrix[N, p] X_centered;\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n}\nparameters {\n  // likelihood parameters\n  real alpha_star;         // centered intercept\n  vector[p] beta;          // population coefficients\n  real&lt;lower = 0&gt; sigma;   // nugget error term\n  // GP parameters\n  real&lt;lower = 0&gt; tau;     // GP scale for intercept\n  real&lt;lower = 0&gt; rho;     // GP length for intercept\n  vector[n] z;             // standard normal\n  // hyperparameters\n  real&lt;lower = 0&gt; sigma_beta;     // variance for coefficients\n}\ntransformed parameters {\n    // compute spatial intercept\n  matrix[n,n] LC = L_matern32(u, tau, rho, delta);\n  vector[n] theta = LC * z; // spatial intercept\n}\nmodel {\n  // likelihood\n  target += normal_lupdf(Y | alpha_star + X_centered * beta + theta[Ids], sigma);\n  // likelihood parameters\n  target += normal_lupdf(alpha_star | 0, 4);\n  target += normal_lupdf(beta | 0, sigma_beta);\n  target += normal_lupdf(sigma | 0, 2);\n  // GP parameters\n  target += normal_lupdf(tau | 0, 4);\n  target += inv_gamma_lupdf(rho | 5, 5);\n  target += normal_lupdf(z | 0, 1);  \n  // hyperparameters\n  target += normal_lupdf(sigma_beta | 0, 2);\n}\ngenerated quantities {\n  // intercepts\n  real alpha = alpha_star - X_bar * beta;\n  vector[n] alphas = alpha_star + theta;\n  // covariance\n  corr_matrix[n] Phi = cov2cor(add_diag(gp_matern32_cov(u, tau, rho), delta));\n  // posterior predictive distribution for the observed locations\n  array[N] real Y_pred = normal_rng(alpha + X * beta + theta[Ids], sigma);\n  // posterior predictive distribution across a new grid of locations\n  vector[q] theta_new = theta_new_rng(u, u_new, tau, rho, z, delta);\n  array[q] real Y_new = normal_rng(alpha + X_new * beta + theta_new, sigma);\n  // log-likelihood for loo\n  array[N] real log_lik;\n  for (i in 1:N) log_lik[i] = normal_lpdf(Y[i] | alpha + X[i, ] * beta + theta[Ids[i]], sigma);\n}"
  },
  {
    "objectID": "slides/18-geospatial.html#function-to-compute-mathbfl",
    "href": "slides/18-geospatial.html#function-to-compute-mathbfl",
    "title": "Geospatial Modeling",
    "section": "Function to compute \\(\\mathbf{L}\\)",
    "text": "Function to compute \\(\\mathbf{L}\\)\n\nfunctions {\n    matrix L_matern32(array[] vector x, real tau, real rho, real delta) { \n        matrix[size(x), size(x)] cov;\n        cov = add_diag(gp_matern32_cov(x, tau, rho), delta);\n        return cholesky_decompose(cov);\n    }\n    ...\n}"
  },
  {
    "objectID": "slides/18-geospatial.html#function-to-predict-boldsymboltheta",
    "href": "slides/18-geospatial.html#function-to-predict-boldsymboltheta",
    "title": "Geospatial Modeling",
    "section": "Function to predict \\(\\boldsymbol{\\theta}^*\\)",
    "text": "Function to predict \\(\\boldsymbol{\\theta}^*\\)\n\nfunctions {\n  vector theta_new_rng(array[] vector x, array[] vector x_new, real tau, \n                        real rho, vector z_obs, real delta) {\n    int n = size(x);\n    int q = size(x_new);\n    vector[q] theta_new;\n      { // everything declared inside of {} will only exist in that local environment\n      matrix[n, n] LC = L_matern32(x, tau, rho, delta);\n      vector[n] theta_obs = LC * z_obs;\n      matrix[n, q] Cplus = gp_matern32_cov(x, x_new, tau, rho);\n      matrix[n, q] LCinv_Cplus = mdivide_left_tri_low(LC, Cplus);\n      vector[n] LCinv_theta_obs = mdivide_left_tri_low(LC, theta_obs);\n      vector[q] theta_new_mu = LCinv_Cplus' * LCinv_theta_obs;\n      matrix[q, q] C_star = add_diag(gp_matern32_cov(x_new, tau, rho), delta);\n      matrix[q, q] theta_new_cov = C_star - LCinv_Cplus' * LCinv_Cplus;\n      theta_new = multi_normal_rng(theta_new_mu, theta_new_cov);\n    }\n    return theta_new;\n    }\n    ...\n}"
  },
  {
    "objectID": "slides/18-geospatial.html#modeling",
    "href": "slides/18-geospatial.html#modeling",
    "title": "Geospatial Modeling",
    "section": "Modeling",
    "text": "Modeling\nWe specify the following model:\n\\[\\begin{aligned}\nY_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n\\boldsymbol{\\theta} | \\tau,\\rho &\\sim N(\\mathbf{0}_n,\\mathbf{C})\\\\\n\\alpha^* &\\sim N(0,4^2)\\\\\n\\beta_j | \\sigma_{\\beta} &\\sim N(0,\\sigma_{\\beta}^2), \\quad j = 1,\\ldots,p\\\\\n\\sigma &\\sim \\text{Half-Normal}(0, 2^2)\\\\\n\\tau &\\sim \\text{Half-Normal}(0, 4^2)\\\\\n\\rho &\\sim \\text{Inv-Gamma}(5, 5)\\\\\n\\sigma_{\\beta} &\\sim \\text{Half-Normal}(0, 2^2)\n\\end{aligned}\\]\nWhere \\(N = 490\\), \\(n = 29\\), \\(\\mathbf{x}_j(\\mathbf{u}_i) = (\\text{age}_{ij}/10, \\text{urban}_i)\\), and \\(\\mathbf{C}\\) is the Matérn 3/2."
  },
  {
    "objectID": "slides/18-geospatial.html#posterior-summaries",
    "href": "slides/18-geospatial.html#posterior-summaries",
    "title": "Geospatial Modeling",
    "section": "Posterior summaries",
    "text": "Posterior summaries\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=4000; warmup=2000; thin=1; \npost-warmup draws per chain=2000, total post-warmup draws=8000.\n\n            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nalpha      11.44    0.07 2.04  5.29 11.10 12.08 12.61 13.41   937 1.01\nbeta[1]     0.00    0.00 0.07 -0.15 -0.04  0.00  0.04  0.14  6759 1.00\nbeta[2]     0.01    0.00 0.19 -0.39 -0.06  0.00  0.08  0.41  3647 1.00\nsigma       1.76    0.00 0.06  1.64  1.72  1.76  1.80  1.88  3787 1.00\ntau         1.72    0.04 1.26  0.43  0.90  1.35  2.10  5.20   937 1.00\nrho         0.90    0.01 0.58  0.35  0.54  0.73  1.05  2.42  1837 1.00\nsigma_beta  0.31    0.01 0.44  0.01  0.06  0.15  0.36  1.57  3734 1.00\n\nSamples were drawn using NUTS(diag_e) at Fri Mar 14 09:53:01 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/18-geospatial.html#traceplots",
    "href": "slides/18-geospatial.html#traceplots",
    "title": "Geospatial Modeling",
    "section": "Traceplots",
    "text": "Traceplots"
  },
  {
    "objectID": "slides/18-geospatial.html#traceplots-1",
    "href": "slides/18-geospatial.html#traceplots-1",
    "title": "Geospatial Modeling",
    "section": "Traceplots",
    "text": "Traceplots"
  },
  {
    "objectID": "slides/18-geospatial.html#traceplots-2",
    "href": "slides/18-geospatial.html#traceplots-2",
    "title": "Geospatial Modeling",
    "section": "Traceplots",
    "text": "Traceplots"
  },
  {
    "objectID": "slides/18-geospatial.html#posterior-predictive-check",
    "href": "slides/18-geospatial.html#posterior-predictive-check",
    "title": "Geospatial Modeling",
    "section": "Posterior predictive check",
    "text": "Posterior predictive check"
  },
  {
    "objectID": "slides/18-geospatial.html#posterior-correlation-matrix",
    "href": "slides/18-geospatial.html#posterior-correlation-matrix",
    "title": "Geospatial Modeling",
    "section": "Posterior correlation matrix",
    "text": "Posterior correlation matrix"
  },
  {
    "objectID": "slides/18-geospatial.html#visualize-the-predicted-hemoglobin",
    "href": "slides/18-geospatial.html#visualize-the-predicted-hemoglobin",
    "title": "Geospatial Modeling",
    "section": "Visualize the predicted hemoglobin",
    "text": "Visualize the predicted hemoglobin"
  },
  {
    "objectID": "slides/18-geospatial.html#posterior-predictive-distribution-1",
    "href": "slides/18-geospatial.html#posterior-predictive-distribution-1",
    "title": "Geospatial Modeling",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution"
  },
  {
    "objectID": "slides/18-geospatial.html#correlation-as-a-function-of-distance",
    "href": "slides/18-geospatial.html#correlation-as-a-function-of-distance",
    "title": "Geospatial Modeling",
    "section": "Correlation as a function of distance",
    "text": "Correlation as a function of distance"
  },
  {
    "objectID": "prepare/prepare-mar20.html",
    "href": "prepare/prepare-mar20.html",
    "title": "Prepare for March 20 lecture",
    "section": "",
    "text": "📖 Read Bayesian Spatial Analysis by Lawson and Banerjee 2009.\n✅ Work on HW 04 which is due Tuesday March 25 before class."
  },
  {
    "objectID": "ae/ae-07-geospatial.html",
    "href": "ae/ae-07-geospatial.html",
    "title": "AE 07: Geospatial",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-07-geospatial.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-07-geospatial.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 07: Geospatial",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-07-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-07.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-07-geospatial.html#democratic-republic-of-conge-demographic-and-health-survey",
    "href": "ae/ae-07-geospatial.html#democratic-republic-of-conge-demographic-and-health-survey",
    "title": "AE 07: Geospatial",
    "section": "Democratic Republic of Conge Demographic and Health Survey",
    "text": "Democratic Republic of Conge Demographic and Health Survey\nWe will look at a sample of women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. There are ~8600 women who are nested in ~500 survey clusters. The variables in the dataset are as follows.\n\nloc_id: location id (i.e. survey cluster).\nhemoglobin: hemoglobin level (g/dL).\nanemia: anemia classifications.\nage: age in years.\nurban: urban vs. rural.\nLATNUM: latitude.\nLONGNUM: longitude.\nmean_hemoglobin: average hemoglobin at each community (g/dL).\ncommunity_size: number of participants at each community.\nmean_age: average age of participants at each community (years).\n\nThe data set is available in your AE repos and is called drc.\n\ndrc &lt;- readRDS(\"drc.rds\")"
  },
  {
    "objectID": "ae/ae-07-geospatial.html#exercise-1",
    "href": "ae/ae-07-geospatial.html#exercise-1",
    "title": "AE 07: Geospatial",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit the logistic regression model: \\[\\begin{align*}\nY_i &\\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\\\\n\\text{logit}(\\pi_i) &= \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta},\n\\end{align*}\\] where \\(Y_i\\) is the binary indicator for a length of stay being greater than 6 hours and \\(\\mathbf{x}_i = (black_i, other_i)\\) contains the covariates for race, with white as the reference category. Place weakly-informative priors. Evaluate model convergence, check model fit with posterior predictive checks. Present the posterior mean and 95% credible interval for the odds ratio for black race.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-07-geospatial.html#exercise-2",
    "href": "ae/ae-07-geospatial.html#exercise-2",
    "title": "AE 07: Geospatial",
    "section": "Exercise 2",
    "text": "Exercise 2\nFit the additive log ratio regression model for \\(Y_i \\in \\{1,\\ldots,K\\}\\) for \\(i = 1,\\ldots,n\\), \\[\\log \\left(\\frac{P(Y_i = k)}{P(Y_i = K)} \\right)= \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\},\\] where \\(K\\) is chosen as reference. Place weakly-informative priors. Evaluate model convergence, check model fit with posterior predictive checks. Present the posterior mean and 95% credible interval for the odds ratio for black race.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-07-geospatial.html#exercise-3",
    "href": "ae/ae-07-geospatial.html#exercise-3",
    "title": "AE 07: Geospatial",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit the proportional odds regression model for ordinal \\(Y_i \\in \\{1,\\ldots,K\\}\\) for \\(i = 1,\\ldots,n\\), \\[\\log \\frac{P(Y_i\\leq k)}{P(Y_i &gt; k)} = \\text{logit}P(Y_i\\leq k) = \\alpha_k - \\mathbf{x}_i \\boldsymbol{\\beta}.\\] Place weakly-informative priors. Evaluate model convergence, check model fit with posterior predictive checks. Present the posterior mean and 95% credible interval for the odds ratio for black race.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "ae/ae-07-geospatial.html#data-objects-and-maps",
    "href": "ae/ae-07-geospatial.html#data-objects-and-maps",
    "title": "AE 07: Geospatial",
    "section": "Data Objects and Maps",
    "text": "Data Objects and Maps\nWe will begin by converting our dataset to a sf data object, which can be used for spatial data manipulation. We must specify the coordinates (i.e., latitude/longitude) and the Coordinate Reference System (CRS). CRS is a system that defines how spatial data is represented and mapped to the earth. When you see crs = 4326 in an sf object, it means that the spatial data is using the WGS 84 standard, which is the most commonly used CRS for GPS data and web maps (like Google Maps). The coordinates are in terms of longitude and latitude, and they are based on a global reference framework.\n\ndata_sf &lt;- st_as_sf(drc, coords = c(\"LONGNUM\", \"LATNUM\"), crs = 4326)\nhead(data_sf)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 21.79508 ymin: 0.220128 xmax: 21.79508 ymax: 0.220128\nGeodetic CRS:  WGS 84\n  loc_id hemoglobin     anemia age urban mean_hemoglobin community_size\n1      1       12.5 not anemic  28 rural        11.81053             19\n2      1       12.6 not anemic  42 rural        11.81053             19\n3      1       13.3 not anemic  15 rural        11.81053             19\n4      1       12.9 not anemic  28 rural        11.81053             19\n5      1       10.4       mild  32 rural        11.81053             19\n6      1       12.2 not anemic  42 rural        11.81053             19\n  mean_age                  geometry\n1 30.10526 POINT (21.79508 0.220128)\n2 30.10526 POINT (21.79508 0.220128)\n3 30.10526 POINT (21.79508 0.220128)\n4 30.10526 POINT (21.79508 0.220128)\n5 30.10526 POINT (21.79508 0.220128)\n6 30.10526 POINT (21.79508 0.220128)\n\n\nThe sf data object encodes the coordinates into a geometry which can be used for spatial mapping. This data object is convenient because it seamlessly works with dplyr, ggplot2, and other tidyverse packages for data manipulation and visualization.\nWe also extract the boundary information for our spatial unit. When you use ne_states, you are accessing state or province-level boundary data for countries around the world. This data typically includes geographic outlines (polygons) for the boundaries of states or provinces, and it’s useful for tasks like creating maps that show the subdivisions of a country. We make sure to return the data as a sf class to be consistent with our spatial data. We also will transform to be in the same CRS.\n\ncongo_states_map &lt;- ne_states(country = \"Democratic Republic of the Congo\", returnclass = \"sf\")\ncongo_states_map &lt;- st_transform(congo_states_map, crs = 4326)\n\nWe then use the st_intersection, which is a function to find the area where the geometries of two or more spatial objects overlap. We want to make sure all our points are within the DRC boundaries. Most importantly,this function merges our data with the boundary information.\n\ndata_sf_drc &lt;- st_intersection(data_sf, congo_states_map)\n\nWe can now plot the average hemoglobin for each community.\n\nggplot() +\n  # Plot the map of DRC\n  geom_sf(data = congo_states_map, fill = \"lightblue\", color = \"black\") +\n  # Plot the points from our data\n  geom_sf(data = data_sf_drc, aes(color = mean_hemoglobin), shape = 16, size = 2) +\n  scale_color_viridis_b() + \n  # Customize the plot appearance\n  theme_minimal() +\n  labs(title = \"Communities in the Democratic Republic of Congo\",\n       subtitle = \"Points represent average hemoglobin\",\n       caption = \"Data Source: rnaturalearth\",\n       x = \"Longitude\", y = \"Latitude\", color = \"Hemoglobin (g/dL)\") \n\n\n\n\n\n\n\n\nNote the choice of color scale, scale_color_viridis_b. This is a convenient scale for spatial data. An alternative is a continuous scale, scale_color_viridis_c."
  },
  {
    "objectID": "ae/ae-07-geospatial.html#extracting-data-for-a-subset",
    "href": "ae/ae-07-geospatial.html#extracting-data-for-a-subset",
    "title": "AE 07: Geospatial",
    "section": "Extracting Data for a Subset",
    "text": "Extracting Data for a Subset\nIn the lecture, we analyzed data from one state as a demonstration. Here we will show the code for accomoplishing this. We start by defining our state of interest. We can see all the states here.\n\ntable(data_sf_drc$name) |&gt; kable(col.names = c(\"State\", \"Observations\"))\n\n\n\n\nState\nObservations\n\n\n\n\nBandundu\n1152\n\n\nBas-Congo\n447\n\n\nÉquateur\n1175\n\n\nKasaï-Occidental\n708\n\n\nKasaï-Oriental\n933\n\n\nKatanga\n980\n\n\nKinshasa City\n818\n\n\nManiema\n426\n\n\nNord-Kivu\n567\n\n\nOrientale\n897\n\n\nSud-Kivu\n490\n\n\n\n\n\nWe will work with Sud-Kivu, like we did in the lecture.\n\nstate_of_interest &lt;- c(\"Sud-Kivu\")\ndata_sf_state_of_interest &lt;- data_sf_drc %&gt;% filter(name %in% state_of_interest)\n\nWe can then plot the average hemoglobin across our state of interest.\n\nggplot() +\n  # Plot our state of interest\n  geom_sf(data = congo_states_map %&gt;% filter(name %in% state_of_interest), fill = \"lightblue\", color = \"black\", size = 0.2) +\n  # Plot the points from your data\n  geom_sf(data = data_sf_state_of_interest, aes(color = mean_hemoglobin), shape = 16, size = 2) +\n  scale_color_viridis_b() + \n  theme_minimal() +\n  labs(title = \"Map of Sud-Kivu in the DRC\",\n       caption = \"Data Source: rnaturalearth\",\n       color = \"Hemoglobin (g/dL)\") +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank()) +\n  coord_sf()  # Ensures the map is properly projected"
  },
  {
    "objectID": "ae/ae-07-geospatial.html#obtain-a-grid-for-prediction",
    "href": "ae/ae-07-geospatial.html#obtain-a-grid-for-prediction",
    "title": "AE 07: Geospatial",
    "section": "Obtain a Grid for Prediction",
    "text": "Obtain a Grid for Prediction\nTo obtain a grid for prediction, we need to obtain the boundary box for the spatial object.\n\nbbox &lt;- st_bbox(congo_states_map %&gt;% filter(name %in% state_of_interest))\nbbox\n\n     xmin      ymin      xmax      ymax \n26.821237 -4.999953 29.259357 -1.666668 \n\n\nWe then specify a \\(20 \\times 20\\) grid across the boundaries of the state.\n\nn_lat &lt;- 20\nn_long &lt;- 20\nlatitudes &lt;- seq(bbox[\"ymin\"], bbox[\"ymax\"], length.out = n_lat)\nlongitudes &lt;- seq(bbox[\"xmin\"], bbox[\"xmax\"], length.out = n_long)\n\nCreate a data frame with all combinations of latitudes and longitudes and convert to an sf data object.\n\ngrid_points &lt;- expand.grid(lat = latitudes, long = longitudes)\ngrid_sf &lt;- st_as_sf(grid_points, coords = c(\"long\", \"lat\"), crs = 4326)\n\nWe will then make sure to only keep points within the Sid-Kivu boundary using st_within (points must be within the polygon).\n\ngrid_inside_drc &lt;- grid_sf[st_within(grid_sf, congo_states_map %&gt;% filter(name %in% state_of_interest), sparse = FALSE), ]\n\nFinally, we can plot the grid points within the state, both the original grid and the points within the state.\n# Plot the whole grid\nggplot() +\n  geom_sf(data = congo_states_map %&gt;% filter(name %in% state_of_interest), fill = \"lightblue\", color = \"black\") +\n  geom_sf(data = grid_sf, color = \"red\", shape = 16, size = 1) +\n  theme_minimal() +\n  labs(title = \"Grid of Points across Sud-Kivu\",\n       x = \"Longitude\", y = \"Latitude\")\n# Plot the grid points within the states\nggplot() +\n  geom_sf(data = congo_states_map %&gt;% filter(name %in% state_of_interest), fill = \"lightblue\", color = \"black\") +\n  geom_sf(data = grid_inside_drc, color = \"red\", shape = 16, size = 1) +\n  theme_minimal() +\n  labs(title = \"Grid of Points WITHIN Sud-Kivu\",\n       x = \"Longitude\", y = \"Latitude\")"
  },
  {
    "objectID": "ae/ae-07-geospatial.html#fit-a-bayesian-spatial-model",
    "href": "ae/ae-07-geospatial.html#fit-a-bayesian-spatial-model",
    "title": "AE 07: Geospatial",
    "section": "Fit a Bayesian Spatial Model",
    "text": "Fit a Bayesian Spatial Model\nWe will fit the model introduced in the lecture. We begin by preparing the data for the Stan model. We start by defining the vector \\(\\mathbf{Y}\\) and the matrix \\(\\mathbf{X}\\), being careful to not include an intercept. We then define the location identifier. We also define \\(N = \\sum_{i=1}^{n} n_i\\), the number of spatial locations, \\(n\\), and the number of predictors, \\(p\\).\n\nY &lt;- data_sf_state_of_interest$hemoglobin\nX &lt;- model.matrix(~ I(age / 10) + as.factor(urban), data = data_sf_state_of_interest)[, -1]\nIds &lt;- as.numeric(as.factor(data_sf_state_of_interest$loc_id))\nN &lt;- length(Y)\np &lt;- ncol(X)\nn &lt;- length(unique(Ids))\n\nWe then define both the observed coordinates, \\(\\mathbf{u}\\), and the new coordinates where predictions are desired, \\(\\mathbf{u}^*\\). Note that \\(\\mathbf{u}\\) should be \\(n\\) dimensional, meaning it only includes the \\(n\\) unique locations, not all \\(N\\) locations. We also define \\(d\\) the dimension of the spatial location vector (i.e., \\(d=2\\)), and \\(q\\), the number of new spatial locations.\n\nu &lt;- st_coordinates(data_sf_state_of_interest)[!duplicated(data_sf_state_of_interest$loc_id), ]\nu_new &lt;- st_coordinates(grid_inside_drc)\nd &lt;- ncol(u)\nq &lt;- nrow(u_new)\n\nFinally, we define the matrix \\(\\mathbf{X}^*\\), which is an \\(q \\times p\\) dimensional matrix that contains the predictor values for each of the new locations. We just specify the overall average mean age and specify everyone to be rural.\n\nX_new &lt;- matrix(cbind(mean(data_sf_state_of_interest$age), 0), nrow = q, ncol = p, byrow = TRUE)\n\nWe can now define the Stan data object.\n\nstan_data &lt;- list(\n  N = N,\n  p = p,\n  n = n,\n  d = d, \n  q = q,\n  Y = Y,\n  X = X,\n  Ids = Ids,\n  u = u,\n  u_new = u_new,\n  X_new = X_new\n)\n\nWe can then compile and fit the spatial model, which is saved in geospatial.stan, and is available in your AE 07 repo. I specify a few additional arguments to run the model for more iterations and don’t save a few parameters that are not needed for inference. I also played with control to get optimal convergence. For this model it is useful to specify options(mc.cores = 4).\n\noptions(mc.cores = 4)\ngeospatial_model &lt;- stan_model(file = \"geospatial.stan\")\nfit_spatial &lt;- sampling(geospatial_model, stan_data, \n                        iter = 4000, \n                        control = list(\"adapt_delta\" = 0.99), \n                        pars = c(\"z\", \"LK\", \"theta_new\", \"lp__\"), \n                        include = FALSE)\n\nWe can do all of the typical evaluation of MCMC convergence, posterior predictive checks, and posterior summaries. Here I will show you the code for mapping the posterior predictive distribution across the predictive grid. We begin by extracting the posterior predictive distribution for our grid and then compute posterior mean and standard deviation.\n\nY_new &lt;- rstan::extract(fit_spatial, pars = \"Y_new\")$Y_new\nmeans &lt;- apply(Y_new, 2, mean)\nsds &lt;- apply(Y_new, 2, sd)\n\nWe then add these predictions to the sf spatial grid object.\n\ngrid_inside_drc$prediction_mean &lt;- means\ngrid_inside_drc$prediction_sd &lt;- sds\n\nWe are now ready to create the visualizations.\n# Means\nggplot() +\n  geom_sf(data = grid_inside_drc, aes(color = prediction_mean), shape = 15, size = 10) +\n  scale_color_viridis_c() + \n  geom_sf(data = congo_states_map %&gt;% filter(name %in% state_of_interest), fill = NA, color = \"black\", size = 1) +\n  geom_sf(data = data_sf_state_of_interest[!duplicated(data_sf_state_of_interest$loc_id), ], col = \"gray\", shape = 16, size = 2) +\n  theme_minimal() +\n  labs(title = \"Posterior Predictive Distribution Mean\",\n       subtitle = paste(\"State:\", paste(state_of_interest, collapse = \", \")),\n       caption = \"Data Source: rnaturalearth\",\n       color = \"Posterior Mean\") +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank()) +\n  coord_sf()  # Ensures the map is properly projected\n# Standard deviations\nggplot() +\n  # Plot the points from your data\n  geom_sf(data = grid_inside_drc, aes(color = prediction_sd), shape = 15, size = 10) +\n  scale_color_viridis_c() + \n  geom_sf(data = congo_states_map %&gt;% filter(name %in% state_of_interest), fill = NA, color = \"black\", size = 1) +\n  geom_sf(data = data_sf_state_of_interest[!duplicated(data_sf_state_of_interest$loc_id), ], col = \"gray\", shape = 16, size = 2) +\n  theme_minimal() +\n  labs(title = \"Posterior Predictive Distribution Standard Deviation (SD)\",\n       subtitle = paste(\"State:\", paste(state_of_interest, collapse = \", \")),\n       caption = \"Data Source: rnaturalearth\",\n       color = \"Posterior SD\") +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank()) +\n  coord_sf()  # Ensures the map is properly projected"
  },
  {
    "objectID": "slides/19-disease-mapping.html#review-of-last-lecture",
    "href": "slides/19-disease-mapping.html#review-of-last-lecture",
    "title": "Disease Mapping",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nDuring our last lecture, we learned about Gaussian processes.\nWe learned how to apply Gaussian processes to longitudinal (or time-series) data.\nThe longitudinal setting is one-dimensional (i.e., time). Today we will learn about applying Gaussian processes in two-dimensions (i.e. space)."
  },
  {
    "objectID": "slides/19-disease-mapping.html#motivating-an-areal-data-analysis",
    "href": "slides/19-disease-mapping.html#motivating-an-areal-data-analysis",
    "title": "Disease Mapping",
    "section": "Motivating an Areal Data Analysis",
    "text": "Motivating an Areal Data Analysis"
  },
  {
    "objectID": "slides/19-disease-mapping.html#goal-of-an-areal-data-spatial-analysis",
    "href": "slides/19-disease-mapping.html#goal-of-an-areal-data-spatial-analysis",
    "title": "Disease Mapping",
    "section": "Goal of an Areal Data Spatial Analysis",
    "text": "Goal of an Areal Data Spatial Analysis\n\nAccount for noise due to spatial variability in order to provide smoothed estimates across space"
  },
  {
    "objectID": "slides/19-disease-mapping.html#spatial-correlation-areal-data",
    "href": "slides/19-disease-mapping.html#spatial-correlation-areal-data",
    "title": "Disease Mapping",
    "section": "Spatial Correlation: Areal Data",
    "text": "Spatial Correlation: Areal Data\n\nHow to induce spatial correlation between areal units?\n\nDistances between centroids (possibly population weighted); may be inappropriate for oddly shaped regions of varying sizes (great for equal sized grid though).\nNeighborhood structure of your spatial region; are two regions neighbors?\n\nCorrelation introduced through spatial random effects.\nThe default model for areal data in the Bayesian setting is called the conditionally autoregressive (CAR) model."
  },
  {
    "objectID": "slides/19-disease-mapping.html#prepare-for-next-class",
    "href": "slides/19-disease-mapping.html#prepare-for-next-class",
    "title": "Disease Mapping",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 05, which is due April 8.\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Guest lecture by Prof. Hwanhee Hong on Bayesian Meta-Analysis"
  },
  {
    "objectID": "slides/19-disease-mapping.html#bayesian-model-fitting",
    "href": "slides/19-disease-mapping.html#bayesian-model-fitting",
    "title": "Disease Mapping",
    "section": "Bayesian Model Fitting",
    "text": "Bayesian Model Fitting\n\nCAR models are preferred in the Bayesian setting due to the conditional definition of the model\n\\(\\theta_i\\) parameters are typically updated individually.\nDepending on the likelihood, conjugacy may be available, useful for Gibbs sampling.\nToday, CAR models can be implemented using probabilistic programming languages (e.g., Stan)."
  },
  {
    "objectID": "slides/19-disease-mapping.html#inducing-spatial-dependency",
    "href": "slides/19-disease-mapping.html#inducing-spatial-dependency",
    "title": "Disease Mapping",
    "section": "Inducing Spatial Dependency",
    "text": "Inducing Spatial Dependency\nToday, we will look at the intrinsic CAR (ICAR) process for a vector \\(\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_n)^\\top\\), \\(\\boldsymbol{\\theta} | \\tau^2 \\sim \\text{ICAR}\\left(\\tau^2\\right)\\). Under this specification, the following conditional distributions are induced.\n\\[\\theta_{i} | \\boldsymbol{\\theta}_{-i}, \\tau^2 \\sim \\mathcal N \\left({\\frac{\\sum_{j=1}^n w_{ij}\\theta_{j}}{\\sum_{j=1}^n w_{ij}}},\\frac{\\tau^2}{\\sum_{j=1}^n w_{ij}}\\right)\\]\n\n\\(\\boldsymbol\\theta_{-j}\\): Vector of \\(\\theta_{i}\\) parameters with \\(\\theta_{j}\\) removed\n\\(\\boldsymbol{\\beta}_0 | \\tau_0^2 \\sim \\text{ICAR}\\left(\\tau_0^2\\right),\\quad \\boldsymbol{\\beta}_1 | \\tau_1^2 \\sim \\text{ICAR}\\left(\\tau_1^2\\right)\\)\nNot a proper prior distribution (implications for MCMC algorithms!)"
  },
  {
    "objectID": "slides/19-disease-mapping.html#a-proper-car-process",
    "href": "slides/19-disease-mapping.html#a-proper-car-process",
    "title": "Disease Mapping",
    "section": "A Proper CAR Process",
    "text": "A Proper CAR Process\n\\[\\boldsymbol{\\theta} = \\left\\{\\theta_1, \\ldots, \\theta_n\\right\\}^{\\text{T}};\\  \\boldsymbol{\\theta} | \\rho, \\tau^2 \\sim \\text{Leroux CAR}\\left(\\rho,\\tau^2\\right)\\]\n\\[\\theta_i | \\boldsymbol{\\theta}_{-i}, \\rho, \\tau^2 \\sim \\mathcal N \\left({\\frac{\\rho \\sum_{j=1}^n w_{ij} \\theta_j}{\\rho \\sum_{j=1}^n w_{ij} + 1 - \\rho}}, \\frac{\\tau^2}{\\rho \\sum_{j=1}^n w_{ij} + 1 - \\rho}\\right)\\]\n\nProper for \\(\\rho \\in [0,1)\\)\n\\(\\rho=1\\) gives us the ICAR model\n\n\\(\\rho\\) is given a prior distribution and estimated by the data\n\\(\\text{COR}(\\theta_k,\\theta_j|\\boldsymbol{\\theta}_{-kj}) = \\frac{\\rho w_{kj}}{\\sqrt{(\\rho \\sum_{i=1}^n w_{ki} + 1 - \\rho)(\\rho \\sum_{i=1}^n w_{ji} + 1-\\rho)}}\\)"
  },
  {
    "objectID": "slides/19-disease-mapping.html#motivating-data",
    "href": "slides/19-disease-mapping.html#motivating-data",
    "title": "Disease Mapping",
    "section": "Motivating Data",
    "text": "Motivating Data\nToday, we will motivate areal spatial data analysis and disease mapping by studying 2020 COVID mortality at the county-level in North Carolina. The data object covid_nc_2020 is an sf object.\n\nVariables are:\n\nname: county name.\npopulation: 2020 population.\nobs_deaths: observed number of COVID-related deaths in 2020.\nest_deaths: estimated number of COVID-related deaths in 2020.\nsmr: standardized mortality ratio.\nage: precentage of residents over 60 years of age.\npoverty: percentage of residents below the poverty line.\ngeometry: contains centroid and boundary information for each county."
  },
  {
    "objectID": "slides/21-scalable-1.html#review-of-the-last-lecture",
    "href": "slides/21-scalable-1.html#review-of-the-last-lecture",
    "title": "Scalable Gaussian Processes #1",
    "section": "Review of the last lecture",
    "text": "Review of the last lecture\nTBU"
  },
  {
    "objectID": "slides/21-scalable-1.html#motivating-dataset",
    "href": "slides/21-scalable-1.html#motivating-dataset",
    "title": "Scalable Gaussian Processes #1",
    "section": "Motivating dataset",
    "text": "Motivating dataset\nRecall we worked with a dataset on women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. Variables are:\n\nloc_id: location id (i.e. survey cluster).\nhemoglobin: hemoglobin level (g/dL).\nanemia: anemia classifications.\nage: age in years.\nurban: urban vs. rural.\nLATNUM: latitude.\nLONGNUM: longitude."
  },
  {
    "objectID": "slides/21-scalable-1.html#motivating-dataset-1",
    "href": "slides/21-scalable-1.html#motivating-dataset-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "Motivating dataset",
    "text": "Motivating dataset\n\n\n  loc_id hemoglobin     anemia age urban   LATNUM  LONGNUM\n1      1       12.5 not anemic  28 rural 0.220128 21.79508\n2      1       12.6 not anemic  42 rural 0.220128 21.79508\n3      1       13.3 not anemic  15 rural 0.220128 21.79508\n4      1       12.9 not anemic  28 rural 0.220128 21.79508\n5      1       10.4       mild  32 rural 0.220128 21.79508\n6      1       12.2 not anemic  42 rural 0.220128 21.79508\n\n\n\n\n\n\n\n\nModeling goals:\n\n\n\n\nLearn the associations between age and urbanicity and hemoglobin, accounting for unmeasured spatial confounders.\nCreate a predicted map of hemoglobin across the spatial surface controlling for age and urbanicity, with uncertainty quantification."
  },
  {
    "objectID": "slides/21-scalable-1.html#map-of-sud-kivu-state",
    "href": "slides/21-scalable-1.html#map-of-sud-kivu-state",
    "title": "Scalable Gaussian Processes #1",
    "section": "Map of Sud-Kivu state",
    "text": "Map of Sud-Kivu state\nLast time, we focused on one state with ~500 women at ~30 locations."
  },
  {
    "objectID": "slides/21-scalable-1.html#map-of-the-drc",
    "href": "slides/21-scalable-1.html#map-of-the-drc",
    "title": "Scalable Gaussian Processes #1",
    "section": "Map of the DRC",
    "text": "Map of the DRC\nToday we will extend the analysis to the full dataset with ~8,600 observations at ~500 locations."
  },
  {
    "objectID": "slides/21-scalable-1.html#modeling",
    "href": "slides/21-scalable-1.html#modeling",
    "title": "Scalable Gaussian Processes #1",
    "section": "Modeling",
    "text": "Modeling\n\\[\\begin{align*}\n  Y_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\\]\nData objects:\n\n\\(i \\in \\{1,\\dots,n\\}\\) indexes unique locations.\n\\(j \\in \\{1,\\dots,n_i\\}\\) indexes individuals at each location.\n\\(Y_j(\\mathbf{u}_i)\\) denotes the hemoglobin level of individual \\(j\\) at location \\(\\mathbf{u}_i\\).\n\\(\\mathbf{x}_j(\\mathbf{u}_i) = (\\text{age}_{ij}/10,\\text{urban}_i) \\in \\mathbb{R}^{1 \\times p}\\), where \\(p=2\\) is the number of predictors (excluding intercept)."
  },
  {
    "objectID": "slides/21-scalable-1.html#modeling-1",
    "href": "slides/21-scalable-1.html#modeling-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "Modeling",
    "text": "Modeling\n\\[\\begin{align*}\n  Y_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\\]\nPopulation parameters:\n\n\\(\\alpha \\in \\mathbb{R}\\) is the intercept.\n\\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\) is the regression coefficients.\n\\(\\sigma^2 \\in \\mathbb{R}^+\\) is the overall residual error (nugget).\n\nLocation-specific parameters:\n\n\\(\\mathbf{u}_i = (\\text{longitude}_i,\\text{latitude}_i) \\in \\mathbb{R}^2\\) denotes coordinates of location \\(i\\).\n\\(\\theta(\\mathbf{u}_i)\\) denotes the spatial intercept at location \\(\\mathbf{u}_i\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#location-specific-notation",
    "href": "slides/21-scalable-1.html#location-specific-notation",
    "title": "Scalable Gaussian Processes #1",
    "section": "Location-specific notation",
    "text": "Location-specific notation\n\\[\\mathbf{Y}(\\mathbf{u}_i) = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i)\\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}(\\mathbf{u}_i), \\quad \\boldsymbol{\\epsilon}(\\mathbf{u}_i) \\sim N_{n_i}(\\mathbf{0},\\sigma^2\\mathbf{I})\\]\n\n\\(\\mathbf{Y}(\\mathbf{u}_i) = (Y_1(\\mathbf{u}_i),\\ldots,Y_{n_i}(\\mathbf{u}_i))^\\top\\).\n\\(\\mathbf{X}(\\mathbf{u}_i)\\) is an \\(n_i \\times p\\) dimensional matrix with rows \\(\\mathbf{x}_j(\\mathbf{u}_i)\\).\n\\(\\boldsymbol{\\epsilon}(\\mathbf{u}_i) = (\\epsilon_i(\\mathbf{u}_i),\\ldots,\\epsilon_{n_i}(\\mathbf{u}_i))^\\top\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#full-data-notation",
    "href": "slides/21-scalable-1.html#full-data-notation",
    "title": "Scalable Gaussian Processes #1",
    "section": "Full data notation",
    "text": "Full data notation\n\\[\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})\\]\n\n\\(\\mathbf{Y} = (\\mathbf{Y}(\\mathbf{u}_1)^\\top,\\ldots,\\mathbf{Y}(\\mathbf{u}_{n})^\\top)^\\top \\in \\mathbb{R}^N\\), with \\(N = \\sum_{i=1}^n n_i\\).\n\\(\\mathbf{X} \\in \\mathbb{R}^{N \\times p}\\) stacks \\(\\mathbf{X}(\\mathbf{u}_i)\\).\n\\(\\boldsymbol{\\theta} = (\\theta(\\mathbf{u}_1),\\ldots,\\theta(\\mathbf{u}_n))^\\top \\in \\mathbb{R}^n\\).\n\\(\\mathbf{Z}\\) is an \\(N \\times n\\) dimensional block diagonal binary matrix. Each row contains a single 1 in column \\(i\\) that corresponds to the location of \\(Y_j(\\mathbf{u}_i)\\). \\[\n\\begin{align}\n\\mathbf{Z} = \\begin{bmatrix}\n\\mathbf{1}_{n_1} & \\mathbf{0} & \\dots & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{1}_{n_2} & \\dots & \\mathbf{0}  \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{0} & \\dots & \\mathbf{0} & \\mathbf{1}_{n_n}\n\\end{bmatrix}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#analysis-of-the-full-anemia-dataset",
    "href": "slides/21-scalable-1.html#analysis-of-the-full-anemia-dataset",
    "title": "Scalable Gaussian Processes #1",
    "section": "Analysis of the full Anemia dataset",
    "text": "Analysis of the full Anemia dataset\nConsider the following model:\n\\[\\begin{align*}\n  Y_{ij} = \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0,\\sigma^2).\n\\end{align*}\\]\nData Objects:\n\n\\(i \\in \\{1,\\dots,n\\}\\) indexes unique locations.\n\\(j \\in \\{1,\\dots,n_i\\}\\) indexes individuals at each location.\n\\(Y_{ij}\\) denotes the hemoglobin level of individual \\(j\\) at location \\(i\\).\n\\(\\mathbf{x}_{ij} \\in \\mathbb{R}^p\\), \\(p=3\\), are covariates including an intercept, age (years), and urban (binary)."
  },
  {
    "objectID": "slides/21-scalable-1.html#analysis-of-the-full-anemia-dataset-1",
    "href": "slides/21-scalable-1.html#analysis-of-the-full-anemia-dataset-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "Analysis of the full Anemia dataset",
    "text": "Analysis of the full Anemia dataset\nConsider the following model:\n\\[\\begin{align*}\n  Y_{ij} = \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0,\\sigma^2).\n\\end{align*}\\]\nPopulation Parameters:\n\n\\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\) is the regression coefficients.\n\\(\\sigma^2 \\in \\mathbb{R}^+\\) is the overall residual error (nugget)."
  },
  {
    "objectID": "slides/21-scalable-1.html#analysis-of-the-full-anemia-dataset-2",
    "href": "slides/21-scalable-1.html#analysis-of-the-full-anemia-dataset-2",
    "title": "Scalable Gaussian Processes #1",
    "section": "Analysis of the full Anemia dataset",
    "text": "Analysis of the full Anemia dataset\nConsider the following model:\n\\[\\begin{align*}\n  Y_{ij} = \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0,\\sigma^2).\n\\end{align*}\\]\nLocation-specific Parameters:\n\n\\(\\theta(\\mathbf{u}_i)\\) denotes the spatial intercept at location \\(\\mathbf{u}_i\\).\n\\(\\mathbf{u}_i \\in \\mathbb{R}^d\\) denotes the spatial location of location \\(i\\). For example, \\(\\mathbf{u}_i = (\\text{latitude}_i, \\text{longitude}_i)\\), so that \\(d = 2\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#analysis-of-the-full-anemia-dataset-3",
    "href": "slides/21-scalable-1.html#analysis-of-the-full-anemia-dataset-3",
    "title": "Scalable Gaussian Processes #1",
    "section": "Analysis of the full Anemia dataset",
    "text": "Analysis of the full Anemia dataset\nRewriting the model in a vectorized form (as a LMM):\n\\[\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}(\\mathbf{u}_i) + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).\\]\n\n\\(\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})\\) are location-specific observations.\n\\(\\boldsymbol{\\epsilon}_i = (\\epsilon_{i1},\\ldots,\\epsilon_{in_i})\\), such that \\(\\epsilon_{ij} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\).\n\n\\[\\begin{align*}\n\\mathbf{Y}_i = \\mathbf{X}_i\\boldsymbol{\\beta} + \\mathbf{Z}_i\\boldsymbol{\\theta}(\\mathbf{u}) + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N(0,\\sigma^2 \\mathbf{I}),\n\\end{align*}\\]\nwhere\n\n\\(\\mathbf{y} \\in \\mathbb{R}^{n}\\), \\(X \\in \\mathbb{R}^{n \\times p}\\), and \\(\\boldsymbol{\\theta}(\\mathbf{u}) \\in \\mathbb{R}^{q}\\)\n\\(W \\in \\mathbb{R}^{n \\times q}\\) assigns the spatial intercepts to each individual\n\nConsider a Bayesian analysis with\n\n\\(\\boldsymbol{\\theta}(u) \\sim GP(0,K(\\psi))\\) for some covariance kernel \\(K\\) with parameter \\(\\psi\\)\n\\(\\boldsymbol{\\beta} \\sim N(0,\\tau^2)\\)\nand appropriate priors for \\(\\sigma^2\\), \\(\\tau^2\\), and \\(\\psi\\)"
  },
  {
    "objectID": "slides/21-scalable-1.html#analysis-of-the-full-anemia-dataset-4",
    "href": "slides/21-scalable-1.html#analysis-of-the-full-anemia-dataset-4",
    "title": "Scalable Gaussian Processes #1",
    "section": "Analysis of the full Anemia dataset",
    "text": "Analysis of the full Anemia dataset\nRewriting the model in a vectorized form: \\[\n\\begin{align}\n  \\mathbf{y} = X\\boldsymbol{\\beta} + W\\boldsymbol{\\theta}(\\mathbf{u}) + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N(0,\\sigma^2 \\mathbf{I}),\n\\end{align}\n\\] where\n\n\\(\\mathbf{y} \\in \\mathbb{R}^{n}\\), \\(X \\in \\mathbb{R}^{n \\times p}\\), and \\(\\boldsymbol{\\theta}(\\mathbf{u}) \\in \\mathbb{R}^{q}\\)\n\\(W \\in \\mathbb{R}^{n \\times q}\\) assigns the spatial intercepts to each individual\n\nConsider a Bayesian analysis with\n\n\\(\\boldsymbol{\\theta}(u) \\sim GP(0,K(\\psi))\\) for some covariance kernel \\(K\\) with parameter \\(\\psi\\)\n\\(\\boldsymbol{\\beta} \\sim N(0,\\tau^2)\\)\nand appropriate priors for \\(\\sigma^2\\), \\(\\tau^2\\), and \\(\\psi\\)"
  },
  {
    "objectID": "slides/21-scalable-1.html#inference-goals",
    "href": "slides/21-scalable-1.html#inference-goals",
    "title": "Scalable Gaussian Processes #1",
    "section": "Inference goals",
    "text": "Inference goals\nOur goal is to [TBU]:\n\nstudy the associations between the outcome and the covariates\nlearn the spatial intercept surface\n\nTo achieve these goals, we need to\n\nperform Bayesian model fitting to obtain posterior samples of \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\theta}(\\mathbf{u})\\) and other parameters\nmake out-of-sample predictions for \\(\\boldsymbol{\\theta}(\\mathbf{u}^*)\\), spatial intercepts at \\(q^*\\) new locations \\(\\mathbf{u}^*=(u_1^*,\\dots,u_{q^*})\\)"
  },
  {
    "objectID": "slides/21-scalable-1.html#computational-issues-with-gp",
    "href": "slides/21-scalable-1.html#computational-issues-with-gp",
    "title": "Scalable Gaussian Processes #1",
    "section": "Computational issues with GP",
    "text": "Computational issues with GP\nEffectively, the prior for \\(\\boldsymbol{\\theta}\\) is \\[\\boldsymbol{\\theta} | \\tau,\\rho \\sim N_n(\\mathbf{0},\\mathbf{C}), \\quad \\mathbf{C} \\in \\mathbb{R}^{n \\times n}.\\] Matérn 3/2 is an isotropic covariance function, \\(C(\\mathbf{u}_i, \\mathbf{u}_j) = C(\\|\\mathbf{u}_i-\\mathbf{u}_j\\|)\\).\n\\[\\mathbf{C} = \\begin{bmatrix}\nC(\\mathbf{0}) & C(\\|\\mathbf{u}_1 - \\mathbf{u}_2\\|) & \\cdots & C(\\|\\mathbf{u}_1 - \\mathbf{u}_n\\|)\\\\\nC(\\|\\mathbf{u}_1 - \\mathbf{u}_2\\|) & C(\\mathbf{0}) & \\cdots & C(\\|\\mathbf{u}_2 - \\mathbf{u}_n\\|)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nC(\\|\\mathbf{u}_{1} - \\mathbf{u}_n\\|) & C(\\|\\mathbf{u}_2 - \\mathbf{u}_n\\|) & \\cdots & C(\\mathbf{0})\\\\\n\\end{bmatrix}.\\]\nThis is not scalable because we need to invert an \\(n \\times n\\) dense covariance matrix for each MCMC iteration, which requires \\(\\mathcal{O}(n^3)\\) floating point operations (flops), and \\(\\mathcal{O}(n^2)\\) memory."
  },
  {
    "objectID": "slides/21-scalable-1.html#scalable-gp-methods-overview",
    "href": "slides/21-scalable-1.html#scalable-gp-methods-overview",
    "title": "Scalable Gaussian Processes #1",
    "section": "Scalable GP methods overview",
    "text": "Scalable GP methods overview\nThe computational issues motivated exploration in scalable GP methods. Existing scalable methods broadly fall under two categories.\n\n\nSparsity methods\n\nsparsity in \\(\\mathbf{C}\\), e.g., covariance tapering (Furrer, Genton, and Nychka (2006)).\nsparsity in \\(\\mathbf{C}^{-1}\\), e.g., Vecchia approximation (Vecchia (1988)) and nearest-neighbor GP (Datta et al. (2016)).\n\nLow-rank methods\n\napproximate \\(\\mathbf{C}\\) on a low-dimensional subspace.\ne.g., process convolution (Higdon (2002)), inducing point method(Snelson and Ghahramani (2005))."
  },
  {
    "objectID": "slides/21-scalable-1.html#hilbert-space-method-for-gp",
    "href": "slides/21-scalable-1.html#hilbert-space-method-for-gp",
    "title": "Scalable Gaussian Processes #1",
    "section": "Hilbert space method for GP",
    "text": "Hilbert space method for GP\n\nSolin and Särkkä (2020) introduced a Hilbert space method for reduced-rank Gaussian process regression (HSGP).\nRiutort-Mayol et al. (2023) discussed how to practically implement HSGP.\nTutorial codes are available in different probabilistic programming languages:\n\nstan\nNumPyro\npyMC"
  },
  {
    "objectID": "slides/21-scalable-1.html#outline",
    "href": "slides/21-scalable-1.html#outline",
    "title": "Scalable Gaussian Processes #1",
    "section": "Outline",
    "text": "Outline\nLecture today:\n\nWhat is HSGP?\nTheory behind HSGP\nWhy HSGP is scalable?\nHow to use HSGP to make out-of-sample predictions?\n\nLecture on Thursday:\n\nParameter tuning for HSGP\nHow to implement HSGP in stan\nCodes demo for the Anemia dataset\nApplication exercise"
  },
  {
    "objectID": "slides/21-scalable-1.html#key-idea-of-hsgp",
    "href": "slides/21-scalable-1.html#key-idea-of-hsgp",
    "title": "Scalable Gaussian Processes #1",
    "section": "Key idea of HSGP",
    "text": "Key idea of HSGP\nHSGP approximates \\(\\Sigma\\) with\n\\[\\Sigma \\approx \\Phi(\\mathbf{u}) \\Lambda(\\psi) \\Phi(\\mathbf{u})^T,\\] where\n\nlet \\(m\\) denote the number of basis functions\n\\(\\Phi(\\mathbf{u}) \\in \\mathbb{R}^{q \\times m}\\) only depends on the observed locations\n\\(\\Lambda(\\psi) \\in \\mathbb{R}^{m \\times m}\\) is diagonal\n\nComputation cost for the GP in each MCMC iteration reduces to \\(\\mathcal{O}(qm + m)\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#definitions",
    "href": "slides/21-scalable-1.html#definitions",
    "title": "Scalable Gaussian Processes #1",
    "section": "Definitions",
    "text": "Definitions\nLet \\(\\mathcal{U} \\in \\mathbb{R}^d\\), \\(d=2\\) be the geographical area of interest. For any locations \\(u,u' \\in \\mathcal{U}\\), let \\(r=u-u'\\). A covariance function \\(K\\) is\n\nstationary (translation invariant) if\n\n\\[K(u,u') = K(r)\\]\n\nisotropic (rotation invariant) if\n\n\\[K(u,u')=K(\\|r\\|)\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#spectral-density",
    "href": "slides/21-scalable-1.html#spectral-density",
    "title": "Scalable Gaussian Processes #1",
    "section": "Spectral density",
    "text": "Spectral density\nBy the Bochner’s theorem, a bounded stationary covariance function \\(K\\) can be represented as\n\\[K(r) = \\frac 1{(2\\pi)^d} \\int \\exp(i w^Tr) \\mu(dw)\\]\nfor some positive finite measure \\(\\mu\\). If \\(\\mu\\) has a density \\(S(w)\\) with respect to the Lebesgue measure, it is called the spectral density.\nBy the Wiener-Khintchin theorem, \\(K(r)\\) and \\(S(w)\\) are Fourier duals: \\[\n\\begin{align}\n  K(r) &= \\frac 1{(2\\pi)^d} \\int \\exp(iw^Tr) S(w) dw \\\\\n  S(w) &= \\int K(r) \\exp(-i w^Tr) dr.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#polynomial-expansion-of-the-spectral-density",
    "href": "slides/21-scalable-1.html#polynomial-expansion-of-the-spectral-density",
    "title": "Scalable Gaussian Processes #1",
    "section": "Polynomial expansion of the spectral density",
    "text": "Polynomial expansion of the spectral density\nFor an isotropic covariance function \\(K(\\|r\\|)\\), the corresponding spectral density \\(S(w)=S(\\|w\\|)\\).\nFor regular covariance functions, \\(S\\) admits a closed form polynomial expansion of \\(\\|w\\|^2\\):\n\\[S(\\|w\\|) = a_0 + a_1(\\|w\\|^2) + a_2(\\|w\\|^2)^2 + \\dots \\tag{1}\\]\nNote that \\(-\\|w\\|^2\\) is the transfer function for the Laplace operator \\(\\nabla^2\\). Let \\(\\mathcal{F}\\) denote the Fourier transform operator, \\(f\\) denote a twice-differentiable real-valued function,\n\\[[\\mathcal{F}(\\nabla^2 f)](w) = -\\|w\\|^2 [\\mathcal{F}(f)](w).\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#connection-with-the-laplacian",
    "href": "slides/21-scalable-1.html#connection-with-the-laplacian",
    "title": "Scalable Gaussian Processes #1",
    "section": "Connection with the Laplacian",
    "text": "Connection with the Laplacian\nHence applying inverse Fourier transform on both sides of equation (1), we have\n\\[\\mathcal{K} = a_0 + a_1(-\\nabla^2) + a_2(-\\nabla^2)^2 + \\dots \\tag{2}\\]\nwhere \\(\\mathcal{K}\\) is the covariance operator such that\n\\[\\mathcal{K}f(u) = \\int K(u,u')f(u')du'.\\]\nEquation (2) shows the connection between \\(\\nabla^2\\) and \\(\\mathcal{K}\\). If we are able to approximate \\(\\nabla^2\\), we can approximate \\(\\mathcal{K}\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#eigenfunction-expansion-of-the-laplacian",
    "href": "slides/21-scalable-1.html#eigenfunction-expansion-of-the-laplacian",
    "title": "Scalable Gaussian Processes #1",
    "section": "Eigenfunction expansion of the Laplacian",
    "text": "Eigenfunction expansion of the Laplacian\nOn a compact set \\(\\Omega \\subset \\mathbb{R}^d\\), \\(-\\nabla^2\\) has a discrete spectrum. Consider the eigenvalue problem for \\(-\\nabla^2\\) with Dirichlet boundary conditions: \\[\n\\begin{align}\n  \\begin{cases}\n    -\\nabla^2 \\phi_j(x) = \\lambda_j \\phi_j(x), & x \\in \\Omega \\\\\n    \\phi_j(x) = 0, & x \\in \\partial \\Omega\n  \\end{cases}\n\\end{align}\n\\] With sufficiently smooth boundary \\(\\partial \\Omega\\), eigenvalues and eigenfunctions exist. And because \\(-\\nabla^2\\) is positive definite Hermitian,\n\nall the eigenvalues \\(\\lambda_j\\)’s are real and positive\nthe eigenfunctions \\(\\phi_j\\)’s are orthonormal\n\n\\[\\int_\\Omega \\phi_i(u) \\phi_j(u) du = 1_{(i=j)}.\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#eigenfunction-expansion-of-the-laplacian-1",
    "href": "slides/21-scalable-1.html#eigenfunction-expansion-of-the-laplacian-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "Eigenfunction expansion of the Laplacian",
    "text": "Eigenfunction expansion of the Laplacian\nTherefore on \\(\\Omega\\), for sufficiently smooth function \\(f\\),\n\\[-\\nabla^2 f(u) = \\int L(u,u')f(u')du',\\] where the kernel \\[L(u,u') = \\sum_{j=1}^\\infty \\lambda_j \\phi_j(u) \\phi_j(u').\\]\nBy orthonormality,\n\\[L^s(u,u')=\\sum_{j=1}^\\infty \\lambda_j^s \\phi_j(u) \\phi_j(u').\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#low-rank-approximation",
    "href": "slides/21-scalable-1.html#low-rank-approximation",
    "title": "Scalable Gaussian Processes #1",
    "section": "Low rank approximation",
    "text": "Low rank approximation\nTherefore on \\(\\Omega\\) under the boundary conditions,\n\\[\n\\begin{align}\n  \\mathcal{K}f(u) &= [a_0 + a_1(-\\nabla^2) + a_2(-\\nabla^2)^2 + \\dots]f(u) \\\\\n  &= \\int [a_0 + a_1 L(u,u') + a_2L^2(u,u') + \\dots]f(u')du'.\n\\end{align}\n\\] Recall \\(\\mathcal{K}f(u)= \\int K(u,u') f(u') du'\\). Hence\n\\[\n\\begin{align}\n  K(u,u') &= a_0 + a_1 L(u,u') + a_2L^2(u,u') + \\dots \\\\\n  &= \\sum_{j=1}^\\infty (a_0 + a_1 \\lambda_j + a_2 \\lambda_j^2 + \\dots) \\phi_j(u) \\phi_j(u') \\\\\n  &= \\sum_{j=1}^\\infty S(\\sqrt{\\lambda_j})\\phi_j(u) \\phi_j(u').\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#low-rank-approximation-1",
    "href": "slides/21-scalable-1.html#low-rank-approximation-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "Low rank approximation",
    "text": "Low rank approximation\nOverall, we can approximate the covariance function with\n\\[\n\\begin{align}\n  K(u,u') &\\approx \\sum_{j=1}^\\infty S(\\sqrt{\\lambda_j})\\phi_j(u) \\phi_j(u') \\\\\n  &\\approx \\sum_{j=1}^m S(\\sqrt{\\lambda_j})\\phi_j(u) \\phi_j(u').\n\\end{align}\n\\]\n\nthe spectral density \\(S\\) depends on the covariance function \\(K\\)\nthe eigenvalue \\(\\lambda_j\\)’s and eigenfunction \\(\\phi_j\\)’s only depend on the chosen compact domain \\(\\Omega\\) and are independent of \\(K\\)\neven with an infinite sum, this is still an approximation as the eigenexpansion is restricted to \\(\\Omega\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#key-takeaways",
    "href": "slides/21-scalable-1.html#key-takeaways",
    "title": "Scalable Gaussian Processes #1",
    "section": "Key takeaways",
    "text": "Key takeaways\nHSGP approximates the GP covariance function via an eigenfunction expansion of the Laplace operator in a compact set \\(\\Omega\\).\nHSGP only works:\n\nfor covariance functions which admits a power spectral density, e.g., the Matern family.\nunder a user-specified compact domain \\(\\Omega\\).\nunder a user-specified number of basis functions \\(m\\). The approximation can be made arbitrarily accurate as \\(m\\) and \\(\\Omega\\) increase.\nfor \\(d \\le 3\\), at most \\(4\\), because given an accuracy level, \\(m\\) scales exponentially in \\(d\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-summary",
    "href": "slides/21-scalable-1.html#hsgp-summary",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP Summary",
    "text": "HSGP Summary\nRecall, we want to model a spatial intercept \\[\\boldsymbol{\\theta}(\\mathbf{u}) \\sim N(0,\\Sigma), \\quad \\Sigma \\in \\mathbb{R}^{q \\times q}, \\quad \\Sigma_{ij}=K(u_i,u_j|\\psi).\\]\nFor isotropic and nice covariance function \\(K\\), we can use HSGP to approximate \\(\\Sigma\\) as\n\\[\n\\begin{align}\n  &\\Sigma_{ik} \\approx \\sum_{j=1}^m S_{K,\\psi}(\\sqrt{\\lambda_j})\\phi_j(u_i) \\phi_j(u_k) \\\\\n  \\implies & \\Sigma \\approx \\Phi(\\mathbf{u})\\Lambda(\\psi)\\Phi(\\mathbf{u})^T, \\quad \\text{where}\n\\end{align}\n\\]\n\n\\(\\Lambda(\\psi) =\\) diag\\((S_{K,\\psi}(\\sqrt{\\lambda_1}), \\dots, S_{K,\\psi}(\\sqrt{\\lambda_m}))\\)\n\\(\\Phi(\\mathbf{u})\\) is a \\(q \\times m\\) matrix with \\(\\Phi(\\mathbf{u})_{ij} = \\phi_j(\\mathbf{u}_i)\\)"
  },
  {
    "objectID": "slides/21-scalable-1.html#why-hsgp-is-scalable",
    "href": "slides/21-scalable-1.html#why-hsgp-is-scalable",
    "title": "Scalable Gaussian Processes #1",
    "section": "Why HSGP is scalable",
    "text": "Why HSGP is scalable\n\\[\\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top.\\]\n\n\\(\\boldsymbol{\\Phi}\\) only depends on \\(\\boldsymbol{\\Theta}\\) and the observed locations, can be pre-calculated.\nNo matrix inversion.\nEach MCMC iteration requires \\(\\mathcal{O}(nm + m)\\) flops, vs \\(\\mathcal{O}(n^3)\\) for a full GP.\nIdeally \\(m \\ll n\\), but HSGP can be faster even for \\(m&gt;n\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#model-reparameterization",
    "href": "slides/21-scalable-1.html#model-reparameterization",
    "title": "Scalable Gaussian Processes #1",
    "section": "Model reparameterization",
    "text": "Model reparameterization\nUnder HSGP, approximately \\[\\boldsymbol{\\theta} \\overset{d}{=} \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}, \\quad \\mathbf{b} \\sim N_m(0,\\mathbf{I}).\\]\nTherefore we can reparameterize the model as\n\\[\n\\begin{align}\n  \\mathbf{Y} &= \\alpha \\mathbf{1}_{N} + X\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon} \\\\\n  &\\approx \\alpha \\mathbf{1}_{N} + X\\boldsymbol{\\beta} + \\underbrace{\\mathbf{Z}\\boldsymbol{\\Phi} \\mathbf{S}^{1/2}}_{\\mathbf{W}}\\mathbf{b} + \\boldsymbol{\\epsilon}\n\\end{align}\n\\]\nNote the resemblance to linear regression:\n\n\\(\\mathbf{W} \\in \\mathbb{R}^{n \\times m}\\) is a known design matrix given parameters \\(\\tau\\) and \\(\\rho\\).\n\\(\\mathbf{b}\\) is an unknown parameter vector with prior \\(N_m(0,\\mathbf{I})\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-model-in-stan",
    "href": "slides/21-scalable-1.html#hsgp-model-in-stan",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP model in stan",
    "text": "HSGP model in stan\nSimilarly, we can use the reparameterized model in stan.\nStan documentation calls it the non-centered parameterization, and suggests it’s usually more computationally efficient.\n\ntransformed data {\n  matrix[q,m] PHI = ...;\n}\nparameters {\n  vector[m] z; // standard normal\n}\nmodel {\n  vector[q] theta = PHI * sqrt_Lambda .* z;\n  \n  target += normal_lupdf(y | X * beta + W * theta, sigma);\n  target += normal_lupdf(z | 0, 1);\n  ...\n}"
  },
  {
    "objectID": "slides/21-scalable-1.html#out-of-sample-prediction",
    "href": "slides/21-scalable-1.html#out-of-sample-prediction",
    "title": "Scalable Gaussian Processes #1",
    "section": "Out-of-sample prediction",
    "text": "Out-of-sample prediction\nWe are also interested in \\(\\boldsymbol{\\theta}(\\mathbf{u}^*)\\), the spatial intercepts at \\(q^*\\) new locations \\(u^*_1, \\dots u^*_{q^*}\\). Recall under the GP, \\[\n\\begin{align}\n  \\begin{pmatrix}\n    \\boldsymbol{\\theta}(\\mathbf{u}) \\\\\n    \\boldsymbol{\\theta}(\\mathbf{u}^*)\n  \\end{pmatrix} \\sim N \\left(0,\n  \\begin{pmatrix}\n    \\Sigma & C \\\\\n    C^T & \\Sigma^*\n  \\end{pmatrix} \\right),\n\\end{align}\n\\] where \\(\\Sigma^* \\in \\mathbb{R}^{q^* \\times q^*}\\) is the covariance matrix for \\(\\boldsymbol{\\theta}(\\mathbf{u}^*)\\), and \\(C\\) is the cross covariance matrix between \\(\\boldsymbol{\\theta}(\\mathbf{u})\\) and \\(\\boldsymbol{\\theta}(\\mathbf{u}^*)\\).\nThe conditional distribution is \\[\\boldsymbol{\\theta}(\\mathbf{u}^*) \\mid \\boldsymbol{\\theta}(\\mathbf{u}) \\sim N(C^T \\Sigma^{-1}\\boldsymbol{\\theta}(\\mathbf{u}), \\Sigma^*-C^T\\Sigma^{-1}C).\\] For each posterior sample \\(\\boldsymbol{\\theta}(\\mathbf{u})^{(s)}\\), we obtain a posterior predictive sample \\(\\boldsymbol{\\theta}(\\mathbf{u}^*)^{(s)}\\) by drawing from \\(\\boldsymbol{\\theta}(\\mathbf{u}^*) \\mid \\boldsymbol{\\theta}(\\mathbf{u})^{(s)}\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#out-of-sample-prediction-under-hsgp",
    "href": "slides/21-scalable-1.html#out-of-sample-prediction-under-hsgp",
    "title": "Scalable Gaussian Processes #1",
    "section": "Out-of-sample prediction under HSGP",
    "text": "Out-of-sample prediction under HSGP\nUnder HSGP, approximately \\[\n\\begin{align}\n  \\begin{pmatrix}\n    \\boldsymbol{\\theta}(\\mathbf{u}) \\\\\n    \\boldsymbol{\\theta}(\\mathbf{u}^*)\n  \\end{pmatrix} \\sim N \\left(0,\n  \\begin{pmatrix}\n    \\Phi(\\mathbf{u})\\Lambda(\\psi) \\Phi(\\mathbf{u})^T & \\Phi(\\mathbf{u})\\Lambda(\\psi) \\Phi(\\mathbf{u}^*)^T \\\\\n    \\Phi(\\mathbf{u}^*)\\Lambda(\\psi) \\Phi(\\mathbf{u})^T & \\Phi(\\mathbf{u}^*)\\Lambda(\\psi) \\Phi(\\mathbf{u}^*)^T\n  \\end{pmatrix} \\right).\n\\end{align}\n\\]\n\nif \\(m \\ge q\\), use the same approach\nif \\(m &lt; q\\) so that \\(\\Phi(\\mathbf{u})\\Lambda(\\psi) \\Phi(\\mathbf{u})^T\\) is not invertible, \\[\\boldsymbol{\\theta}(\\mathbf{u}^*) \\mid \\boldsymbol{\\theta}(\\mathbf{u}) = (\\Phi(\\mathbf{u}^*)\\Lambda(\\psi) \\Phi(\\mathbf{u})^T) \\Sigma^+\\boldsymbol{\\theta}(\\mathbf{u})\\] where \\(\\Sigma^+\\) is the pseudo inverse of \\(\\Phi(\\mathbf{u})\\Lambda(\\psi) \\Phi(\\mathbf{u})^T\\) such that \\(\\Phi(\\mathbf{u})\\Lambda(\\psi) \\Phi(\\mathbf{u})^T\\Sigma^+ = \\mathbf{I}\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#out-of-sample-prediction-under-hsgp-1",
    "href": "slides/21-scalable-1.html#out-of-sample-prediction-under-hsgp-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "Out-of-sample prediction under HSGP",
    "text": "Out-of-sample prediction under HSGP\nUnder the reparamterized model, \\[\n\\begin{align}\n  \\boldsymbol{\\theta}(\\mathbf{u}^*) \\mid \\boldsymbol{\\theta}(\\mathbf{u}) &= (\\Phi(\\mathbf{u}^*)\\Lambda(\\psi) \\Phi(\\mathbf{u})^{\\top})   \\Sigma^+\\boldsymbol{\\theta}(\\mathbf{u}) \\\\\n  &= (\\Phi(\\mathbf{u}^*)\\Lambda(\\psi) \\Phi(\\mathbf{u})^T) \\Sigma^+\\Phi(\\mathbf{u}) \\Lambda(\\psi)^{1/2}\\mathbf{z} \\\\\n  &= \\Phi(\\mathbf{u}^*)\\Lambda(\\psi)^{1/2}\\mathbf{z}.\n\\end{align}\n\\]\nTherefore for each posterior sample of \\(\\mathbf{z}\\) and \\(\\psi\\), we can obtain a posterior predictive sample for \\(\\boldsymbol{\\theta}(\\mathbf{u}^*)\\) as \\[\\boldsymbol{\\theta}(\\mathbf{u}^*)^{(s)} = \\Phi(\\mathbf{u}^*)\\Lambda(\\psi^{(s)})^{1/2}\\mathbf{z}^{(s)}.\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-prediction-in-stan",
    "href": "slides/21-scalable-1.html#hsgp-prediction-in-stan",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP prediction in stan",
    "text": "HSGP prediction in stan\nEasy to implement in stan.\n\ntransformed data {\n  matrix[q_pred,m] PHI_pred = ...;\n}\ngenerated quantities {\n  vector[q_pred] theta_pred = PHI_pred * sqrt_Lambda .* z;\n}"
  },
  {
    "objectID": "slides/21-scalable-1.html#recap",
    "href": "slides/21-scalable-1.html#recap",
    "title": "Scalable Gaussian Processes #1",
    "section": "Recap",
    "text": "Recap\nHSGP is a low rank approximation method for GP.\n\\[\n\\begin{align}\n  \\mathbf{C}_{ij} \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j), \\quad \\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top,\n\\end{align}\n\\]\n\nfor covariance function \\(C\\) which admits a power spectral density.\non a box \\(\\boldsymbol{\\Theta} \\subset \\mathbb{R}^d\\).\nwith \\(m\\) number of basis functions.\n\nWe have talked about:\n\nwhy HSGP is scalable.\nhow to do posterior sampling and posterior predictive sampling in stan."
  },
  {
    "objectID": "slides/21-scalable-1.html#next-lecture",
    "href": "slides/21-scalable-1.html#next-lecture",
    "title": "Scalable Gaussian Processes #1",
    "section": "Next lecture",
    "text": "Next lecture\n\nHow to choose the compact domain \\(\\Omega\\)?\nHow to choose number of basis function \\(m\\)?\nWhich covariance functions can we use HSGP for?\nHow to implement HSGP?"
  },
  {
    "objectID": "slides/21-scalable-1.html#references",
    "href": "slides/21-scalable-1.html#references",
    "title": "Scalable Gaussian Processes #1",
    "section": "References",
    "text": "References\n\n\nDatta, Abhirup, Sudipto Banerjee, Andrew O Finley, and Alan E Gelfand. 2016. “Hierarchical Nearest-Neighbor Gaussian Process Models for Large Geostatistical Datasets.” Journal of the American Statistical Association 111 (514): 800–812.\n\n\nFurrer, Reinhard, Marc G Genton, and Douglas Nychka. 2006. “Covariance Tapering for Interpolation of Large Spatial Datasets.” Journal of Computational and Graphical Statistics 15 (3): 502–23.\n\n\nHigdon, Dave. 2002. “Space and Space-Time Modeling Using Process Convolutions.” In Quantitative Methods for Current Environmental Issues, 37–56. Springer.\n\n\nRiutort-Mayol, Gabriel, Paul-Christian Bürkner, Michael R Andersen, Arno Solin, and Aki Vehtari. 2023. “Practical Hilbert Space Approximate Bayesian Gaussian Processes for Probabilistic Programming.” Statistics and Computing 33 (1): 17.\n\n\nSnelson, Edward, and Zoubin Ghahramani. 2005. “Sparse Gaussian Processes Using Pseudo-Inputs.” Advances in Neural Information Processing Systems 18.\n\n\nSolin, Arno, and Simo Särkkä. 2020. “Hilbert Space Methods for Reduced-Rank Gaussian Process Regression.” Statistics and Computing 30 (2): 419–46.\n\n\nVecchia, Aldo V. 1988. “Estimation and Model Identification for Continuous Spatial Processes.” Journal of the Royal Statistical Society Series B: Statistical Methodology 50 (2): 297–312."
  },
  {
    "objectID": "slides/19-disease-mapping.html#disease-mapping",
    "href": "slides/19-disease-mapping.html#disease-mapping",
    "title": "Disease Mapping",
    "section": "Disease Mapping",
    "text": "Disease Mapping\nBayesian hierarchical models enable to obtain smoothed disease relative risks by including covariates and random effects to borrow information from neighboring areas. Spatial disease risk models are commonly specified using a Poisson distribution for the observed number of cases (\\(Y_i\\)) with mean equal to the expected number of cases (\\(E_i\\)) times the relative risk (\\(\\lambda_i\\)) corresponding to area \\(i\\), \\(i = 1,\\ldots,n\\),\n\\[\\begin{aligned}\nY_i \\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i)\\\\\n\\log \\lambda_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\theta_i + \\epsilon_i\n\\end{aligned}\\]\nHere, the logarithm of \\(\\lambda_i\\) is expressed as a sum of fixed effects to quantify the effects of the covariates on the disease risk, and random effects that represent residual variation that is not explained by the available covariates. The fixed effects are expressed using a vector of intercept and\nSpatial random effects \\(\\theta_i\\) that smooth data according to a neighborhood structure are included to acknowledge that data may be spatially correlated, and relative risks in neighboring areas may be more similar than relative risks in areas that are further away Unstructured exchangeable components \\(\\epsilon_i\\) are also included to model uncorrelated noise.\nThe relative risk \\(\\lambda_i\\) quantifies whether an area \\(i\\) has higher (\\(\\lambda_i &gt; 1\\)) or lower (\\(\\lambda_i &lt; 1\\)) risk than the average risk in the standard population (e.g., the whole population of the study region). For example, \\(\\lambda_i = 2\\) indicates the risk of area \\(i\\) is two times the average risk in the standard population."
  },
  {
    "objectID": "slides/19-disease-mapping.html#standardized-mortality-ratios",
    "href": "slides/19-disease-mapping.html#standardized-mortality-ratios",
    "title": "Disease Mapping",
    "section": "Standardized Mortality Ratios",
    "text": "Standardized Mortality Ratios"
  },
  {
    "objectID": "slides/19-disease-mapping.html#compute-adjacency-matrix",
    "href": "slides/19-disease-mapping.html#compute-adjacency-matrix",
    "title": "Disease Mapping",
    "section": "Compute Adjacency Matrix",
    "text": "Compute Adjacency Matrix\nTo compute the adjacency matrix of an sf data object we can use the spdep library.\n\nneighbors &lt;- spdep::poly2nb(covid_nc_2020) # computes the neighborhood structure\nW &lt;- spdep::nb2mat(neighbors, style = \"B\", zero.policy = TRUE) # converts to an n x n matrix\n\n\nstyle = \"B\" specifies binary encoding (1 if neighbors, 0 if not).\nzero.policy = TRUE ensures the function works even if some counties do not have neighbors."
  },
  {
    "objectID": "slides/19-disease-mapping.html#prepare-adjacnecy-for-stan",
    "href": "slides/19-disease-mapping.html#prepare-adjacnecy-for-stan",
    "title": "Disease Mapping",
    "section": "Prepare Adjacnecy for Stan",
    "text": "Prepare Adjacnecy for Stan\n\n# Get the row-column pairs where the adjacency matrix is 1\n# This will return all non-zero indices in the adjacency matrix\nneighbor_pairs &lt;- which(adj_matrix == 1, arr.ind = TRUE)\n\n# Filter out the upper triangle (to avoid repeating edges)\nneighbor_pairs_lower &lt;- neighbor_pairs[neighbor_pairs[, 1] &lt; neighbor_pairs[, 2], ]"
  },
  {
    "objectID": "slides/19-disease-mapping.html#better-adjacency-plot",
    "href": "slides/19-disease-mapping.html#better-adjacency-plot",
    "title": "Disease Mapping",
    "section": "Better Adjacency Plot",
    "text": "Better Adjacency Plot"
  },
  {
    "objectID": "slides/19-disease-mapping.html#fit-the-stan-model",
    "href": "slides/19-disease-mapping.html#fit-the-stan-model",
    "title": "Disease Mapping",
    "section": "Fit the Stan Model",
    "text": "Fit the Stan Model\n\nX &lt;- model.matrix(~ age + poverty, data = covid_nc_2020)[, -1]\nstan_data &lt;- list(\n  n = nrow(covid_nc_2020),\n  p = ncol(X),\n  n_edges = nrow(neighbor_pairs_lower),\n  node1 = neighbor_pairs_lower[, 1],\n  node2 = neighbor_pairs_lower[, 2],\n  Y = covid_nc_2020$obs_deaths,\n  E = covid_nc_2020$est_deaths,\n  X = X\n)\nicar &lt;- stan_model(\"icar.stan\")\nfit_icar &lt;- sampling(icar, stan_data, pars = c(\"z1\", \"z2\", \"epsilon\", \"log_mu\", \"lp__\"), include = FALSE, iter = 10000)"
  },
  {
    "objectID": "slides/19-disease-mapping.html#looking-at-smr-observed-versus-ppd",
    "href": "slides/19-disease-mapping.html#looking-at-smr-observed-versus-ppd",
    "title": "Disease Mapping",
    "section": "Looking at SMR observed versus PPD",
    "text": "Looking at SMR observed versus PPD"
  },
  {
    "objectID": "slides/19-disease-mapping.html#final-map",
    "href": "slides/19-disease-mapping.html#final-map",
    "title": "Disease Mapping",
    "section": "Final Map",
    "text": "Final Map"
  },
  {
    "objectID": "slides/19-disease-mapping.html#compute-ppd",
    "href": "slides/19-disease-mapping.html#compute-ppd",
    "title": "Disease Mapping",
    "section": "Compute PPD",
    "text": "Compute PPD\n\nY_pred &lt;- rstan::extract(fit_icar, pars = \"Y_pred\")$Y_pred\nppd_oe &lt;- sweep(Y_pred, 2, stan_data$E, \"/\")\nppd_mean &lt;- apply(ppd_oe, 2, mean)\nppd_sd &lt;- apply(ppd_oe, 2, sd)\ncovid_nc_2020$ppd_mean &lt;- ppd_mean\ncovid_nc_2020$ppd_sd &lt;- ppd_sd"
  },
  {
    "objectID": "slides/19-disease-mapping.html#standardized-mortality-ratios-1",
    "href": "slides/19-disease-mapping.html#standardized-mortality-ratios-1",
    "title": "Disease Mapping",
    "section": "Standardized Mortality Ratios",
    "text": "Standardized Mortality Ratios\nThe Standardized Mortality Ratio (SMR) is a ratio of the observed number of deaths to the expected number of deaths in a population.\nIt is often used in epidemiology and public health to assess mortality in a population relative to a standard or reference population.\nFormula:\n\\[\\text{SMR} = \\frac{\\text{Total Observed Deaths}}{\\text{Total Expected Deaths}}\\]"
  },
  {
    "objectID": "slides/19-disease-mapping.html#smr-simplified-example",
    "href": "slides/19-disease-mapping.html#smr-simplified-example",
    "title": "Disease Mapping",
    "section": "SMR: Simplified Example",
    "text": "SMR: Simplified Example\nWe will calculate SMR using the overall population proportions without adjusting for age, sex, or other factors.\nConsider the following data for different counties:\n\n\n\nCounty\nObserved Deaths\nPopulation\n\n\n\n\nCounty A\n10\n20,000\n\n\nCounty B\n15\n30,000\n\n\nCounty C\n5\n10,000"
  },
  {
    "objectID": "slides/19-disease-mapping.html#smr-step-1---calculate-expected-deaths",
    "href": "slides/19-disease-mapping.html#smr-step-1---calculate-expected-deaths",
    "title": "Disease Mapping",
    "section": "SMR: Step 1 - Calculate Expected Deaths",
    "text": "SMR: Step 1 - Calculate Expected Deaths\nFirst, we calculate the Expected Deaths for each county based on the overall death rate in the standard population.\nFor simplicity, let’s assume the standard death rate is 0.0005 for all counties.\n\\[\\text{Expected Deaths} = \\text{Death Rate} \\times \\text{Population}\\]\nFor each county:\n\nCounty A: \\(0.0005 \\times 20,000 = 10\\)\nCounty B: \\(0.0005 \\times 30,000 = 15\\)\nCounty C: \\(0.0005 \\times 10,000 = 5\\)"
  },
  {
    "objectID": "slides/19-disease-mapping.html#smr-step-2---calculate-total-observed-and-expected-deaths",
    "href": "slides/19-disease-mapping.html#smr-step-2---calculate-total-observed-and-expected-deaths",
    "title": "Disease Mapping",
    "section": "SMR: Step 2 - Calculate Total Observed and Expected Deaths",
    "text": "SMR: Step 2 - Calculate Total Observed and Expected Deaths\nNext, sum the observed deaths and expected deaths across all counties:\n\nTotal Observed Deaths = $ 10 + 15 + 5 = 30 $\nTotal Expected Deaths = $ 10 + 15 + 5 = 30 $"
  },
  {
    "objectID": "slides/19-disease-mapping.html#smr-step-3---calculate-smr",
    "href": "slides/19-disease-mapping.html#smr-step-3---calculate-smr",
    "title": "Disease Mapping",
    "section": "SMR: Step 3 - Calculate SMR",
    "text": "SMR: Step 3 - Calculate SMR\nNow, we compute the Standardized Mortality Ratio (SMR):\n\\[\\text{SMR} = \\frac{\\text{Total Observed Deaths}}{\\text{Total Expected Deaths}} = \\frac{30}{30} = 1\\]\nAn SMR of 1 means that the observed number of deaths is exactly what was expected based on the standard population."
  },
  {
    "objectID": "slides/19-disease-mapping.html#interpretation-of-smr",
    "href": "slides/19-disease-mapping.html#interpretation-of-smr",
    "title": "Disease Mapping",
    "section": "Interpretation of SMR",
    "text": "Interpretation of SMR\nThe Standardized Mortality Ratio (SMR) helps in understanding the relative mortality in a population compared to a standard population.\n\nSMR = 1: The observed deaths are equal to the expected deaths.\nSMR &gt; 1: The observed deaths exceed the expected deaths, suggesting excess mortality.\nSMR &lt; 1: The observed deaths are fewer than the expected deaths, suggesting lower mortality.\n\nIn this simplified version, we only considered overall population proportions. In real-world applications, age, sex, and other demographic factors are typically accounted for to provide a more accurate SMR."
  },
  {
    "objectID": "slides/19-disease-mapping.html#standardized-mortality-ratios-2",
    "href": "slides/19-disease-mapping.html#standardized-mortality-ratios-2",
    "title": "Disease Mapping",
    "section": "Standardized Mortality Ratios",
    "text": "Standardized Mortality Ratios"
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html",
    "href": "ae/ae-08-disease-mapping.html",
    "title": "AE 08: Disease Mapping",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-08-disease-mapping.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 08: Disease Mapping",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-08-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-08.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#covid-death",
    "href": "ae/ae-08-disease-mapping.html#covid-death",
    "title": "AE 08: Disease Mapping",
    "section": "COVID Death",
    "text": "COVID Death\nToday, we will motivate areal spatial data analysis and disease mapping by studying deaths attributed to COVID in 2020 across North Carolina at the county level. The data object covid_nc_2020 is an sf object.\n\nVariables are:\n\nname: county name.\npopulation: 2020 population.\nobs_deaths: observed number of COVID-related deaths in 2020.\nest_deaths: estimated number of COVID-related deaths in 2020.\nsmr: standardized mortality ratio.\nage: precentage of residents over 60 years of age.\npoverty: percentage of residents below the poverty line.\ngeometry: contains centroid and boundary information for each county.\n\n\nThe data set is available in your AE repos and is called covid_nc_2020.shp.\n\ncovid_nc_2020 &lt;- st_read(\"covid_nc_2020.shp\")"
  },
  {
    "objectID": "slides/19-disease-mapping.html#covid-mortality",
    "href": "slides/19-disease-mapping.html#covid-mortality",
    "title": "Disease Mapping",
    "section": "COVID Mortality",
    "text": "COVID Mortality"
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-1-introduction-to-disease-mapping",
    "href": "slides/19-disease-mapping.html#slide-1-introduction-to-disease-mapping",
    "title": "Disease Mapping",
    "section": "Slide 1: Introduction to Disease Mapping",
    "text": "Slide 1: Introduction to Disease Mapping\nWhat is Disease Mapping?\nDisease mapping is a way of visualizing and analyzing geographic variations in health outcomes, such as mortality or disease incidence, across different regions (e.g., counties or neighborhoods).\nIt helps us identify regions with unusually high or low health outcomes, which could be indicative of underlying health disparities."
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-2-the-challenge-with-raw-health-data",
    "href": "slides/19-disease-mapping.html#slide-2-the-challenge-with-raw-health-data",
    "title": "Disease Mapping",
    "section": "Slide 2: The Challenge with Raw Health Data",
    "text": "Slide 2: The Challenge with Raw Health Data\nThe Problem with Raw Counts\nImagine you want to compare the number of deaths across counties in a state, like North Carolina. If we simply look at raw death counts, we might be misled:\n\nLarger counties with more people may have more deaths simply due to their larger population.\nSmaller counties may appear “healthier” simply because they have fewer people, not because they have lower mortality rates.\n\nThus, raw death counts are not enough to draw meaningful comparisons."
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-3-disease-mapping-needs-adjustments",
    "href": "slides/19-disease-mapping.html#slide-3-disease-mapping-needs-adjustments",
    "title": "Disease Mapping",
    "section": "Slide 3: Disease Mapping Needs Adjustments",
    "text": "Slide 3: Disease Mapping Needs Adjustments\nThe Need for Adjustment in Disease Mapping\n\nTo make fair comparisons between regions of different sizes, we need to adjust for population size.\nWithout these adjustments, it’s hard to determine if a county’s high death count is due to its population size or if there’s something unique about the county (e.g., healthcare access, environmental factors) that increases the risk of mortality.\nThis is where we need more nuanced measures to adjust for population size and allow for better comparisons.\nToday we will talk about the standardized mortality ratio (SMR)"
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-4-introducing-the-standardized-mortality-ratio-smr",
    "href": "slides/19-disease-mapping.html#slide-4-introducing-the-standardized-mortality-ratio-smr",
    "title": "Disease Mapping",
    "section": "Slide 4: Introducing the Standardized Mortality Ratio (SMR)",
    "text": "Slide 4: Introducing the Standardized Mortality Ratio (SMR)\nWhat is SMR?\nThe Standardized Mortality Ratio (SMR) is a way of comparing the observed number of deaths in a population to the number of deaths we would expect, given the population’s characteristics (such as population size).\nIt adjusts for differences in population size, allowing us to identify areas where deaths are higher or lower than we would expect."
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-5-the-formula-for-smr",
    "href": "slides/19-disease-mapping.html#slide-5-the-formula-for-smr",
    "title": "Disease Mapping",
    "section": "Slide 5: The Formula for SMR",
    "text": "Slide 5: The Formula for SMR\nSMR Formula\nThe SMR for a given county is calculated as:\n[ = ]\nWhere: - Observed Deaths is the actual number of deaths in the county. - Expected Deaths is the number of deaths we would expect in the county, given the population proportion of that county compared to the total population.\nThe Expected Deaths are calculated by multiplying the total number of deaths in the entire state by the proportion of the population in that county."
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-6-example-data",
    "href": "slides/19-disease-mapping.html#slide-6-example-data",
    "title": "Disease Mapping",
    "section": "Slide 6: Example Data",
    "text": "Slide 6: Example Data\n\n\n\nCounty\nObserved Deaths\nPopulation\nPopulation Proportion\n\n\n\n\nCounty A\n10\n20,000\n0.2\n\n\nCounty B\n15\n30,000\n0.3\n\n\nCounty C\n5\n10,000\n0.5"
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-7-step-1---calculate-expected-deaths",
    "href": "slides/19-disease-mapping.html#slide-7-step-1---calculate-expected-deaths",
    "title": "Disease Mapping",
    "section": "Slide 7: Step 1 - Calculate Expected Deaths",
    "text": "Slide 7: Step 1 - Calculate Expected Deaths\nThe Expected Deaths for each county are calculated by multiplying the total deaths by the population proportion for that county:\n[ = 30 = 6 ] [ = 30 = 9 ] [ = 30 = 15 ]"
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-8-step-2---compute-smr-for-each-county",
    "href": "slides/19-disease-mapping.html#slide-8-step-2---compute-smr-for-each-county",
    "title": "Disease Mapping",
    "section": "Slide 8: Step 2 - Compute SMR for Each County",
    "text": "Slide 8: Step 2 - Compute SMR for Each County\nNow, we calculate the SMR by dividing the observed deaths by the expected deaths:\n[ = = 1.67 ] [ = = 1.67 ] [ = = 0.33 ]"
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-9-interpreting-smr",
    "href": "slides/19-disease-mapping.html#slide-9-interpreting-smr",
    "title": "Disease Mapping",
    "section": "Slide 9: Interpreting SMR",
    "text": "Slide 9: Interpreting SMR\nWhat Do These Numbers Mean?\n\nSMR = 1: The observed number of deaths matches the expected number of deaths.\nSMR &gt; 1: More deaths than expected (excess mortality).\nSMR &lt; 1: Fewer deaths than expected (lower mortality).\n\nIn our example: - County A and County B have excess mortality, with SMR of 1.67. - County C has fewer deaths than expected, with SMR of 0.33."
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-10-why-use-smr-in-disease-mapping",
    "href": "slides/19-disease-mapping.html#slide-10-why-use-smr-in-disease-mapping",
    "title": "Disease Mapping",
    "section": "Slide 10: Why Use SMR in Disease Mapping?",
    "text": "Slide 10: Why Use SMR in Disease Mapping?\nThe Advantage of SMR\nWithout adjusting for population size, we may misunderstand the health outcomes in a region. Larger regions may appear to have more deaths, but this could be because they have larger populations.\nSMR allows us to: - Make meaningful comparisons across counties of different sizes. - Identify areas with excess mortality (SMR &gt; 1) and areas with lower-than-expected mortality (SMR &lt; 1).\nIn disease mapping, SMR helps us better understand spatial health disparities and identify regions that may need targeted public health interventions."
  },
  {
    "objectID": "slides/19-disease-mapping.html#slide-11-conclusion",
    "href": "slides/19-disease-mapping.html#slide-11-conclusion",
    "title": "Disease Mapping",
    "section": "Slide 11: Conclusion",
    "text": "Slide 11: Conclusion\nThe SMR is a crucial tool in disease mapping, helping us adjust for differences in population size and draw more accurate conclusions about health disparities.\nIn this presentation, we showed how disease mapping motivates the need for SMR, which provides a more nuanced and accurate picture of mortality patterns across regions.\nUsing SMR in disease mapping allows us to uncover important patterns and make informed decisions about public health priorities."
  },
  {
    "objectID": "slides/19-disease-mapping.html#introduction-to-disease-mapping",
    "href": "slides/19-disease-mapping.html#introduction-to-disease-mapping",
    "title": "Disease Mapping",
    "section": "Introduction to Disease Mapping",
    "text": "Introduction to Disease Mapping\n\nDisease mapping is a way of visualizing and analyzing geographic variations in health outcomes, such as mortality or disease incidence, across different regions (e.g., counties or neighborhoods).\nIt helps us identify regions with unusually high or low health outcomes, which could be indicative of underlying health disparities."
  },
  {
    "objectID": "slides/19-disease-mapping.html#the-challenge-with-observed-health-data",
    "href": "slides/19-disease-mapping.html#the-challenge-with-observed-health-data",
    "title": "Disease Mapping",
    "section": "The Challenge with Observed Health Data",
    "text": "The Challenge with Observed Health Data\nImagine you want to compare the number of deaths across counties in a state, like North Carolina. If we simply look at observed death counts, we might be misled:\n\nLarger counties with more people may have more deaths simply due to their larger population.\nSmaller counties may appear “healthier” simply because they have fewer people, not because they have lower mortality rates.\n\nThus, observed death counts are not enough to draw meaningful comparisons."
  },
  {
    "objectID": "slides/19-disease-mapping.html#introducing-the-standardized-mortality-ratio",
    "href": "slides/19-disease-mapping.html#introducing-the-standardized-mortality-ratio",
    "title": "Disease Mapping",
    "section": "Introducing the Standardized Mortality Ratio",
    "text": "Introducing the Standardized Mortality Ratio\n\nSMR is a way of comparing the observed number of deaths in a population to the number of deaths we would expect, given the population’s characteristics (such as population size).\nIt adjusts for differences in population size, allowing us to identify areas where deaths are higher or lower than we would expect.\nThe SMR for a given county is calculated as:\n\n\\[\\text{SMR} = \\frac{\\text{Observed Deaths}}{\\text{Expected Deaths}}\\]\n\nThe Expected Deaths are calculated by multiplying the total number of deaths in the entire state by the proportion of the population in that county."
  },
  {
    "objectID": "slides/19-disease-mapping.html#example-data",
    "href": "slides/19-disease-mapping.html#example-data",
    "title": "Disease Mapping",
    "section": "Example Data",
    "text": "Example Data\n\n\n\nCounty\nObserved Deaths\nPopulation\nPopulation Proportion\n\n\n\n\nCounty A\n10\n30,000\n0.3\n\n\nCounty B\n15\n50,000\n0.5\n\n\nCounty C\n5\n20,000\n0.2\n\n\nTotal\n30\n100,000\n1.0"
  },
  {
    "objectID": "slides/19-disease-mapping.html#step-1---calculate-expected-deaths",
    "href": "slides/19-disease-mapping.html#step-1---calculate-expected-deaths",
    "title": "Disease Mapping",
    "section": "Step 1 - Calculate Expected Deaths",
    "text": "Step 1 - Calculate Expected Deaths\nThe Expected Deaths for each county are calculated by multiplying the total deaths by the population proportion for that county:\n\\[\\begin{aligned}\n\\text{Expected Deaths for County A} = 30 \\times 0.3 &= 9\\\\\n\\text{Expected Deaths for County B} = 30 \\times 0.5 &= 15\\\\\n\\text{Expected Deaths for County C} = 30 \\times 0.2 &= 6\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/19-disease-mapping.html#step-2---compute-smr-for-each-county",
    "href": "slides/19-disease-mapping.html#step-2---compute-smr-for-each-county",
    "title": "Disease Mapping",
    "section": "Step 2 - Compute SMR for Each County",
    "text": "Step 2 - Compute SMR for Each County\nNow, we calculate the SMR by dividing the observed deaths by the expected deaths:\n\\[\\begin{aligned}\n\\text{SMR for County A} = \\frac{10}{6} &= 1.67\\\\\n\\text{SMR for County B} = \\frac{15}{9} &= 1.67\\\\\n\\text{SMR for County C} = \\frac{5}{15} &= 0.33\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/19-disease-mapping.html#nterpreting-smr",
    "href": "slides/19-disease-mapping.html#nterpreting-smr",
    "title": "Disease Mapping",
    "section": "nterpreting SMR",
    "text": "nterpreting SMR\nWhat Do These Numbers Mean?\n\nSMR = 1: The observed number of deaths matches the expected number of deaths.\nSMR &gt; 1: More deaths than expected (excess mortality).\nSMR &lt; 1: Fewer deaths than expected (lower mortality).\n\nIn our example:\n\nCounty A and County B have excess mortality, with SMR of 1.67.\nCounty C has fewer deaths than expected, with SMR of 0.33."
  },
  {
    "objectID": "slides/19-disease-mapping.html#why-use-smr-in-disease-mapping",
    "href": "slides/19-disease-mapping.html#why-use-smr-in-disease-mapping",
    "title": "Disease Mapping",
    "section": "Why Use SMR in Disease Mapping?",
    "text": "Why Use SMR in Disease Mapping?\nSMR allows us to:\n\nMake meaningful comparisons across counties of different sizes.\nIdentify areas with excess mortality (SMR &gt; 1) and areas with lower-than-expected mortality (SMR &lt; 1).\n\nIn disease mapping, SMR helps us better understand spatial health disparities and identify regions that may need targeted public health interventions."
  },
  {
    "objectID": "slides/19-disease-mapping.html#the-challenge-with-observed-data",
    "href": "slides/19-disease-mapping.html#the-challenge-with-observed-data",
    "title": "Disease Mapping",
    "section": "The Challenge with Observed Data",
    "text": "The Challenge with Observed Data\nImagine you want to compare the number of deaths across counties in a state, like North Carolina. If we simply look at observed death counts, we might be misled:\n\nLarger counties with more people may have more deaths simply due to their larger population.\nSmaller counties may appear “healthier” simply because they have fewer people, not because they have lower mortality rates.\n\nThus, observed death counts are not enough to draw meaningful comparisons."
  },
  {
    "objectID": "slides/19-disease-mapping.html#the-challenge-with-observed-data-1",
    "href": "slides/19-disease-mapping.html#the-challenge-with-observed-data-1",
    "title": "Disease Mapping",
    "section": "The Challenge with Observed Data",
    "text": "The Challenge with Observed Data\n\nTo make fair comparisons between regions of different sizes, we need to adjust for population size (and sometimes demographics).\nWithout these adjustments, it’s hard to determine if a county’s high death count is due to its population size or if there’s something unique about the county (e.g., healthcare access, environmental factors) that increases the risk of mortality.\nThis is where we need more nuanced measures to adjust for population size and allow for better comparisons.\nToday we will talk about the standardized mortality ratio (SMR)."
  },
  {
    "objectID": "slides/19-disease-mapping.html#standardized-mortality-ratio",
    "href": "slides/19-disease-mapping.html#standardized-mortality-ratio",
    "title": "Disease Mapping",
    "section": "Standardized Mortality Ratio",
    "text": "Standardized Mortality Ratio\n\nSMR is a way of comparing the observed number of deaths in a population to the number of deaths we would expect, given the population’s characteristics (such as population size).\nIt adjusts for differences in population, allowing us to identify areas where deaths are higher or lower than we would expect.\n\n\\[\\text{SMR} = \\frac{\\text{Observed Deaths}}{\\text{Expected Deaths}}\\]\n\nExpected Deaths is calculated by multiplying the total deaths across the state by the proportion of the population in that county."
  },
  {
    "objectID": "slides/19-disease-mapping.html#step-2---compute-smr",
    "href": "slides/19-disease-mapping.html#step-2---compute-smr",
    "title": "Disease Mapping",
    "section": "Step 2 - Compute SMR",
    "text": "Step 2 - Compute SMR\nNow, we calculate the SMR by dividing the observed deaths by the expected deaths:\n\\[\\begin{aligned}\n\\text{SMR for County A} &= \\frac{10}{9} = 1.11\\\\\n\\text{SMR for County B} &= \\frac{15}{15} = 1\\\\\n\\text{SMR for County C} &= \\frac{5}{6} = 0.83\n\\end{aligned}\\]\nWhat do these numbers mean?"
  },
  {
    "objectID": "slides/19-disease-mapping.html#interpreting-smr",
    "href": "slides/19-disease-mapping.html#interpreting-smr",
    "title": "Disease Mapping",
    "section": "Interpreting SMR",
    "text": "Interpreting SMR\n\nSMR = 1: The observed number of deaths matches the expected number of deaths.\nSMR &gt; 1: More deaths than expected (excess mortality).\nSMR &lt; 1: Fewer deaths than expected (lower mortality).\n\nIn our example:\n\nCounty A has excess mortality, with SMR of 1.11.\nCounty B has as many deaths as expected, with SMR of 1.\nCounty C has fewer deaths than expected, with SMR of 0.83."
  },
  {
    "objectID": "slides/19-disease-mapping.html#create-a-figure-with-plambda_i-1",
    "href": "slides/19-disease-mapping.html#create-a-figure-with-plambda_i-1",
    "title": "Disease Mapping",
    "section": "Create a figure with \\(P(\\lambda_i > 1)\\)",
    "text": "Create a figure with \\(P(\\lambda_i &gt; 1)\\)\n\ncovid_nc_2020$lambda_prob &lt;- apply(lambda, 2, function(x) mean(x &gt; 1))\ncovid_nc_2020$lambda_prob_binary &lt;- 1 * (covid_nc_2020$lambda_prob &gt; 0.95)\ncovid_nc_2020$lambda_prob_binary &lt;- as.factor(covid_nc_2020$lambda_prob_binary)\nlevels(covid_nc_2020$lambda_prob_binary) &lt;- c(\"No\", \"Yes\")\n\nggplot(data = covid_nc_2020) +\n  geom_sf(fill = \"lightblue\", color = \"black\") +\n  geom_sf(aes(fill = lambda_prob), shape = 16, size = 2) +\n  theme_minimal() +\n  labs(\n    title = \"Posterior Probability that SMR is greater than 1\",\n    fill = \"Probability\"\n  ) +\n  scale_fill_viridis_c() +  # Custom labels for the breaks\n  coord_sf()  # Ensures the map is properly projected\n\n\n\n\n\n\n\nggplot(data = covid_nc_2020) +\n  geom_sf(fill = \"lightblue\", color = \"black\") +\n  geom_sf(aes(fill = as.factor(lambda_prob_binary)), shape = 16, size = 2) +\n  theme_minimal() +\n  labs(\n    title = \"Posterior Indicator that SMR &gt; 1\",\n    fill = \"SMR &gt; 1\"\n  ) +\n  # scale_fill_viridis_b() +  # Custom labels for the breaks\n  coord_sf()  # Ensures the map is properly projected"
  },
  {
    "objectID": "slides/19-disease-mapping.html#writing-down-a-model-for-smr",
    "href": "slides/19-disease-mapping.html#writing-down-a-model-for-smr",
    "title": "Disease Mapping",
    "section": "Writing down a model for SMR",
    "text": "Writing down a model for SMR\nDefine \\(Y_i\\) and \\(E_i\\) as the observed and expected mortality counts at county \\(i\\) (\\(i = 1\\ldots,n\\)). We can model the observed counts as follows:\n\\[Y_i | \\lambda_i \\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i).\\]\n\nRecall that for a random variable \\(Y \\sim \\text{Poisson}(\\lambda)\\), \\(\\mathbb{E}[Y] = \\lambda\\) and \\(\\mathbb{V}(Y) = \\lambda\\).\nWe have: \\(\\mathbb{E}[Y_i | \\lambda_i] = E_i \\lambda_i \\implies \\mathbb{E}\\left[(Y_i / E_i) | \\lambda_i\\right] = \\lambda_i.\\)\n\nUnder this parameterization \\(\\lambda_i\\) is the SMR."
  },
  {
    "objectID": "slides/19-disease-mapping.html#disease-mapping-model",
    "href": "slides/19-disease-mapping.html#disease-mapping-model",
    "title": "Disease Mapping",
    "section": "Disease Mapping Model",
    "text": "Disease Mapping Model\nThe parameter \\(\\lambda_i\\), sometimes also called relative risk, is modeled as follows:\n\\[\\begin{aligned}\nY_i | \\lambda_i &\\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i)\\\\\n\\log \\lambda_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\theta_i + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{aligned}\\]\nwhere \\(\\mathbf{x}_i \\in \\mathbb{R}^{p \\times 1}\\) contains county-level predictors.\nPopulation parameters:\n\n\\(\\alpha \\in \\mathbb{R}\\) is a population intercept.\n\\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\) is a vector of population coefficients.\n\\(\\sigma \\in \\mathbb{R}^+\\) is a residual error term."
  },
  {
    "objectID": "slides/19-disease-mapping.html#lattice-data-areal-data",
    "href": "slides/19-disease-mapping.html#lattice-data-areal-data",
    "title": "Disease Mapping",
    "section": "Lattice Data (Areal Data)",
    "text": "Lattice Data (Areal Data)\n\nData observed at the level of an areal unit\n\nCounty Level Sudden Infant Death Syndrome Counts"
  },
  {
    "objectID": "slides/19-disease-mapping.html#lattice-data-areal-data-1",
    "href": "slides/19-disease-mapping.html#lattice-data-areal-data-1",
    "title": "Disease Mapping",
    "section": "Lattice Data (Areal Data)",
    "text": "Lattice Data (Areal Data)\n\nBirmingham Tract Level Poverty Levels"
  },
  {
    "objectID": "slides/19-disease-mapping.html#goals-of-areal-spatial-data-analysis",
    "href": "slides/19-disease-mapping.html#goals-of-areal-spatial-data-analysis",
    "title": "Disease Mapping",
    "section": "Goals of Areal Spatial Data Analysis",
    "text": "Goals of Areal Spatial Data Analysis\nThe goal of areal spatial data analysis is to understand how spatial patterns (e.g., mortality rates, disease incidence) vary across different geographic areas (e.g., counties, neighborhoods).\nIt helps us identify:\n\nClusters: Areas with similar characteristics (e.g., high mortality, disease prevalence).\nOutliers: Areas that deviate significantly from the overall pattern (e.g., unexpectedly high mortality rates).\nSpatial Dependence: Whether values in one area are correlated with values in nearby areas (e.g., neighboring counties with similar health outcomes)."
  },
  {
    "objectID": "slides/19-disease-mapping.html#why-we-care-about-spatial-patterns",
    "href": "slides/19-disease-mapping.html#why-we-care-about-spatial-patterns",
    "title": "Disease Mapping",
    "section": "Why We Care About Spatial Patterns",
    "text": "Why We Care About Spatial Patterns\n\nLocal Insights: Spatial analysis helps identify local variations in health outcomes that may not be apparent when analyzing data at a higher (e.g., state or national) level.\nTargeted Interventions: Understanding spatial patterns allows for targeted public health interventions tailored to regions that need attention (e.g., areas with unusually high mortality rates).\nIdentifying Spatial Clusters: By recognizing clusters of high or low rates, we can investigate potential common causes (e.g., environmental factors, access to healthcare, socioeconomic conditions)."
  },
  {
    "objectID": "ae/ae-07-geospatial.html#visualizing-the-correlation",
    "href": "ae/ae-07-geospatial.html#visualizing-the-correlation",
    "title": "AE 07: Geospatial",
    "section": "Visualizing the Correlation",
    "text": "Visualizing the Correlation\nFinally, we will visualize the posterior correlation function. We begin by extracting the posterior mean value of the covariance function tuning parameter.\n\nrho_mean &lt;- mean(rstan::extract(fit_spatial, pars = \"rho\")$rho)\n\nWe can then define the Matérn 3/2 covariance function, which is a function of the distance between two points, \\(||\\mathbf{h}||\\), and \\(\\rho\\).\n\nmatern_3_2 &lt;- function(h, rho) (1 + sqrt(3) * h / rho) * exp(- sqrt(3) * h / rho)\n\nWe can use this function to compute the correlation matrix for our observed locations. We begin by computing the distance matrix and then compute the correlation matrix at the posterior mean of \\(\\rho\\). We only compute correlation at the unique distances.\n\nD &lt;- as.matrix(dist(u))\ndists &lt;- data.frame(distance_geodisic = unique(D[D != 0]))\ndists$correlation &lt;- matern_3_2(dists$distance_geodisic, rho = rho_mean)\n\nNext we will use the geodist package to convert the geodisic distance of our observed locations to miles. Note that this function converts distance to meters by default, so we divide by 1,609 to get miles.\n\ndist_miles &lt;- geodist::geodist_vec(\n  x1 = u[, 1],\n  y1 = u[, 2],\n  paired = TRUE,\n  measure = \"haversine\"\n) / 1609\n\nThis produces an \\(n \\times n\\) matrix of distances. We extract the unique distances like we did for the original geodisic distances.\n\ndists$distances_miles &lt;- unique(dist_miles[dist_miles != 0])\n\nFinally, we can plot these correlations for both distance units.\n\ncolnames(dists) &lt;- c(\"Geodisic\", \"Correlation\", \"Mile\")\ndat_fig &lt;- pivot_longer(dists, cols = c(\"Geodisic\", \"Mile\"), names_to = \"Distance\")\nggplot(dat_fig, aes(x = value, y = Correlation)) + \n  geom_line() + \n  facet_grid(. ~ Distance, scales = \"free_x\") + \n  labs(x = \"Distance\")"
  },
  {
    "objectID": "slides/21-scalable1.html#review-of-the-last-lecture",
    "href": "slides/21-scalable1.html#review-of-the-last-lecture",
    "title": "Scalable Gaussian Processes #1",
    "section": "Review of the last lecture",
    "text": "Review of the last lecture\nTBU"
  },
  {
    "objectID": "slides/21-scalable1.html#motivating-dataset",
    "href": "slides/21-scalable1.html#motivating-dataset",
    "title": "Scalable Gaussian Processes #1",
    "section": "Motivating dataset",
    "text": "Motivating dataset\nRecall we worked with a dataset on women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. Variables are:\n\nloc_id: location id (i.e. survey cluster).\nhemoglobin: hemoglobin level (g/dL).\nanemia: anemia classifications.\nage: age in years.\nurban: urban vs. rural.\nLATNUM: latitude.\nLONGNUM: longitude."
  },
  {
    "objectID": "slides/21-scalable1.html#motivating-dataset-1",
    "href": "slides/21-scalable1.html#motivating-dataset-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "Motivating dataset",
    "text": "Motivating dataset\n\n\n  loc_id hemoglobin     anemia age urban   LATNUM  LONGNUM\n1      1       12.5 not anemic  28 rural 0.220128 21.79508\n2      1       12.6 not anemic  42 rural 0.220128 21.79508\n3      1       13.3 not anemic  15 rural 0.220128 21.79508\n4      1       12.9 not anemic  28 rural 0.220128 21.79508\n5      1       10.4       mild  32 rural 0.220128 21.79508\n6      1       12.2 not anemic  42 rural 0.220128 21.79508\n\n\n\n\n\n\n\n\nModeling goals:\n\n\n\nLearn the associations between age and urbanality and hemoglobin, accounting for unmeasured spatial confounders.\nCreate a predicted map of hemoglobin across the spatial surface controlling for age and urbanality, with uncertainty quantification."
  },
  {
    "objectID": "slides/21-scalable1.html#map-of-sud-kivu-state",
    "href": "slides/21-scalable1.html#map-of-sud-kivu-state",
    "title": "Scalable Gaussian Processes #1",
    "section": "Map of Sud-Kivu state",
    "text": "Map of Sud-Kivu state\nLast time, we focused on one state with ~500 observations at ~30 locations."
  },
  {
    "objectID": "slides/21-scalable1.html#map-of-the-drc",
    "href": "slides/21-scalable1.html#map-of-the-drc",
    "title": "Scalable Gaussian Processes #1",
    "section": "Map of the DRC",
    "text": "Map of the DRC\nToday we will extend the analysis to the full dataset with ~8,600 observations at ~500 locations."
  },
  {
    "objectID": "slides/21-scalable1.html#modeling",
    "href": "slides/21-scalable1.html#modeling",
    "title": "Scalable Gaussian Processes #1",
    "section": "Modeling",
    "text": "Modeling\n\\[\\begin{align*}\n  Y_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\\]\nData objects:\n\n\\(i \\in \\{1,\\dots,n\\}\\) indexes unique locations.\n\\(j \\in \\{1,\\dots,n_i\\}\\) indexes individuals at each location.\n\\(Y_j(\\mathbf{u}_i)\\) denotes the observation of individual \\(j\\) at location \\(\\mathbf{u}_i\\).\n\\(\\mathbf{x}_j(\\mathbf{u}_i) = (\\text{age}_{ij}/10,\\text{urban}_i) \\in \\mathbb{R}^{1 \\times p}\\), where \\(p=2\\) is the number of predictors (excluding intercept)."
  },
  {
    "objectID": "slides/21-scalable1.html#modeling-1",
    "href": "slides/21-scalable1.html#modeling-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "Modeling",
    "text": "Modeling\n\\[\\begin{align*}\n  Y_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\\]\nPopulation parameters:\n\n\\(\\alpha \\in \\mathbb{R}\\) is the intercept.\n\\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\) is the regression coefficients.\n\\(\\sigma^2 \\in \\mathbb{R}^+\\) is the overall residual error (nugget).\n\nLocation-specific parameters:\n\n\\(\\mathbf{u}_i = (\\text{latitude}_i, \\text{longitude}_i) \\in \\mathbb{R}^2\\) denotes coordinates of location \\(i\\).\n\\(\\theta(\\mathbf{u}_i)\\) denotes the spatial intercept at location \\(\\mathbf{u}_i\\)."
  },
  {
    "objectID": "slides/21-scalable1.html#location-specific-notation",
    "href": "slides/21-scalable1.html#location-specific-notation",
    "title": "Scalable Gaussian Processes #1",
    "section": "Location-specific notation",
    "text": "Location-specific notation\n\\[\\mathbf{Y}(\\mathbf{u}_i) = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i)\\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}(\\mathbf{u}_i), \\quad \\boldsymbol{\\epsilon}(\\mathbf{u}_i) \\sim N_{n_i}(\\mathbf{0},\\sigma^2\\mathbf{I})\\]\n\n\\(\\mathbf{Y}(\\mathbf{u}_i) = (Y_1(\\mathbf{u}_i),\\ldots,Y_{n_i}(\\mathbf{u}_i))^\\top\\).\n\\(\\mathbf{X}(\\mathbf{u}_i)\\) is an \\(n_i \\times p\\) dimensional matrix with rows \\(\\mathbf{x}_j(\\mathbf{u}_i)\\).\n\\(\\boldsymbol{\\epsilon}(\\mathbf{u}_i) = (\\epsilon_i(\\mathbf{u}_i),\\ldots,\\epsilon_{n_i}(\\mathbf{u}_i))^\\top\\)."
  },
  {
    "objectID": "slides/21-scalable1.html#full-data-notation",
    "href": "slides/21-scalable1.html#full-data-notation",
    "title": "Scalable Gaussian Processes #1",
    "section": "Full data notation",
    "text": "Full data notation\n\\[\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})\\]\n\n\\(\\mathbf{Y} = (\\mathbf{Y}(\\mathbf{u}_1)^\\top,\\ldots,\\mathbf{Y}(\\mathbf{u}_{n})^\\top)^\\top \\in \\mathbb{R}^N\\), with \\(N = \\sum_{i=1}^n n_i\\).\n\\(\\mathbf{X} \\in \\mathbb{R}^{N \\times p}\\) stacks \\(\\mathbf{X}(\\mathbf{u}_i)\\).\n\\(\\boldsymbol{\\theta} = (\\theta(\\mathbf{u}_1),\\ldots,\\theta(\\mathbf{u}_n))^\\top \\in \\mathbb{R}^n\\).\n\\(\\mathbf{Z}\\) is an \\(N \\times n\\) dimensional block diagonal binary matrix. Each row contains a single 1 in column \\(i\\) that corresponds to the location of \\(Y_j(\\mathbf{u}_i)\\). \\[\n\\begin{align}\n\\mathbf{Z} = \\begin{bmatrix}\n\\mathbf{1}_{n_1} & \\mathbf{0} & \\dots & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{1}_{n_2} & \\dots & \\mathbf{0}  \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{0} & \\dots & \\mathbf{0} & \\mathbf{1}_{n_n}\n\\end{bmatrix}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable1.html#modeling-2",
    "href": "slides/21-scalable1.html#modeling-2",
    "title": "Scalable Gaussian Processes #1",
    "section": "Modeling",
    "text": "Modeling\nWe specify the following model: \\[\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})\\] with priors\n\n\\(\\boldsymbol{\\theta}(\\mathbf{u}) | \\tau,\\rho \\sim GP(\\mathbf{0},C(\\cdot,\\cdot))\\), where \\(C\\) is the Matérn 3/2 covariance function with magnitude \\(\\tau\\) and length scale \\(\\rho\\)\n\\(\\alpha^* \\sim N(0,4^2)\\). This is the intercept after centering \\(\\mathbf{X}\\).\n\\(\\beta_j | \\sigma_{\\beta} \\sim N(0,\\sigma_{\\beta}^2)\\), \\(j \\in \\{1,\\dots,p\\}\\)\n\\(\\sigma \\sim \\text{Half-Normal}(0, 2^2)\\)\n\\(\\tau \\sim \\text{Half-Normal}(0, 4^2)\\)\n\\(\\rho \\sim \\text{Inv-Gamma}(5, 5)\\)\n\\(\\sigma_{\\beta} \\sim \\text{Half-Normal}(0, 2^2)\\)"
  },
  {
    "objectID": "slides/21-scalable1.html#computational-issues-with-gp",
    "href": "slides/21-scalable1.html#computational-issues-with-gp",
    "title": "Scalable Gaussian Processes #1",
    "section": "Computational issues with GP",
    "text": "Computational issues with GP\nEffectively, the prior for \\(\\boldsymbol{\\theta}\\) is \\[\\boldsymbol{\\theta} | \\tau,\\rho \\sim N_n(\\mathbf{0},\\mathbf{C}), \\quad \\mathbf{C} \\in \\mathbb{R}^{n \\times n}.\\] Matérn 3/2 is an isotropic covariance function, \\(C(\\mathbf{u}_i, \\mathbf{u}_j) = C(\\|\\mathbf{u}_i-\\mathbf{u}_j\\|)\\).\n\\[\\mathbf{C} = \\begin{bmatrix}\nC(0) & C(\\|\\mathbf{u}_1 - \\mathbf{u}_2\\|) & \\cdots & C(\\|\\mathbf{u}_1 - \\mathbf{u}_n\\|)\\\\\nC(\\|\\mathbf{u}_1 - \\mathbf{u}_2\\|) & C(0) & \\cdots & C(\\|\\mathbf{u}_2 - \\mathbf{u}_n\\|)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nC(\\|\\mathbf{u}_{1} - \\mathbf{u}_n\\|) & C(\\|\\mathbf{u}_2 - \\mathbf{u}_n\\|) & \\cdots & C(0)\\\\\n\\end{bmatrix}.\\]\nThis is not scalable because we need to invert an \\(n \\times n\\) dense covariance matrix for each MCMC iteration, which requires \\(\\mathcal{O}(n^3)\\) floating point operations (flops), and \\(\\mathcal{O}(n^2)\\) memory."
  },
  {
    "objectID": "slides/21-scalable1.html#scalable-gp-methods-overview",
    "href": "slides/21-scalable1.html#scalable-gp-methods-overview",
    "title": "Scalable Gaussian Processes #1",
    "section": "Scalable GP methods overview",
    "text": "Scalable GP methods overview\nThe computational issues motivated exploration in scalable GP methods. Existing scalable methods broadly fall under two categories.\n\nSparsity methods\n\n\nsparsity in \\(\\mathbf{C}\\), e.g., covariance tapering (@furrer2006covariance).\nsparsity in \\(\\mathbf{C}^{-1}\\), e.g., Vecchia approximation (@vecchia1988estimation) and nearest-neighbor GP (@datta2016hierarchical).\n\n\nLow-rank methods\n\n\napproximate \\(\\mathbf{C}\\) on a low-dimensional subspace.\ne.g., process convolution (@higdon2002space), inducing point method(@snelson2005sparse)."
  },
  {
    "objectID": "slides/21-scalable1.html#hilbert-space-method-for-gp",
    "href": "slides/21-scalable1.html#hilbert-space-method-for-gp",
    "title": "Scalable Gaussian Processes #1",
    "section": "Hilbert space method for GP",
    "text": "Hilbert space method for GP\n\n@solin2020hilbert introduced a Hilbert space method for reduced-rank Gaussian process regression (HSGP).\n@riutort2023practical discussed how to practically implement HSGP.\nTutorial codes are available in different probabilistic programming languages:\n\nstan\nNumPyro\npyMC"
  },
  {
    "objectID": "slides/21-scalable1.html#lecture-plan",
    "href": "slides/21-scalable1.html#lecture-plan",
    "title": "Scalable Gaussian Processes #1",
    "section": "Lecture plan",
    "text": "Lecture plan\nToday:\n\nHow does HSGP work\nWhy HSGP is scalable\nHow to use HSGP for Bayesian geospatial model fitting and posterior predictive sampling\n\nThursday:\n\nParameter tuning for HSGP\nHow to implement HSGP in stan"
  },
  {
    "objectID": "slides/21-scalable1.html#hsgp-approximation",
    "href": "slides/21-scalable1.html#hsgp-approximation",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation",
    "text": "HSGP approximation\nGiven:\n\nan isotropic covariance function \\(C\\) which admits a power spectral density, e.g., the Matérn family, and\na compact domain \\(\\boldsymbol{\\Theta} \\in \\mathbb{R}^d\\) with smooth boundaries. For our purposes, we only consider boxes, e.g., \\([-1,1] \\times [-1,1]\\).\n\nHSGP approximates the \\((i,j)\\) element of the corresponding \\(n \\times n\\) covariance matrix \\(\\mathbf{C}\\) as \\[\\mathbf{C}_{ij}=C(\\|\\mathbf{u}_i - \\mathbf{u}_j\\|) \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j).\\]"
  },
  {
    "objectID": "slides/21-scalable1.html#hsgp-approximation-1",
    "href": "slides/21-scalable1.html#hsgp-approximation-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation",
    "text": "HSGP approximation\n\\[\\mathbf{C}_{ij}=C(\\|\\mathbf{u}_i - \\mathbf{u}_j\\|) \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j).\\]\n\n\\(s_k \\in \\mathbb{R}^+\\) are positive scalars which depends on the covariance function \\(C\\) and its parameters \\(\\tau\\) and \\(\\rho\\).\n\\(\\phi_k: \\boldsymbol{\\Theta} \\to \\mathbb{R}\\) are basis functions which only depends on \\(\\boldsymbol{\\Theta}\\).\n\\(m\\) is the number of basis functions. Note: even with an infinite sum (\\(m \\to \\infty\\)), this remains an approximation (see @solin2020hilbert)."
  },
  {
    "objectID": "slides/21-scalable1.html#hsgp-approximation-2",
    "href": "slides/21-scalable1.html#hsgp-approximation-2",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation",
    "text": "HSGP approximation\nIn matrix notation,\n\\[\\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top.\\]\n\n\\(\\boldsymbol{\\Phi} \\in \\mathbb{R}^{n \\times m}\\) is a feature matrix. Only depends on \\(\\boldsymbol{\\Theta}\\) and the observed locations.\n\\(\\mathbf{S} \\in \\mathbb{R}^{m \\times m}\\) is diagonal. Depends on the covariance function \\(C\\) and parameters \\(\\tau\\) and \\(\\rho\\).\n\n\\[\n\\begin{align}\n  \\boldsymbol{\\Phi} = \\begin{bmatrix}\n  \\phi_1(\\mathbf{u}_1) & \\dots & \\phi_m(\\mathbf{u}_1) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\phi_1(\\mathbf{u}_n) & \\dots & \\phi_m(\\mathbf{u}_n)\n  \\end{bmatrix}, \\quad\n  \\mathbf{S} = \\begin{bmatrix}\n  s_1 &  &  \\\\\n  & \\ddots &  \\\\\n  &  & s_m\n  \\end{bmatrix}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable1.html#why-hsgp-is-scalable",
    "href": "slides/21-scalable1.html#why-hsgp-is-scalable",
    "title": "Scalable Gaussian Processes #1",
    "section": "Why HSGP is scalable",
    "text": "Why HSGP is scalable\n\\[\\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top.\\]\n\n\\(\\boldsymbol{\\Phi}\\) only depends on \\(\\boldsymbol{\\Theta}\\) and the observed locations, can be pre-calculated.\nNo matrix inversion.\nEach MCMC iteration requires \\(\\mathcal{O}(nm + m)\\) flops, vs \\(\\mathcal{O}(n^3)\\) for a full GP.\nIdeally \\(m \\ll n\\), but HSGP can be faster even for \\(m&gt;n\\)."
  },
  {
    "objectID": "slides/21-scalable1.html#model-reparameterization",
    "href": "slides/21-scalable1.html#model-reparameterization",
    "title": "Scalable Gaussian Processes #1",
    "section": "Model reparameterization",
    "text": "Model reparameterization\nUnder HSGP, approximately \\[\\boldsymbol{\\theta} \\overset{d}{=} \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}, \\quad \\mathbf{b} \\sim N_m(0,\\mathbf{I}).\\]\nTherefore we can reparameterize the model as\n\\[\n\\begin{align}\n  \\mathbf{Y} &= \\alpha \\mathbf{1}_{N} + X\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon} \\\\\n  &\\approx \\alpha \\mathbf{1}_{N} + X\\boldsymbol{\\beta} + \\underbrace{\\mathbf{Z}\\boldsymbol{\\Phi} \\mathbf{S}^{1/2}}_{\\mathbf{W}}\\mathbf{b} + \\boldsymbol{\\epsilon}\n\\end{align}\n\\]\nNote the resemblance to linear regression:\n\n\\(\\mathbf{W} \\in \\mathbb{R}^{n \\times m}\\) is a known design matrix given parameters \\(\\tau\\) and \\(\\rho\\).\n\\(\\mathbf{b}\\) is an unknown parameter vector with prior \\(N_m(0,\\mathbf{I})\\)."
  },
  {
    "objectID": "slides/21-scalable1.html#hsgp-in-stan",
    "href": "slides/21-scalable1.html#hsgp-in-stan",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP in stan",
    "text": "HSGP in stan\nSimilarly, we can use the reparameterized model in stan.\nThis is called the non-centered parameterization in stan documentation. It’s recommended for computational efficiency for hierarchical models.\n\ntransformed data {\n  matrix[n,m] PHI;\n  matrix[N,m] Z;\n}\nparameters {\n  real alpha;\n  real&lt;lower=0&gt; sigma;\n  vector[p] beta;\n  vector[m] b;\n  vector&lt;lower=0&gt;[m] sqrt_S;\n}\nmodel {\n  vector[n] theta = PHI * (sqrt_S .* b);\n  target += normal_lupdf(y | alpha + X * beta + Z * theta, sigma);\n  target += normal_lupdf(b | 0, 1);\n  ...\n}"
  },
  {
    "objectID": "slides/21-scalable1.html#posterior-predictive-distribution",
    "href": "slides/21-scalable1.html#posterior-predictive-distribution",
    "title": "Scalable Gaussian Processes #1",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nWe want to make predictions for \\(\\mathbf{Y}^* = (Y(\\mathbf{u}_{n+1}),\\ldots, Y(\\mathbf{u}_{n+q}))^\\top\\), observations at \\(q\\) new locations. Define \\(\\boldsymbol{\\theta}^* = (\\theta(\\mathbf{u}_{n+1}),\\ldots,\\theta(\\mathbf{u}_{n+q}))^\\top\\), \\(\\boldsymbol{\\Omega} = (\\alpha,\\boldsymbol{\\beta},\\sigma,\\tau,\\rho)\\). Recall:\n\\[\\begin{align*}\n  f(\\mathbf{Y}^* | \\mathbf{Y}) &= \\int f(\\mathbf{Y}^*, \\boldsymbol{\\theta}^*, \\boldsymbol{\\theta}, \\boldsymbol{\\Omega} | \\mathbf{Y}) d\\boldsymbol{\\theta}^* d\\boldsymbol{\\theta} d\\boldsymbol{\\Omega}\\\\\n  &= \\int \\underbrace{f(\\mathbf{Y}^* | \\boldsymbol{\\theta}^*, \\boldsymbol{\\Omega})}_{(1)} \\underbrace{f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})}_{(2)} \\underbrace{f(\\boldsymbol{\\theta},\\boldsymbol{\\Omega} | \\mathbf{Y})}_{(3)} d\\boldsymbol{\\theta}^* d\\boldsymbol{\\theta} d\\boldsymbol{\\Omega}\\\\\n\\end{align*}\\]\n\nLikelihood: \\(f(\\mathbf{Y}^* | \\boldsymbol{\\theta}^*, \\boldsymbol{\\Omega})\\)  – remains the same as for GP \nKriging: \\(f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})\\)  – we will focus on this next \nPosterior distribution: \\(f(\\boldsymbol{\\theta},\\boldsymbol{\\Omega} | \\mathbf{Y})\\)  – we have just discussed"
  },
  {
    "objectID": "slides/21-scalable1.html#kriging",
    "href": "slides/21-scalable1.html#kriging",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging",
    "text": "Kriging\nRecall under the GP prior,\n\\[\\begin{bmatrix}\n    \\boldsymbol{\\theta}\\\\\n    \\boldsymbol{\\theta}^*\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega} \\sim N_{n+q}\\left(\\begin{bmatrix}\n    \\mathbf{0}_n \\\\\n    \\mathbf{0}_q\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\mathbf{C} & \\mathbf{C}_{+}\\\\\n    \\mathbf{C}_{+}^\\top & \\mathbf{C}^*\n  \\end{bmatrix}\\right),\\]\nwhere \\(\\mathbf{C}\\) is the covariance of \\(\\boldsymbol{\\theta}\\), \\(\\mathbf{C}^*\\) is the covariance of \\(\\boldsymbol{\\theta}^*\\), and \\(\\mathbf{C}_{+}\\) is the cross covariance matrix between \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\theta}^*\\).\nTherefore by properties of multivariate normal, \\[\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) \\sim N_q(\\mathbb{E}_{\\boldsymbol{\\theta}^*},\\mathbb{V}_{\\boldsymbol{\\theta}^*}), \\quad \\text{where}\\] \\[\n\\begin{align}\n  \\mathbb{E}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}_+^\\top \\mathbf{C}^{-1} \\boldsymbol{\\theta}\\\\\n  \\mathbb{V}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}^* - \\mathbf{C}_+^\\top \\mathbf{C}^{-1} \\mathbf{C}_+.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable1.html#kriging-under-hsgp",
    "href": "slides/21-scalable1.html#kriging-under-hsgp",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging under HSGP",
    "text": "Kriging under HSGP\nUnder HSGP, \\(\\mathbf{C}^* \\approx \\boldsymbol{\\Phi}^* \\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}\\), \\(\\mathbf{C}_+ \\approx \\boldsymbol{\\Phi} \\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}\\), where \\[\n\\begin{align}\n  \\boldsymbol{\\Phi}^* \\in \\mathbb{R}^{q \\times m} = \\begin{bmatrix}\n  \\phi_1(\\mathbf{u}_{n+1}) & \\dots & \\phi_m(\\mathbf{u}_{n+1}) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\phi_1(\\mathbf{u}_{n+q}) & \\dots & \\phi_m(\\mathbf{u}_{n+q})\n  \\end{bmatrix}\n\\end{align}\n\\] is the feature matrix for the new locations. Therefore approximately \\[\n\\begin{align}\n  \\begin{bmatrix}\n    \\boldsymbol{\\theta} \\\\\n    \\boldsymbol{\\theta}^*\n  \\end{bmatrix} \\sim N_{n\n  +q} \\left(\\begin{bmatrix}\n    \\mathbf{0}_n \\\\\n    \\mathbf{0}_q\n  \\end{bmatrix},\n  \\begin{bmatrix}\n    \\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top & \\boldsymbol{\\Phi}\\mathbf{S} \\boldsymbol{\\Phi}^{*\\top} \\\\\n    \\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top & \\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}\n  \\end{bmatrix} \\right).\n\\end{align}\n\\] Claim \\(\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) = (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta}\\), where \\(\\mathbf{A}^\\dagger\\) denotes a generalized inverse of matrix \\(\\mathbf{A}\\) such that \\(\\mathbf{A}\\mathbf{A}^\\dagger = \\mathbf{I}\\)."
  },
  {
    "objectID": "slides/21-scalable1.html#kriging-under-hsgp-1",
    "href": "slides/21-scalable1.html#kriging-under-hsgp-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging under HSGP",
    "text": "Kriging under HSGP\nClaim \\(\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) = (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta}.\\)\nSketch proof below, see details in class:\n\nBy properties of multivariate normal, \\(\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) \\sim N_q(\\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS},\\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS})\\), \\[\n\\begin{align}\n  \\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta}\\\\\n  \\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}) - (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger}(\\boldsymbol{\\Phi}\\mathbf{S} \\boldsymbol{\\Phi}^{*\\top}).\n\\end{align}\n\\]\nShow if \\(\\boldsymbol{\\Phi}\\) has full column rank, which is true under HSGP, then for any matrix \\(\\mathbf{A}\\) of proper dimension, \\[\\mathbf{S} \\boldsymbol{\\Phi}^\\top(\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger}\\boldsymbol{\\Phi}\\mathbf{A} = \\mathbf{A} \\tag{1}\\] Therefore \\(\\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS} \\equiv \\mathbf{0}\\)."
  },
  {
    "objectID": "slides/21-scalable1.html#kriging-under-hsgp-2",
    "href": "slides/21-scalable1.html#kriging-under-hsgp-2",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging under HSGP",
    "text": "Kriging under HSGP\nUnder the reparameterized model, \\(\\boldsymbol{\\theta} = \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}\\), for \\(\\mathbf{b} \\sim N_m(0,\\mathbf{I}).\\) Therefore \\[\n\\begin{align}\n  \\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta},\\boldsymbol{\\Omega}) &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta} \\\\\n  &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger}(\\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}) \\\\\n  &= \\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b}. \\quad (\\text{by equation (1) in the last slide})\n\\end{align}\n\\]\nDuring MCMC sampling, we can obtain posterior predictive samples for \\(\\boldsymbol{\\theta}^*\\) through posterior samples of \\(\\mathbf{b}\\) and \\(\\mathbf{S}\\). Let superscript \\((s)\\) denote the \\(s\\)th posterior sample:\n\\[\\boldsymbol{\\theta}^{*(s)} = \\boldsymbol{\\Phi}^* \\mathbf{S}^{(s) 1/2} \\mathbf{b}^{(s)}.\\]"
  },
  {
    "objectID": "slides/21-scalable1.html#kriging-under-hsgp-alternative-view",
    "href": "slides/21-scalable1.html#kriging-under-hsgp-alternative-view",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging under HSGP – alternative view",
    "text": "Kriging under HSGP – alternative view\nUnder the reparameterized model, there is another (perhaps more intuitive) way to recognize the kriging distribution under HSGP.\nWe model \\(\\boldsymbol{\\theta} = \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}\\), where \\(\\mathbf{b}\\) is treated as the unknown parameter. Therefore for kriging: \\[\n\\begin{align}\n  \\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta},\\boldsymbol{\\Omega}) &= \\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b} \\mid (\\mathbf{b},\\boldsymbol{\\Omega}) \\\\\n  &=\\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable1.html#hsgp-kriging-in-stan",
    "href": "slides/21-scalable1.html#hsgp-kriging-in-stan",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP kriging in stan",
    "text": "HSGP kriging in stan\nKriging under HSGP can be easily implemented in stan.\n\ntransformed data {\n  matrix[q,m] PHI_new;\n}\nparameters {\n  vector[m] b;\n  vector&lt;lower=0&gt;[m] sqrt_S;\n}\ngenerated quantities {\n  vector[q] theta_new = PHI_new * (sqrt_S .* b);\n}"
  },
  {
    "objectID": "slides/21-scalable1.html#recap",
    "href": "slides/21-scalable1.html#recap",
    "title": "Scalable Gaussian Processes #1",
    "section": "Recap",
    "text": "Recap\nHSGP is a low rank approximation method for GP.\n\\[\n\\begin{align}\n  \\mathbf{C}_{ij} \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j), \\quad \\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top,\n\\end{align}\n\\]\n\nfor covariance function \\(C\\) which admits a power spectral density.\non a box \\(\\boldsymbol{\\Theta} \\subset \\mathbb{R}^d\\).\nwith \\(m\\) number of basis functions.\n\nWe have talked about:\n\nwhy HSGP is scalable.\nhow to do posterior sampling and posterior predictive sampling in stan."
  },
  {
    "objectID": "slides/21-scalable1.html#hsgp-parameters",
    "href": "slides/21-scalable1.html#hsgp-parameters",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP parameters",
    "text": "HSGP parameters\n@solin2020hilbert showed that HSGP approximation can be made arbitrarily accurate as \\(\\boldsymbol{\\Theta}\\) and \\(m\\) increase.\nBut how to choose:\n\nsize of the box \\(\\boldsymbol{\\Theta}\\).\nnumber of basis functions \\(m\\)."
  },
  {
    "objectID": "slides/21-scalable1.html#hsgp-approximation-box",
    "href": "slides/21-scalable1.html#hsgp-approximation-box",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation box",
    "text": "HSGP approximation box\nDue to the design of HSGP, the approximation is less accurate near the boundaries of \\(\\boldsymbol{\\Theta}\\).\n\nSuppose all the coordinates are centered. Let \\[S_l = \\max_i |\\mathbf{u}_{il}|, \\quad l=1,\\dots,d, \\quad i= 1, \\dots, (n+q)\\] such that \\(\\boldsymbol{\\Theta}_S = \\prod_{l=1}^d [-S_l,S_l]\\) is the smallest box which contains all observed and prediction locations. We should at least ensure \\(\\boldsymbol{\\Theta} \\supset \\boldsymbol{\\Theta}_S\\).\nWe want the box to be large enough to ensure good boundary accuracy. Let \\(c_l \\ge 1\\) be boundary factors, we consider \\[\\boldsymbol{\\Theta} = \\prod_{l=1}^d [-L_l,L_l], \\quad L_l = c_l S_l.\\]"
  },
  {
    "objectID": "slides/21-scalable1.html#hsgp-approximation-box-1",
    "href": "slides/21-scalable1.html#hsgp-approximation-box-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation box",
    "text": "HSGP approximation box\n\n\n\n\n\nHow much the approximation accuracy deteriorates towards the boundaries depends on smoothness of the true surface.\n\nE.g., the larger the length scale \\(\\rho\\), the smoother the surface, a smaller box can be used for the same level of boundary accuracy."
  },
  {
    "objectID": "slides/21-scalable1.html#hsgp-approximation-box-2",
    "href": "slides/21-scalable1.html#hsgp-approximation-box-2",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation box",
    "text": "HSGP approximation box\n\n\n\n\n\nThe larger the box,\n\nthe more basis functions we need for the same level of overall accuracy,\nhence higher run time."
  },
  {
    "objectID": "slides/21-scalable1.html#hsgp-basis-functions",
    "href": "slides/21-scalable1.html#hsgp-basis-functions",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP basis functions",
    "text": "HSGP basis functions\nThe total number of basis functions \\(m = \\prod_{l=1}^d m_l\\), i.e., we need to decide on \\(m_l\\)’s, the number of basis functions for each dimension.\n\nThe higher the \\(m\\), the better the overall approximation accuracy, the higher the runtime.\n\\(m\\) scales exponentially in \\(d\\), hence the HSGP computation complexity \\(\\mathcal{O}(mn+m)\\) also scales exponentially in \\(d\\). Therefore HSGP is only recommended for \\(d \\le 3\\), at most \\(4\\)."
  },
  {
    "objectID": "slides/21-scalable1.html#relationship-between-c-and-m",
    "href": "slides/21-scalable1.html#relationship-between-c-and-m",
    "title": "Scalable Gaussian Processes #1",
    "section": "Relationship between \\(c\\) and \\(m\\)",
    "text": "Relationship between \\(c\\) and \\(m\\)\n@riutort2023practical used simulations with a squared exponential covariance function to investigate the relationship between \\(c\\) and \\(m\\).\n\n\n\n\n\nNotice that \\(c\\) needs to be above a certain minimum value to achieve a reasonable boundary accuracy."
  },
  {
    "objectID": "slides/21-scalable1.html#relationship-between-c-m-and-rho",
    "href": "slides/21-scalable1.html#relationship-between-c-m-and-rho",
    "title": "Scalable Gaussian Processes #1",
    "section": "Relationship between \\(c\\), \\(m\\) and \\(\\rho\\)",
    "text": "Relationship between \\(c\\), \\(m\\) and \\(\\rho\\)\nTo summarize:\n\nThe boundary factor \\(c\\) needs to be above a minimum value, otherwise HSGP approximation is poor no matter how large \\(m\\) is.\nAs \\(\\rho\\) increases, the surface is less smooth,\n\n\n\\(c\\) needs to increase to retain boundary accuracy.\n\\(m\\) needs to increase to retain overall accuracy.\n\n\nAs \\(c\\) increases, \\(m\\) needs to increase to retain overall accuracy.\nAs \\(m\\) increases, run time increases. Hence we want to minimize \\(m\\) and \\(c\\) while maintaining certain accuracy level.\n\nAnd, we do not know \\(\\rho\\). So how to choose \\(c\\) and \\(m\\)?"
  },
  {
    "objectID": "slides/21-scalable1.html#prepare-for-next-class",
    "href": "slides/21-scalable1.html#prepare-for-next-class",
    "title": "Scalable Gaussian Processes #1",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 05 which is due Apr 8\nComplete reading to prepare for Thursday’s lecture\nThursday’s lecture:\n\n\nParameter tuning for HSGP\nHow to implement HSGP in stan"
  },
  {
    "objectID": "slides/21-scalable1.html#references",
    "href": "slides/21-scalable1.html#references",
    "title": "Scalable Gaussian Processes #1",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/19-disease-mapping.html#review-of-last-week",
    "href": "slides/19-disease-mapping.html#review-of-last-week",
    "title": "Disease Mapping",
    "section": "",
    "text": "Last week, we learned about Gaussian processes.\nWe learned how to apply Gaussian processes to longitudinal (or time-series) and geospatial data.\nFocused on making predictions at new locations across the spatial surface.\nToday we will focus on areal spatial data, which has different goals associated with it than point-referenced spatial data."
  },
  {
    "objectID": "slides/19-disease-mapping.html#disease-mapping-model-1",
    "href": "slides/19-disease-mapping.html#disease-mapping-model-1",
    "title": "Disease Mapping",
    "section": "Disease Mapping Model",
    "text": "Disease Mapping Model\nThe parameter \\(\\lambda_i\\), sometimes also called relative risk, is modeled as follows:\n\\[\\begin{aligned}\nY_i | \\lambda_i &\\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i)\\\\\n\\log \\lambda_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\theta_i + \\epsilon_i,\\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{aligned}\\]\nSpatial Error Term:\n\n\\(\\theta_i \\in \\mathbb{R}\\) is a location-specific parameter that smooths data according to a neighborhood structure.\n\\(\\theta_i\\) induces spatial correlation, such that \\(\\lambda_i\\) in neighboring areas will be more similar."
  },
  {
    "objectID": "slides/19-disease-mapping.html#disease-mapping-model-2",
    "href": "slides/19-disease-mapping.html#disease-mapping-model-2",
    "title": "Disease Mapping",
    "section": "Disease Mapping Model",
    "text": "Disease Mapping Model\nThe relative risk \\(\\lambda_i\\) quantifies whether an area \\(i\\) has higher (\\(\\lambda_i &gt; 1\\)) or lower (\\(\\lambda_i &lt; 1\\)) risk than the average risk in the standard population (e.g., the whole population of the study region). For example, \\(\\lambda_i = 2\\) indicates the risk of area \\(i\\) is two times the average risk in the standard population."
  },
  {
    "objectID": "slides/19-disease-mapping.html#adjacency-matrix",
    "href": "slides/19-disease-mapping.html#adjacency-matrix",
    "title": "Disease Mapping",
    "section": "Adjacency Matrix",
    "text": "Adjacency Matrix\n\nWe will define the matrix \\(\\mathbf{W} \\in \\mathbb{R}^{n \\times n}\\) as the adjacency matrix.\n\nThis is sometimes called a proximity matrix or neighborhood matrix.\n\nEach entry (\\(w_{ij} = [\\mathbf{W}]_{ij}\\)) is given by: \\[w_{ij} = 1(i \\sim j) = \\left\\{ \\begin{array}{ll}\n       1 & \\mbox{if $i$ and $j$ share a border};\\\\\n       0 & \\mbox{otherwise}.\\end{array} \\right.\\]\nIn some cases, \\(w_{ij}\\) can be generalized to be non-binary."
  },
  {
    "objectID": "slides/19-disease-mapping.html#car-model",
    "href": "slides/19-disease-mapping.html#car-model",
    "title": "Disease Mapping",
    "section": "CAR Model",
    "text": "CAR Model\n\\[f(\\boldsymbol{\\theta} | \\tau^2) \\propto \\exp\\left\\{-\\frac{1}{2\\tau^2}\\boldsymbol{\\theta}^\\top \\left(\\mathbf{D}_w - \\mathbf{W}\\right) \\boldsymbol{\\theta}\\right\\},\\]\nwhere \\(\\mathbf{D}_w\\) is diagonal with \\([\\mathbf{D}_w]_{ii} = w_{i+}\\) and \\(w_{i+} = \\sum_{i=1}^n w_{ij}\\) (i.e., \\(w_{i+}\\) is the number of neighbors for locations \\(i\\))."
  },
  {
    "objectID": "slides/19-disease-mapping.html#visualzing-the-adjacency-matrix",
    "href": "slides/19-disease-mapping.html#visualzing-the-adjacency-matrix",
    "title": "Disease Mapping",
    "section": "Visualzing the Adjacency Matrix",
    "text": "Visualzing the Adjacency Matrix"
  },
  {
    "objectID": "slides/19-disease-mapping.html#compute-the-non-zero-edges-for-stan",
    "href": "slides/19-disease-mapping.html#compute-the-non-zero-edges-for-stan",
    "title": "Disease Mapping",
    "section": "Compute the Non-Zero Edges for Stan",
    "text": "Compute the Non-Zero Edges for Stan\n\n# Get the row-column pairs where the adjacency matrix is 1\n# This will return all non-zero indices in the adjacency matrix\nneighbor_pairs &lt;- which(W == 1, arr.ind = TRUE)\n\n# Filter out the upper triangle (to avoid repeating edges)\nneighbor_pairs_lower &lt;- neighbor_pairs[neighbor_pairs[, 1] &lt; neighbor_pairs[, 2], ]"
  },
  {
    "objectID": "slides/19-disease-mapping.html#visualzing-the-adjacency-matrix-1",
    "href": "slides/19-disease-mapping.html#visualzing-the-adjacency-matrix-1",
    "title": "Disease Mapping",
    "section": "Visualzing the Adjacency Matrix",
    "text": "Visualzing the Adjacency Matrix"
  },
  {
    "objectID": "slides/19-disease-mapping.html#icar-model",
    "href": "slides/19-disease-mapping.html#icar-model",
    "title": "Disease Mapping",
    "section": "ICAR Model",
    "text": "ICAR Model\nToday, we will look at the intrinsic CAR (ICAR) process for a vector \\(\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_n)^\\top\\), \\(\\boldsymbol{\\theta} | \\tau^2 \\sim \\text{ICAR}\\left(\\tau^2\\right)\\). Under this specification, the following joint distribution is given:\n\\[f(\\boldsymbol{\\theta} | \\tau^2) \\propto \\exp\\left\\{-\\frac{1}{2\\tau^2}\\boldsymbol{\\theta}^\\top \\left(\\mathbf{D}_w - \\mathbf{W}\\right) \\boldsymbol{\\theta}\\right\\},\\]\nwhere \\(\\mathbf{D}_w\\) is diagonal with \\([\\mathbf{D}_w]_{ii} = w_{i+}\\) and \\(w_{i+} = \\sum_{j=1}^n w_{ij}\\) (i.e., \\(w_{i+}\\) is the number of neighbors for locations \\(i\\)).\n\n\\(\\left(\\mathbf{D}_w - \\mathbf{W}\\right)\\) is singular, so \\(\\left(\\mathbf{D}_w - \\mathbf{W}\\right)^{-1}\\) does not exist and this distribution is improper.\nWe can still use this as a prior for \\(\\boldsymbol{\\theta}\\) and get a proper posterior!"
  },
  {
    "objectID": "slides/19-disease-mapping.html#icar-model-conditional-distributinos",
    "href": "slides/19-disease-mapping.html#icar-model-conditional-distributinos",
    "title": "Disease Mapping",
    "section": "ICAR Model: Conditional Distributinos",
    "text": "ICAR Model: Conditional Distributinos\nThe joint distribution on the previous slide can be written as a \\(n\\) conditional distributions:\n\\[\\theta_{i} | \\boldsymbol{\\theta}_{-i}, \\tau^2 \\sim  N \\left({\\frac{\\sum_{j=1}^n w_{ij}\\theta_{j}}{w_{i+}}},\\frac{\\tau^2}{w_{i+}}\\right)\\]\n\n\\(\\boldsymbol\\theta_{-j}\\): Vector of \\(\\theta_{i}\\) parameters with \\(\\theta_{j}\\) removed.\nThe mean is an average of the neighbors values.\nThe variance shrinks as a function of the number of neighbors."
  },
  {
    "objectID": "slides/19-disease-mapping.html#another-equivalent-specification",
    "href": "slides/19-disease-mapping.html#another-equivalent-specification",
    "title": "Disease Mapping",
    "section": "Another Equivalent Specification",
    "text": "Another Equivalent Specification\nPairwise difference specification:\n\\[f(\\boldsymbol{\\theta} | \\tau^2) \\propto \\exp\\left\\{-\\frac{1}{2\\tau^2}\\sum_{i \\sim j} w_{ij} (\\theta_i - \\theta_j)^2\\right\\},\\]\n\nThe impropriety of the distribution can also be seen here, because we can add any constant to all \\(\\theta_i\\) and the distribution is unaffected.\nA constraint such as \\(\\sum_{i=1}^n \\theta_i = 0\\) would provide the needed centering.\nWe will use this specification in Stan."
  },
  {
    "objectID": "slides/19-disease-mapping.html#full-disease-mapping-model",
    "href": "slides/19-disease-mapping.html#full-disease-mapping-model",
    "title": "Disease Mapping",
    "section": "Full Disease Mapping Model",
    "text": "Full Disease Mapping Model\nThe full model can be written as:\n\\[\\begin{aligned}\nY_i | \\lambda_i &\\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i)\\\\\n\\log \\lambda_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\theta_i + \\epsilon_i,\\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n\\boldsymbol{\\theta} | \\tau &\\sim \\text{ICAR}\\left(\\tau^2\\right)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\\\\\n\\end{aligned}\\]\nwhere \\(\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta}, \\sigma, \\tau)\\).\n\n\\(\\mu_i = \\exp\\{\\log E_i + \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\theta_i + \\epsilon_i\\}\\)."
  },
  {
    "objectID": "slides/19-disease-mapping.html#posterior-distribution",
    "href": "slides/19-disease-mapping.html#posterior-distribution",
    "title": "Disease Mapping",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\nDefine \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\). The posterior can be written as:\n\\[\\begin{aligned}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega},\\boldsymbol{\\theta})\\\\\n&\\propto f(\\mathbf{Y} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega})\\\\\n&\\propto f(\\boldsymbol{\\Omega}) \\prod_{i=1}^n f({Y}_i | \\lambda_i) f(\\theta_i | \\tau^2) .\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/19-disease-mapping.html#adding-the-icar-prior-to-stan",
    "href": "slides/19-disease-mapping.html#adding-the-icar-prior-to-stan",
    "title": "Disease Mapping",
    "section": "Adding the ICAR prior to Stan",
    "text": "Adding the ICAR prior to Stan\nWe will use the pairwise differences specification, so we need the unique pairs of neighbors. We will define \\(n_{edges}\\) as the number of non-zero edges. The following is added to the Stan data code chunk.\n\ndata {\n  int&lt;lower=0&gt; n;\n  int&lt;lower=0&gt; n_edges;\n  array[n_edges] int&lt;lower = 1, upper = n&gt; node1; // node1[i] adjacent to node2[i]\n  array[n_edges] int&lt;lower = 1, upper = n&gt; node2; // and node1[i] &lt; node2[i]\n  ...\n}"
  },
  {
    "objectID": "slides/19-disease-mapping.html#adding-the-icar-prior-to-stan-1",
    "href": "slides/19-disease-mapping.html#adding-the-icar-prior-to-stan-1",
    "title": "Disease Mapping",
    "section": "Adding the ICAR prior to Stan",
    "text": "Adding the ICAR prior to Stan\nWe can then add the following to the parameters and model Stan code chunks, where we leverage Stan’s ability to perform multi-indexing and vectorization!\n\nparameters {\n  vector[n] z;\n}\ntransformed parameters {\n  vector[n] theta = tau * z;\n}\nmodel {\n  target += -0.5 * dot_self(z[node1] - z[node2]);\n  // soft sum-to-zero constraint on z,\n  // equivalent to mean(z) ~ normal(0,0.01)\n  sum(z) ~ normal(0, 0.01 * n);\n}"
  },
  {
    "objectID": "slides/19-disease-mapping.html#extracting-the-non-zero-edges-for-stan",
    "href": "slides/19-disease-mapping.html#extracting-the-non-zero-edges-for-stan",
    "title": "Disease Mapping",
    "section": "Extracting the non-zero edges for Stan",
    "text": "Extracting the non-zero edges for Stan\nOur goal is to get the row-column pairs from \\(\\mathbf{W}\\) where the \\(w_{ij} = 1\\). This will return all non-zero indices in the adjacency matrix.\n\nneighbor_pairs &lt;- which(W == 1, arr.ind = TRUE)\n\nSince \\(\\mathbf{W}\\) is symmetric, we only need to keep the edges from above the diagonal to avoid repeating edges.\n\nneighbor_pairs_lower &lt;- neighbor_pairs[neighbor_pairs[, 1] &lt; neighbor_pairs[, 2], ]\nn_edges &lt;- nrow(neighbor_pairs_lower)\nnode1 &lt;- neighbor_pairs_lower[, 1]\nnode2 &lt;- neighbor_pairs_lower[, 2]"
  },
  {
    "objectID": "slides/19-disease-mapping.html#extracting-non-zero-edges-for-stan",
    "href": "slides/19-disease-mapping.html#extracting-non-zero-edges-for-stan",
    "title": "Disease Mapping",
    "section": "Extracting non-zero edges for Stan",
    "text": "Extracting non-zero edges for Stan\nOur goal is to get the row-column pairs from \\(\\mathbf{W}\\) where the \\(w_{ij} = 1\\). This will return all non-zero indices in the adjacency matrix.\n\nneighbor_pairs &lt;- which(W == 1, arr.ind = TRUE)\n\nSince \\(\\mathbf{W}\\) is symmetric, we only need to keep the edges from above the diagonal to avoid repeating edges.\n\nneighbor_pairs_lower &lt;- neighbor_pairs[neighbor_pairs[, 1] &lt; neighbor_pairs[, 2], ]\nn_edges &lt;- nrow(neighbor_pairs_lower)\nnode1 &lt;- neighbor_pairs_lower[, 1]\nnode2 &lt;- neighbor_pairs_lower[, 2]"
  },
  {
    "objectID": "slides/19-disease-mapping.html#modeling",
    "href": "slides/19-disease-mapping.html#modeling",
    "title": "Disease Mapping",
    "section": "Modeling",
    "text": "Modeling\nWe specify the following model:\n\\[\\begin{aligned}\nY_i | \\lambda_i &\\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i)\\\\\n\\log \\lambda_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\theta_i + \\epsilon_i,\\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n\\boldsymbol{\\theta} | \\tau &\\sim \\text{ICAR}\\left(\\tau^2\\right)\\\\\n\\alpha^* &\\sim N(0,3^2)\\\\\n\\beta_j &\\sim N(0,3^2), \\quad j = 1,\\ldots,p\\\\\n\\sigma &\\sim \\text{Half-Normal}(0, 3^2)\\\\\n\\tau &\\sim \\text{Half-Normal}(0, 3^2)\\\\\n\\end{aligned}\\]\nWhere \\(n = 100\\), \\(\\mathbf{x}_i = (\\text{age}_{i}, \\text{poverty}_i)\\)."
  },
  {
    "objectID": "slides/19-disease-mapping.html#full-stan-model-for-icar",
    "href": "slides/19-disease-mapping.html#full-stan-model-for-icar",
    "title": "Disease Mapping",
    "section": "Full Stan Model for ICAR",
    "text": "Full Stan Model for ICAR\n\n// saved in icar.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 0&gt; n_edges;\n  array[n_edges] int&lt;lower = 1, upper = n&gt; node1; // node1[i] adjacent to node2[i]\n  array[n_edges] int&lt;lower = 1, upper = n&gt; node2; // and node1[i] &lt; node2[i]\n  array[n] int&lt;lower = 0&gt; Y;\n  vector&lt;lower = 0&gt;[n] E;\n  matrix[n, p] X;\n}\ntransformed data {\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n  vector[n] logE = log(E);\n}\nparameters {\n  real alpha_star;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma; // precision of heterogeneous effects\n  real&lt;lower = 0&gt; tau; // precision of spatial effects\n  vector[n] z1; // spatial effects\n  vector[n] z2; // heterogeneous effects\n}\ntransformed parameters {\n  vector[n] theta = tau * z1;     // spatial effects\n  vector[n] epsilon = sigma * z2; // heterogeneous effects\n}\nmodel {\n  Y ~ poisson_log(logE + alpha_star + X_centered * beta + theta + epsilon);\n  // the following computes the ICAR prior on theta (through the standardized version z1)\n  target += -0.5 * dot_self(z1[node1] - z1[node2]);\n  // soft sum-to-zero constraint on theta)\n  sum(z1) ~ normal(0, 0.001 * n); // equivalent to mean(z1) ~ normal(0, 0.001)\n  // heterogeneous effects\n  z2 ~ std_normal();\n  // population parameters\n  alpha_star ~ normal(0, 3);\n  beta ~ normal(0, 3);\n  sigma ~ normal(0, 3);\n  tau ~ normal(0, 3);\n}\ngenerated quantities {\n  real alpha = alpha_star - X_bar * beta;\n  vector[n] log_mu = logE + alpha_star + X_centered * beta + theta + epsilon;\n  vector[n] lambda = exp(log_mu - logE);\n  vector[n] mu = exp(log_mu);\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = poisson_log_rng(log_mu[i]);\n    log_lik[i] = poisson_log_lpmf(Y[i] | log_mu[i]);\n  }\n}"
  },
  {
    "objectID": "slides/19-disease-mapping.html#examine-model-summaries",
    "href": "slides/19-disease-mapping.html#examine-model-summaries",
    "title": "Disease Mapping",
    "section": "Examine model summaries",
    "text": "Examine model summaries\n\nprint(fit_icar, pars = c(\"alpha\", \"alpha_star\", \"beta\", \"sigma\", \"tau\"))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=10000; warmup=5000; thin=1; \npost-warmup draws per chain=5000, total post-warmup draws=20000.\n\n            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nalpha      -0.99    0.01 0.42 -1.81 -1.27 -0.99 -0.71 -0.17  6567    1\nalpha_star  0.08    0.00 0.05 -0.01  0.05  0.08  0.12  0.18  6187    1\nbeta[1]     0.99    0.01 1.05 -1.06  0.28  0.99  1.70  3.06  5966    1\nbeta[2]     4.66    0.01 1.13  2.40  3.92  4.67  5.43  6.86  6293    1\nsigma       0.42    0.00 0.06  0.30  0.39  0.43  0.46  0.53  1087    1\ntau         0.25    0.01 0.17  0.01  0.11  0.22  0.36  0.64   550    1\n\nSamples were drawn using NUTS(diag_e) at Fri Mar 21 15:37:56 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/19-disease-mapping.html#compute-posterior-distribution-of-lambda_i",
    "href": "slides/19-disease-mapping.html#compute-posterior-distribution-of-lambda_i",
    "title": "Disease Mapping",
    "section": "Compute posterior distribution of \\(\\lambda_i\\)",
    "text": "Compute posterior distribution of \\(\\lambda_i\\)\n\nlambda &lt;- rstan::extract(fit_icar, pars = \"lambda\")$lambda\nlambda_mean &lt;- apply(lambda, 2, mean)\nlambda_sd &lt;- apply(lambda, 2, sd)\ncovid_nc_2020$lambda_mean &lt;- lambda_mean\ncovid_nc_2020$lambda_sd &lt;- lambda_sd"
  },
  {
    "objectID": "slides/19-disease-mapping.html#looking-at-smr-observed-versus-lambda_i",
    "href": "slides/19-disease-mapping.html#looking-at-smr-observed-versus-lambda_i",
    "title": "Disease Mapping",
    "section": "Looking at SMR observed versus \\(\\lambda_i\\)",
    "text": "Looking at SMR observed versus \\(\\lambda_i\\)"
  },
  {
    "objectID": "slides/19-disease-mapping.html#examine-traceplots",
    "href": "slides/19-disease-mapping.html#examine-traceplots",
    "title": "Disease Mapping",
    "section": "Examine traceplots",
    "text": "Examine traceplots\n\nrstan::traceplot(fit_icar, pars = c(\"alpha\", \"alpha_star\", \"beta\", \"sigma\", \"tau\"))"
  },
  {
    "objectID": "slides/19-disease-mapping.html#examine-traceplots-1",
    "href": "slides/19-disease-mapping.html#examine-traceplots-1",
    "title": "Disease Mapping",
    "section": "Examine traceplots",
    "text": "Examine traceplots\n\nrstan::traceplot(fit_icar, pars = \"theta[1]\")"
  },
  {
    "objectID": "slides/19-disease-mapping.html#mapping-plambda_i-1-mathbfy",
    "href": "slides/19-disease-mapping.html#mapping-plambda_i-1-mathbfy",
    "title": "Disease Mapping",
    "section": "Mapping \\(P(\\lambda_i > 1 | \\mathbf{Y})\\)",
    "text": "Mapping \\(P(\\lambda_i &gt; 1 | \\mathbf{Y})\\)"
  },
  {
    "objectID": "slides/19-disease-mapping.html#mapping-plambda_i-1-mathbfy-1",
    "href": "slides/19-disease-mapping.html#mapping-plambda_i-1-mathbfy-1",
    "title": "Disease Mapping",
    "section": "Mapping \\(P(\\lambda_i > 1 | \\mathbf{Y})\\)",
    "text": "Mapping \\(P(\\lambda_i &gt; 1 | \\mathbf{Y})\\)\nBinary indicator of \\(P(\\lambda_i &gt; 1 | \\mathbf{Y}) &gt; 0.95\\):"
  },
  {
    "objectID": "slides/19-disease-mapping.html#posterior-smr-across-north-carolina",
    "href": "slides/19-disease-mapping.html#posterior-smr-across-north-carolina",
    "title": "Disease Mapping",
    "section": "Posterior SMR Across North Carolina",
    "text": "Posterior SMR Across North Carolina"
  },
  {
    "objectID": "prepare/prepare-mar25.html",
    "href": "prepare/prepare-mar25.html",
    "title": "Prepare for March 25 lecture",
    "section": "",
    "text": "📖 Read Disease risk modeling by Moraga.\n📖 Review Spatial Models in Stan: Intrinsic Auto-Regressive Models for Areal Data by Morris 2019. Review through the section titled: Fitting the Model to the Scotland Lip Cancer Dataset.\n✅ Work on HW 05 which is due Tuesday April 8 before class."
  },
  {
    "objectID": "slides/19-disease-mapping.html#using-the-non-centered-parameterization",
    "href": "slides/19-disease-mapping.html#using-the-non-centered-parameterization",
    "title": "Disease Mapping",
    "section": "Using the non-centered parameterization",
    "text": "Using the non-centered parameterization\nIn Stan it is more computationally efficient to use a non-centered parameterization. We define, \\(\\mathbf{z} \\in \\mathbb{R}^n\\) and give it the following prior:\n\\[\\mathbf{z} \\sim \\text{ICAR}(\\tau^2 = 1), \\quad \\sum_{i=1}^n z_i = 0.\\]\nWe can then recover \\(\\boldsymbol{\\theta}\\) by computing \\(\\boldsymbol{\\theta} = \\tau \\mathbf{z}.\\)"
  },
  {
    "objectID": "slides/19-disease-mapping.html#non-centered-parameterization",
    "href": "slides/19-disease-mapping.html#non-centered-parameterization",
    "title": "Disease Mapping",
    "section": "Non-centered parameterization",
    "text": "Non-centered parameterization\nIn Stan it is more computationally efficient to use a non-centered parameterization. We define, \\(\\mathbf{z} \\in \\mathbb{R}^n\\) and give it the following prior:\n\\[\\mathbf{z} \\sim \\text{ICAR}(\\tau^2 = 1), \\quad \\sum_{i=1}^n z_i = 0.\\]\nWe can then recover \\(\\boldsymbol{\\theta}\\) by computing \\(\\boldsymbol{\\theta} = \\tau \\mathbf{z}.\\)"
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#the-durham-neighborhood-compass",
    "href": "ae/ae-08-disease-mapping.html#the-durham-neighborhood-compass",
    "title": "AE 08: Disease Mapping",
    "section": "The Durham Neighborhood Compass",
    "text": "The Durham Neighborhood Compass\n\nThe Durham Neighborhood Compass is a public data portal developed by the City and County of Durham, NC. It provides neighborhood-level indicators across many topics, including: health (e.g., diabetes, asthma), education, housing and development, crime and safety, and demographics and equity. Most indicators are available at the census tract level, allowing for spatial analysis of neighborhoods within Durham County. Data are updated regularly and reflect community conditions over time. Today we will focus on adult diabetes prevalence by census tract in 2019. The data is available in dnc.csv and can be downloaded directly from the Durham Neighborhood Compass website.\n\nThe variables in the dataset are as follows and are specific to each census tract in 2019.\n\nGEOID: location id for each census tract.\ndiabetes: percentage of adults with diabetes.\neducation: percentage of adults with a bachelors degree or higher.\nincome: median household income, in $10,000.\ncolor: percentage of people of color.\n\nThe data set is available in your AE repos and is called dnc.\n\ndnc &lt;- read_csv(\"data/dnc.csv\")"
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#creating-an-analysis-dataset",
    "href": "ae/ae-08-disease-mapping.html#creating-an-analysis-dataset",
    "title": "AE 08: Disease Mapping",
    "section": "Creating an analysis dataset",
    "text": "Creating an analysis dataset\nWe will begin by extracting the spatial information for Durham County, NC using the tigris R package.\n\ndurham_tracts &lt;- tracts(\n  state = \"NC\", \n  county = \"Durham\", \n  year = 2019,\n  class = \"sf\",\n  progress_bar = FALSE\n)\n\nThis object is an sf data object. The relevant variables are GEOID, which contains a unique identifier of each county and geometry, which contains the boundary information. For analyzing our data, we will first merge dnc with the sf data object. We begin by preparing the data for merging.\n\ndurham_merged &lt;- durham_tracts %&gt;%\n  mutate(GEOID = as.numeric(GEOID)) %&gt;%\n  left_join(dnc , by = \"GEOID\")\n\nWe can now visualize the adult diabetes proportion across the Durham county census tracts.\nggplot(durham_merged) +\n  geom_sf(aes(fill = diabetes), color = \"gray20\") +\n  scale_fill_viridis_c(option = \"plasma\", na.value = \"lightgray\") +\n  labs(title = \"Adult Diabetes Rate by Census Tract (Durham, 2019)\",\n       fill = \"% with Diabetes\") +\n  scale_fill_viridis_c() + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nNote that there are three locations that do not have any data, represented in gray. These locations correspond to census tracts with a zero residents. For the analysis we will remove them."
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#computing-the-adjacnecy-matrix",
    "href": "ae/ae-08-disease-mapping.html#computing-the-adjacnecy-matrix",
    "title": "AE 08: Disease Mapping",
    "section": "Computing the adjacnecy matrix",
    "text": "Computing the adjacnecy matrix\nTo fit the model, we required an adjacency matrix, \\(\\mathbf{W}\\). We only want to compute our adjacency matrix for the locations with non-zero residents. Thus, we create a dataset that removes census tracts with NA for diabetes.\n\ndurham_model_data &lt;- durham_merged %&gt;%\n  filter(!is.na(diabetes))\n\nWe then use functions for the R package spdep to create a neighbor list and adjacency matrix.\n\nnb &lt;- poly2nb(durham_model_data)\nW &lt;- nb2mat(nb, style = \"B\", zero.policy = TRUE)\n\nWe can visualize the adjacency matrix. To do this we first get the centroids of each census tract.\n\ncentroids &lt;- st_centroid(durham_model_data)\ncentroids &lt;- centroids %&gt;% mutate(id = 1:nrow(centroids))\n\nWe then compute the edges and create a data frame with the indices of the neighboring census tracts.\n\nneighbor_pairs &lt;- which(W == 1, arr.ind = TRUE)\nedges &lt;- data.frame(\n  from = neighbor_pairs[, 1],\n  to = neighbor_pairs[, 2]\n)\n\nThis plot shows the adjacency structure among Durham County census tracts used in the spatial model. The function geom_sf() draws the outlines of the tracts and also plots red dots at each tract centroid to show where the adjacency links originate. The function geom_segment() draws blue lines between neighboring tracts, based on their centroids.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we can compute the needed objects for Stan.\n\nneighbor_pairs_lower &lt;- neighbor_pairs[neighbor_pairs[, 1] &lt; neighbor_pairs[, 2], ]\nn_edges &lt;- nrow(neighbor_pairs_lower)\nnode1 &lt;- neighbor_pairs_lower[, 1]\nnode2 &lt;- neighbor_pairs_lower[, 2]"
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#computing-the-adjacency-matrix",
    "href": "ae/ae-08-disease-mapping.html#computing-the-adjacency-matrix",
    "title": "AE 08: Disease Mapping",
    "section": "Computing the adjacency matrix",
    "text": "Computing the adjacency matrix\nTo fit the model, we required an adjacency matrix, \\(\\mathbf{W}\\). We only want to compute our adjacency matrix for the locations with non-zero residents. Thus, we create a dataset that removes census tracts with NA for diabetes.\n\ndurham_model_data &lt;- durham_merged %&gt;%\n  filter(!is.na(diabetes))\n\nWe then use functions for the R package spdep to create a neighbor list and adjacency matrix.\n\nnb &lt;- poly2nb(durham_model_data)\nW &lt;- nb2mat(nb, style = \"B\", zero.policy = TRUE)\n\nWe can visualize the adjacency matrix. To do this we first get the centroids of each census tract.\n\ncentroids &lt;- st_centroid(durham_model_data)\ncentroids &lt;- centroids %&gt;% mutate(id = 1:nrow(centroids))\n\nWe then compute the edges and create a data frame with the indices of the neighboring census tracts.\n\nneighbor_pairs &lt;- which(W == 1, arr.ind = TRUE)\nedges &lt;- data.frame(\n  from = neighbor_pairs[, 1],\n  to = neighbor_pairs[, 2]\n)\n\nThis plot shows the adjacency structure among Durham County census tracts used in the spatial model. The function geom_sf() draws the outlines of the tracts and also plots red dots at each tract centroid to show where the adjacency links originate. The function geom_segment() draws blue lines between neighboring tracts, based on their centroids.\nggplot(data = durham_model_data) +\n  geom_sf(fill = NA, color = \"black\") +\n  geom_segment(data = edges,\n               aes(x = st_coordinates(centroids)[from, 1], y = st_coordinates(centroids)[from, 2],\n                   xend = st_coordinates(centroids)[to, 1], yend = st_coordinates(centroids)[to, 2]),\n               color = \"blue\", size = 1.5) +\n  geom_sf(data = centroids, aes(geometry = geometry), color = \"red\", size = 3) +\n  theme_minimal() +\n  coord_sf() +\n  labs(x = \"Longitude\", y = \"Latitude\") +\n  ggtitle(\"Census Tract Adjacency Map\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nFinally, we can compute the needed objects for the ICAR prior.\n\nneighbor_pairs_lower &lt;- neighbor_pairs[neighbor_pairs[, 1] &lt; neighbor_pairs[, 2], ]\nn_edges &lt;- nrow(neighbor_pairs_lower)\nnode1 &lt;- neighbor_pairs_lower[, 1]\nnode2 &lt;- neighbor_pairs_lower[, 2]"
  },
  {
    "objectID": "hw/hw-05.html#exercise-1",
    "href": "hw/hw-05.html#exercise-1",
    "title": "HW 05: Geospatial Modeling",
    "section": "Exercise 1",
    "text": "Exercise 1\nBased on AE-07, process the drc data to create an sf data object that can be used for spatial analyses. Create a map that visualizes the proportion of participants with anemia at each community in Sud-Kivu.",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "hw/hw-05.html#data",
    "href": "hw/hw-05.html#data",
    "title": "HW 05: Geospatial Modeling",
    "section": "Data",
    "text": "Data\nWe will look at a sample of women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. There are ~8600 women who are nested in ~500 survey clusters. The variables in the dataset are as follows.\n\nloc_id: location id (i.e. survey cluster).\nhemoglobin: hemoglobin level (g/dL).\nanemia: anemia classifications.\nage: age in years.\nurban: urban vs. rural.\nLATNUM: latitude.\nLONGNUM: longitude.\nmean_hemoglobin: average hemoglobin at each community (g/dL).\ncommunity_size: number of participants at each community.\nmean_age: average age of participants at each community (years).\n\nThe data set is the same one that was used in AE-07 and available in your HW repos.\n\ndrc &lt;- readRDS(\"drc.rds\")",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "hw/hw-05.html#modeling",
    "href": "hw/hw-05.html#modeling",
    "title": "HW 05: Geospatial Modeling",
    "section": "Modeling",
    "text": "Modeling\nIn this homework, we will focus on the Sud-Kivu state within the DRC. Researchers are interested in mapping risk of anemia across Sud-Kivu. Anemia is a condition in which the body lacks enough healthy red blood cells to carry adequate oxygen to tissues. It is commonly associated with low levels of hemoglobin, the oxygen-carrying protein in red blood cells. Anemia can result from malnutrition, especially deficiencies in iron, folate, or vitamin B12, and is linked to a range of negative outcomes including fatigue, impaired cognitive development, weakened immunity, and poor pregnancy outcomes. In children and women, anemia is often a marker of underlying nutritional and health disparities.\nTo study anemia, researchers would like to fit a logistic regression model using a Bayesian hierarchical model with spatially varying intercepts. Define \\(Y_{ij} \\in \\{0,1\\}\\) as the indicator of anemia at location \\(i\\) (\\(i = 1,\\ldots,n\\)) for participant \\(j\\) (\\(j = 1,\\ldots, n_i\\)).\n\ndrc &lt;- drc %&gt;%\n  mutate(anemic = if_else(anemia == \"not anemic\", 0, 1))\n\nWe will fit the following model:\n\\[\\begin{align*}\nY_j(\\mathbf{u}_i) | \\alpha, \\boldsymbol{\\beta}, \\theta(\\mathbf{u}_i) &\\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_{j}(\\mathbf{u}_i))\\\\\n\\text{logit}(\\pi_{j}(\\mathbf{u}_i)) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i)\\\\\n\\boldsymbol{\\theta} | \\tau,\\rho &\\sim N(\\mathbf{0}_n,\\mathbf{C})\\\\\n\\alpha^* &\\sim N(0,4^2)\\\\\n\\beta_j | \\sigma_{\\beta} &\\sim N(0,\\sigma_{\\beta}^2), \\quad j = 1,\\ldots,p\\\\\n\\tau &\\sim \\text{Half-Normal}(0, 4^2)\\\\\n\\rho &\\sim \\text{Inv-Gamma}(5, 5)\\\\\n\\sigma_{\\beta} &\\sim \\text{Half-Normal}(0, 2^2),\n\\end{align*}\\] where \\(\\boldsymbol{\\theta} = (\\theta(\\mathbf{u}_1),\\ldots,\\theta(\\mathbf{u}_n))^\\top\\), \\(N = \\sum_{i=1}^n n_i = 490\\), \\(n = 29\\), \\(\\mathbf{x}_j(\\mathbf{u}_i) = (\\text{age}_{ij}/10, \\text{urban}_i)\\), and \\(\\mathbf{C}\\) a covariance matrix produced using the Matérn 3/2 covariance function with scale parameter \\(\\tau\\) and length scale \\(\\rho\\). The parameter \\(\\alpha^*\\) is the intercept after centering, such that \\(\\text{logit}(\\pi_{j}(\\mathbf{u}_i)) = \\alpha^* + \\mathbf{x}_j^*(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i)\\), where \\(\\mathbf{x}_j^*(\\mathbf{u}_i) = \\mathbf{x}_j(\\mathbf{u}_i) - \\bar{\\mathbf{x}}_j(\\mathbf{u}_i)\\) and \\(\\bar{\\mathbf{x}}_j(\\mathbf{u}_i) = \\frac{1}{N} \\sum_{i=1}^n \\sum_{j=1}^{n_i} \\mathbf{x}_j(\\mathbf{u}_i)\\).",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "hw/hw-05.html#exercise-2",
    "href": "hw/hw-05.html#exercise-2",
    "title": "HW 05: Geospatial Modeling",
    "section": "Exercise 2",
    "text": "Exercise 2\nCreate a \\(25 \\times 25\\) grid of points to be used for prediction, and only keep those points within Sud-Kivu. Visualize the grid.",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "hw/hw-05.html#exercise-3",
    "href": "hw/hw-05.html#exercise-3",
    "title": "HW 05: Geospatial Modeling",
    "section": "Exercise 3",
    "text": "Exercise 3\nResearchers are interested in understanding the a priori correlation between the anemia probability between two communities across Sud-Kivu. Visualize the correlation as a function of distance between two locations given in miles, evaluated at the prior mean for \\(\\rho\\). What is the a priori correlation between two locations that are 25 miles apart?",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "hw/hw-05-blank.html",
    "href": "hw/hw-05-blank.html",
    "title": "HW 05",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "hw/hw-05.html#exercise-4",
    "href": "hw/hw-05.html#exercise-4",
    "title": "HW 05: Geospatial Modeling",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit the model detailed above. Present evidence of MCMC convergence.",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "hw/hw-05.html#exercise-5",
    "href": "hw/hw-05.html#exercise-5",
    "title": "HW 05: Geospatial Modeling",
    "section": "Exercise 5",
    "text": "Exercise 5\nResearchers would like to understand the a posteriori correlation between the anemia probability between two communities across Sud-Kivu. Repeat Exercise 3 using the posterior mean of \\(\\rho\\). Visualize the correlations from the prior and posterior on the same figure. How does the posterior correlation compare to the prior?",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "hw/hw-05.html#exercise-6",
    "href": "hw/hw-05.html#exercise-6",
    "title": "HW 05: Geospatial Modeling",
    "section": "Exercise 6",
    "text": "Exercise 6\nPresent posterior means and intervals for the odds ratios for age and urbanality. Are any of these predictors associated with anemia?",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "hw/hw-05.html#exercise-7",
    "href": "hw/hw-05.html#exercise-7",
    "title": "HW 05: Geospatial Modeling",
    "section": "Exercise 7",
    "text": "Exercise 7\nMap the posterior mean probability of anemia across Sud-Kivu, \\(\\pi(\\mathbf{u}_i)\\). To make this map, we must specify \\(x(\\mathbf{u}_i) = (2.8, 0)\\), such that the age is 28 years (average age in Sud-Kivu) and we assume locations are rural (most common community location). Describe any spatial patterns that arise.",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "hw/hw-05.html#exercise-8",
    "href": "hw/hw-05.html#exercise-8",
    "title": "HW 05: Geospatial Modeling",
    "section": "Exercise 8",
    "text": "Exercise 8\nCreate a map of the posterior standard deviation that corresponds to your map in Exercise 7. Describe any spatial patterns that arise.",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "hw/hw-05.html#exercise-9",
    "href": "hw/hw-05.html#exercise-9",
    "title": "HW 05: Geospatial Modeling",
    "section": "Exercise 9",
    "text": "Exercise 9\nResearchers would like to identify locations in Sud-Kivu where a public health intervention is warranted. The are intrerested in intervening in locations where the posterior probability of anemia is greater than 0.4, \\(p_i = P(\\pi(\\mathbf{u}_i) &gt; 0.4 | \\mathbf{Y})\\). In particular, any locations with \\(p_i\\) greater than 0.5 \\(\\left(h_i = 1(p_i &gt; 0.5)\\right)\\) indicates a hot spot region where intervention is needed. Visualize both the \\(p_i\\) and \\(h_i\\) and make a statement about where intervention is warranted.",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "ae/ae-07-geospatial.html#democratic-republic-of-congo-demographic-and-health-survey",
    "href": "ae/ae-07-geospatial.html#democratic-republic-of-congo-demographic-and-health-survey",
    "title": "AE 07: Geospatial",
    "section": "Democratic Republic of Congo Demographic and Health Survey",
    "text": "Democratic Republic of Congo Demographic and Health Survey\nWe will look at a sample of women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. There are ~8600 women who are nested in ~500 survey clusters. The variables in the dataset are as follows.\n\nloc_id: location id (i.e. survey cluster).\nhemoglobin: hemoglobin level (g/dL).\nanemia: anemia classifications.\nage: age in years.\nurban: urban vs. rural.\nLATNUM: latitude.\nLONGNUM: longitude.\nmean_hemoglobin: average hemoglobin at each community (g/dL).\ncommunity_size: number of participants at each community.\nmean_age: average age of participants at each community (years).\n\nThe data set is available in your AE repos and is called drc.\n\ndrc &lt;- readRDS(\"drc.rds\")"
  },
  {
    "objectID": "prepare/prepare-mar27.html",
    "href": "prepare/prepare-mar27.html",
    "title": "Prepare for March 27 lecture",
    "section": "",
    "text": "📖 Review Meta-analysis of rare adverse events in randomized clinical trials: Bayesian and frequentist methods by Hong et al. 2020.\n📖 Review Safety and efficacy of hydroxychloroquine as prophylactic against COVID-19 in healthcare workers: a meta-analysis of randomised clinical trials by Hong et al. 2023.\n✅ Work on HW 05 which is due Tuesday April 8 before class."
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#fitting-the-model",
    "href": "ae/ae-08-disease-mapping.html#fitting-the-model",
    "title": "AE 08: Disease Mapping",
    "section": "Fitting the Model",
    "text": "Fitting the Model\nWe can then compute the design matrix, making sure to remove the intercept.\n\nX &lt;- model.matrix(~ education + income + color, data = durham_model_data)[, -1]\n\nAnd then we can create the Stan data object.\n\nstan_data &lt;- list(\n  n = nrow(durham_model_data),\n  p = ncol(X),\n  n_edges = n_edges,\n  node1 = node1,\n  node2 = node2,\n  Y = durham_model_data$diabetes,\n  X = X\n)\n\nWe will now fit the model and print the posterior summaries and MCMC convergence diagnostics.\n\nicar &lt;- stan_model(file = \"icar.stan\")\nfit_icar &lt;- sampling(icar, stan_data, \n                     iter = 5000, control = list(adapt_delta = 0.99))\nprint(fit_icar, pars = c(\"alpha_star\", \"alpha\", \"beta\", \"sigma\", \"tau\"))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nalpha_star  0.13       0 0.00  0.13  0.13  0.13  0.13  0.13 20490 1.00\nalpha       0.20       0 0.03  0.14  0.18  0.20  0.22  0.25  2169 1.00\nbeta[1]    -0.09       0 0.03 -0.14 -0.11 -0.09 -0.07 -0.03  1931 1.00\nbeta[2]     0.00       0 0.00 -0.01 -0.01  0.00  0.00  0.00  5675 1.00\nbeta[3]     0.02       0 0.02 -0.02  0.01  0.02  0.04  0.07  2660 1.00\nsigma       0.02       0 0.00  0.01  0.01  0.02  0.02  0.02   541 1.01\ntau         0.02       0 0.01  0.00  0.01  0.02  0.03  0.04   591 1.01\n\nSamples were drawn using NUTS(diag_e) at Tue Mar 25 10:52:25 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nNote that I increased adapt_delta from its default of 0.8 to help with the convergence. We will can also look at traceplots, which look good. The number of effective samples is quite small for the variance parameters, which is a bit concerning, however since the \\(\\hat{R}\\) and traceplots look good, we can proceed with inference.\n\nrstan::traceplot(fit_icar, pars = c(\"alpha_star\", \"alpha\", \"beta\", \"sigma\", \"tau\"))"
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#posterior-predictive-distribution",
    "href": "ae/ae-08-disease-mapping.html#posterior-predictive-distribution",
    "title": "AE 08: Disease Mapping",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\nWe will now look at the posterior predictive distributions, which we can use to map the diabetes proportion across Durham census tracts. We begin by extracting the posterior predictive distribution and then compute summaries.\n\nY_pred &lt;- rstan::extract(fit_icar, pars = \"Y_pred\")$Y_pred\nppd_mean &lt;- apply(Y_pred, 2, mean)\nppd_sd &lt;- apply(Y_pred, 2, sd)\n\nWe then merge the predictions back to the original dataset, with all census tracts, including those with the NA for diabetes, so that our maps are across all census tracts. We begin by creating a data object that contains our predictions that can then be merged into the full data.\n\nmodel_geoids &lt;- durham_merged %&gt;%\n  filter(!is.na(diabetes)) %&gt;%\n  pull(GEOID)\npredictions &lt;- tibble(GEOID = model_geoids,\n                      ppd_mean = ppd_mean,\n                      ppd_sd = ppd_sd)\n\nWe now merge predictions back into the full spatial object for plotting.\n\ndurham_plot_data &lt;- durham_merged %&gt;%\n  left_join(predictions, by = \"GEOID\")\n\nThe code to create a map of the observed diabetes proportion and the posterior mean is below. Note that I make sure to have each plot be on the same color scale, so they are comparable. I first compute the maximum value needed for the upper bound and then use the limits functions.\n\nmax_limit &lt;- max(c(durham_merged$diabetes,\n                   durham_plot_data$ppd_mean), na.rm = TRUE)\n\nThe maps can be created as follows.\nggplot(durham_plot_data) +\n  geom_sf(aes(fill = diabetes), color = \"gray20\") +\n  scale_fill_viridis_c(option = \"plasma\", na.value = \"lightgray\") +\n  labs(title = \"Adult Diabetes Rate by Census Tract (Durham, 2019)\",\n       fill = \"% with Diabetes\") +\n  scale_fill_viridis_c(\n    limits = c(0, max_limit),\n    na.value = \"lightgray\"  # Gray out missing tracts\n  ) + \n  theme_minimal()\nggplot(durham_plot_data) +\n  geom_sf(aes(fill = ppd_mean), color = \"black\") +\n  scale_fill_viridis_c(\n    limits = c(0, max_limit),\n    na.value = \"lightgray\"  # Gray out missing tracts\n  ) +\n  labs(\n    title = \"Predicted Diabetes Rate by Census Tract (ICAR Model)\",\n    fill = \"Predicted %\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#exercise-1",
    "href": "ae/ae-08-disease-mapping.html#exercise-1",
    "title": "AE 08: Disease Mapping",
    "section": "Exercise 1",
    "text": "Exercise 1\nExamine the two side-by-side plots above and compare and contrast them. What changes to do you observe?\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#exercise-2",
    "href": "ae/ae-08-disease-mapping.html#exercise-2",
    "title": "AE 08: Disease Mapping",
    "section": "Exercise 2",
    "text": "Exercise 2\nCreate a map of the posterior predictive standard deviation across the Durham census tracts. What patterns do you observe?\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-08-disease-mapping.html#exercise-3",
    "href": "ae/ae-08-disease-mapping.html#exercise-3",
    "title": "AE 08: Disease Mapping",
    "section": "Exercise 3",
    "text": "Exercise 3\nPresent posterior summaries of \\(\\boldsymbol{\\beta}\\). Which predictors are associated with adult diabetes?\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "prepare/prepare-apr01.html",
    "href": "prepare/prepare-apr01.html",
    "title": "Prepare for April 1 lecture",
    "section": "",
    "text": "📖 Read Sections 1-3 of Practical Hilbert space approximate Bayesian Gaussian processes for probabilistic programming by Riutort-Mayol et al. 2022. You don’t need to fully understand everything in the paper!\n✅ Work on HW 05 which is due Tuesday April 8 before class."
  },
  {
    "objectID": "prepare/prepare-apr03.html",
    "href": "prepare/prepare-apr03.html",
    "title": "Prepare for April 3 lecture",
    "section": "",
    "text": "📖 Review corrected proof from Tuesday’s lecture.\n📖 Read Sections 4-6 of Practical Hilbert space approximate Bayesian Gaussian processes for probabilistic programming by Riutort-Mayol et al. 2022. You don’t need to fully understand everything in the paper!\n✅ Work on HW 05 which is due Tuesday April 8 before class."
  },
  {
    "objectID": "slides/21-scalable-1.html#review-of-previous-lectures",
    "href": "slides/21-scalable-1.html#review-of-previous-lectures",
    "title": "Scalable Gaussian Processes #1",
    "section": "",
    "text": "Two weeks ago, we learned about:\n\nGaussian processes, and\nHow to use Gaussian processes for\n\nlongitudinal data\ngeospatial data"
  },
  {
    "objectID": "slides/21-scalable-1.html#map-of-the-sud-kivu-state",
    "href": "slides/21-scalable-1.html#map-of-the-sud-kivu-state",
    "title": "Scalable Gaussian Processes #1",
    "section": "Map of the Sud-Kivu state",
    "text": "Map of the Sud-Kivu state\nLast time, we focused on one state with ~500 observations at ~30 locations."
  },
  {
    "objectID": "slides/21-scalable-1.html#prediction-for-the-sud-kivu-state",
    "href": "slides/21-scalable-1.html#prediction-for-the-sud-kivu-state",
    "title": "Scalable Gaussian Processes #1",
    "section": "Prediction for the Sud-Kivu state",
    "text": "Prediction for the Sud-Kivu state\nAnd we created a \\(20 \\times 20\\) grid for prediction of the spatial intercept surface over the Sud-Kivu state."
  },
  {
    "objectID": "slides/21-scalable-1.html#prediction-for-the-drc",
    "href": "slides/21-scalable-1.html#prediction-for-the-drc",
    "title": "Scalable Gaussian Processes #1",
    "section": "Prediction for the DRC",
    "text": "Prediction for the DRC\nAnd we will make predictions on a \\(30 \\times 30\\) grid over the DRC."
  },
  {
    "objectID": "slides/21-scalable-1.html#modeling-2",
    "href": "slides/21-scalable-1.html#modeling-2",
    "title": "Scalable Gaussian Processes #1",
    "section": "Modeling",
    "text": "Modeling\nWe specify the following model: \\[\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})\\] with priors\n\n\\(\\boldsymbol{\\theta}(\\mathbf{u}) | \\tau,\\rho \\sim GP(\\mathbf{0},C(\\cdot,\\cdot))\\), where \\(C\\) is the Matérn 3/2 covariance function with magnitude \\(\\tau\\) and length scale \\(\\rho\\).\n\\(\\alpha^* \\sim N(0,4^2)\\). This is the intercept after centering \\(\\mathbf{X}\\).\n\\(\\beta_j | \\sigma_{\\beta} \\sim N(0,\\sigma_{\\beta}^2)\\), \\(j \\in \\{1,\\dots,p\\}\\)\n\\(\\sigma \\sim \\text{Half-Normal}(0, 2^2)\\)\n\\(\\tau \\sim \\text{Half-Normal}(0, 4^2)\\)\n\\(\\rho \\sim \\text{Inv-Gamma}(5, 5)\\)\n\\(\\sigma_{\\beta} \\sim \\text{Half-Normal}(0, 2^2)\\)"
  },
  {
    "objectID": "slides/21-scalable-1.html#lecture-plan",
    "href": "slides/21-scalable-1.html#lecture-plan",
    "title": "Scalable Gaussian Processes #1",
    "section": "Lecture plan",
    "text": "Lecture plan\nToday:\n\nHow does HSGP work\nWhy HSGP is scalable\nHow to use HSGP for Bayesian geospatial model fitting and posterior predictive sampling\n\n\nThursday:\n\nParameter tuning for HSGP\nHow to implement HSGP in stan"
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-approximation",
    "href": "slides/21-scalable-1.html#hsgp-approximation",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation",
    "text": "HSGP approximation\nGiven:\n\nan isotropic covariance function \\(C\\) which admits a power spectral density, e.g., the Matérn family, and\na compact domain \\(\\boldsymbol{\\Theta} \\in \\mathbb{R}^d\\) with smooth boundaries. For our purposes, we only consider boxes, e.g., \\([-1,1] \\times [-1,1]\\).\n\nHSGP approximates the \\((i,j)\\) element of the corresponding \\(n \\times n\\) covariance matrix \\(\\mathbf{C}\\) as \\[\\mathbf{C}_{ij}=C(\\|\\mathbf{u}_i - \\mathbf{u}_j\\|) \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j).\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-approximation-1",
    "href": "slides/21-scalable-1.html#hsgp-approximation-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation",
    "text": "HSGP approximation\n\\[\\mathbf{C}_{ij}=C(\\|\\mathbf{u}_i - \\mathbf{u}_j\\|) \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j).\\]\n\n\\(s_k \\in \\mathbb{R}^+\\) are positive scalars which depends on the covariance function \\(C\\) and its parameters \\(\\tau\\) and \\(\\rho\\).\n\\(\\phi_k: \\boldsymbol{\\Theta} \\to \\mathbb{R}\\) are basis functions which only depends on \\(\\boldsymbol{\\Theta}\\).\n\\(m\\) is the number of basis functions. Note: even with an infinite sum (i.e., \\(m \\to \\infty\\)), this remains an approximation (see Solin and Särkkä (2020))."
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-approximation-2",
    "href": "slides/21-scalable-1.html#hsgp-approximation-2",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation",
    "text": "HSGP approximation\nIn matrix notation,\n\\[\\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top.\\]\n\n\\(\\boldsymbol{\\Phi} \\in \\mathbb{R}^{n \\times m}\\) is a feature matrix. Only depends on \\(\\boldsymbol{\\Theta}\\) and the observed locations.\n\\(\\mathbf{S} \\in \\mathbb{R}^{m \\times m}\\) is diagonal. Depends on the covariance function \\(C\\) and parameters \\(\\tau\\) and \\(\\rho\\).\n\n\\[\n\\begin{align}\n  \\boldsymbol{\\Phi} = \\begin{bmatrix}\n  \\phi_1(\\mathbf{u}_1) & \\dots & \\phi_m(\\mathbf{u}_1) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\phi_1(\\mathbf{u}_n) & \\dots & \\phi_m(\\mathbf{u}_n)\n  \\end{bmatrix}, \\quad\n  \\mathbf{S} = \\begin{bmatrix}\n  s_1 &  &  \\\\\n  & \\ddots &  \\\\\n  &  & s_m\n  \\end{bmatrix}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-in-stan",
    "href": "slides/21-scalable-1.html#hsgp-in-stan",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP in stan",
    "text": "HSGP in stan\nSimilarly, we can use the reparameterized model in stan.\nThis is called the non-centered parameterization in stan documentation. It’s recommended for computational efficiency for hierarchical models.\n\ntransformed data {\n  matrix[n,m] PHI;\n  matrix[N,m] Z;\n  matrix[N,p] X_centered;\n}\nparameters {\n  real alpha_star;\n  real&lt;lower=0&gt; sigma;\n  vector[p] beta;\n  vector[m] b;\n  vector&lt;lower=0&gt;[m] sqrt_S;\n  ...\n}\nmodel {\n  vector[n] theta = PHI * (sqrt_S .* b);\n  target += normal_lupdf(y | alpha_star + X_centered * beta + Z * theta, sigma);\n  target += normal_lupdf(b | 0, 1);\n  ...\n}"
  },
  {
    "objectID": "slides/21-scalable-1.html#posterior-predictive-distribution",
    "href": "slides/21-scalable-1.html#posterior-predictive-distribution",
    "title": "Scalable Gaussian Processes #1",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nWe want to make predictions for \\(\\mathbf{Y}^* = (Y(\\mathbf{u}_{n+1}),\\ldots, Y(\\mathbf{u}_{n+q}))^\\top\\), observations at \\(q\\) new locations. Define \\(\\boldsymbol{\\theta}^* = (\\theta(\\mathbf{u}_{n+1}),\\ldots,\\theta(\\mathbf{u}_{n+q}))^\\top\\), \\(\\boldsymbol{\\Omega} = (\\alpha,\\boldsymbol{\\beta},\\sigma,\\tau,\\rho)\\). Recall:\n\\[\\begin{align*}\n  f(\\mathbf{Y}^* | \\mathbf{Y}) &= \\int f(\\mathbf{Y}^*, \\boldsymbol{\\theta}^*, \\boldsymbol{\\theta}, \\boldsymbol{\\Omega} | \\mathbf{Y}) d\\boldsymbol{\\theta}^* d\\boldsymbol{\\theta} d\\boldsymbol{\\Omega}\\\\\n  &= \\int \\underbrace{f(\\mathbf{Y}^* | \\boldsymbol{\\theta}^*, \\boldsymbol{\\Omega})}_{(1)} \\underbrace{f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})}_{(2)} \\underbrace{f(\\boldsymbol{\\theta},\\boldsymbol{\\Omega} | \\mathbf{Y})}_{(3)} d\\boldsymbol{\\theta}^* d\\boldsymbol{\\theta} d\\boldsymbol{\\Omega}\\\\\n\\end{align*}\\]\n\nLikelihood: \\(f(\\mathbf{Y}^* | \\boldsymbol{\\theta}^*, \\boldsymbol{\\Omega})\\) – remains the same as for GP\nKriging: \\(f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})\\) – we will focus on this next\nPosterior distribution: \\(f(\\boldsymbol{\\theta},\\boldsymbol{\\Omega} | \\mathbf{Y})\\) – we have just discussed"
  },
  {
    "objectID": "slides/21-scalable-1.html#kriging",
    "href": "slides/21-scalable-1.html#kriging",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging",
    "text": "Kriging\nRecall under the GP prior,\n\\[\\begin{bmatrix}\n    \\boldsymbol{\\theta}\\\\\n    \\boldsymbol{\\theta}^*\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega} \\sim N_{n+q}\\left(\\begin{bmatrix}\n    \\mathbf{0}_n \\\\\n    \\mathbf{0}_q\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\mathbf{C} & \\mathbf{C}_{+}\\\\\n    \\mathbf{C}_{+}^\\top & \\mathbf{C}^*\n  \\end{bmatrix}\\right),\\]\nwhere \\(\\mathbf{C}\\) is the covariance of \\(\\boldsymbol{\\theta}\\), \\(\\mathbf{C}^*\\) is the covariance of \\(\\boldsymbol{\\theta}^*\\), and \\(\\mathbf{C}_{+}\\) is the cross covariance matrix between \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\theta}^*\\).\nTherefore by properties of multivariate normal, \\[\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) \\sim N_q(\\mathbb{E}_{\\boldsymbol{\\theta}^*},\\mathbb{V}_{\\boldsymbol{\\theta}^*}), \\quad \\text{where}\\] \\[\n\\begin{align}\n  \\mathbb{E}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}_+^\\top \\mathbf{C}^{-1} \\boldsymbol{\\theta}\\\\\n  \\mathbb{V}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}^* - \\mathbf{C}_+^\\top \\mathbf{C}^{-1} \\mathbf{C}_+.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#kriging-under-hsgp",
    "href": "slides/21-scalable-1.html#kriging-under-hsgp",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging under HSGP",
    "text": "Kriging under HSGP\nUnder HSGP, \\(\\mathbf{C}^* \\approx \\boldsymbol{\\Phi}^* \\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}\\), \\(\\mathbf{C}_+ \\approx \\boldsymbol{\\Phi} \\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}\\), where \\[\n\\begin{align}\n  \\boldsymbol{\\Phi}^* \\in \\mathbb{R}^{q \\times m} = \\begin{bmatrix}\n  \\phi_1(\\mathbf{u}_{n+1}) & \\dots & \\phi_m(\\mathbf{u}_{n+1}) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\phi_1(\\mathbf{u}_{n+q}) & \\dots & \\phi_m(\\mathbf{u}_{n+q})\n  \\end{bmatrix}\n\\end{align}\n\\] is the feature matrix for the new locations. Therefore approximately \\[\n\\begin{align}\n  \\begin{bmatrix}\n    \\boldsymbol{\\theta} \\\\\n    \\boldsymbol{\\theta}^*\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega} \\sim N_{n\n  +q} \\left(\\begin{bmatrix}\n    \\mathbf{0}_n \\\\\n    \\mathbf{0}_q\n  \\end{bmatrix},\n  \\begin{bmatrix}\n    \\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top & \\boldsymbol{\\Phi}\\mathbf{S} \\boldsymbol{\\Phi}^{*\\top} \\\\\n    \\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top & \\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}\n  \\end{bmatrix} \\right).\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#kriging-under-hsgp-1",
    "href": "slides/21-scalable-1.html#kriging-under-hsgp-1",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging under HSGP",
    "text": "Kriging under HSGP\nAgain by properties of multivariate normal, \\[\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) \\overset{?}{\\sim} N_q(\\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS},\\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS}),\\]\n\\[\n\\begin{align}\n  \\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{-1} \\boldsymbol{\\theta}\\\\\n  \\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}) - (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{-1}(\\boldsymbol{\\Phi}\\mathbf{S} \\boldsymbol{\\Phi}^{*\\top}).\n\\end{align}\n\\]\n\n\nIf \\(m \\ge n\\), \\((\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)\\) is invertible, this is the kriging distribution under HSGP.\nBut what if \\(m &lt; n\\)?"
  },
  {
    "objectID": "slides/21-scalable-1.html#kriging-under-hsgp-2",
    "href": "slides/21-scalable-1.html#kriging-under-hsgp-2",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging under HSGP",
    "text": "Kriging under HSGP\nIf \\(m \\le n\\), claim \\(\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) = (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta},\\) where \\(\\mathbf{A}^\\dagger\\) denotes a generalized inverse of matrix \\(\\mathbf{A}\\) such that \\(\\mathbf{A}\\mathbf{A}^{\\dagger}\\mathbf{A} = \\mathbf{A}\\). Sketch proof below, see details in class.\n\nBy properties of multivariate normal, \\(\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) \\sim N_q(\\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS},\\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS})\\), \\[\n\\begin{align}\n  \\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta}\\\\\n  \\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}) - (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger \\top}(\\boldsymbol{\\Phi}\\mathbf{S} \\boldsymbol{\\Phi}^{*\\top}).\n\\end{align}\n\\]\nShow if \\(\\boldsymbol{\\Phi}\\) has full column rank, which is true under HSGP, then \\[\n\\begin{align}\n  \\mathbf{S} \\boldsymbol{\\Phi}^\\top(\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger \\top}\\boldsymbol{\\Phi}\\mathbf{S} = \\mathbf{S} \\tag{1} \\\\\n  \\mathbf{S} \\boldsymbol{\\Phi}^\\top(\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger}\\boldsymbol{\\Phi}\\mathbf{S} = \\mathbf{S} \\tag{2}.\n\\end{align}\n\\] Equation (1) is sufficient to show \\(\\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS} \\equiv \\mathbf{0}\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#kriging-under-hsgp-alternative-view",
    "href": "slides/21-scalable-1.html#kriging-under-hsgp-alternative-view",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging under HSGP – alternative view",
    "text": "Kriging under HSGP – alternative view\nUnder the reparameterized model, there is another (perhaps more intuitive) way to recognize the kriging distribution under HSGP when \\(m \\le n\\).\nWe model \\(\\boldsymbol{\\theta} = \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}\\), where \\(\\mathbf{b}\\) is treated as the unknown parameter. Therefore for kriging: \\[\n\\begin{align}\n  \\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta},\\boldsymbol{\\Omega}) &= \\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b} \\mid (\\mathbf{b},\\mathbf{S},\\boldsymbol{\\Omega}) \\\\\n  &=\\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-kriging-in-stan",
    "href": "slides/21-scalable-1.html#hsgp-kriging-in-stan",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP kriging in stan",
    "text": "HSGP kriging in stan\nIf \\(m \\le n\\), kriging under HSGP can be easily implemented in stan.\n\ntransformed data {\n  matrix[q,m] PHI_new;\n  ...\n}\nparameters {\n  vector[m] b;\n  vector&lt;lower=0&gt;[m] sqrt_S;\n  ...\n}\nmodel {\n  ...\n}\ngenerated quantities {\n  vector[q] theta_new = PHI_new * (sqrt_S .* b);\n}\n\n\nIf \\(m&gt;n\\), we need to invert an \\(n \\times n\\) matrix \\((\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)\\) for kriging, which could be computationally prohibitive."
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-parameters",
    "href": "slides/21-scalable-1.html#hsgp-parameters",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP parameters",
    "text": "HSGP parameters\nSolin and Särkkä (2020) showed that HSGP approximation can be made arbitrarily accurate as \\(\\boldsymbol{\\Theta}\\) and \\(m\\) increase.\n\nBut how to choose:\n\nsize of the box \\(\\boldsymbol{\\Theta}\\).\nnumber of basis functions \\(m\\).\n\n\n\n\n\n\n\n\n\nOur goal:\n\n\n\nMinimize the run time while maintaining reasonable approximation accuracy.\n\n\n\n\nNote: we treat estimation of the GP magnitude parameter \\(\\tau\\) as a separate problem, and only consider approximation accuracy of HSGP in terms of the correlation function."
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-approximation-box",
    "href": "slides/21-scalable-1.html#hsgp-approximation-box",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation box",
    "text": "HSGP approximation box\nDue to the design of HSGP, the approximation is less accurate near the boundaries of \\(\\boldsymbol{\\Theta}\\).\n\nSuppose all the coordinates are centered. Let \\[S_l = \\max_i |\\mathbf{u}_{il}|, \\quad l=1,\\dots,d, \\quad i= 1, \\dots, (n+q)\\] such that \\(\\boldsymbol{\\Theta}_S = \\prod_{l=1}^d [-S_l,S_l]\\) is the smallest box which contains all observed and prediction locations. We should at least ensure \\(\\boldsymbol{\\Theta} \\supset \\boldsymbol{\\Theta}_S\\).\nWe want the box to be large enough to ensure good boundary accuracy. Let \\(c_l \\ge 1\\) be boundary factors, we consider \\[\\boldsymbol{\\Theta} = \\prod_{l=1}^d [-L_l,L_l], \\quad L_l = c_l S_l.\\]"
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-approximation-box-and-rho",
    "href": "slides/21-scalable-1.html#hsgp-approximation-box-and-rho",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation box and \\(\\rho\\)",
    "text": "HSGP approximation box and \\(\\rho\\)\n\n\n\n\n\nHow much the approximation accuracy deteriorates towards the boundaries depends on smoothness of the true surface.\n\nthe larger the length scale \\(\\rho\\), the smoother the surface, a smaller box (smaller \\(c\\)) can be used for the same level of boundary accuracy."
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-approximation-box-and-m",
    "href": "slides/21-scalable-1.html#hsgp-approximation-box-and-m",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP approximation box and \\(m\\)",
    "text": "HSGP approximation box and \\(m\\)\n\n\n\n\n\nThe larger the box,\n\nthe more basis functions we need for the same level of overall accuracy,\nhence higher run time."
  },
  {
    "objectID": "slides/21-scalable-1.html#zooming-out-doesnt-simplify-the-problem",
    "href": "slides/21-scalable-1.html#zooming-out-doesnt-simplify-the-problem",
    "title": "Scalable Gaussian Processes #1",
    "section": "Zooming out doesn’t simplify the problem",
    "text": "Zooming out doesn’t simplify the problem\n\n\n\n\n\n\nIf we scale the coordinates by a constant \\(b\\), the length scale \\(\\rho\\) of the underlying GP also needs to be approximately scaled by \\(b\\) to capture the same level of details in the data.\nWe can effectively think of the length scale parameter as \\((\\rho/\\|\\mathbf{S}\\|)\\)."
  },
  {
    "objectID": "slides/21-scalable-1.html#hsgp-basis-functions",
    "href": "slides/21-scalable-1.html#hsgp-basis-functions",
    "title": "Scalable Gaussian Processes #1",
    "section": "HSGP basis functions",
    "text": "HSGP basis functions\nThe total number of basis functions \\(m = \\prod_{l=1}^d m_l\\), i.e., we need to decide on \\(m_l\\)’s, the number of basis functions for each dimension.\n\n\\(m\\) scales exponentially in \\(d\\), hence the HSGP computation complexity \\(\\mathcal{O}(nm+m)\\) also scales exponentially in \\(d\\). Therefore HSGP is only recommended for \\(d \\le 3\\), at most \\(4\\).\nThe higher the \\(m\\), the better the overall approximation accuracy, the higher the runtime."
  },
  {
    "objectID": "slides/21-scalable-1.html#relationship-between-c-and-m",
    "href": "slides/21-scalable-1.html#relationship-between-c-and-m",
    "title": "Scalable Gaussian Processes #1",
    "section": "Relationship between \\(c\\) and \\(m\\)",
    "text": "Relationship between \\(c\\) and \\(m\\)\nRiutort-Mayol et al. (2023) used simulations with a squared exponential covariance function to investigate the relationship between \\(c\\) and \\(m\\).\n\n\n\n\n\nNotice that \\(c\\) needs to be above a certain minimum value to achieve a reasonable boundary accuracy."
  },
  {
    "objectID": "slides/21-scalable-1.html#relationship-between-c-m-and-rho",
    "href": "slides/21-scalable-1.html#relationship-between-c-m-and-rho",
    "title": "Scalable Gaussian Processes #1",
    "section": "Relationship between \\(c\\), \\(m\\) and \\(\\rho\\)",
    "text": "Relationship between \\(c\\), \\(m\\) and \\(\\rho\\)\nLet’s quickly recap. For simplicity, let \\(d=1\\),\n\n\nAs \\((\\rho/S)\\) decreases, the surface is less smooth,\n\n\\(c\\) needs to increase to retain boundary accuracy.\n\\(m\\) needs to increase to retain overall accuracy.\n\nAs \\(c\\) increases, \\(m\\) needs to increase to retain overall accuracy.\nAs \\(m\\) increases, run time increases.\n\n\n\nHence we want to minimize \\(m\\) and \\(c\\) while maintaining approximation accuracy."
  },
  {
    "objectID": "slides/21-scalable-1.html#empirical-functional-form",
    "href": "slides/21-scalable-1.html#empirical-functional-form",
    "title": "Scalable Gaussian Processes #1",
    "section": "Empirical functional form",
    "text": "Empirical functional form\n\nGiven \\(c\\) and \\(\\rho/S\\), let \\(m_C(c,\\rho/S)\\) denote the minimum number of basis functions needed for a near 100% accuracy under covariance function \\(C\\).\nRiutort-Mayol et al. (2023) used extensive simulations to obtain an empirical function form of \\(m_C(c,\\rho/S)\\) for frequently used Matérn covariance functions. E.g., for Matérn 3/2,\n\n\\[m_{3/2}(c,\\rho/S)=3.24 \\frac{c}{\\rho/S}, \\quad c \\ge 4.5 \\frac{\\rho}S, \\quad c \\ge 1.2.\\]\nNotice:\n\nlinear proportionality between \\(m\\), \\(c\\) and \\(\\rho/S\\).\nfor a given \\(\\rho/S\\), there exists a minimum \\(c\\), below which the approximation is poor no matter how large \\(m\\) is."
  },
  {
    "objectID": "slides/21-scalable-1.html#question",
    "href": "slides/21-scalable-1.html#question",
    "title": "Scalable Gaussian Processes #1",
    "section": "Question",
    "text": "Question\nBUT, in real applications, we do not know \\(\\rho\\).\n\nSo how to use \\(m_C(c,\\rho/S)\\) to help choose \\(c\\) and \\(m\\)?"
  },
  {
    "objectID": "slides/21-scalable-1.html#prepare-for-next-class",
    "href": "slides/21-scalable-1.html#prepare-for-next-class",
    "title": "Scalable Gaussian Processes #1",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 05 which is due Apr 8\nComplete reading to prepare for Thursday’s lecture\nThursday’s lecture:\n\nParameter tuning for HSGP\nHow to implement HSGP in stan"
  },
  {
    "objectID": "slides/22-scalable-2.html#geospatial-analysis-on-hemoglobin-dataset",
    "href": "slides/22-scalable-2.html#geospatial-analysis-on-hemoglobin-dataset",
    "title": "Scalable Gaussian Processes #2",
    "section": "",
    "text": "We wanted to perform geospatial analysis on a dataset with ~8,600 observations at ~500 locations, and make predictions at ~440 locations on a grid."
  },
  {
    "objectID": "slides/22-scalable-2.html#geospatial-model",
    "href": "slides/22-scalable-2.html#geospatial-model",
    "title": "Scalable Gaussian Processes #2",
    "section": "Geospatial model",
    "text": "Geospatial model\nWe specify the following model: \\[\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})\\] with priors\n\n\\(\\boldsymbol{\\theta} | \\tau,\\rho \\sim GP(\\mathbf{0},C(\\cdot,\\cdot))\\), where \\(C\\) is the Matérn 3/2 covariance function with magnitude \\(\\tau\\) and length scale \\(\\rho\\)\n\\(\\alpha^* \\sim N(0,4^2)\\). This is the intercept after centering \\(\\mathbf{X}\\).\n\\(\\beta_j | \\sigma_{\\beta} \\sim N(0,\\sigma_{\\beta}^2)\\), \\(j \\in \\{1,\\dots,p\\}\\)\n\\(\\sigma \\sim \\text{Half-Normal}(0, 2^2)\\)\n\\(\\tau \\sim \\text{Half-Normal}(0, 4^2)\\)\n\\(\\rho \\sim \\text{Inv-Gamma}(5, 5)\\)\n\\(\\sigma_{\\beta} \\sim \\text{Half-Normal}(0, 2^2)\\)"
  },
  {
    "objectID": "slides/22-scalable-2.html#review-of-the-last-lecture",
    "href": "slides/22-scalable-2.html#review-of-the-last-lecture",
    "title": "Scalable Gaussian Processes #2",
    "section": "Review of the last lecture",
    "text": "Review of the last lecture\n\n\nGaussian process (GP) is not scalable as it requires \\(\\mathcal{O}(n^3)\\) flops per MCMC iteration.\nIntroduced HSGP, a Hilbert space low-rank approximation method for GP. \\[\\mathbf{C} \\approx \\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^T, \\quad \\text{where}\\]\n\n\\(\\boldsymbol{\\Phi} \\in \\mathbb{R}^{n \\times m}\\) only depends on the approximation box \\(\\boldsymbol{\\Theta}\\) and observed locations.\n\\(\\mathbf{S} \\in \\mathbb{R}^{m \\times m}\\) is diagonal. It depends on the covariance function \\(C\\) and parameters \\(\\tau\\) and \\(\\rho\\).\n\\(m\\) is the number of basis functions.\n\nModel reparameterization under HSGP.\nBayesian model fitting and kriging under HSGP."
  },
  {
    "objectID": "slides/22-scalable-2.html#parameter-tuning-for-hsgp",
    "href": "slides/22-scalable-2.html#parameter-tuning-for-hsgp",
    "title": "Scalable Gaussian Processes #2",
    "section": "Parameter tuning for HSGP",
    "text": "Parameter tuning for HSGP\n\n\nTo implement HSGP, we need to decide on:\n\nnumber of basis functions \\(m=\\prod_{l=1}^d m_l\\)\n\none number \\(m_l\\) for each dimension of the GP.\n\nsize of the approximation box \\(\\boldsymbol{\\Theta}\\)\n\nscale of the coordinates \\(\\mathbf{S}=(S_1,S_2)\\).\nboundary factors \\(\\mathbf{c}=(c_1,c_2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur goal:\n\n\n\nMinimize the run time while maintaining reasonable approximation accuracy."
  },
  {
    "objectID": "slides/22-scalable-2.html#review-of-what-we-discussed-last-time",
    "href": "slides/22-scalable-2.html#review-of-what-we-discussed-last-time",
    "title": "Scalable Gaussian Processes #2",
    "section": "Review of what we discussed last time",
    "text": "Review of what we discussed last time\nFor simplicity, let \\(d=1\\)\n\n\nEstimation of the GP magnitude \\(\\tau\\) is treated as a separate problem. We only consider approximation accuracy in terms of the correlation matrix.\nScaling the coordinates do not change the problem. We consider the length scale parameter as \\((\\rho/S)\\).\nAs \\((\\rho/S)\\) decreases, the surface is less smooth,\n\n\\(c\\) needs to increase to retain boundary accuracy.\n\\(m\\) needs to increase to retain overall accuracy.\n\nFor a given \\(\\rho/S\\), there exists a minimum \\(c\\), below which the approximation is poor no matter how large \\(m\\) is. This minimum value increases as \\(\\rho/S\\) increases.\nAs \\(c\\) increases, \\(m\\) needs to increase to retain overall accuracy."
  },
  {
    "objectID": "slides/22-scalable-2.html#review-of-what-we-discussed-last-time-1",
    "href": "slides/22-scalable-2.html#review-of-what-we-discussed-last-time-1",
    "title": "Scalable Gaussian Processes #2",
    "section": "Review of what we discussed last time",
    "text": "Review of what we discussed last time\n\n\nAs \\(m\\) increases, run time increases. Hence we want to minimize \\(m\\) and \\(c\\) while maintaining certain accuracy level.\nRiutort-Mayol et al. (2023) presented an empirical functional form of \\(m\\) as a function of \\(c\\) and \\(\\rho/S\\) for Matérn 3/2 covariance function: \\[m(c,\\rho/S)=3.24 \\frac{c}{\\rho/S}, \\quad c \\ge 4.5 \\frac{\\rho}S, \\quad c \\ge 1.2.\\]\nNote we also have\n\n\\(\\rho(m,c,S)=3.42 Sc/m\\): the minimum \\(\\rho\\) (least smooth surface) that can be well approximated given \\(c\\), \\(m\\) and \\(S\\).\n\\(c(\\rho,S)=\\min(4.5\\rho/S,1.2)\\): the minimum \\(c\\) for HSGP to work.\n\n\n\n\nQuestion: in real applications, we do not know \\(\\rho\\). So how to proceed?"
  },
  {
    "objectID": "slides/22-scalable-2.html#an-iterative-algorithm",
    "href": "slides/22-scalable-2.html#an-iterative-algorithm",
    "title": "Scalable Gaussian Processes #2",
    "section": "An iterative algorithm",
    "text": "An iterative algorithm\nPseudo-codes for HSGP parameter tuning assuming \\(d=1\\).\n\nu = center(observed and prediction locations)\nS = box size (u)\nmax_iter = 30\n\n# initialization\nj = 0\ncheck = FALSE\nrho = 0.5*S # the practical paper recommends setting the initial guess of rho to be 0.5 to 1 times S\nc = c(rho/S) # minimum c given rho and S\nm = m(c,rho/S) # minimum m given c, and rho/S\nL = c*S\ndiagnosis = logical(max_iter) # store checking results for each iteration\n\nwhile (!check & j&lt;=max_iter){\n  \n  fit = runHSGP(L,m) # stan run\n  j = j + 1\n\n  rho_hat = mean(fit$rho) # obtain fitted value for rho\n  # check the fitted is larger than the minimum rho that can be well approximated\n  diagnosis[j] = (rho_hat + 0.01 &gt;= rho)\n  if (j==1) {\n    \n    if (diagnosis[j]){\n      # if the diagnosis check is passed, do one more run just to make sure\n      m = m + 2\n      c = c(rho_hat/S)\n      rho = rho(m,c,S)\n    } else {\n      # if the check failed, update our knowledge about rho\n      rho = rho_hat\n      c = c(rho/S)\n      m = m(c,rho/S)\n    }\n  } else {\n    if (diagnosis[j] & diagnosis[j-2]){\n      # if the check passed for the last two runs, we finish tuning\n      check = TRUE\n    } else if (diagnosis[j] & !diagnosis[j-2]){\n      # if the check failed last time but passed this time, do one more run\n      m = m + 2\n      c = c(rho_hat/S)\n      rho = rho(m,c,S)      \n    } else if (!diagnosis[j]){\n      # if the check failed, update our knowledge about rho\n      rho = rho_hat\n      c = c(rho/S)\n      m = m(c,rho/S)\n    }\n  }\n  L = c*S\n}"
  },
  {
    "objectID": "slides/22-scalable-2.html#hsgp-implementation-codes",
    "href": "slides/22-scalable-2.html#hsgp-implementation-codes",
    "title": "Scalable Gaussian Processes #2",
    "section": "HSGP implementation codes",
    "text": "HSGP implementation codes\nPlease clone the repo for AE 09 for HSGP implementation codes."
  },
  {
    "objectID": "slides/22-scalable-2.html#side-notes-on-hsgp-implementation",
    "href": "slides/22-scalable-2.html#side-notes-on-hsgp-implementation",
    "title": "Scalable Gaussian Processes #2",
    "section": "Side notes on HSGP implementation",
    "text": "Side notes on HSGP implementation\nA few random things to keep in mind for implementation in practice:\n\n\nIf your HSGP run is suspiciously VERY slow, check the number of basis functions being used in the run and make sure it is reasonable.\nCheck whether \\(m \\le n\\) before using the kriging results.\nBecause HSGP is a low-rank approximation method, the GP magnitude parameter \\(\\tau\\) will always be overestimated. However, we can account for this and use a bias-adjusted \\(\\tau\\) instead. See the AE 09 stan codes for parameter tau_adj.\nIf \\(d&gt;1\\), we need to do parameter tuning for each dimension."
  },
  {
    "objectID": "slides/22-scalable-2.html#gp-vs-hsgp-spatial-intercept-posterior-mean",
    "href": "slides/22-scalable-2.html#gp-vs-hsgp-spatial-intercept-posterior-mean",
    "title": "Scalable Gaussian Processes #2",
    "section": "GP vs HSGP spatial intercept posterior mean",
    "text": "GP vs HSGP spatial intercept posterior mean"
  },
  {
    "objectID": "slides/22-scalable-2.html#gp-vs-hsgp-spatial-intercept-posterior-sd",
    "href": "slides/22-scalable-2.html#gp-vs-hsgp-spatial-intercept-posterior-sd",
    "title": "Scalable Gaussian Processes #2",
    "section": "GP vs HSGP spatial intercept posterior SD",
    "text": "GP vs HSGP spatial intercept posterior SD"
  },
  {
    "objectID": "slides/22-scalable-2.html#gp-vs-hsgp-parameter-posterior-density",
    "href": "slides/22-scalable-2.html#gp-vs-hsgp-parameter-posterior-density",
    "title": "Scalable Gaussian Processes #2",
    "section": "GP vs HSGP parameter posterior density",
    "text": "GP vs HSGP parameter posterior density"
  },
  {
    "objectID": "slides/22-scalable-2.html#gp-vs-hsgp-correlation-function",
    "href": "slides/22-scalable-2.html#gp-vs-hsgp-correlation-function",
    "title": "Scalable Gaussian Processes #2",
    "section": "GP vs HSGP correlation function",
    "text": "GP vs HSGP correlation function"
  },
  {
    "objectID": "slides/22-scalable-2.html#gp-vs-hsgp-effective-sample-size",
    "href": "slides/22-scalable-2.html#gp-vs-hsgp-effective-sample-size",
    "title": "Scalable Gaussian Processes #2",
    "section": "GP vs HSGP effective sample size",
    "text": "GP vs HSGP effective sample size"
  },
  {
    "objectID": "slides/22-scalable-2.html#prepare-for-next-class",
    "href": "slides/22-scalable-2.html#prepare-for-next-class",
    "title": "Scalable Gaussian Processes #2",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 05 which is due Apr 8\nComplete reading to prepare for Tuesday’s lecture\nTuesday’s lecture: Bayesian Clustering"
  },
  {
    "objectID": "slides/22-scalable-2.html#references",
    "href": "slides/22-scalable-2.html#references",
    "title": "Scalable Gaussian Processes #2",
    "section": "References",
    "text": "References\n\n\nRiutort-Mayol, Gabriel, Paul-Christian Bürkner, Michael R Andersen, Arno Solin, and Aki Vehtari. 2023. “Practical Hilbert Space Approximate Bayesian Gaussian Processes for Probabilistic Programming.” Statistics and Computing 33 (1): 17.\n\n\nSolin, Arno, and Simo Särkkä. 2020. “Hilbert Space Methods for Reduced-Rank Gaussian Process Regression.” Statistics and Computing 30 (2): 419–46."
  },
  {
    "objectID": "ae/ae-09-hsgp.html",
    "href": "ae/ae-09-hsgp.html",
    "title": "AE 09: Scalable Gaussian processes",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-09-hsgp.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-09-hsgp.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 09: Scalable Gaussian processes",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-09-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-09.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-09-hsgp.html#democratic-republic-of-conge-demographic-and-health-survey",
    "href": "ae/ae-09-hsgp.html#democratic-republic-of-conge-demographic-and-health-survey",
    "title": "AE 09: Scalable Gaussian processes",
    "section": "Democratic Republic of Conge Demographic and Health Survey",
    "text": "Democratic Republic of Conge Demographic and Health Survey\nWe continue working on the hemoglobin dataset, with a sample of women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. There are ~8600 women who are nested in ~500 survey clusters. The variables in the dataset are as follows.\n\nloc_id: location id (i.e. survey cluster).\nhemoglobin: hemoglobin level (g/dL).\nanemia: anemia classifications.\nage: age in years.\nurban: urban vs. rural.\nLATNUM: latitude.\nLONGNUM: longitude.\nmean_hemoglobin: average hemoglobin at each community (g/dL).\ncommunity_size: number of participants at each community.\nmean_age: average age of participants at each community (years).\n\nThe data set is available in your AE repos and is called drc.\n\ndata &lt;- readRDS(\"drc.rds\")"
  },
  {
    "objectID": "ae/ae-09-hsgp.html#center-the-coordinates",
    "href": "ae/ae-09-hsgp.html#center-the-coordinates",
    "title": "AE 09: Scalable Gaussian processes",
    "section": "Center the coordinates",
    "text": "Center the coordinates\nWe first center the coordinates. In real applications, we might want to consider more sophisticated methods to properly transform longitude and latitude to Euclidean coordinates, but here we directly use them as Euclidean coordinates for simplicity.\n\n### center coordinates\nx_range &lt;- round(range(data$LONGNUM),2)\ny_range &lt;- c(-13,5)\n\n# center the locations\ndata_loc_centered &lt;- data %&gt;%\n  mutate(LONGNUM = (LONGNUM - mean(x_range)),\n         LATNUM = (LATNUM - mean(y_range)))\n\n# extract unique locations\nu &lt;- data_loc_centered %&gt;%\n  filter(!duplicated(cbind(LONGNUM,LATNUM))) %&gt;%\n  dplyr::select(LONGNUM,LATNUM)"
  },
  {
    "objectID": "ae/ae-09-hsgp.html#obtain-a-grid-for-prediction",
    "href": "ae/ae-09-hsgp.html#obtain-a-grid-for-prediction",
    "title": "AE 09: Scalable Gaussian processes",
    "section": "Obtain a Grid for Prediction",
    "text": "Obtain a Grid for Prediction\nNext, we obtain locations for prediction. We first create a \\(30 \\times 30\\) grid over DRC, and then extract points that are within DRC. This leaves us with about ~440 new locations. Remember to center these coordinates as well!\n\ncongo_states_map &lt;- ne_states(country = \"Democratic Republic of the Congo\", returnclass = \"sf\")\ncongo_states_map &lt;- st_transform(congo_states_map, crs = 4326)\nbbox &lt;- st_bbox(congo_states_map)\n\nlatitudes &lt;- seq(bbox[\"ymin\"], bbox[\"ymax\"], length.out = 30)\nlongitudes &lt;- seq(bbox[\"xmin\"], bbox[\"xmax\"], length.out = 30)\ngrid_points &lt;- expand.grid(lat = latitudes, long = longitudes)\n\n# Convert the grid into an sf object\ngrid_sf &lt;- st_as_sf(grid_points, coords = c(\"long\", \"lat\"), crs = 4326)\nin_drc &lt;- st_within(grid_sf, congo_states_map, sparse = FALSE)\nin_drc &lt;- apply(in_drc,1,any)\n\ngrid_inside_drc &lt;- grid_sf[in_drc, ]\n\nu_new &lt;- st_coordinates(grid_inside_drc)\n\nu_new[,1] &lt;- (u_new[,1] - mean(x_range))\nu_new[,2] &lt;- (u_new[,2] - mean(y_range))\n\n# Plot all the grid points\nggplot() +\n  geom_sf(data = congo_states_map, fill = \"lightblue\", color = \"black\") +\n  geom_sf(data = grid_sf, color = \"red\", shape = 16, size = 1) +\n  theme_minimal() +\n  labs(title = \"Grid of Points across DRC\",\n       caption = \"\",\n       # subtitle = \"Red points represent valid grid points inside DRC boundaries\",\n       x = \"Longitude\", y = \"Latitude\")\n# Plot the grid points within the states\nggplot() +\n  geom_sf(data = congo_states_map, fill = \"lightblue\", color = \"black\") +\n  geom_sf(data = grid_inside_drc, color = \"red\", shape = 16, size = 1) +\n  theme_minimal() +\n  labs(title = \"Grid of Points WITHIN DRC\",\n       caption = \"Data Source: rnaturalearth\",\n       # subtitle = \"Red points represent valid grid points inside DRC boundaries\",\n       x = \"Longitude\", y = \"Latitude\")"
  },
  {
    "objectID": "ae/ae-09-hsgp.html#exercise-1",
    "href": "ae/ae-09-hsgp.html#exercise-1",
    "title": "AE 09: Scalable Gaussian processes",
    "section": "Exercise 1",
    "text": "Exercise 1\nHow many iterations did HSGP parameter tuning process take? How many basis functions were used in the final HSGP run?\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-09-hsgp.html#exercise-2",
    "href": "ae/ae-09-hsgp.html#exercise-2",
    "title": "AE 09: Scalable Gaussian processes",
    "section": "Exercise 2",
    "text": "Exercise 2\nLook at the traceplots for a few parameters and comment on the mixing of the MCMC chain.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-09-hsgp.html#exercise-3",
    "href": "ae/ae-09-hsgp.html#exercise-3",
    "title": "AE 09: Scalable Gaussian processes",
    "section": "Exercise 3",
    "text": "Exercise 3\nObtain posterior mean and 95% credible interval for the regression coefficients. Interpret them under our context. Are age and urbanicity significantly associated with female hemoglobin level?\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-09-hsgp.html#exercise-4",
    "href": "ae/ae-09-hsgp.html#exercise-4",
    "title": "AE 09: Scalable Gaussian processes",
    "section": "Exercise 4",
    "text": "Exercise 4\nUse codes provided in the last AE to create visualizations of the surface of posterior mean and posterior SD of the spatial intercept. You can use the following codes to extract the posterior mean and SD. Note that due to the identifiability issue discussed in class, here we look at \\(\\alpha\\mathbf{1}+\\boldsymbol{\\theta}^*\\).\n\nres_HSGP &lt;- runspec$fit\np_theta_HSGP &lt;- as.matrix(res_HSGP,pars=\"theta_new\")\np_alpha_HSGP &lt;- as.matrix(res_HSGP,pars=\"alpha\")\np_intercept_HSGP &lt;- p_theta_HSGP + c(p_alpha_HSGP)\np_intercept_HSGP_m &lt;- colMeans(p_intercept_HSGP)\np_intercept_HSGP_sd &lt;- apply(p_intercept_HSGP,2,sd)\n\nDescribe the spatial pattern of hemoglobin level for females in DRC, and think of possible reasons based on your knowledge about DRC.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "ae/ae-09-hsgp.html#democratic-republic-of-congo-demographic-and-health-survey",
    "href": "ae/ae-09-hsgp.html#democratic-republic-of-congo-demographic-and-health-survey",
    "title": "AE 09: Scalable Gaussian processes",
    "section": "Democratic Republic of Congo Demographic and Health Survey",
    "text": "Democratic Republic of Congo Demographic and Health Survey\nWe continue working on the hemoglobin dataset, with a sample of women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. There are ~8600 women who are nested in ~500 survey clusters. The variables in the dataset are as follows.\n\nloc_id: location id (i.e. survey cluster).\nhemoglobin: hemoglobin level (g/dL).\nanemia: anemia classifications.\nage: age in years.\nurban: urban vs. rural.\nLATNUM: latitude.\nLONGNUM: longitude.\nmean_hemoglobin: average hemoglobin at each community (g/dL).\ncommunity_size: number of participants at each community.\nmean_age: average age of participants at each community (years).\n\nThe data set is available in your AE repos and is called drc.\n\ndata &lt;- readRDS(\"drc.rds\")"
  },
  {
    "objectID": "slides/21-scalable-1.html#kriging-under-hsgp-3",
    "href": "slides/21-scalable-1.html#kriging-under-hsgp-3",
    "title": "Scalable Gaussian Processes #1",
    "section": "Kriging under HSGP",
    "text": "Kriging under HSGP\nUnder the reparameterized model, \\(\\boldsymbol{\\theta} = \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}\\), for \\(\\mathbf{b} \\sim N_m(0,\\mathbf{I}).\\) Therefore \\[\n\\begin{align}\n  \\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta},\\boldsymbol{\\Omega}) &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta} \\\\\n  &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger}(\\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}) \\\\\n  &= \\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b}. \\quad (\\text{by equation (2) in the last slide})\n\\end{align}\n\\]\nDuring MCMC sampling, we can obtain posterior predictive samples for \\(\\boldsymbol{\\theta}^*\\) through posterior samples of \\(\\mathbf{b}\\) and \\(\\mathbf{S}\\). Let superscript \\((s)\\) denote the \\(s\\)th posterior sample:\n\\[\\boldsymbol{\\theta}^{*(s)} = \\boldsymbol{\\Phi}^* \\mathbf{S}^{(s) 1/2} \\mathbf{b}^{(s)}.\\]"
  },
  {
    "objectID": "slides/23-clustering.html#learning-objectives",
    "href": "slides/23-clustering.html#learning-objectives",
    "title": "Bayesian Clustering",
    "section": "",
    "text": "We will introduce the basic mixture modeling framework as a mechanism for model-based clustering and describe computational and inferential challenges.\nVariations of the popular finite Gaussian mixture model (GMM) will be introduced to cluster patients according to ED length-of-stay.\nWe present an implementation of mixture modeling in Stan and discuss challenges therein.\nFinally, various posterior summaries will be explored."
  },
  {
    "objectID": "slides/23-clustering.html#finding-subtypes",
    "href": "slides/23-clustering.html#finding-subtypes",
    "title": "Bayesian Clustering",
    "section": "Finding subtypes",
    "text": "Finding subtypes\nRevisiting data on patients admitted to the emergency department (ED) from the MIMIC-IV-ED demo.\n\n\n\n\n\n\n\n\n\nCan we identify subgroups within this population?"
  },
  {
    "objectID": "slides/23-clustering.html#the-usual-setup",
    "href": "slides/23-clustering.html#the-usual-setup",
    "title": "Bayesian Clustering",
    "section": "The usual setup",
    "text": "The usual setup\nMost models introduced in this course are of the form:\n\\[f\\left(Y_i\\mid X_i\\right) = f\\left(Y_i\\mid \\boldsymbol{\\theta}_i\\left(X_i\\right)\\right).\\]\n\n\\(f(\\cdot)\\) is the density or distribution function of an assumed family (e.g., Gaussian, binomial),\n\\(\\boldsymbol{\\theta}_i\\) is a parameter (or parameters) that may depend on individual covariates \\(X_i\\)."
  },
  {
    "objectID": "slides/23-clustering.html#the-usual-setup-1",
    "href": "slides/23-clustering.html#the-usual-setup-1",
    "title": "Bayesian Clustering",
    "section": "The usual setup",
    "text": "The usual setup\n\\[f\\left(Y_i\\mid X_i\\right) = f\\left(Y_i\\mid \\boldsymbol{\\theta}_i\\left(X_i\\right)\\right)\\]\nLinear regression:\n\n\\(f\\) is the Gaussian density function, and \\(\\boldsymbol{\\theta}_i(X_i)=(X_i\\beta, \\sigma^2)^\\top\\)\n\nBinary classification:\n\n\\(f\\) is the Bernoulli mass function, and \\(\\boldsymbol{\\theta}_i(X_i)=\\text{logit}(X_i\\beta)^{-1}\\)"
  },
  {
    "objectID": "slides/23-clustering.html#limitations-of-the-usual-setup",
    "href": "slides/23-clustering.html#limitations-of-the-usual-setup",
    "title": "Bayesian Clustering",
    "section": "Limitations of the usual setup",
    "text": "Limitations of the usual setup\nSuppose patients \\(i=1,\\dots,n\\) are administered a diagnostic test. Their outcome \\(Y_i\\) depends only on whether or not they have previously received treatment: \\(X_i=1\\) if yes and \\(X_i=0\\) otherwise. Suppose the diagnostic test has Gaussian-distributed measurement error, so \\[Y_i\\mid X_i \\sim N(\\alpha + \\beta X_i, \\sigma^2).\\] Now, suppose past treatment information is not included in patients’ record—we cannot condition on \\(X_i\\). Marginalizing, \\[\\begin{align*}\nf(Y_i) &= P(X_i=1)\\times N(Y_i\\mid \\alpha + \\beta, \\sigma^2) \\\\\n      & +P(X_i=0)\\times N(Y_i\\mid \\alpha, \\sigma^2).\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/23-clustering.html#limitations-of-the-usual-setup-1",
    "href": "slides/23-clustering.html#limitations-of-the-usual-setup-1",
    "title": "Bayesian Clustering",
    "section": "Limitations of the usual setup",
    "text": "Limitations of the usual setup\n\nn &lt;- 500; mu &lt;- c(1,4.5); s2 &lt;- 1\nx &lt;- sample(1:2, n, T); y &lt;- rnorm(n, mu[x], sqrt(s2))\nggplot(data.frame(y = y), aes(x = y)) + \n  geom_histogram() + \n  labs(x = \"Y\", y = \"Count\")"
  },
  {
    "objectID": "slides/23-clustering.html#limitations-of-the-usual-setup-2",
    "href": "slides/23-clustering.html#limitations-of-the-usual-setup-2",
    "title": "Bayesian Clustering",
    "section": "Limitations of the usual setup",
    "text": "Limitations of the usual setup\n\nfit &lt;- lm(y ~ 1)\nggplot(data.frame(residuals = fit$residuals), aes(x = residuals)) + \n  geom_histogram() + \n  labs(x = \"Residuals\", y = \"Count\")\n\n\n\n\n\n\n\n\nNormality of residuals?"
  },
  {
    "objectID": "slides/23-clustering.html#mixture-model",
    "href": "slides/23-clustering.html#mixture-model",
    "title": "Bayesian Clustering",
    "section": "Mixture Model",
    "text": "Mixture Model\nMotivation for using a mixture model: Standard distributional families are not sufficiently expressive.\n\nThe inflexibility of the model may be due to unobserved heterogeneity (e.g., unrecorded treatment history).\n\nGenerically, \\[f(Y_i) = \\sum_{h = 1}^k \\pi_h \\times f_h(Y_i).\\]\nUses of mixture models:\n\nModeling weird densities/distributions (e.g., bimodal).\nLearning latent groups/clusters."
  },
  {
    "objectID": "slides/23-clustering.html#mixture-model-1",
    "href": "slides/23-clustering.html#mixture-model-1",
    "title": "Bayesian Clustering",
    "section": "Mixture Model",
    "text": "Mixture Model\n\\[f(Y_i) = \\sum_{h=1}^k \\pi_h\\times f_h(Y_i)\\]\n\nThis mixture is comprised of \\(k\\) components indexed by \\(h=1,\\dots,k\\). For each component, we have a probability density (or mass) function \\(f_h\\) and a mixture weight \\(\\pi_h\\), where \\(\\sum_{h=1}^k \\pi_k=1\\).\nWhen \\(k\\) is finite, we call this a finite mixture model for \\(Y_i\\).\nIt is common to let, \\[f_h(Y_i) = f(Y_i\\mid \\boldsymbol{\\theta}_h).\\]\n\nThe component densities share a functional form and differ in their parameters."
  },
  {
    "objectID": "slides/23-clustering.html#gaussian-mixture-model",
    "href": "slides/23-clustering.html#gaussian-mixture-model",
    "title": "Bayesian Clustering",
    "section": "Gaussian Mixture Model",
    "text": "Gaussian Mixture Model\nLetting \\(f_h(Y_i) = N(Y_i\\mid \\mu_h, \\sigma^2_h)\\) for \\(h=1,\\dots,k\\), yields the Gaussian mixture model. For \\(Y_i\\in\\mathbb{R}\\), \\[f(Y_i) = \\sum_{h=1}^k \\pi_h N\\left({Y}_i\\mid \\mu_h, \\sigma^2_h\\right)\\]\nFor multivariate outcomes \\(\\mathbf{Y}_i\\in\\mathbb{R}^p\\),\n\\[ f(\\mathbf{Y}_i) = \\sum_{h=1}^k \\pi_h N_p\\left(\\mathbf{Y}_i\\mid\\boldsymbol{\\mu}_h, \\boldsymbol{\\Sigma}_h\\right).\\]"
  },
  {
    "objectID": "slides/23-clustering.html#gaussian-mixture-model-1",
    "href": "slides/23-clustering.html#gaussian-mixture-model-1",
    "title": "Bayesian Clustering",
    "section": "Gaussian Mixture Model",
    "text": "Gaussian Mixture Model\nConsider a mixture model with 3 groups:\n\nMixture 1: \\(\\mu_1 = -1.5, \\sigma_1 = 1\\).\nMixture 2: \\(\\mu_2 = 0, \\sigma_2 = 1.5\\).\nMixture 3: \\(\\mu_3 = 2, \\sigma_3 = 0.6\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice, both means \\(\\mu_h\\) and variances \\(\\sigma^2_h\\) vary across clusters."
  },
  {
    "objectID": "slides/23-clustering.html#generative-perspective-on-gmm",
    "href": "slides/23-clustering.html#generative-perspective-on-gmm",
    "title": "Bayesian Clustering",
    "section": "Generative perspective on GMM",
    "text": "Generative perspective on GMM\nTo simulate from a \\(k\\)-component Gaussian mixture with means \\(\\mu_1,\\dots,\\mu_k\\), variances \\(\\sigma_1^2,\\dots,\\sigma^2_k\\), and weights \\(\\pi_1,\\dots,\\pi_k\\):\n\nSample the component indicator \\(z_i\\in \\{1, \\dots,k\\}\\) with probabilities: \\[P(z_i=h) = \\pi_h \\iff z_i \\sim \\text{Categorical}(k, \\{\\pi_1,\\ldots,\\pi_k\\}).\\]\nGiven \\(z_i\\), sample \\(Y_i\\) from the appropriate component: \\[\\left(Y_i\\mid z_i =h\\right) \\sim N\\left(\\mu_h, \\sigma^2_h\\right).\\]"
  },
  {
    "objectID": "slides/23-clustering.html#generative-perspective-on-gmm-1",
    "href": "slides/23-clustering.html#generative-perspective-on-gmm-1",
    "title": "Bayesian Clustering",
    "section": "Generative perspective on GMM",
    "text": "Generative perspective on GMM\n\nn &lt;- 500 \nmu &lt;- c(1, 4.5)\ns2 &lt;- 1 \n# implicit: pi = c(0.5, 0.5)\nz &lt;- sample(1:2, n, TRUE)\ny &lt;- rnorm(n, mu[z], sqrt(s2))\n\nThis is essentially the code used to simulate the missing treatment history example."
  },
  {
    "objectID": "slides/23-clustering.html#marginalizing-component-indicators",
    "href": "slides/23-clustering.html#marginalizing-component-indicators",
    "title": "Bayesian Clustering",
    "section": "Marginalizing Component Indicators",
    "text": "Marginalizing Component Indicators\nThe label \\(z_i\\) indicates which component \\(Y_i\\) is drawn from—think of this as the cluster label: \\(f\\left(Y_i\\mid z_i=h\\right) = N\\left(Y_i\\mid \\mu_h,\\sigma^2_h \\right).\\)\nBut \\(z_i\\) is unknown, so we marginalize to obtain:\n\\[\\begin{align*}\nf(Y_i) &= \\int_\\mathcal{Z}f\\left(Y_i\\mid z\\right) f(z)dz \\\\\n&= \\sum_{h=1}^k f\\left(Y_i\\mid z=h\\right) P(z=h) \\\\\n&= \\sum_{h=1}^k N\\left(Y_i\\mid \\mu_h,\\sigma^2_h \\right) \\times \\pi_h.\n\\end{align*}\\]\nThis is key to implementing in Stan."
  },
  {
    "objectID": "slides/23-clustering.html#gaussian-mixture-in-stan",
    "href": "slides/23-clustering.html#gaussian-mixture-in-stan",
    "title": "Bayesian Clustering",
    "section": "Gaussian mixture in Stan",
    "text": "Gaussian mixture in Stan\nComponent indicators \\(z_i\\) are discrete parameters, which cannot be estimated in Stan. As before, suppose \\(f(Y_i) = \\sum_{h=1}^k  \\pi_h N\\left(Y_i\\mid \\mu_h,\\sigma^2_h \\right)\\).\nThe log-likelihood is:\n\\[\\begin{align*}\n\\log f(Y_i)\n&= \\log \\sum_{h=1}^k  \\exp \\left(\\log\\left[\\pi_h N\\left(Y_i\\mid \\mu_h,\\sigma^2_h \\right) \\right]\\right)\\\\\n&= \\verb|log_sum_exp| \\left[\\log\\pi_1 + \\log N\\left(Y_i\\mid \\mu_1,\\sigma^2_1 \\right),\\right. \\\\\n&\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\dots, \\\\\n&\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\left.\\log\\pi_k + \\log N\\left(Y_i\\mid \\mu_k,\\sigma^2_k \\right) \\right],\n\\end{align*}\\]\nlog_sum_exp is a Stan function."
  },
  {
    "objectID": "slides/23-clustering.html#gaussian-mixture-in-stan-1",
    "href": "slides/23-clustering.html#gaussian-mixture-in-stan-1",
    "title": "Bayesian Clustering",
    "section": "Gaussian mixture in Stan",
    "text": "Gaussian mixture in Stan\n\n// saved in mixture1.stan\ndata {\n  int&lt;lower = 1&gt; k;          // number of mixture components\n  int&lt;lower = 1&gt; n;          // number of data points\n  array[n] real Y;           // observations\n}\nparameters {\n  simplex[k] pi; // mixing proportions\n  ordered[k] mu; // means of the mixture components\n  vector&lt;lower=0&gt;[k] sigma; // sds of the mixture components\n}\nmodel {\n  target += normal_lpdf(mu |0.0, 10.0);\n  target += exponential_lpdf(sigma | 1.0);\n  vector[k] log_probs = log(pi);\n  for (i in 1:n){\n    vector[k] lps = log_probs;\n    for (h in 1:k){\n      lps[h] += normal_lpdf(Y[i] | mu[h], sigma[h]);\n    }\n    target += log_sum_exp(lps);\n  }\n}\n\nOf note: simplex and ordered types."
  },
  {
    "objectID": "slides/23-clustering.html#first-fit",
    "href": "slides/23-clustering.html#first-fit",
    "title": "Bayesian Clustering",
    "section": "First fit",
    "text": "First fit\n\ned &lt;- read.csv(\"exam1_data.csv\")\ndat &lt;- list(Y = (ed$los - mean(ed$los)),\n            n = length(ed$los),\n            k = 2)\nmod1 &lt;- stan_model(\"mixture1.stan\")\nfit1 &lt;- sampling(mod1, data=dat, chains=4, iter=5000, control=list(\"adapt_delta\"=0.99))\nprint(fit1, pars = c(\"pi\", \"mu\", \"sigma\"), probs = c(0.025, 0.975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n          mean se_mean   sd  2.5% 97.5% n_eff Rhat\npi[1]     0.37    0.17 0.24  0.16  0.83     2 6.35\npi[2]     0.63    0.17 0.24  0.17  0.84     2 6.35\nmu[1]    -3.17    0.09 0.51 -4.49 -2.49    33 1.05\nmu[2]     0.45    3.96 5.69 -3.17 13.08     2 5.14\nsigma[1] 13.25    4.48 6.46  2.09 20.11     2 4.97\nsigma[2]  5.03    3.28 4.68  2.03 14.78     2 7.56\n\nSamples were drawn using NUTS(diag_e) at Sat Mar 22 13:54:38 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/23-clustering.html#what-is-going-on",
    "href": "slides/23-clustering.html#what-is-going-on",
    "title": "Bayesian Clustering",
    "section": "What is going on?",
    "text": "What is going on?\n\npairs(fit1, pars = c(\"mu\", \"sigma\"))"
  },
  {
    "objectID": "slides/23-clustering.html#bimodal-posterior",
    "href": "slides/23-clustering.html#bimodal-posterior",
    "title": "Bayesian Clustering",
    "section": "Bimodal posterior",
    "text": "Bimodal posterior\n\n\n\n\n\n\n\n\n\n\nIn one mode, \\(\\sigma^2_1 \\ll \\sigma^2_2\\) and in the other, \\(\\sigma^2_1\\gg\\sigma^2_2\\)"
  },
  {
    "objectID": "slides/23-clustering.html#bimodal-posterior-1",
    "href": "slides/23-clustering.html#bimodal-posterior-1",
    "title": "Bayesian Clustering",
    "section": "Bimodal posterior",
    "text": "Bimodal posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Gaussian clusters have light tails, so outlying values of \\(Y\\) force large values of \\(\\sigma^2_h\\). When \\(\\sigma^2_h\\) is large, small changes to \\(\\mu_h\\) have little impact on the log-likelihood, and the ordering constraint is not sufficient to identify the clusters."
  },
  {
    "objectID": "slides/23-clustering.html#things-to-consider-when-your-mixture-model-is-mixed-up",
    "href": "slides/23-clustering.html#things-to-consider-when-your-mixture-model-is-mixed-up",
    "title": "Bayesian Clustering",
    "section": "Things to consider when your mixture model is mixed up",
    "text": "Things to consider when your mixture model is mixed up\nMixture modeling, especially when clusters are of interest, can be fickle.\n\nDifferent mixtures can give similar fit to data, leading to multimodal posteriors that are difficult to sample from (previous slides).\nClusters will depend on your choice of \\(f_h\\)—a Gaussian mixture model can only find Gaussian-shaped clusters.\nIncreasing \\(k\\) often improves fit, but may muddle cluster interpretation."
  },
  {
    "objectID": "slides/23-clustering.html#things-to-consider-when-your-mixture-model-is-mixed-up-1",
    "href": "slides/23-clustering.html#things-to-consider-when-your-mixture-model-is-mixed-up-1",
    "title": "Bayesian Clustering",
    "section": "Things to consider when your mixture model is mixed up",
    "text": "Things to consider when your mixture model is mixed up\n\nEmploy informative priors.\nVary the number of clusters.\nChange the form of the kernel."
  },
  {
    "objectID": "slides/23-clustering.html#updated-model",
    "href": "slides/23-clustering.html#updated-model",
    "title": "Bayesian Clustering",
    "section": "Updated model",
    "text": "Updated model\n\n// saved in mixture2.stan\ndata {\n  int&lt;lower = 1&gt; k;          // number of mixture components\n  int&lt;lower = 1&gt; n;          // number of data points\n  array[n] real Y;         // observations\n}\nparameters {\n  simplex[k] pi; // mixing proportions\n  ordered[k] mu; // means of the mixture components\n  vector&lt;lower = 0&gt;[k] sigma; // sds of the mixture components\n  vector&lt;lower = 1&gt;[k] nu;\n}\nmodel {\n  target += normal_lpdf(mu | 0.0, 10.0);\n  target += normal_lpdf(sigma | 2.0, 0.5);\n  target += gamma_lpdf(nu | 5.0, 0.5);\n  vector[k] log_probs = log(pi);\n  for (i in 1:n){\n    vector[k] lps = log_probs;\n    for (h in 1:k){\n      lps[h] += student_t_lpdf(Y[i] | nu[h], mu[h], sigma[h]);\n    }\n    target += log_sum_exp(lps);\n  }\n}\n\n\nInformative prior on \\(\\sigma^2_h\\).\nMixture of Student-t."
  },
  {
    "objectID": "slides/23-clustering.html#updated-model-fit",
    "href": "slides/23-clustering.html#updated-model-fit",
    "title": "Bayesian Clustering",
    "section": "Updated model fit",
    "text": "Updated model fit\n\nmod2 &lt;- stan_model(\"mixture2.stan\")\nfit2 &lt;- sampling(mod2, data=dat, chains=4, iter=5000, control=list(\"adapt_delta\"=0.99))\nprint(fit2, pars=c(\"pi\", \"mu\", \"sigma\", \"nu\"))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\npi[1]     0.81    0.00 0.04  0.73  0.79  0.81  0.83  0.87  5567    1\npi[2]     0.19    0.00 0.04  0.13  0.17  0.19  0.21  0.27  5567    1\nmu[1]    -2.96    0.00 0.22 -3.39 -3.10 -2.96 -2.81 -2.53  7318    1\nmu[2]     8.30    0.02 1.10  6.03  7.66  8.35  9.01 10.28  4306    1\nsigma[1]  2.20    0.00 0.17  1.88  2.09  2.20  2.31  2.55  5987    1\nsigma[2]  2.82    0.00 0.40  2.06  2.55  2.81  3.09  3.64  6745    1\nnu[1]    11.98    0.04 4.44  5.14  8.75 11.38 14.57 22.22  9860    1\nnu[2]     1.54    0.00 0.43  1.03  1.25  1.46  1.74  2.50  9286    1\n\nSamples were drawn using NUTS(diag_e) at Sat Mar 29 12:52:38 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/23-clustering.html#updated-model-results",
    "href": "slides/23-clustering.html#updated-model-results",
    "title": "Bayesian Clustering",
    "section": "Updated model results",
    "text": "Updated model results"
  },
  {
    "objectID": "slides/23-clustering.html#from-marginal-mixture-model-to-clusters",
    "href": "slides/23-clustering.html#from-marginal-mixture-model-to-clusters",
    "title": "Bayesian Clustering",
    "section": "From marginal mixture model to clusters",
    "text": "From marginal mixture model to clusters\nStan cannot directly infer categorical component indicators \\(z_i\\). Instead, for each individual, we compute\n\\[\\begin{align*}\nP\\left(z_i = h \\mid Y_i, \\boldsymbol{\\mu},\\boldsymbol{\\sigma},\\boldsymbol{\\pi} \\right) &= \\frac{f(Y_i\\mid z_i = h, \\mu_h,\\sigma_h)P(z_i=h\\mid \\pi_h)}{\\sum_{h'=1}^k f(Y_i\\mid z_i = h', \\mu_{h'},\\sigma_{h'})P(z_i=h'\\mid \\pi_{h'})}\\\\\n&= \\frac{N(Y_i | \\mu_{h},\\sigma_{h})\\pi_{h}}{\\sum_{h' = 1}^k N(Y_i | \\mu_{h'},\\sigma_{h'})\\pi_{h'}} = p_{ih}.\n\\end{align*}\\]\nGiven these cluster membership probabilities, we can recover cluster indicators through simulation: \\[(z_i\\mid -) \\sim \\text{Categorical}\\left(k, \\left\\{ p_{i1},\\dots,p_{ik}  \\right\\}\\right).\\]"
  },
  {
    "objectID": "slides/23-clustering.html#from-marginal-mixture-model-to-clusters-1",
    "href": "slides/23-clustering.html#from-marginal-mixture-model-to-clusters-1",
    "title": "Bayesian Clustering",
    "section": "From marginal mixture model to clusters",
    "text": "From marginal mixture model to clusters\n\n...\n\ngenerated quantities {\n  matrix[n,k] lPrZik;\n  int&lt;lower=1, upper=k&gt; z[n];\n  for (i in 1:n){\n    for (h in 1:k){\n      lPrZik[i,h] = log(pi[h]) + student_t_lpdf(Y[i] | nu[h], mu[h], sigma[h]);\n    }\n    lPrZik[i] -= log(sum(exp(lPrZik[i])));\n    z[i] = categorical_rng(exp(lPrZik[i]'));\n  }\n}"
  },
  {
    "objectID": "slides/23-clustering.html#co-clustering-probabilities",
    "href": "slides/23-clustering.html#co-clustering-probabilities",
    "title": "Bayesian Clustering",
    "section": "Co-clustering probabilities",
    "text": "Co-clustering probabilities\nRecovering \\(z_i\\) allows us to make the following pairwise comparison: what is the probability that unit \\(i\\) and unit \\(j\\) are in the same cluster? This is the “co-clustering probability”.\nIt is common to arrange these probabilities in a co-clustering matrix \\(\\mathbf{C}\\), where the \\(i,j\\) entry is given by, \\[C_{ij}=P\\left( z_i=z_j\\mid- \\right)\\approx \\frac{1}{S}\\sum_{s=1}^S \\mathbb{1}\\left[z_i^{(s)}=z_j^{(s)}\\right].\\]"
  },
  {
    "objectID": "slides/23-clustering.html#co-clustering-probabilities-1",
    "href": "slides/23-clustering.html#co-clustering-probabilities-1",
    "title": "Bayesian Clustering",
    "section": "Co-clustering probabilities",
    "text": "Co-clustering probabilities"
  },
  {
    "objectID": "slides/23-clustering.html#how-do-our-results-change-when-we-use-more-components",
    "href": "slides/23-clustering.html#how-do-our-results-change-when-we-use-more-components",
    "title": "Bayesian Clustering",
    "section": "How do our results change when we use more components?",
    "text": "How do our results change when we use more components?\n\\(k=3\\)"
  },
  {
    "objectID": "slides/23-clustering.html#how-do-our-results-change-when-we-use-more-components-1",
    "href": "slides/23-clustering.html#how-do-our-results-change-when-we-use-more-components-1",
    "title": "Bayesian Clustering",
    "section": "How do our results change when we use more components?",
    "text": "How do our results change when we use more components?\n\\(k=4\\)"
  },
  {
    "objectID": "slides/23-clustering.html#co-clusterings-across-k",
    "href": "slides/23-clustering.html#co-clusterings-across-k",
    "title": "Bayesian Clustering",
    "section": "Co-clusterings across \\(k\\)",
    "text": "Co-clusterings across \\(k\\)\n\n\n\n\n\n\n\n\n\nThe same general pattern persists when more clusters are used, indicating that \\(k=2\\) is a reasonable choice."
  },
  {
    "objectID": "slides/23-clustering.html#prepare-for-next-class",
    "href": "slides/23-clustering.html#prepare-for-next-class",
    "title": "Bayesian Clustering",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nReminder: On Thursday, we will have a in-class live-coding exercise.\nBegin working on Exam 02, which is due for feedback on April 15."
  },
  {
    "objectID": "slides/22-scalable-2.html",
    "href": "slides/22-scalable-2.html",
    "title": "Scalable Gaussian Processes #2",
    "section": "",
    "text": "We wanted to perform geospatial analysis on a dataset with ~8,600 observations at ~500 locations, and make predictions at ~440 locations on a grid."
  },
  {
    "objectID": "slides/22-scalable-2.html#hsgp-parameters",
    "href": "slides/22-scalable-2.html#hsgp-parameters",
    "title": "Scalable Gaussian Processes #2",
    "section": "HSGP parameters",
    "text": "HSGP parameters\nSolin and Särkkä (2020) showed that HSGP approximation can be made arbitrarily accurate as \\(\\boldsymbol{\\Theta}\\) and \\(m\\) increase.\n\n\n\n\n\n\n\n\nOur goal:\n\n\n\n\nMinimize the run time while maintaining reasonable approximation accuracy.\nFind minimum \\(\\boldsymbol{\\Theta}\\) and \\(m\\) with reasonable accuracy.\n\n\n\n\n\n\nNote: we treat estimation of the GP magnitude parameter \\(\\tau\\) as a separate problem, and only consider approximation accuracy of HSGP in terms of the correlation function."
  },
  {
    "objectID": "slides/22-scalable-2.html#hsgp-approximation-box",
    "href": "slides/22-scalable-2.html#hsgp-approximation-box",
    "title": "Scalable Gaussian Processes #2",
    "section": "HSGP approximation box",
    "text": "HSGP approximation box\nDue to the design of HSGP, the approximation is less accurate near the boundaries of \\(\\boldsymbol{\\Theta}\\).\n\nSuppose all the coordinates are centered. Let \\[S_l = \\max_i |\\mathbf{u}_{il}|, \\quad l=1,\\dots,d, \\quad i= 1, \\dots, (n+q)\\] such that \\(\\boldsymbol{\\Theta}_S = \\prod_{l=1}^d [-S_l,S_l]\\) is the smallest box which contains all observed and prediction locations. We should at least ensure \\(\\boldsymbol{\\Theta} \\supset \\boldsymbol{\\Theta}_S\\).\nWe want the box to be large enough to ensure good boundary accuracy. Let \\(c_l \\ge 1\\) be boundary factors, we consider \\[\\boldsymbol{\\Theta} = \\prod_{l=1}^d [-L_l,L_l], \\quad L_l = c_l S_l.\\]"
  },
  {
    "objectID": "slides/22-scalable-2.html#hsgp-approximation-box-and-rho",
    "href": "slides/22-scalable-2.html#hsgp-approximation-box-and-rho",
    "title": "Scalable Gaussian Processes #2",
    "section": "HSGP approximation box and \\(\\rho\\)",
    "text": "HSGP approximation box and \\(\\rho\\)\n\n\n\n\n\nHow much the approximation accuracy deteriorates towards the boundaries depends on smoothness of the true surface.\n\nthe larger the length scale \\(\\rho\\), the smoother the surface, a smaller box (smaller \\(c\\)) can be used for the same level of boundary accuracy."
  },
  {
    "objectID": "slides/22-scalable-2.html#hsgp-approximation-box-and-m",
    "href": "slides/22-scalable-2.html#hsgp-approximation-box-and-m",
    "title": "Scalable Gaussian Processes #2",
    "section": "HSGP approximation box and \\(m\\)",
    "text": "HSGP approximation box and \\(m\\)\n\n\n\n\n\nThe larger the box,\n\nthe more basis functions we need for the same level of overall accuracy,\nhence higher run time."
  },
  {
    "objectID": "slides/22-scalable-2.html#zooming-out-doesnt-simplify-the-problem",
    "href": "slides/22-scalable-2.html#zooming-out-doesnt-simplify-the-problem",
    "title": "Scalable Gaussian Processes #2",
    "section": "Zooming out doesn’t simplify the problem",
    "text": "Zooming out doesn’t simplify the problem\n\n\n\n\n\n\nIf we scale the coordinates by a constant \\(b\\), the length scale \\(\\rho\\) of the underlying GP also needs to be approximately scaled by \\(b\\) to capture the same level of details in the data.\nWe can effectively think of the length scale parameter as \\((\\rho/\\|\\mathbf{S}\\|)\\)."
  },
  {
    "objectID": "slides/22-scalable-2.html#hsgp-basis-functions",
    "href": "slides/22-scalable-2.html#hsgp-basis-functions",
    "title": "Scalable Gaussian Processes #2",
    "section": "HSGP basis functions",
    "text": "HSGP basis functions\nThe total number of basis functions \\(m = \\prod_{l=1}^d m_l\\), i.e., we need to decide on \\(m_l\\)’s, the number of basis functions for each dimension.\n\n\\(m\\) scales exponentially in \\(d\\), hence the HSGP computation complexity \\(\\mathcal{O}(nm+m)\\) also scales exponentially in \\(d\\). Therefore HSGP is only recommended for \\(d \\le 3\\), at most \\(4\\).\nThe higher the \\(m\\), the better the overall approximation accuracy, the higher the runtime."
  },
  {
    "objectID": "slides/22-scalable-2.html#relationship-between-c-m-and-rhos",
    "href": "slides/22-scalable-2.html#relationship-between-c-m-and-rhos",
    "title": "Scalable Gaussian Processes #2",
    "section": "Relationship between \\(c\\), \\(m\\) and \\(\\rho/S\\)",
    "text": "Relationship between \\(c\\), \\(m\\) and \\(\\rho/S\\)\nLet’s quickly recap. For simplicity, let \\(d=1\\),\n\n\nAs \\((\\rho/S)\\) decreases, the surface is less smooth,\n\n\\(c\\) needs to increase to retain boundary accuracy.\n\\(m\\) needs to increase to retain overall accuracy.\n\nAs \\(c\\) increases, \\(m\\) needs to increase to retain overall accuracy.\nAs \\(m\\) increases, run time increases."
  },
  {
    "objectID": "slides/22-scalable-2.html#empirical-functional-form",
    "href": "slides/22-scalable-2.html#empirical-functional-form",
    "title": "Scalable Gaussian Processes #2",
    "section": "Empirical functional form",
    "text": "Empirical functional form\nStill assuming \\(d=1\\). If \\(\\rho\\) is known to us,\n\ngiven \\(c\\) and \\(\\rho/S\\) and the covariance function \\(C\\), we can compute \\(m(c,\\rho/S)\\), the minimum number of basis functions needed for a near 100% approximation accuracy of the correlation matrix.\nRiutort-Mayol et al. (2023) used extensive simulations to obtain an empirical function form of \\(m(c,\\rho/S)\\) for frequently used Matérn covariance functions. E.g., for Matérn 3/2,\n\n\\[m_{3/2}(c,\\rho/S)=3.24 \\frac{c}{\\rho/S}, \\quad c \\ge 4.5 \\frac{\\rho}S, \\quad c \\ge 1.2.\\]"
  },
  {
    "objectID": "slides/22-scalable-2.html#empirical-functional-form-1",
    "href": "slides/22-scalable-2.html#empirical-functional-form-1",
    "title": "Scalable Gaussian Processes #2",
    "section": "Empirical functional form",
    "text": "Empirical functional form\n\\[m(c,\\rho/S)=3.24 \\frac{c}{\\rho/S}, \\quad c \\ge 4.5 \\frac{\\rho}S, \\quad c \\ge 1.2.\\]\n\n\nNotice the linear proportionality between \\(m\\), \\(c\\) and \\(\\rho/S\\).\nFor a given \\(\\rho/S\\), there exists a minimum \\(c(\\rho/S)=\\min(4.5\\rho/S,1.2)\\), below which the approximation is poor no matter how large \\(m\\) is.\nFrom \\(m(c,\\rho/S)\\), we also have \\[\\rho(m,c,S)=3.42 S \\frac cm,\\] the minimum \\(\\rho\\) (least smooth surface) that can be well approximated given \\(c\\), \\(m\\) and \\(S\\)."
  },
  {
    "objectID": "slides/22-scalable-2.html#question",
    "href": "slides/22-scalable-2.html#question",
    "title": "Scalable Gaussian Processes #2",
    "section": "Question",
    "text": "Question\nBUT, in real applications, we do not know \\(\\rho\\).\n\nSo how to make use of \\(m(c,\\rho/S)\\) to help choose \\(c\\) and \\(m\\)?"
  },
  {
    "objectID": "slides/22-scalable-2.html#side-notes-on-hsgp-implementation-1",
    "href": "slides/22-scalable-2.html#side-notes-on-hsgp-implementation-1",
    "title": "Scalable Gaussian Processes #2",
    "section": "Side notes on HSGP implementation",
    "text": "Side notes on HSGP implementation\nA few random things to keep in mind for implementation in practice:\n\n\nIt is possible to use different length scale parameters for each dimension. See demo codes here for examples.\nThe iterative algorithm described in Riutort-Mayol et al. (2023) (i.e., pseudo-codes on slide 15) can be further improved:\n\nit sometimes stops at a place.\nit sometimes runs into a circular loop. See AE 09 stan codes for one possible fix.\nit errs on the safe side and only changes \\(m\\) if it might be too small.\n\nDue to identifiability issues, we always look at the spatial intercept \\(\\alpha \\mathbf{1}+\\boldsymbol{\\theta}\\) together instead of just \\(\\boldsymbol{\\theta}\\)."
  },
  {
    "objectID": "prepare/prepare-apr08.html",
    "href": "prepare/prepare-apr08.html",
    "title": "Prepare for April 8 lecture",
    "section": "",
    "text": "📖 Review BDA3 Section 22.1 to get an introduction to finite mixture models.\n✅ Work on Exam 02, which will be assigned after class on Tuesday.\n✅ Prepare for the in-class live-coding exercise on Thursday."
  },
  {
    "objectID": "ae/ae-10-clustering.html",
    "href": "ae/ae-10-clustering.html",
    "title": "AE 10: Bayesian Clustering",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-10-clustering.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-10-clustering.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 10: Bayesian Clustering",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-10-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-10.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-10-clustering.html#fitting-the-model",
    "href": "ae/ae-10-clustering.html#fitting-the-model",
    "title": "AE 10: Bayesian Clustering",
    "section": "Fitting the Model",
    "text": "Fitting the Model\nBecause clustering is generally an unsupervised task, no covariates are used to fit the model. We will use a centered outcome \\(Y_i - \\bar{Y}\\) to simplify prior specification. In addition to the outcome \\(Y\\), we must specify the number of components (clusters) \\(k\\) used to fit the mixture. The Stan data object is given by\n\nstan_data &lt;- list(Y = (ed_los$los - mean(ed_los$los)),\n                  n = length(ed_los$los),\n                  k = 2)\n\nWe will now fit the model and print the posterior summaries and MCMC convergence diagnostics.\nGaussian components:\n\nmixture_gauss &lt;- stan_model(\"mixture1.stan\")\nfit_mixture_k2g &lt;- sampling(mixture_gauss, data=stan_data, iter=5000, chains=4, control=list(\"adapt_delta\"=0.99))\n\nprint(fit_mixture_k2g, pars = c(\"pi\", \"mu\", \"sigma\"))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\npi[1]     0.37    0.17 0.24  0.16  0.21  0.25  0.47  0.83     2 6.35\npi[2]     0.63    0.17 0.24  0.17  0.53  0.75  0.79  0.84     2 6.35\nmu[1]    -3.17    0.09 0.51 -4.49 -3.37 -3.07 -2.86 -2.49    33 1.05\nmu[2]     0.45    3.96 5.69 -3.17 -2.86 -2.67 -0.50 13.08     2 5.14\nsigma[1] 13.25    4.48 6.46  2.09  9.72 16.12 17.47 20.11     2 4.97\nsigma[2]  5.03    3.28 4.68  2.03  2.27  2.43  5.04 14.78     2 7.56\n\nSamples were drawn using NUTS(diag_e) at Sat Mar 22 13:54:38 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nTraceplots and pair plots of posterior samples reveal bimodality:\n\nrstan::traceplot(fit_mixture_k2g, pars = c(\"pi\", \"mu\", \"sigma\"))\n\n\n\n\n\n\n\n\n\npairs(fit_mixture_k2g, pars = c(\"mu\", \"sigma\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior means are not meaningful in this case\nStudent t components:\n\nmixture_t &lt;- stan_model(\"mixture2.stan\")\nfit_mixture_k2t &lt;- sampling(mixture_t, data=stan_data, iter=5000, chains=4)\nprint(fit_mixture_k2t, pars = c(\"pi\", \"mu\", \"sigma\", \"nu\"))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\npi[1]     0.81    0.00 0.04  0.73  0.79  0.81  0.83  0.87  5567    1\npi[2]     0.19    0.00 0.04  0.13  0.17  0.19  0.21  0.27  5567    1\nmu[1]    -2.96    0.00 0.22 -3.39 -3.10 -2.96 -2.81 -2.53  7318    1\nmu[2]     8.30    0.02 1.10  6.03  7.66  8.35  9.01 10.28  4306    1\nsigma[1]  2.20    0.00 0.17  1.88  2.09  2.20  2.31  2.55  5987    1\nsigma[2]  2.82    0.00 0.40  2.06  2.55  2.81  3.09  3.64  6745    1\nnu[1]    11.98    0.04 4.44  5.14  8.75 11.38 14.57 22.22  9860    1\nnu[2]     1.54    0.00 0.43  1.03  1.25  1.46  1.74  2.50  9286    1\n\nSamples were drawn using NUTS(diag_e) at Sat Mar 29 12:52:38 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\nrstan::traceplot(fit_mixture_k2t, pars = c(\"pi\", \"mu\", \"sigma\", \"nu\"))"
  },
  {
    "objectID": "ae/ae-10-clustering.html#visualizing-textprz_iz_j-",
    "href": "ae/ae-10-clustering.html#visualizing-textprz_iz_j-",
    "title": "AE 10: Bayesian Clustering",
    "section": "Visualizing \\(\\text{Pr}(z_i=z_j|-)\\)",
    "text": "Visualizing \\(\\text{Pr}(z_i=z_j|-)\\)\nFor t mixture:"
  },
  {
    "objectID": "ae/ae-10-clustering.html#exercise-1",
    "href": "ae/ae-10-clustering.html#exercise-1",
    "title": "AE 10: Bayesian Clustering",
    "section": "Exercise 1",
    "text": "Exercise 1\nRevisit the Gaussian mixture model fit above. Diagnostic criteria indicate that the posterior samples collected may not faithfully represent the true posterior. Traceplots and pair plots indicate that different chains explored different regions of the posterior. Discuss the following: Why might the true posterior be multimodal? Is this a problem with the model or with the way we compute the posterior?\nAnswer:\nPoints students may consider: - Without the ordered constraint, the model is trivially multimodal with modes corresponding to switched labels. - The ordered constraint on \\(\\mu\\) is intended to resolve non-identifiability (label switching). If, under the data generating process \\(\\mu_1\\approx\\mu_2\\) however, then the constraint fails to prevent switching between the variances. I.e., ordering \\(\\mu\\) does not resolve label switching in a variance mixture of Gaussians model. - Notice that the chains do not mix over modes—each chain is apparently stuck in one of the two modes. Hence, no single chain is actually exploring the disjoint region of high posterior probability. This is a limitation of the posterior computation method. - If chains did mix over modes, however, summarizing the posterior would be nontrivial."
  },
  {
    "objectID": "ae/ae-10-clustering.html#exercise-2",
    "href": "ae/ae-10-clustering.html#exercise-2",
    "title": "AE 10: Bayesian Clustering",
    "section": "Exercise 2",
    "text": "Exercise 2\nExtend the Gaussian mixture model Stan code to simulate data Y_pred under the posterior predictive. Compare the density of predicted data to the empirical density of observed data. Do you observe anything surprising?\nAnswer:\nThis is a bit tricky because the posterior probability that large \\(Y\\) comes from the small component is numerically zero.\nHopefully students not that many of the marginal posteriors for Y_pred are well-behaved in terms of ESS and Rhat. Maybe we can estimate the density well without nailing the parameters. The density overlay is reasonable with obvious problems around the interval \\((5,15)\\).\n\n# add code here\n\n\ngenerated quantities {\n  array[n] real Y_pred;\n  matrix[n,k] lPrZik;\n  int&lt;lower=1, upper=k&gt; z[n];\n  \n  for (i in 1:n){\n    for (h in 1:k){\n      lPrZik[i,h] = log(pi[h]) + normal_lpdf(Y[i] | mu[h], sigma[h]);\n    }\n    lPrZik[i] -= log(sum(exp(lPrZik[i])));\n    \n    // Numerical zeros due to light tails complicate predictive inference\n    if (is_inf(exp(lPrZik[i,1]))){\n      z[i] = 1;\n      Y_pred[i] = normal_rng(mu[z[i]], sigma[z[i]]);\n      continue;\n    }\n    if (is_inf(exp(lPrZik[i,2]))){\n      z[i] = 2;\n      Y_pred[i] = normal_rng(mu[z[i]], sigma[z[i]]);\n      continue;\n    }\n    \n    z[i] = categorical_rng(exp(lPrZik[i]'));\n    \n    Y_pred[i] = normal_rng(mu[z[i]], sigma[z[i]]);\n  }\n}\n\n\nEx2mod &lt;- stan_model(\"Ex2.stan\")\nEx2fit &lt;- sampling(Ex2mod, data=stan_data, chains=4, iter=4000,\n                   control=list(\"adapt_delta\"=0.99))\n\nEx2samps &lt;- extract(Ex2fit)\nplot(density(Ex2samps$Y_pred[Ex2samps$Y_pred&gt;-10 & Ex2samps$Y_pred&lt;100]))\n\n\nggplot(data.frame(x1=c(Ex2samps$Y_pred), x2=stan_data$Y)) + \n  geom_density(aes(x=x1), bw=1, color=\"blue\") +\n  geom_density(aes(x=x2), bw=1) + \n  ggtitle(\"Fitted density (blue) + data (black)\")\n\n\n\n\n\n\n\n\n\nggplot(data.frame(x1=c(Ex2samps$Y_pred), x2=stan_data$Y)) + \n  geom_density(aes(x=x1), bw=1, color=\"blue\") +\n  geom_density(aes(x=x2), bw=1) + \n  xlim(min(stan_data$Y)-1, max(stan_data$Y)+1) +\n  ggtitle(\"Fitted density (blue) + data (black)\", subtitle = \"Truncated to sample range\")\n\nWarning: Removed 114357 rows containing non-finite outside the scale range\n(`stat_density()`)."
  },
  {
    "objectID": "ae/ae-10-clustering.html#exercise-3",
    "href": "ae/ae-10-clustering.html#exercise-3",
    "title": "AE 10: Bayesian Clustering",
    "section": "Exercise 3",
    "text": "Exercise 3\nExtend the student t mixture model Stan code to simulate data Y_pred under the posterior predictive. Compare to the predicted data under the Gaussian mixture model and comment on any similarities/differences.\nAnswer:\nProbabilities are better behaved here thanks to the heavy tails. The predictive density has extreme tails (unsurprising given \\(\\nu_1\\approx 1.5\\)). The tails are outlandish in context. However, the mode at \\((5,15)\\) is better modeled.\n\n# add code here\n\n\ngenerated quantities {\n  array[n] real Y_pred;\n  matrix[n,k] lPrZik;\n  \n  int&lt;lower=1, upper=k&gt; z[n];\n  for (i in 1:n){\n    for (h in 1:k){\n      lPrZik[i,h] = log(pi[h]) + student_t_lpdf(Y[i] | nu[h], mu[h], sigma[h]);\n    }\n    lPrZik[i] -= log(sum(exp(lPrZik[i])));\n    z[i] = categorical_rng(exp(lPrZik[i]'));\n    \n    Y_pred[i] = student_t_rng(nu[z[i]], mu[z[i]], sigma[z[i]]);\n  }\n}\n\n\nEx3mod &lt;- stan_model(\"Ex3.stan\")\nEx3fit &lt;- sampling(Ex3mod, data=stan_data, chains=4, iter=4000)\n\nEx3samps &lt;- extract(Ex3fit)\nplot(density(Ex3samps$Y_pred[Ex3samps$Y_pred&gt;-10 & Ex3samps$Y_pred&lt;100]))\n\n\nggplot(data.frame(x1=c(Ex3samps$Y_pred), x2=stan_data$Y)) + \n  geom_density(aes(x=x2), bw=1) +\n  geom_density(aes(x=x1), bw=1, color=\"blue\") +\n  ggtitle(\"Fitted density (blue) + data (black)\")\n\n\n\n\n\n\n\n\n\nggplot(data.frame(x1=c(Ex3samps$Y_pred), x2=stan_data$Y)) + \n  geom_density(aes(x=x1), bw=1, color=\"blue\") +\n  geom_density(aes(x=x2), bw=1) + \n  xlim(min(stan_data$Y)-1, max(stan_data$Y)+1) +\n  ggtitle(\"Fitted density (blue) + data (black)\", subtitle = \"Truncated to sample range\")\n\nWarning: Removed 29799 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nggplot(data.frame(x1=c(Ex2samps$Y_pred), x2=c(Ex3samps$Y_pred))) + \n  geom_density(aes(x=x1), bw=1, color=\"red\") +\n  geom_density(aes(x=x2), bw=1, color=\"blue\") + \n  ggtitle(\"t-mixture density (blue) + Gaussiam mixture (red)\")\n\n\n\n\n\n\n\n\n\nggplot(data.frame(x1=c(Ex2samps$Y_pred), x2=c(Ex3samps$Y_pred))) + \n  geom_density(aes(x=x1), bw=1, color=\"red\") +\n  geom_density(aes(x=x2), bw=1, color=\"blue\") + \n  ggtitle(\"t-mixture density (blue) + Gaussiam mixture (red)\", subtitle = \"Truncated to sample range\") +\n  xlim(min(stan_data$Y)-1, max(stan_data$Y)+1) \n\nWarning: Removed 114357 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 29799 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "slides/23-clustering.html#characterizing-the-clusters",
    "href": "slides/23-clustering.html#characterizing-the-clusters",
    "title": "Bayesian Clustering",
    "section": "Characterizing the Clusters",
    "text": "Characterizing the Clusters"
  },
  {
    "objectID": "slides/23-clustering.html#characterizing-the-clusters-1",
    "href": "slides/23-clustering.html#characterizing-the-clusters-1",
    "title": "Bayesian Clustering",
    "section": "Characterizing the Clusters",
    "text": "Characterizing the Clusters"
  },
  {
    "objectID": "slides/21-scalable-1.html",
    "href": "slides/21-scalable-1.html",
    "title": "Scalable Gaussian Processes #1",
    "section": "",
    "text": "Two weeks ago, we learned about:\n\nGaussian processes, and\nHow to use Gaussian processes for\n\nlongitudinal data\ngeospatial data"
  },
  {
    "objectID": "slides/18-geospatial.html",
    "href": "slides/18-geospatial.html",
    "title": "Geospatial Modeling",
    "section": "",
    "text": "During our last lecture, we learned about Gaussian processes.\nWe learned how to apply Gaussian processes to longitudinal (or time-series) data.\nThe longitudinal setting is one-dimensional (i.e., time). Today we will learn about applying Gaussian processes in two-dimensions (i.e. space)."
  },
  {
    "objectID": "slides/06-model-checking.html",
    "href": "slides/06-model-checking.html",
    "title": "Model checking",
    "section": "",
    "text": "On Thursday, we learned about\n\nDifferent types of priors that can be specified\nPosterior summaries: point estimates, intervals, and probabilities\nPosterior predictive distributions\nWe learned about the generated quantities code chunk\n\nToday, we will dive into (1) methods for assessing MCMC convergence, and (2) model performance techniques."
  },
  {
    "objectID": "slides/23-clustering.html",
    "href": "slides/23-clustering.html",
    "title": "Bayesian Clustering",
    "section": "",
    "text": "We will introduce the basic mixture modeling framework as a mechanism for model-based clustering and describe computational and inferential challenges.\nVariations of the popular finite Gaussian mixture model (GMM) will be introduced to cluster patients according to ED length-of-stay.\nWe present an implementation of mixture modeling in Stan and discuss challenges therein.\nFinally, various posterior summaries will be explored."
  },
  {
    "objectID": "slides/08-workflow.html",
    "href": "slides/08-workflow.html",
    "title": "Bayesian Workflow",
    "section": "",
    "text": "On Thursday, we learned about various ways compare models.\n\nAIC, DIC, WAIC\nLOO-CV/LOO-IC\n\nToday, we will put these concepts within the larger framework of the Bayesian workflow."
  },
  {
    "objectID": "slides/14-missing.html",
    "href": "slides/14-missing.html",
    "title": "Missing Data",
    "section": "",
    "text": "Last, we learned about classification for binary and multiclass problems."
  },
  {
    "objectID": "slides/16-longitudinal.html",
    "href": "slides/16-longitudinal.html",
    "title": "Longitudinal Data",
    "section": "",
    "text": "During our last lecture, we introduced correlated (or dependent) data sources.\nWe discussed the idea of accounting for dependencies within a group using group-specific parameters.\nWe introduced the random intercept model and studied the induced correlation (forced to be positive) in the marginal model.\nToday we will look at longitudinal data and introduce a simple model that accounts for group-level changes."
  },
  {
    "objectID": "slides/19-disease-mapping.html",
    "href": "slides/19-disease-mapping.html",
    "title": "Disease Mapping",
    "section": "",
    "text": "Last week, we learned about Gaussian processes.\nWe learned how to apply Gaussian processes to longitudinal (or time-series) and geospatial data.\nFocused on making predictions at new locations across the spatial surface.\nToday we will focus on areal spatial data, which has different goals associated with it than point-referenced spatial data."
  },
  {
    "objectID": "slides/04-stan.html",
    "href": "slides/04-stan.html",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "",
    "text": "On Thursday, we performed posterior inference for Bayesian linear regression using Gibbs and Metropolis sampling.\n\nWe obtained correlated samples from the posterior using MCMC.\nGibbs required a lot math!\nMetropolis required tuning!\n\nToday we will introduce Stan, a probabilistic programming language that uses Hamiltonian Monte Carlo to perform general Bayesian inference."
  }
]