[
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you‚Äôre having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you‚Äôll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it‚Äôs been resolved. If there‚Äôs a deadline coming up soon, post on the course forum to let us know that there‚Äôs an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don‚Äôt anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you‚Äôve tried and the errors you see (including verbatim errors and/or screenshots).",
    "crumbs": [
      "Computing",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\nüîó for Duke Container Manager\n\n\nCourse GitHub organization\nüîó for GitHub\n\n\nCourse Canvas site\nüîó for Canvas\n\n\nAssignment submission\nüîó to Gradescope\n\n\nZoom links\nüîó on Canvas",
    "crumbs": [
      "Useful links"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA602 on the Reservations available menu on the right. You only need to do this once, and when you do, you‚Äôll see this container moved to the My reservations menu on the left. We are sharing a container with STA 602 this semester!\nNext, click on STA602 under My reservations to access the RStudio instance you‚Äôll use for the course.",
    "crumbs": [
      "Computing",
      "Access"
    ]
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#lectures",
    "href": "support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#office-hours",
    "href": "support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!\nMake a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone‚Äôs office hours here.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#ed-discussion",
    "href": "support.html#ed-discussion",
    "title": "Course support",
    "section": "Ed Discussion",
    "text": "Ed Discussion\nOutside of class and office hours, any general questions about course content or assignments should be posted on Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters that are not appropriate for the class discussion forum (e.g.¬†illness, accommodations, etc.), you may email me at sib2@duke.edu. If you email me, please include ‚ÄúBIOSTAT 725‚Äù in the subject line. Barring extenuating circumstances, I will respond to BIOSTAT 725 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#mental-health-and-wellness",
    "href": "support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\n\nDukeReach: Provides comprehensive outreach services to identify and support students in managing all aspects of well being. If you have concerns about a student‚Äôs behavior or health visit the website for resources and assistance. Go to studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS): CAPS services include individual, group, and couples counseling services, health coaching, psychiatric services, and workshops and discussions. (919) 660-1000 or students.duke.edu/wellness/caps\nTimelyCare (formerly known as Blue Devils Care): An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#technology-accommodations",
    "href": "support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\n\nWhile we encourage students to use their own laptops for work in this course, we will provide the opportunity to use Duke‚Äôs computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#course-materials-costs",
    "href": "support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#assistance-with-zoom-or-canvas",
    "href": "support.html#assistance-with-zoom-or-canvas",
    "title": "Course support",
    "section": "Assistance with Zoom or Canvas",
    "text": "Assistance with Zoom or Canvas\nFor technical help with Canvas or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Canvas here.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "hw/hw-01-blank.html",
    "href": "hw/hw-01-blank.html",
    "title": "HW 01",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-04-blank.html",
    "href": "hw/hw-04-blank.html",
    "title": "HW 04",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-05-blank.html",
    "href": "hw/hw-05-blank.html",
    "title": "HW 05",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "This course will teach students how to analyze biomedical data from a Bayesian inference perspective with a strong emphasis on using real-world data, including electronic health records, wearables, and imaging data. The course will begin by introducing the machinery of Bayesian statistics through the lens of linear regression, giving enough context for students with no prior experience with Bayesian statistics. A history of computational approaches used in Bayesian statistics will be given before ultimately landing on Stan, a state-of-the-art probabilistic programming language that makes Bayesian inference accessible as a viable data science tool. The course will then branch out from regression and introduce Bayesian versions of machine learning tools, including regularization and classification. The course will then emphasize Bayesian hierarchical models, including Gaussian process models for temporal and spatial data; and clustering. Additional topics may be discussed from the Bayesian perspective, including causal inference, and meta-analysis. While an applied course, the methods will be introduced from a mathematical perspective, allowing students to obtain a fundamental understanding of the introduced models. Students will learn computational skills for implementing Bayesian models using R and Stan. By the end of this course, students will be well-equipped to tackle complex problems in biomedical research using Bayesian inference.\n\n\nBIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#pre-requisites",
    "href": "overview.html#pre-requisites",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "BIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#teaching-assistants",
    "href": "overview.html#teaching-assistants",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\nName\nRole\nOffice Hours\nLocation\n\n\nChristine Shen\nTA\nMon 1:30 - 3:30pm\nOld Chemistry 203A\n\n\nJiang Shu\nTA\nWed 4:45 ‚Äì 6:45pm\nHock (TBD)",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "hw/hw-03-blank.html",
    "href": "hw/hw-03-blank.html",
    "title": "HW 03",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/hw-02-blank.html",
    "href": "hw/hw-02-blank.html",
    "title": "HW 02",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-00-blank.html",
    "href": "hw/hw-00-blank.html",
    "title": "HW 00",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "Below are freely available resources to learn or review the following in R: data wrangling, data visualization, Quarto basics.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#in-depth-introduction",
    "href": "computing-r-resources.html#in-depth-introduction",
    "title": "Resources for learning R",
    "section": "In-depth introduction",
    "text": "In-depth introduction\nCoursera: Data Visualization and Transformation with R by Mine √áetinkaya-Rundel and Elijah Meyer\n\nIncludes videos, readings, practice exercise, quizzes, and other resources\nYou can select content within the modules you want to complete.\nFocus on Modules 2 and 3. Review the content in Module 1 as needed.\nClick here for instructions to register for Coursera for free as a Duke student",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#in-depth-review",
    "href": "computing-r-resources.html#in-depth-review",
    "title": "Resources for learning R",
    "section": "In-depth review",
    "text": "In-depth review\nData Science with R videos by Mine √áetinkaya-Rundel and Elijah Meyer\n\nVideos from the data science Coursera course\nFocus on videos on visualizing and summarizing data\nYou need to join the Coursera course to access the files from the code along videos.\n\nLearn R: An interactive introduction to data analysis with R\n\nHands-on tutorial that can be completed within the site (no RStudio required)\nFocus on Chapters 4 - 6",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#shorter-review",
    "href": "computing-r-resources.html#shorter-review",
    "title": "Resources for learning R",
    "section": "Shorter review",
    "text": "Shorter review\nR for Data Science (2nd ed) by Hadley Wickham, Mine √áetinkaya-Rundel, and Garrett Grolemund\n\nFocus on Chapters 1 - 3, 10",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nTidy Modeling with R by Max Kuhn & Julia Silge\nPosit Cheatsheets\nR workshops by Duke Center for Data and Visualization Sciences",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#stan-resources",
    "href": "computing-r-resources.html#stan-resources",
    "title": "Resources for learning R",
    "section": "Stan resources",
    "text": "Stan resources\n\nRStan Getting Started by Stan Development Team\nBrief Introduction to Stan by Mark Lai",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\ntopic\nprepare\nslides\nae\nhw\nnotes\n\n\n\n\n1\nTh\nJan 8\nWelcome + What Is Bayesian Health Data Science?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW00 assigned\n\n\n2\nTu\nJan 13\nMonte Carlo Sampling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 15\nMarkov Chain Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 01 assigned, HW00 due\n\n\n3\nTu\nJan 20\nProbabilistic Programming (Intro to Stan!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 22\nPriors, Posteriors, and PPDs!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\nTu\nJan 27\nModel Checking\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 29\nModel Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nHW 02 assigned, HW 01 due\n\n\n5\nTu\nFeb 3\nBayesian Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 5\nNonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\nTu\nFeb 10\nRobust Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 12\nRegularization\n\n\n\n\n\n\n\n\n\n\n\n\nHW 03 assigned, HW 02 due\n\n\n7\nTu\nFeb 17\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 19\nMulticlass Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\nTu\nFeb 24\nHierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 26\nLongitudinal Data\n\n\n\n\n\n\n\n\n\n\n\n\nExam 01 assigned, HW03 due\n\n\n9\nTu\nMar 3\nExam 1 Office Hours\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 5\nGaussian Processes\n\n\n\n\n\n\n\n\n\n\n\n\nHW04 assigned, Exam 01 due\n\n\n10\nTu\nMar 10\nNO CLASS: Spring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 12\nNO CLASS: Spring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\nTu\nMar 17\nGeospatial Modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 19\nScalable Gaussian Processes #1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\nTu\nMar 24\nScalable Gaussian Processes #2\n\n\n\n\n\n\n\n\n\n\n\n\nHW 05 assigned, HW04 due\n\n\n\nTh\nMar 26\nDisease Mapping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\nTu\nMar 31\nMissing Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nApr 2\nBayesian Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\nTu\nApr 7\nTBD Topic/Live-Coding Review\n\n\n\n\n\n\n\n\n\n\n\n\nHW 05 due, Exam 02 assigned\n\n\n\nTh\nApr 9\nLive-Coding Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\nTu\nApr 14\nExam 02 Office Hours\n\n\n\n\n\n\n\n\n\n\n\n\nExam 02 due for feedback\n\n\nExam period\nTu\nApr 29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExam 02 due by 5:00pm",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "BIOSTAT 725 Syllabus",
    "section": "",
    "text": "Lecture\nTue & Thu 11:45am - 1pm\nHock 10089\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nRole\nOffice Hours\nLocation\n\n\n\n\nProf.¬†Sam Berchuck\nInstructor\nMon 4-6pm\nor by appointment\nHock 9028\n\n\nChristine Shen\nTA\n\nMon 1:30 - 3:30pm\nOld Chemistry 203A\n\n\nJiang Shu\nTA\n\nWed 4:45 ‚Äì 6:45pm\nHock (Room TBD)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "BIOSTAT 725 Syllabus",
    "section": "",
    "text": "Lecture\nTue & Thu 11:45am - 1pm\nHock 10089\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nRole\nOffice Hours\nLocation\n\n\n\n\nProf.¬†Sam Berchuck\nInstructor\nMon 4-6pm\nor by appointment\nHock 9028\n\n\nChristine Shen\nTA\n\nMon 1:30 - 3:30pm\nOld Chemistry 203A\n\n\nJiang Shu\nTA\n\nWed 4:45 ‚Äì 6:45pm\nHock (Room TBD)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course description",
    "text": "Course description\nThis course will teach students how to analyze biomedical data from a Bayesian inference perspective with a strong emphasis on using real-world data, including electronic health records, wearables, and imaging data. The course will begin by introducing the machinery of Bayesian statistics through the lens of linear regression, giving enough context for students with no prior experience with Bayesian statistics. A history of computational approaches used in Bayesian statistics will be given before ultimately landing on Stan, a state-of-the-art probabilistic programming language that makes Bayesian inference accessible as a viable data science tool. The course will then branch out from regression and introduce Bayesian versions of machine learning tools, including regularization and classification. The course will then emphasize Bayesian hierarchical models, including Gaussian process models for temporal and spatial data; and clustering. Additional topics may be discussed from the Bayesian perspective, including causal inference, and meta-analysis. While an applied course, the methods will be introduced from a mathematical perspective, allowing students to obtain a fundamental understanding of the introduced models. Students will learn computational skills for implementing Bayesian models using R and Stan. By the end of this course, students will be well-equipped to tackle complex problems in biomedical research using Bayesian inference.\n\nPrerequisites\nBIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nunderstand fundamental concepts of Bayesian statistics, including prior and posterior, and predictive distributions,\nimplement the Bayesian workflow, including model building, checking, and refinement,\nuse probabilistic programming software for Bayesian analysis (e.g., Stan),\napply Bayesian techniques to real-world health data,\ncommunicate Bayesian analysis results effectively to both technical and non-technical audiences, and\nidentify opportunities for using Bayesian statistics in your research and/or job.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course materials",
    "text": "Course materials\nWhile there is no official textbook for the course; readings will primarily be made available as they are assigned. We will use the statistical software R. Students will be encourage to download the required software on their own laptops. As a courtesy, students will also be able to access R and the required software through Docker containers provided by Duke Office of Information Technology. See the computing page for more information.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course community",
    "text": "Course community\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke‚Äôs Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean or director of graduate studies (DGS) are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nPronouns\nPronouns are meaningful tools to communicate identities and experiences, and using pronouns supports a campus environment where all community members can thrive. Please update your gender pronouns in Duke Hub. You can find instructions to do so here. You can learn more at the Center for Sexual and Gender Diversity‚Äôs website.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, biostat725-sp26.netlify.app.\nLinks to Zoom meetings may be found in Canvas. Periodic announcements will be sent via email and Canvas Announcements. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nEmail\nIf you have questions about assignment extensions, accommodations, or any other matter, please email me directly at sib2@duke.edu. If you email me, please include ‚ÄúBIOSTAT 725‚Äù in the subject line. Barring extenuating circumstances, I will respond to BIOSTAT 725 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#five-tips-for-success",
    "href": "syllabus.html#five-tips-for-success",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success in this course depends very much on you and the effort you put into it. Your TA(s) and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TA, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you‚Äôre not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings and other preparation work.\nDo the homeworks. The earlier you start, the better. It‚Äôs not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example).\nDon‚Äôt procrastinate. The content builds upon what was taught in previous weeks, so if something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, etc. Don‚Äôt let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours and work with a member of the teaching team to help you identify a good (re)starting point.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#getting-help-in-the-course",
    "href": "syllabus.html#getting-help-in-the-course",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Getting help in the course",
    "text": "Getting help in the course\n\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours1 to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them! \n\nCheck out the Support page for more resources.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#what-to-expect-in-the-course",
    "href": "syllabus.html#what-to-expect-in-the-course",
    "title": "BIOSTAT 725 Syllabus",
    "section": "What to expect in the course",
    "text": "What to expect in the course\n\nLectures\nLectures are designed to be interactive, so you gain experience applying new concepts and learning from each other. My role as instructor is to introduce you to new methods, tools, and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities during the lectures. You are expected to prepare for class by completing assigned readings, attend all lecture sessions, and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded based on completing what we do in class.\nYou are expected to bring a laptop, tablet, or any device with internet and a keyboard to each class so that you can participate in the in-class exercises. Please make sure your device is fully charged before you come to class, as the number of outlets in the classroom will not be sufficient to accommodate everyone.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\nYou will be assessed based on four components: homework, exams, live coding, and application exercises.\n\nHomework\nIn homework, you will apply what you‚Äôve learned during lecture to complete data analysis tasks and explain the underlying mathematics, with a focus on the computation and communication. Homework assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and submitted as a PDF for grading in Gradescope. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. HW0 will not be graded for credit.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams\nThere will be two exams in this course. Each exam will be an open-note take-home assessment. Through these exams you have the opportunity to demonstrate what you‚Äôve learned in the course thus far. The exams will focus on both conceptual understanding of the applied and mathematical content and application through analysis and computational tasks. The exams will be based on content in reading assignments, lectures, application exercises, and homework assignments. More detail about the exams will be given during the semester.\n\n\nLive Coding\nThere will be an in-class live coding evaluation in this course. The exercise will take place on Thursday, April 9, during the regularly scheduled class period, and will account for 10% of the final course grade. Students will work independently in a real-time setting to implement and analyze a Bayesian model using Stan, drawing on concepts from lectures and course assignments. This will be an open-book exercise: students may use course materials and external online resources, but may not use AI tools to generate written explanations or narrative interpretations (AI may be used for coding and technical assistance only). The goal of the exercise is to assess students‚Äô ability to apply Bayesian modeling concepts and work effectively in Stan under realistic analytical conditions. Materials will be distributed at the start of the session via a Github repo. Students who cannot attend on this date must contact the instructor in advance to arrange an alternative. More information about the project will be provided during the semester.\n\n\nApplication exercises\nYou will get the most out of the course if you actively participate in class. Parts of some lectures will be dedicated to working on Application Exercises (AEs). AEs are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59p ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59p ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nAEs will be graded based on making a good-faith effort to attempt all questions covered in class. You are welcome to, but not required, to work on AEs beyond lecture.\nSuccessful on-time effort on at least 80% of AEs will result in full credit for AEs in the final course grade.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n40%\n\n\nExam 01\n20%\n\n\nExam 02\n20%\n\n\nLive Coding\n10%\n\n\nApplication exercises\n10%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nDuke Community Standard\nAll students must adhere to the Duke Community Standard (DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard, students agree:\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;and\nI will act if the Standard is compromised.\n\n\n\n\n\n\nAcademic honesty\nTL;DR: Don‚Äôt cheat!\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what‚Äôs the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nYou may not discuss or otherwise work with others on the exams. Unauthorized collaboration or using unauthorized materials will be considered a violation for all students involved. More details will be given closer to the exam date.\n\n\n\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g.¬†StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:2 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate‚Äîrather than hinder‚Äîlearning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\nAI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\nNo AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments. In general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask a member of the teaching team.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including academic integrity (e.g., completing one‚Äôs own work, following proper citation of sources, adhering to guidance around group work projects,and more). Ignoring these requirements is a violation of the Duke Community Standard. Any questions and/or concerns regarding academic integrity can be directed to the Office of Student Conduct and Community Standards at conduct@duke.edu.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback in a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework assignment will be dropped to accommodate such circumstances.\n\nHomework may be submitted up to 2 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThe late work policy for exams will be provided with the exam instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a homework assignment by the stated due date, you may email me at sib2@duke.edu before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your director of graduate studies (DGS) know, as they can be a resource. Please let me know if you need help contacting your DGS.\n\n\nRegrade Requests\nRegrade requests must be submitted on Gradescope within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the last day of classes.\n\n\nAttendance policy\nEvery student is expected to attend and participate in lecture. There may be times, however, when you cannot attend class. Lecture recordings will be made available upon request for students who have an excused absence. If you miss a lecture, make sure to review the material and complete the application exercise, if applicable, before the next lecture.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\n\nAcademic accommodations\nIf you need accommodations for this class, you will need to register with the Student Disability Access Office (SDAO) and provide them with documentation related to your needs. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#academic-and-wellness-support",
    "href": "syllabus.html#academic-and-wellness-support",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Academic and wellness support",
    "text": "Academic and wellness support\n\n\n\nCAPS\nDuke Counseling & Psychological Services (CAPS) helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJanuary 7: Classes begin\nJanuary 19: MLK Jr Day. No classes\nJanuary 21: Drop/Add ends\nMarch 7 - 15: Spring Break. No classes\nApril 15: Graduate classes end.\nApril 16: Reading period begins (no classes or projects are due during the reading period).\nApril 26: Reading period ends.\nApril 27 - May 2: Final exam period\n\nClick here for the full Duke academic calendar.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOffice hours are times the teaching team set aside each week to meet with students. Click here to learn more about how to effectively use office hours.‚Ü©Ô∏é\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.‚Ü©Ô∏é‚Ü©Ô∏é",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-berchuck",
    "href": "slides/01-welcome.html#meet-prof.-berchuck",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Meet Prof.¬†Berchuck!",
    "text": "Meet Prof.¬†Berchuck!\n\nEducation and career journey\n\nBS in Statistical Science from Duke University\nPhD in Biostatistics from University of North Carolina - Chapel Hill\nPostdoc in Duke Forge: Duke‚Äôs Center for Actionable Health Data Science\nNIH/NEI Pathway to Independence Fellow (K99/R00)\nAssistant Professor, Department of Biostatistics & Bioinformatics and Statistical Science at Duke; Faculty Affiliate of Duke AI Health\n\nWork focuses on developing data science tools to improve patient experience using biomedical data (including EHR)\nDad of 5 and 7 year old daughters üôÇ"
  },
  {
    "objectID": "slides/01-welcome.html#teaching-assistants-tas",
    "href": "slides/01-welcome.html#teaching-assistants-tas",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Teaching Assistants (TAs)",
    "text": "Teaching Assistants (TAs)\n\nChristine Shen\n\n5th year PhD student in the Department of Statistical Science at Duke\n\nJiang Shu\n\n1st year PhD student in the Department of Biostatistics & Bioinformatics at Duke"
  },
  {
    "objectID": "slides/01-welcome.html#check-in-on-ed-discussion",
    "href": "slides/01-welcome.html#check-in-on-ed-discussion",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Check-in on Ed Discussion!",
    "text": "Check-in on Ed Discussion!\n\nClick on the link or scan the QR code to answer the Ed Discussion poll\nhttps://forms.office.com/r/QTuhmC0wM3"
  },
  {
    "objectID": "slides/01-welcome.html#topics",
    "href": "slides/01-welcome.html#topics",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Topics",
    "text": "Topics\n\nIntroduction to the course\nSyllabus activity\nReview of probability"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-bayesian-health-data-science",
    "href": "slides/01-welcome.html#what-is-bayesian-health-data-science",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is Bayesian Health Data Science?",
    "text": "What is Bayesian Health Data Science?\n\n\nBayesian Health Data Science involves using Bayesian methods to analyze health data, which can include electronic health records (EHR), clinical trial data, and other health-related datasets. These methods are model-based and can appropriately quantify and propagate uncertainty, making them suitable for tackling challenges in health research.\n\nSource: ChatGPT"
  },
  {
    "objectID": "slides/01-welcome.html#why-data-science",
    "href": "slides/01-welcome.html#why-data-science",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Why Data Science?",
    "text": "Why Data Science?\n\nStatistics versus Data Science?\nIntroductory Bayesian statistics courses are often very mathematical and involve intense computation; thus Bayesian methods are not as frequently used in applied settings.\nModern software now exists to lower the mathematical burden and computational intensity of Bayesian statistics, but courses do not reflect this.\nThis course focuses on teaching students Bayesian statistics as a tool for research; or anywhere data science is practiced."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-biostat-725",
    "href": "slides/01-welcome.html#what-is-biostat-725",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is BIOSTAT 725?",
    "text": "What is BIOSTAT 725?\n\n\n\n\n\n Bayes \n\nModeling\n\n\n\n\n+\n\n\n\n\n\n Stan\n\nProbabilistic Programming\n\n\n\nPrerequisites: BIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission."
  },
  {
    "objectID": "slides/01-welcome.html#course-learning-objectives",
    "href": "slides/01-welcome.html#course-learning-objectives",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nunderstand fundamental concepts of Bayesian statistics, including prior and posterior, and predictive distributions,\nimplement the Bayesian workflow, including model building, checking, and refinement,\nuse probabilistic programming software for Bayesian analysis (e.g., Stan),\napply Bayesian techniques to real-world health data,\ncommunicate Bayesian analysis results effectively to both technical and non-technical audiences, and\nidentify opportunities for using Bayesian statistics in your research and/or job."
  },
  {
    "objectID": "slides/01-welcome.html#course-topics",
    "href": "slides/01-welcome.html#course-topics",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course topics",
    "text": "Course topics\n\n\n\nLinear regression\n\nMethods for inference\nPrior elicitation\nPosterior estimation\nUncertainty quantification\nModel assessment\nBayesian workflow\nPrediction\n\n\n\nHealth Datasets\n\n\n\nExtensions\n\nRobust regression\nRegularization\nClassification\nMissing data\n\n\n\nHierarchical Model\n\nGaussian processes\nLongitudinal data\nSpatial data"
  },
  {
    "objectID": "slides/01-welcome.html#course-toolkit",
    "href": "slides/01-welcome.html#course-toolkit",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nWebsite: https://biostat725-sp26.netlify.app/\n\nCentral hub for the course!\nTour of the website\n\nCanvas: https://canvas.duke.edu/courses/75433\n\nGradebook\nAnnouncements\nGradescope\n\nGitHub: github.com/biostat725-sp26\n\nDistribute assignments\nPlatform for version control and collaboration"
  },
  {
    "objectID": "slides/01-welcome.html#computing-toolkit",
    "href": "slides/01-welcome.html#computing-toolkit",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nInference using Stan, a probabilistic programming language (rstan)\nWrite reproducible reports in Quarto\nAccess RStudio through STA725 (STA602) Docker Containers\n\n\n\n\n\n\n\n\n\n\n\n\nAccess assignments\nFacilitates version control and collaboration\nAll work in BIOSTAT 725 course organization"
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity",
    "href": "slides/01-welcome.html#syllabus-activity",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity",
    "text": "Syllabus activity\n\n\nIntroduce yourself to your group members.\nChoose a reporter. This person will share the group‚Äôs summary with the class.\nRead the portion of the syllabus assigned to your group.\nDiscuss the key points and questions you my have.\nThe reporter will share a summary with the class."
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity-assignments",
    "href": "slides/01-welcome.html#syllabus-activity-assignments",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity assignments",
    "text": "Syllabus activity assignments\n\nGroup 1: What to expect in the course\nGroup 2: Homework and Exams\nGroup 3: Live Coding and Application Exercises\nGroup 4: Academic honesty (except AI policy)\nGroup 5: Artificial intelligence policy\nGroup 6: Late work policy and waiver for extenuating circumstances\nGroup 7: Regrade requests and attendance policy and Getting help in the course"
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity-report-out",
    "href": "slides/01-welcome.html#syllabus-activity-report-out",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity report out",
    "text": "Syllabus activity report out\n\nGroup 1: What to expect in the course\nGroup 2: Homework and Exams\nGroup 3: Live Coding and Application Exercises\nGroup 4: Academic honesty (except AI policy)\nGroup 5: Artificial intelligence policy\nGroup 6: Late work policy and waiver for extenuating circumstances\nGroup 7: Regrade requests and attendance policy and Getting help in the course"
  },
  {
    "objectID": "slides/01-welcome.html#grading",
    "href": "slides/01-welcome.html#grading",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n40%\n\n\nExam 01\n20%\n\n\nExam 02\n20%\n\n\nLive Coding\n10%\n\n\nApplication Exercises\n10%\n\n\nTotal\n100%"
  },
  {
    "objectID": "slides/01-welcome.html#five-tips-for-success-in-biostat-725",
    "href": "slides/01-welcome.html#five-tips-for-success-in-biostat-725",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Five tips for success in BIOSTAT 725",
    "text": "Five tips for success in BIOSTAT 725\n\nComplete all the preparation work before class.\nAsk questions in class, and office hours.\nDo the homework; get started on homework early when possible.\nDon‚Äôt procrastinate and don‚Äôt let a week pass by with lingering questions.\nStay up-to-date on announcements on Canvas and sent via email."
  },
  {
    "objectID": "slides/01-welcome.html#review-of-probability",
    "href": "slides/01-welcome.html#review-of-probability",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Review of probability",
    "text": "Review of probability\n\nThe goal of Bayesian statistics is to compute the posterior distribution (i.e., the uncertainty distribution of the parameters, \\(\\boldsymbol{\\theta}\\), after observing the data, \\(\\mathbf{Y}\\)).\nThis is the conditional distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{Y}\\).\nTherefore, we need to review the probability concepts that lead to the conditional distribution of one variable conditioned on another.\n\nProbability mass (pmf) and density (pdf) functions\nJoint distributions\nMarginal and conditional distributions"
  },
  {
    "objectID": "slides/01-welcome.html#random-variables",
    "href": "slides/01-welcome.html#random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Random variables",
    "text": "Random variables\n\n\\(X\\) (capital) is a random variable.\nWe want to compute the probability that \\(X\\) takes on a specific value \\(x\\) (lowercase).\n\nThis is denoted \\(P(X = x)\\).\n\nWe also might want to compute the probability of \\(X\\) being in a set \\(\\mathcal A\\).\n\nThis is denoted \\(P(X \\in \\mathcal A)\\).\n\nThe set of possible values that \\(X\\) can take on is called its support, \\(\\mathcal S\\)."
  },
  {
    "objectID": "slides/01-welcome.html#random-variables---example",
    "href": "slides/01-welcome.html#random-variables---example",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Random variables - example",
    "text": "Random variables - example\n\nExample 1: \\(X\\) is the roll of a die.\n\nThe support is \\(\\mathcal S = \\{1, 2, 3, 4, 5, 6\\}\\).\n\\(P(X = 1) = 1/6\\).\n\nExample 2: \\(X\\) is a newborn baby‚Äôs weight.\n\nThe support is \\(\\mathcal S = (0, \\infty)\\).\n\\(P(X \\in [0, \\infty]) = 1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-probability",
    "href": "slides/01-welcome.html#what-is-probability",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is probability?",
    "text": "What is probability?\nObjective (associated with frequentist)\n\n\\(P(X = x)\\) as a purely mathematical statement.\nIf we repeatedly sampled \\(X\\), then the proportion of draws equal to \\(x\\) converges to \\(P(X = x)\\).\n\nSubjective (associated with Bayesian)\n\n\\(P(X = x)\\) represents an individual‚Äôs degree of belief.\nOften quantified as the amount an individual would be willing to wager that \\(X\\) will be \\(x\\).\n\nA Bayesian analysis makes use of both of these concepts."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-uncertainty",
    "href": "slides/01-welcome.html#what-is-uncertainty",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is uncertainty?",
    "text": "What is uncertainty?\nAleatoric uncertainty (likelihood)\n\nUncontrollable randomness in the experiment.\nFor example, the results of a fair coin flip can never be predicted with certainty.\n\nEpistemic uncertainty (prior/posterior)\n\nUncertainty about a quantity that could theoretically be known.\nFor example, if we flipped a coin infinitely-many times we could know the true probability of a head.\n\nA Bayesian analysis makes use of both of these concepts."
  },
  {
    "objectID": "slides/01-welcome.html#univariate-distributions",
    "href": "slides/01-welcome.html#univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables.\nThe random variable \\(X\\) is discrete if its support \\(\\mathcal S\\) is countable.\nExamples:\n\n\\(X \\in \\{0, 1, 2, 3\\}\\) is the number of successes in 3 trials.\n\\(X \\in \\{0, 1, 2, \\ldots\\}\\) is the number of patients with COVID in Durham County."
  },
  {
    "objectID": "slides/01-welcome.html#univariate-distributions-1",
    "href": "slides/01-welcome.html#univariate-distributions-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables.\nThe random variable \\(X\\) is continuous if its support \\(\\mathcal S\\) is uncountable.\nExamples with \\(\\mathcal S = (0, \\infty)\\):\n\n\\(X &gt; 0\\) is systolic blood pressure.\n\\(X &gt; 0\\) is a patient‚Äôs BMI."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-univariate-distributions",
    "href": "slides/01-welcome.html#discrete-univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete univariate distributions",
    "text": "Discrete univariate distributions\n\nIf \\(X\\) is discrete we describe its distribution with its probability mass function (pmf).\nThe pmf is \\(f(x) = P(X = x)\\).\nThe domain of \\(X\\) is the set of \\(x\\) with \\(f(x) &gt; 0\\).\nWe must have \\(f(x) \\geq 0\\) and \\(\\sum_x f(x) = 1\\).\nThe mean is \\(\\mathbb E[X] = \\sum_x x f(x)\\).\nThe variance is \\(\\mathbb V(X) = \\sum_x(x ‚àí \\mathbb E[X])^2f(x)\\).\nThe last three sums are over \\(X\\)‚Äôs domain."
  },
  {
    "objectID": "slides/01-welcome.html#parametric-families-of-distributions",
    "href": "slides/01-welcome.html#parametric-families-of-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Parametric families of distributions",
    "text": "Parametric families of distributions\n\nA statistical analysis typically proceeds by selecting a pmf that seems to match the distribution of a sample.\nWe rarely know the pmf exactly, but we assume it is from a parametric family of distributions.\nFor example, Binomial(10, 0.5) and Binomial(4, 0.1) are different but both from the binomial family.\nA family of distributions have the same equation for the pmf but differ by some unknown parameters \\(\\boldsymbol{\\theta}\\).\nWe must estimate these parameters."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-univariate-distributions",
    "href": "slides/01-welcome.html#continuous-univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nIf \\(X\\) is continuous we describe its distribution with the probability density function (pdf) \\(f(x) \\geq 0\\).\nSince there are uncountably many possible values, \\(P(X = x) = 0\\) for all \\(x\\).\nProbabilities are computed as areas under the pdf curve \\[P(a &lt; X &lt; b) = \\int_a^b f(x)dx.\\]\nTherefore, to be valid \\(f(x)\\) must satisfy \\(f(x) \\geq 0\\) and \\[P(‚àí\\infty &lt; X &lt; \\infty) = \\int_{-\\infty}^{\\infty} f(x)dx = 1.\\]"
  },
  {
    "objectID": "slides/01-welcome.html#continuous-univariate-distributions-1",
    "href": "slides/01-welcome.html#continuous-univariate-distributions-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nThe domain is the set of \\(x\\) values with \\(f(x) &gt; 0\\).\nThe mean and the variance are defined similarly to the discrete case but with the sums replaced by integrals.\nThe mean is \\(\\mathbb E[X] = \\int x f(x)dx\\).\nThe variance is \\(\\mathbb V(X) = \\int (x ‚àí \\mathbb E[X])^2 f(x)dx\\)."
  },
  {
    "objectID": "slides/01-welcome.html#joint-distributions",
    "href": "slides/01-welcome.html#joint-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Joint distributions",
    "text": "Joint distributions\n\n\\(\\mathbf{X} = (X_1, \\ldots, X_p)\\) is a random vector (vectors and matrices should be in bold).\nFor notational convenience, let‚Äôs consider only \\(p = 2\\) random variables \\(X\\) and \\(Y\\).\n\\((X, Y)\\) is discrete if it can take on a countable number of values, such as:\n\n\\(X\\) = number of hearts and \\(Y\\) = number of face cards.\n\n\\((X, Y)\\) is continuous if it can take on an uncountable number of values, such as:\n\n\\(X\\) = birthweight and \\(Y\\) = gestational age."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables",
    "href": "slides/01-welcome.html#discrete-random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe joint pmf: \\(f(x, y) = P(X = x, Y = y)\\)\n\n\\(\\sum_x \\sum_y f(x, y) = 1\\)\n\nThe marginal pmf for \\(X\\): \\(f_X(x) = P(X = x) = \\sum_y f(x, y)\\)\nThe marginal pmf for \\(Y\\): \\(f_Y(y) = P(Y = y) = \\sum_x f(x, y)\\)\nThe marginal distribution is the same as univariate distribution as if we ignored the other variable."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables-1",
    "href": "slides/01-welcome.html#discrete-random-variables-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe conditional pmf of \\(Y\\) given \\(X\\) is \\(f(y|x) = P(Y = y|X = x) = \\frac{P(X = x, Y = y)}{P(X = x)} = \\frac{f(x, y)}{f_X (x)}.\\)\n\\(X\\) and \\(Y\\) are independent if \\(f(x, y) = f_X(x)f_Y(y)\\) for all \\(x\\) and \\(y\\).\n\nVariables are dependent if they are not independent.\n\nEquivalently, \\(X\\) and \\(Y\\) are independent if \\(f(x|y) = f_X(x)\\) for all \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables-2",
    "href": "slides/01-welcome.html#discrete-random-variables-2",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nNotation: \\(X_1, \\dots, X_n \\overset{\\mathrm{iid}}{\\sim} f(x)\\) means that \\(X_1, \\ldots, X_n\\) are independent and identically distributed.\nThis implies the joint pmf is \\[P(X_1 = x_1, \\ldots, X_n = x_n) = \\prod_{i=1}^n f(x_i).\\]\nThe same notation and definitions of independence apply to continuous random variables.\nIn this class, assume independence unless otherwise noted."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-random-variables",
    "href": "slides/01-welcome.html#continuous-random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nManipulating joint pdfs is similar to joint pmfs but sums are replaced by integrals.\nThe joint pdf is denoted \\(f(x, y)\\).\nProbabilities are computed as volume under the pdf: \\[P((X, Y) ‚àà A) = \\int_A f(x, y)dxdy\\] where \\(A \\subset \\mathbb{R}^2\\)."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-random-variables-1",
    "href": "slides/01-welcome.html#continuous-random-variables-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nThe marginal pdf of \\(X\\) is \\(f_X(x) = \\int f(x, y)dy\\).\n\\(f_X\\) is the univariate pdf for \\(X\\) as if we never considered \\(Y\\).\nThe conditional pdf of \\(Y\\) given \\(X\\) is \\[f(y|x) = \\frac{f(x, y)}{f_X (x)}.\\]\nProper: \\(\\int f(y|x)dy = \\int \\frac{f(x,y)}{f_X(x)}dy = \\frac{\\int f(x,y)dy}{f_X(x)} = 1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#defining-joint-distributions-conditionally",
    "href": "slides/01-welcome.html#defining-joint-distributions-conditionally",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Defining joint distributions conditionally",
    "text": "Defining joint distributions conditionally\n\nSpecifying joint distributions is hard.\nEvery joint distribution can be written \\(f(x, y) = f(y|x)f(x)\\).\nTherefore, any joint distribution can be defined by,\n\n\\(X\\)‚Äôs marginal distribution\nThe conditional distribution of \\(Y|X\\)\n\nThe joint problem reduces to two univariate problems.\nThis idea forms the basis of hierarchical modeling."
  },
  {
    "objectID": "slides/01-welcome.html#bayes-rule",
    "href": "slides/01-welcome.html#bayes-rule",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Bayes rule",
    "text": "Bayes rule\n\n\n\n\n\nThomas Bayes, 1701-1761\n\n\nPierre-Simon Laplace, 1749-1827\n\n\\[f(\\boldsymbol{\\theta}|\\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{\\int f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}\\]"
  },
  {
    "objectID": "slides/01-welcome.html#prepare-for-next-week",
    "href": "slides/01-welcome.html#prepare-for-next-week",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Prepare for next week",
    "text": "Prepare for next week\n\nComplete HW 00 tasks\nReview syllabus\nComplete reading to prepare for Tuesday‚Äôs lecture\nTuesday‚Äôs lecture: Monte Carlo Sampling"
  },
  {
    "objectID": "slides/01-welcome.html#the-table-game",
    "href": "slides/01-welcome.html#the-table-game",
    "title": "Welcome to BIOSTAT 725!",
    "section": "The Table Game",
    "text": "The Table Game"
  },
  {
    "objectID": "slides/01-welcome.html",
    "href": "slides/01-welcome.html",
    "title": "Welcome to BIOSTAT 725!",
    "section": "",
    "text": "Education and career journey\n\nBS in Statistical Science from Duke University\nPhD in Biostatistics from University of North Carolina - Chapel Hill\nPostdoc in Duke Forge: Duke‚Äôs Center for Actionable Health Data Science\nNIH/NEI Pathway to Independence Fellow (K99/R00)\nAssistant Professor, Department of Biostatistics & Bioinformatics and Statistical Science at Duke; Faculty Affiliate of Duke AI Health\n\nWork focuses on developing data science tools to improve patient experience using biomedical data (including EHR)\nDad of 5 and 7 year old daughters üôÇ\n\n\n\n\n\n\n\nDr.¬†Youngsoo Baek (PhD)\n\nPhd in Statistical Science from Duke University\nPostdoc in Biostatistics & Bioinformatics\n\nBraden Scherting\n\nPhd candidate in Statistical Science\n\n\n\n\n\n\nClick on the link or scan the QR code to answer the Ed Discussion poll\nhttps://edstem.org/us/courses/68995/discussion/5942168\n\n\n\n\n\n\n\n\n\n\nIntroduction to the course\nSyllabus activity\nReview of probability"
  },
  {
    "objectID": "slides/01-welcome.html#translation",
    "href": "slides/01-welcome.html#translation",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Translation",
    "text": "Translation"
  },
  {
    "objectID": "hw/hw-00.html",
    "href": "hw/hw-00.html",
    "title": "HW 00",
    "section": "",
    "text": "Important\n\n\n\nThis first homework will not be graded, however it will be critical that you complete this on time. All of the computing tools we‚Äôll use in the class will be introduced during this assignemnt.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#rstudio",
    "href": "hw/hw-00.html#rstudio",
    "title": "HW 00",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\nIn this class, you have the option to use RStudio on your laptop (i.e., locally) or on through a container hosted by Duke OIT. My suggestion is for everyone to be comfortable using both options, for the following reasons:\n\nFlexibility: If you‚Äôre laptop has problems right before a due date, it will be helpful to be setup in the container.\nIndependence: It is important to be able to compute on your laptop, because when you graduate you will no longer have access to the Duke containers.\n\nThe container is offered as a convenience and you should take advantage of it when needed. We will now give instructions for using both.\n\nInstalling RStudio on your laptop\n\nMost of you probably already have RStudio installed on your laptop. In case you do not, please follow these instructions to install both R and RStudio, Installation instruction.\nWhen given the option, choose the most recent stable version of both.\n\n\n\nReserve RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers. You will log in using your NetID credentials.\nClick ‚ÄúReserve STA602‚Äù to reserve an RStudio container. Be sure you reserve the container labeled STA602 to ensure you have the computing set up you need for the class. This semester, we are sharing a container with STA 602!\n\nYou only need to reserve a container once per semester.\n\n\nOpen RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA602 to log into the Docker container. You should now see the RStudio environment.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#stan",
    "href": "hw/hw-00.html#stan",
    "title": "HW 00",
    "section": "Stan",
    "text": "Stan\nIn this course, we will use the package rstan as our primary tool for conducting Bayesian inference. The container already has rstan installed, so these steps need to be used for installation on your laptop.\n\nFollow the installation guide here: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n\nMake sure to follow these instructions closely, since prior to installing rstan, you need to configure your R installation to be able to compile C++ code.\n\nStan Hello World!\nOnce rstan has been installed, test to make sure we can load the R package.\n\nlibrary(rstan)\n\nNow we will simulate some data that we can fit with linear regression. We will also define a Stan data object (no need to understand this now, we will go into this in detail in future lectures).\n\n###Set a seed for reproducibility\nset.seed(54)\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- 3 # number of covariates\n\n###True parameter values\nbeta &lt;- matrix(c(rnorm(p + 1)), ncol = 1)\nsigma &lt;- 1.5\n\n###Simulate covariates and outcome\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n###Create a Stan data object\nstan_data &lt;- list(\n  n = n,\n  p = p,\n  Y = Y,  \n  X = X\n)\n\nNow, we can define a Stan model (you do not need to understand this yet, we are just testing!). In RStudio, create a new .stan file called test.stan and then copy and paste the following Stan code. To create a .stan file from RStudio, File -&gt; New File -&gt; Stan File.\n\n// Saved in test.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p + 1] X;\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  Y ~ normal(X * beta, sigma);\n}\n\nNext, we test to see if the model can compile. Note compilation can sometimes take a bit of time.\n\nstan_model &lt;- stan_model(file = \"test.stan\")\n\nOK, great. We will now obtain posterior samples, using default specifications for inference.\n\nfit &lt;- sampling(stan_model, data = stan_data, refresh = 0)\n\nFinally, print some summary estimates for the model parameters.\n\nprint(fit)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n           mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\nbeta[1]    1.89    0.00 0.18    1.54    1.77    1.89    2.01    2.23  4666    1\nbeta[2]    0.20    0.00 0.18   -0.15    0.08    0.20    0.32    0.55  4391    1\nbeta[3]   -0.30    0.00 0.16   -0.62   -0.41   -0.31   -0.19    0.01  5189    1\nbeta[4]    1.58    0.00 0.19    1.22    1.46    1.58    1.71    1.95  4983    1\nsigma      1.71    0.00 0.13    1.49    1.63    1.71    1.79    1.98  4479    1\nlp__    -102.49    0.04 1.61 -106.30 -103.33 -102.13 -101.31 -100.34  1907    1\n\nSamples were drawn using NUTS(diag_e) at Mon Jan  6 15:15:47 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#git-and-github",
    "href": "hw/hw-00.html#git-and-github",
    "title": "HW 00",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nIn addition to R and RStudio, we will use git and GitHub for version control and collaboration.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like ‚ÄúTrack Changes‚Äù features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better). Git is important because:\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)\n\n\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, exams, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username.\n\n\n\nIf you already have a GitHub account, you can move on to the next step.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#connect-rstudio-and-github",
    "href": "hw/hw-00.html#connect-rstudio-and-github",
    "title": "HW 00",
    "section": "Connect RStudio and GitHub",
    "text": "Connect RStudio and GitHub\nNow that you have RStudio and a GitHub account, we will configure git so that RStudio and GitHub communicate with one another.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system. So, if you are using both your laptop and the container, you will need to do this process twice.\n\n\n\nStep 0: Open your RStudio (either the STA602 RStudio container or your laptop).\nStep 1: Type credentials::ssh_setup_github() into the console on the bottom left of the RStudio environment.\nStep 2: R will ask ‚ÄúNo SSH key found. Generate one now?‚Äù Click 1 for yes.\nStep 3: You will generate a key. It will begin with ‚Äússh-rsa‚Ä¶.‚Äù R will then ask ‚ÄúWould you like to open a browser now?‚Äù Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used (e.g., biostat725).\n\n\n\nConfigure git\nThe last thing we need to do is configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"berchuck\",\n  user.email = \"sib2@duke.edu\")\n\nIt may look like nothing happened but you are now ready interact between GitHub and RStudio! We will begin working with RStudio and GitHub in lecture this week.\n\n\n\n\n\n\nNote\n\n\n\nYou should be using the email address you used to create your GitHub account, it‚Äôs ok if it isn‚Äôt your Duke email.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "slides/01-welcome.html#office-hours-poll",
    "href": "slides/01-welcome.html#office-hours-poll",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Office Hours Poll!",
    "text": "Office Hours Poll!\n\nClick on the link or scan the QR code to answer the poll\nhttps://forms.office.com/r/QTuhmC0wM3"
  },
  {
    "objectID": "slides/02-monte-carlo.html#simulating-pi-using-monte-carlo",
    "href": "slides/02-monte-carlo.html#simulating-pi-using-monte-carlo",
    "title": "Monte Carlo Sampling",
    "section": "Simulating \\(\\pi\\) using Monte Carlo",
    "text": "Simulating \\(\\pi\\) using Monte Carlo\nSuppose we are interested in estimating \\(\\pi\\).\n\n\n\nWe can formulate \\(\\pi\\) as a function of the area of a square and circle.\nArea of a circle: \\(A_c = \\pi r^2\\)\nArea of a square: \\(A_s = 4 r^2\\)\nThe ratio of the two areas is: \\(\\frac{A_c}{A_s} = \\frac{\\pi r^2}{4 r^2} \\implies \\pi = \\frac{4 A_c}{A_s}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we have an estimate for the ratio we can solve for \\(\\pi\\). The challenge becomes estimating this ratio."
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-sampling-1",
    "href": "slides/02-monte-carlo.html#monte-carlo-sampling-1",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo sampling",
    "text": "Monte Carlo sampling\n\nWe can take advantage of how quickly a computer can generate pseudo-random numbers.\nThere is a class of algorithms called Monte Carlo sampling that exploit randomness to estimate real world scenarios that would otherwise be difficult to explicitly calculate.\nThe name comes from the Monte Carlo Casino in Monaco, where the primary developer of the method, mathematician Stanislaw Ulam, was inspired by his uncle‚Äôs gambling habits. (This is who Stan was named after!)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-estimation-of-the-ratio",
    "href": "slides/02-monte-carlo.html#monte-carlo-estimation-of-the-ratio",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo estimation of the ratio",
    "text": "Monte Carlo estimation of the ratio\n\nWe can use a Monte Carlo simulation to estimate the area ratio of the circle to the square.\nImagine you randomly drop grains of sand into the area of the square. By counting the total number of sand grains in the square (all of them since you‚Äôre an accurate dropper) to the number of sand grains inside the circle we get this estimate.\nMultiply the estimated ratio by 4 and you get an estimate for \\(\\pi\\).\nThe more grains of sand that are used the more accurate your estimate of \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#algorithm-for-estimating-pi",
    "href": "slides/02-monte-carlo.html#algorithm-for-estimating-pi",
    "title": "Monte Carlo Sampling",
    "section": "Algorithm for estimating \\(\\pi\\)",
    "text": "Algorithm for estimating \\(\\pi\\)\n\nGenerate a random point \\((x, y)\\) inside a square centered at the origin with length 2.\n\n\nThis is equivalent to assuming:\n\n\\(f_{X,Y}(x,y) = f_X(x)f_Y(y)\\)\n\\(f_X(x) = Uniform(-1, 1)\\), \\(f_Y(y) = Uniform(-1, 1)\\)\n\n\n\n\nDetermine whether the point falls inside the unit circle inscribed in the square by checking whether \\(x^2 + y^2 \\leq 1\\).\n\n\n\n\nRepeat steps 1 and 2 for a large number of points (\\(S\\))."
  },
  {
    "objectID": "slides/02-monte-carlo.html#algorithm-for-estimating-pi-1",
    "href": "slides/02-monte-carlo.html#algorithm-for-estimating-pi-1",
    "title": "Monte Carlo Sampling",
    "section": "Algorithm for estimating \\(\\pi\\)",
    "text": "Algorithm for estimating \\(\\pi\\)\n\nGenerate a random point \\((x, y)\\) inside a square centered at the origin with length 2.\nDetermine whether the point falls inside the unit circle inscribed in the square by checking whether \\(x^2 + y^2 \\leq 1\\).\nRepeat steps 1 and 2 for a large number of points (\\(S\\)).\nCalculate the ratio of the number of points that fell inside the circle to the total number of points generated.\n\n\n\nMultiply the ratio by 4 to estimate the value of \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-big-of-a-difference-does-s-make",
    "href": "slides/02-monte-carlo.html#how-big-of-a-difference-does-s-make",
    "title": "Monte Carlo Sampling",
    "section": "How big of a difference does \\(S\\) make",
    "text": "How big of a difference does \\(S\\) make"
  },
  {
    "objectID": "slides/02-monte-carlo.html#estimating-pi-with-increasing-s",
    "href": "slides/02-monte-carlo.html#estimating-pi-with-increasing-s",
    "title": "Monte Carlo Sampling",
    "section": "Estimating \\(\\pi\\) with increasing \\(S\\)",
    "text": "Estimating \\(\\pi\\) with increasing \\(S\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#error-in-estimating-pi",
    "href": "slides/02-monte-carlo.html#error-in-estimating-pi",
    "title": "Monte Carlo Sampling",
    "section": "Error in estimating \\(\\pi\\)",
    "text": "Error in estimating \\(\\pi\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling",
    "href": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling",
    "title": "Monte Carlo Sampling",
    "section": "Intuition behing Monte Carlo sampling",
    "text": "Intuition behing Monte Carlo sampling\n\nAssume \\(X \\sim Uniform(-1,1)\\) and \\(Y \\sim Uniform(-1,1)\\).\nWe can write our problem as, \\(\\pi = 4P(X^2 + Y^2 \\leq 1)\\).\nHow could we do this without Monte Carlo?\nDefine, \\(Z = X^2 + Y^2\\). We could then use change-of-variables to compute the density of \\(Z\\) and then compute \\(P(Z \\leq 1)\\).\n\nThis is generally difficult!"
  },
  {
    "objectID": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling-1",
    "href": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling-1",
    "title": "Monte Carlo Sampling",
    "section": "Intuition behing Monte Carlo sampling",
    "text": "Intuition behing Monte Carlo sampling\n\nInstead, we could write our problem as an expectation and use the law of large numbers,\n\n\\[P(X^2 + Y^2 \\leq 1) = \\mathbb{E}_{X,Y}\\left[1\\left(X^2 + Y^2 \\leq 1\\right)\\right].\\]\n\nRecall: \\(\\mathbb{E}_X[1(A)] = \\int_A f_X(x)dx = P(X \\in A)\\).\nWe then have that, \\[\\frac{1}{S}\\sum_{i = 1}^S 1\\left(X_i^2 + Y_i^2 \\leq 1\\right) \\rightarrow \\mathbb{E}_{X,Y}\\left[1\\left(X^2 + Y^2 \\leq 1\\right)\\right],\\]\n\nwhere \\((X_i,Y_i) \\sim f(X,Y)\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#a-motivating-example",
    "href": "slides/02-monte-carlo.html#a-motivating-example",
    "title": "Monte Carlo Sampling",
    "section": "A motivating example",
    "text": "A motivating example\n\nSuppose we are interested in estimating the prevalence of diabetes in Durham County. We aim to estimate this prevalence by taking a sample of \\(n\\) individuals in Durham County and we record whether or not they have diabetes, \\(Y_i\\).\nWe assume that \\(Z = \\sum_{i=1}^n Y_i \\sim Binomial(n, \\pi)\\) for \\(i = 1,\\ldots,n\\).\nOur goal is to estimate \\(\\pi\\) and perform statistical inference (e.g., point estimation, interval estimation, etc.)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#posterior-inference",
    "href": "slides/02-monte-carlo.html#posterior-inference",
    "title": "Monte Carlo Sampling",
    "section": "Posterior inference",
    "text": "Posterior inference\n\nIn Bayesian statistics, inference is encoded through the posterior distribution,\n\n\\[\\begin{aligned}\nf(\\pi | Z) &= \\frac{f(Z | \\pi)f(\\pi)}{f(Z)},\\\\\n&= \\frac{f(Z | \\pi)f(\\pi)}{\\int f(Z | \\pi)f(\\pi)d\\pi}.\n\\end{aligned}\\]\n\nAll we have to do is specify the likelihood and prior."
  },
  {
    "objectID": "slides/02-monte-carlo.html#likelihood-specification",
    "href": "slides/02-monte-carlo.html#likelihood-specification",
    "title": "Monte Carlo Sampling",
    "section": "Likelihood specification",
    "text": "Likelihood specification\n\\(Z\\) is a Binomial distribution with pmf,\n\\[f(Z | \\pi) = P(Z = z) = {n \\choose z} \\pi^z(1-\\pi)^{n-z},\\]\nwhere \\(z \\in \\{0, 1, \\ldots, n\\}\\).\n\n\\({n \\choose z} = \\frac{n!}{z!(n-z)!} = \\frac{\\Gamma(n+1)}{\\Gamma(z+1)\\Gamma(n-z+1)}\\)\n\\(\\Gamma(x) = (x-1)!\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#prior-specification",
    "href": "slides/02-monte-carlo.html#prior-specification",
    "title": "Monte Carlo Sampling",
    "section": "Prior specification",
    "text": "Prior specification\nWhat do we know about \\(\\pi\\)?\n\n\\(\\pi\\) is continuous.\n\\(\\pi \\in (0,1)\\).\n\nWe should place a distribution on \\(\\pi\\) that permits these properties.\n\nOne option is the Beta distribution, \\(\\pi \\sim Beta(\\alpha,\\beta)\\), \\[f(\\pi) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}.\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-marginal-likelihood",
    "href": "slides/02-monte-carlo.html#computing-the-marginal-likelihood",
    "title": "Monte Carlo Sampling",
    "section": "Computing the marginal likelihood",
    "text": "Computing the marginal likelihood\nUnder our prior specification, we can compute the marginal likelihood, \\(f(Z)\\):\n\\[\\begin{aligned}\nf(Z) &= \\int f(Z | \\pi) f(\\pi) d\\pi\\\\\n&= \\int {n \\choose z} \\pi^z(1-\\pi)^{n-z} \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} d\\pi\\\\\n&= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\int \\underbrace{\\pi^{(\\alpha + z) - 1}(1-\\pi)^{(\\beta + n - z) - 1}}_{Beta\\text{ }kernel} d\\pi\n\\end{aligned}\\]\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample: The kernel of the Beta pdf is \\(\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-marginal-likelihood-1",
    "href": "slides/02-monte-carlo.html#computing-the-marginal-likelihood-1",
    "title": "Monte Carlo Sampling",
    "section": "Computing the marginal likelihood",
    "text": "Computing the marginal likelihood\nUnder our prior specification, we can compute the marginal likelihood, \\(f(Z)\\):\n\\[\\begin{aligned}\nf(Z) &= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\int \\pi^{(\\alpha + z) - 1}(1-\\pi)^{(\\beta + n - z) - 1} d\\pi\\\\\n&= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta + n)}{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#we-can-then-compute-the-posterior",
    "href": "slides/02-monte-carlo.html#we-can-then-compute-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "We can then compute the posterior",
    "text": "We can then compute the posterior\n\\[\\begin{aligned}\nf(\\pi | Z) &= \\frac{f(Z | \\pi) f(\\pi)}{f(Z)}\\\\\n&= \\frac{{n \\choose z} \\pi^z(1-\\pi)^{n-z} \\times \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}}{{n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta) }{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}}\\\\\n&=\\frac{\\Gamma(\\alpha+z)\\Gamma(\\beta+n-z)}{\\Gamma(\\alpha + \\beta + n)}\\pi^{(\\alpha + z) - 1} (1 - \\pi)^{(\\beta + n - z) - 1}\\\\\n&=Beta\\left(\\alpha + z, \\beta + n - z\\right).\n\\end{aligned}\\]\n\nA prior that is considered conjugate yields a posterior with the same distribution.\nThe Beta distribution is conjugate for the Bernoulli/Binomial distributions."
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-posterior",
    "href": "slides/02-monte-carlo.html#computing-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Computing the posterior",
    "text": "Computing the posterior\n\nIn general, computing the marginal likelihood, \\(f(Z)\\), is extremely difficulty.\nAn easier approach is to use the kernel trick.\n\n\\[\\begin{aligned}\nf(\\pi | Z) &\\propto f(Z | \\pi) f(\\pi)\\\\\n&\\propto \\pi^z (1-\\pi)^{n - z} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\\\\n&= \\pi^{\\left(\\alpha + z\\right) - 1} (1-\\pi)^{\\left(\\beta + n - z\\right) - 1}\\\\\n&= Beta\\left(\\alpha + z, \\beta + n - z\\right).\n\\end{aligned}\\]\n\nThis only works when a conjugate prior is used."
  },
  {
    "objectID": "slides/02-monte-carlo.html#lets-inspect-the-posterior",
    "href": "slides/02-monte-carlo.html#lets-inspect-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Let‚Äôs inspect the posterior",
    "text": "Let‚Äôs inspect the posterior\n\nSuppose we conducted a simple random sample of 500 individuals in Durham County and 120 responsed that they had diabetes and 380 that they did not.\nThe posterior becomes, \\(Beta\\left(\\alpha + 120, \\beta + 380\\right)\\).\nWe must choose our prior distribution wisely.\nNote that:\n\n\\(\\mathbb{E}[\\pi] = \\alpha/(\\alpha + \\beta)\\)\n\\(\\mathbb{V}(\\pi) = (\\alpha\\beta)/[(\\alpha + \\beta)^2(\\alpha + \\beta + 1)]\\).\n\nTypically, \\(\\alpha = \\beta = 1\\), which corresponds to a uniform prior on \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#lets-inspect-the-posterior-1",
    "href": "slides/02-monte-carlo.html#lets-inspect-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Let‚Äôs inspect the posterior",
    "text": "Let‚Äôs inspect the posterior"
  },
  {
    "objectID": "slides/02-monte-carlo.html#suppose-my-prior-changes",
    "href": "slides/02-monte-carlo.html#suppose-my-prior-changes",
    "title": "Monte Carlo Sampling",
    "section": "Suppose my prior changes",
    "text": "Suppose my prior changes"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarizing-the-posterior",
    "href": "slides/02-monte-carlo.html#summarizing-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nWe now have a posterior distribution. What do we do now?\n\n\nPosterior means, medians, modes, and variances\n\n\n\n\nJoint, conditional, and marginal probabilities, for example: \\(P(\\pi &lt; c | Z)\\)\n\n\n\n\n\\(\\alpha\\)-quantiles: \\(\\{q_{\\alpha} : P(\\pi &lt; q_{\\alpha} | Z) = \\alpha\\}, \\alpha \\in (0,1)\\)\n\n\n\n\n\\(\\ldots\\)\n\n\n\nHow do we go about computing these summaries?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarization-can-be-complex",
    "href": "slides/02-monte-carlo.html#summarization-can-be-complex",
    "title": "Monte Carlo Sampling",
    "section": "Summarization can be complex",
    "text": "Summarization can be complex\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\pi &lt; c | Z)\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pbeta}\\))\n\n\nThese methods work well for standard posterior quantities and distributions."
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarizing-the-posterior-1",
    "href": "slides/02-monte-carlo.html#summarizing-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution.\n\n\\(P(\\pi \\in A| Z)\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\pi\\), \\(g\\left(\\pi\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\pi_1 - \\pi_2|\\), \\(\\pi_1/\\pi_2\\), \\(\\max\\left\\{\\pi_1,\\ldots,\\pi_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible.\n\nWhat are our options?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-mc-sampling",
    "href": "slides/02-monte-carlo.html#monte-carlo-mc-sampling",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo (MC) sampling",
    "text": "Monte Carlo (MC) sampling\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\pi|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\pi|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#mc-approximation",
    "href": "slides/02-monte-carlo.html#mc-approximation",
    "title": "Monte Carlo Sampling",
    "section": "MC approximation",
    "text": "MC approximation\nLet \\(g\\left(\\pi\\right)\\) be (just about) any function of \\(\\pi\\). The law of large numbers says that if,\n\\[\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\pi|\\mathbf{Y}\\right),\\] then, \\[\\frac{1}{S}\\sum_{s=1}^S g\\left(\\pi^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\pi\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\pi\\right)f\\left(\\pi|\\mathbf{Y}\\right)d\\pi,\\] as \\(S\\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#mc-approximation-1",
    "href": "slides/02-monte-carlo.html#mc-approximation-1",
    "title": "Monte Carlo Sampling",
    "section": "MC approximation",
    "text": "MC approximation\n\nImplications (as \\(S\\rightarrow \\infty\\)):\n\n\\(\\overline{\\pi}=\\frac{1}{S}\\sum_{s=1}^S \\pi^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\pi|\\mathbf{Y}\\right]\\)\n\\(\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\pi^{\\left(s\\right)}-\\overline{\\pi}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\pi|\\mathbf{Y}\\right)\\)\n\\(\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\pi^{\\left(s\\right)}\\leq {c}\\right) \\rightarrow P\\left(\\pi\\leq {c} | \\mathbf{Y}\\right)\\)\n\\(\\alpha\\)-quantile of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}\\)\n\nJust about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample"
  },
  {
    "objectID": "slides/02-monte-carlo.html#posterior-inference-for-arbitrary-functions",
    "href": "slides/02-monte-carlo.html#posterior-inference-for-arbitrary-functions",
    "title": "Monte Carlo Sampling",
    "section": "Posterior inference for arbitrary functions",
    "text": "Posterior inference for arbitrary functions\nInterest in the posterior distribution of a function of \\(\\pi\\), \\(g\\left(\\pi\\right)\\)\n\nMC sampling plan:\n\nDraw \\(\\pi^{\\left(1\\right)}\\) from \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\pi^{\\left(1\\right)}\\right)\\)\n\\(\\ldots\\)\nDraw \\(\\pi^{\\left(S\\right)}\\) from \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\pi^{\\left(S\\right)}\\right)\\)\n\n\\(\\Rightarrow g\\left(\\pi^{\\left(1\\right)}\\right),\\ldots,g\\left(\\pi^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\pi\\right)|\\mathbf{Y}\\right)\\)\nTherefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-many-samples-to-take",
    "href": "slides/02-monte-carlo.html#how-many-samples-to-take",
    "title": "Monte Carlo Sampling",
    "section": "How many samples to take?",
    "text": "How many samples to take?\nWe can use a central limit theorem: \\(\\sqrt{S}\\left(\\overline{\\pi}-\\mathbb{E}[\\pi | \\mathbf{Y}]\\right)/\\sigma \\stackrel{d}{\\rightarrow} \\text{N}\\left(0,1\\right)\\),\n\n\\(\\overline{\\pi}=\\frac{1}{S}\\sum_{s=1}^S \\pi^{\\left(s\\right)}\\)\n\\(\\sigma^2 = \\mathbb{V}\\left(\\pi | \\mathbf{Y}\\right), \\quad \\mathbb{V}\\left(\\overline{\\pi}\\right) = \\frac{1}{S^2}\\sum_{s=1}^S\\mathbb{V}\\left(\\pi^{(s)}\\right) = \\frac{1}{S}\\mathbb{V}\\left(\\pi | \\mathbf{Y}\\right) = \\sigma^2/S\\).\n\n\\(\\implies \\overline{\\pi}\\approx N\\left(\\mathbb{E}[\\pi | \\mathbf{Y}],\\sigma^2/S\\right)\\)\nMC standard error: \\(\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\), \\(\\widehat{\\sigma}^2=\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\pi^{\\left(s\\right)}-\\overline{\\pi}\\right)^2\\)\nApproximate 95% MC confidence interval for the posterior mean: \\(\\overline{\\pi} \\pm 2\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\)\n\nChoose \\(S\\) large enough to report the posterior mean with your desired precision\nReporting MC standard errors for the posterior mean is a good way to indicate that \\(S\\) is large enough"
  },
  {
    "objectID": "slides/02-monte-carlo.html#returning-to-our-posterior",
    "href": "slides/02-monte-carlo.html#returning-to-our-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Returning to our posterior",
    "text": "Returning to our posterior\nLet‚Äôs obtain \\(S = 1,000\\) samples from our posterior.\n\npi_samples &lt;- rbeta(1000, 1 + 120, 1 + 380)\n\nWe can compute the posterior mean and variance.\n\nmean(pi_samples)\n\n[1] 0.2399671\n\nvar(pi_samples)\n\n[1] 0.000354165"
  },
  {
    "objectID": "slides/02-monte-carlo.html#assessing-accuracy",
    "href": "slides/02-monte-carlo.html#assessing-accuracy",
    "title": "Monte Carlo Sampling",
    "section": "Assessing accuracy",
    "text": "Assessing accuracy"
  },
  {
    "objectID": "slides/02-monte-carlo.html#additional-posterior-summaries",
    "href": "slides/02-monte-carlo.html#additional-posterior-summaries",
    "title": "Monte Carlo Sampling",
    "section": "Additional posterior summaries",
    "text": "Additional posterior summaries\n\n# median\nmedian(pi_samples)\n\n[1] 0.2405268\n\n# 95% credible intervals\nquantile(pi_samples, probs = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.2042175 0.2794289 \n\n# evaluating probability\nmean(pi_samples &lt; 0.25)\n\n[1] 0.683\n\n# summarizing arbitrary functions of the parameters\npi_new &lt;- pi_samples^3 - pi_samples\nc(mean(pi_new), quantile(pi_new, probs = c(0.025, 0.975)))\n\n                 2.5%      97.5% \n-0.2268650 -0.2576109 -0.1957007"
  },
  {
    "objectID": "slides/02-monte-carlo.html#poisson-random-variable",
    "href": "slides/02-monte-carlo.html#poisson-random-variable",
    "title": "Monte Carlo Sampling",
    "section": "Poisson random variable",
    "text": "Poisson random variable\nSuppose that \\(Y_i \\stackrel{iid}{\\sim} Poisson(\\lambda)\\) for \\(i = 1,\\ldots,n\\). Assume the following conjugate prior, \\(f(\\lambda) \\sim Gamma(a, b)\\), so that \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). We would like to perform inference for the \\(\\lambda\\). We take \\(S=10,000\\) samples.\n\nset.seed(54) # set seed for replicability\nS &lt;- 10000 # number of Monte Carlo samples\nlambda &lt;- 4 # true value of lambda\nn &lt;- 100 # sample size\nY &lt;- rpois(n, lambda)\na &lt;- 3 # hyperprior\nb &lt;- 1 # hyperprior\nsamples &lt;- rgamma(S, a + sum(Y), b + n) # sample from posterior\n\n\n\n\n\n\nmin\nq1\nmedian\nq3\nmax\nmean\nsd\n\n\n\n\n3.269\n3.904\n4.039\n4.173\n4.773\n4.041\n0.201"
  },
  {
    "objectID": "slides/02-monte-carlo.html#sampling-for-any-distribution",
    "href": "slides/02-monte-carlo.html#sampling-for-any-distribution",
    "title": "Monte Carlo Sampling",
    "section": "Sampling for any distribution",
    "text": "Sampling for any distribution\nMonte Carlo sampling does not have to be used solely for posterior inference. Suppose we are interested in computed summaries for \\(X_i \\stackrel{iid}{\\sim} N(3, 4)\\) for \\(i = 1,\\ldots,n\\). We take \\(S = 10,000\\).\n\nS &lt;- 10000\nsamples &lt;- rnorm(S, 3, 2)\nlibrary(moments)\nkurtosis(samples)\n\n[1] 3.041546\n\nskewness(samples)\n\n[1] -0.0109433\n\nmean(samples &gt; 4) # P(X_i &gt; 4)\n\n[1] 0.3087"
  },
  {
    "objectID": "slides/02-monte-carlo.html#combination-of-random-variables",
    "href": "slides/02-monte-carlo.html#combination-of-random-variables",
    "title": "Monte Carlo Sampling",
    "section": "Combination of random variables",
    "text": "Combination of random variables\nSuppose \\(X_i \\stackrel{iid}{\\sim} N(3, 4)\\) and \\(Y_i \\stackrel{iid}{\\sim} \\chi^2(df=3)\\) for \\(i = 1,\\ldots,n\\). \\(X_i\\) and \\(Y_i\\) are independent. We are interested in summaries of \\(Z_i = X_i / Y_i\\). We take \\(S = 10,000\\).\n\nS &lt;- 10000\nx &lt;- rnorm(S, 3, 2)\ny &lt;- rchisq(S, 3)\nz &lt;- x / y\nmean(z)\n\n[1] 2.961057\n\nmedian(z)\n\n[1] 1.146081\n\nmean(z &gt; 1)\n\n[1] 0.5456"
  },
  {
    "objectID": "slides/02-monte-carlo.html#prepare-for-next-class",
    "href": "slides/02-monte-carlo.html#prepare-for-next-class",
    "title": "Monte Carlo Sampling",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nComplete HW 00 which is due before Thursday‚Äôs class\nComplete reading to prepare for Thursday‚Äôs lecture\nThursday‚Äôs lecture: Markov chain Monte Carlo"
  },
  {
    "objectID": "slides/03-mcmc.html#review-of-last-lecture",
    "href": "slides/03-mcmc.html#review-of-last-lecture",
    "title": "Markov chain Monte Carlo",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we performed posterior inference for a Beta-Binomial model using Monte Carlo estimation.\nToday we will discuss Bayesian estimation of linear regression:\n\n\\[\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n).\\]\n\nThere is no closed form posterior, so we cannot directly use Monte Carlo sampling.\nWe need Markov chain Monte Carlo (MCMC)!\nOur goal is to gain intuition behind MCMC, the workhorse behind Stan."
  },
  {
    "objectID": "slides/03-mcmc.html#defining-the-model",
    "href": "slides/03-mcmc.html#defining-the-model",
    "title": "Markov chain Monte Carlo",
    "section": "Defining the model",
    "text": "Defining the model\nSuppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#defining-the-likelihood",
    "href": "slides/03-mcmc.html#defining-the-likelihood",
    "title": "Markov chain Monte Carlo",
    "section": "Defining the likelihood",
    "text": "Defining the likelihood\nThe individual likelihood contribution for subject \\(i\\) is given by,\n\\[Y_i|\\boldsymbol{\\beta},\\sigma^2 \\stackrel{ind}{\\sim} N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta},\\sigma^2) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta},\\sigma^2),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#matrix-likelihood-specification",
    "href": "slides/03-mcmc.html#matrix-likelihood-specification",
    "title": "Markov chain Monte Carlo",
    "section": "Matrix likelihood specification",
    "text": "Matrix likelihood specification\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2 ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-estimation",
    "href": "slides/03-mcmc.html#linear-regression-estimation",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression estimation",
    "text": "Linear regression estimation\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} \\log f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-assumptions",
    "href": "slides/03-mcmc.html#linear-regression-assumptions",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression assumptions",
    "text": "Linear regression assumptions\n\nLinearity between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\): \\(\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\)\nIndependence of errors: \\(\\epsilon_i\\)‚Äôs are independent Gaussian\nHomoskedasticity: constant \\(\\sigma^2\\) across \\(\\mathbf{x}_i\\)\nNormality of errors: \\(\\epsilon_i \\sim N(0,\\sigma^2)\\)\nNo multicollinearity: \\(\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1}\\) exists"
  },
  {
    "objectID": "slides/03-mcmc.html#posterior-for-linear-regression",
    "href": "slides/03-mcmc.html#posterior-for-linear-regression",
    "title": "Markov chain Monte Carlo",
    "section": "Posterior for linear regression",
    "text": "Posterior for linear regression\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)d\\boldsymbol{\\beta}d\\sigma^2}.\n\\end{aligned}\\]\nNo closed form exists for the posterior. \\[f(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#motivation-for-gibbs-sampling",
    "href": "slides/03-mcmc.html#motivation-for-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Motivation for Gibbs sampling",
    "text": "Motivation for Gibbs sampling\n\nSuppose we were given \\(\\sigma^{2(1)}\\), a single sample from the marginal posterior distribution \\(f\\left(\\sigma^2|\\mathbf{Y}\\right)\\) (from where, who knows?)\nUse the sample to generate \\(\\boldsymbol{\\beta}^{(1)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y},\\sigma^{2(1)}\\right)\\)\n\\(\\left(\\boldsymbol{\\beta}^{(1)},\\sigma^{2(1)}\\right)\\) is a sample from \\(f\\left(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}\\right)\\)\n\\(\\boldsymbol{\\beta}^{(1)}\\) is a sample from \\(f\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)\\)\n\n\n\n\n\n\n\n\nRecall\n\n\n\\(f\\left(\\boldsymbol{\\beta}, \\sigma^{2}|\\mathbf{Y}\\right) = f\\left(\\boldsymbol{\\beta} | \\sigma^{2},\\mathbf{Y}\\right)f\\left(\\sigma^{2}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\nSuppose we can sample from the following two distributions,\n\n\\(f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta})\\)\n\\(f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\sigma^2)\\)\n\nThese are called full conditional distributions.\nSet initial values for \\(\\boldsymbol{\\theta}^{(0)} = (\\boldsymbol{\\beta}^{(0)}, \\sigma^{2(0)})\\). Then, given a current state of parameters \\(\\boldsymbol{\\theta}^{(s)}\\), we can generate a new state as follows:\n\nSample \\(\\boldsymbol{\\beta}^{(s + 1)} \\sim f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^{2(s)})\\)\nSample \\(\\sigma^{2(s + 1)} \\sim f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}^{(s + 1)})\\)\nLet \\(\\boldsymbol{\\theta}^{(s+1)} = (\\boldsymbol{\\beta}^{(s + 1)}, \\sigma^{2(s + 1)})\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#why-does-this-work",
    "href": "slides/03-mcmc.html#why-does-this-work",
    "title": "Markov chain Monte Carlo",
    "section": "Why does this work?",
    "text": "Why does this work?\n\n\\(\\boldsymbol{\\theta}^{(0)}\\) isn‚Äôt a sample from the posterior, it is an arbitrarily chosen initial value\n\\(\\boldsymbol{\\theta}^{(1)}\\) likely isn‚Äôt from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\)\n\\(\\boldsymbol{\\theta}^{(2)}\\) likely isn‚Äôt from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\) and \\(\\boldsymbol{\\theta}^{(1)}\\)\nTheorem: For any initial values, the chain will eventually converge to the posterior\nTheorem: If \\(\\boldsymbol{\\theta}^{(s)}\\) is a sample from the posterior, then \\(\\boldsymbol{\\theta}^{(s+1)}\\) is too"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler",
    "href": "slides/03-mcmc.html#gibbs-sampler",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nUnder mild regulatory conditions that are generally satisfied for most statistical models, one can show that the iteration \\(\\boldsymbol{\\theta}^{(s)}\\) converges in distribution to a draw from the true joint posterior distribution\nSo for \\(s\\) sufficiently large (say, bigger than \\(s_0\\)), \\(\\left\\{\\boldsymbol{\\theta}^{(s)}, s=s_0+1,\\ldots,S\\right\\}\\) is a correlated sample from the true joint posterior (and \\(\\boldsymbol{\\beta}^{(s)}\\) and \\(\\sigma^{2(s)}\\) are samples from the marginals)\nSimilar to Monte Carlo approximation, we can use these samples to estimate posterior quantities of interest"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-1",
    "href": "slides/03-mcmc.html#gibbs-sampler-1",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\n\\(\\boldsymbol{\\theta}^{(t)}\\) depends on \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-1)}\\) only through \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\boldsymbol{\\theta}^{(t)}\\) is conditionally independent of \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-2)}\\) given \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\implies\\) Markov property, so the sequence is called a Markov chain\nWe use the samples similar to MC approximation; therefore, Gibbs sampling is a form of Markov chain Monte Carlo (MCMC)\nWe will cover diagnostics for MCMC in another lecture!"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-1",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-1",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nWe need to compute the full conditionals. Before doing this, we require prior distributions.\nLet‚Äôs assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gaussian,\n\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta",
    "href": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\boldsymbol{\\beta}\\)",
    "text": "Full conditional for \\(\\boldsymbol{\\beta}\\)\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta},\\sigma^2 | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{aligned}\\]\nThis is the kernel of a multivariate normal for \\(\\boldsymbol{\\beta}\\), with \\(\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}\\) and \\(\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\). It‚Äôs easy to see then that, \\(f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\\)\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta-1",
    "href": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta-1",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\boldsymbol{\\beta}\\)",
    "text": "Full conditional for \\(\\boldsymbol{\\beta}\\)\nThe full conditional can be found in closed-form and is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-sigma2",
    "href": "slides/03-mcmc.html#full-conditional-for-sigma2",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\sigma^2\\)",
    "text": "Full conditional for \\(\\sigma^2\\)\nFull conditional for \\(\\sigma^2\\), assuming \\(f(\\sigma^2) \\sim IG(a, b)\\):\n\\[\\sigma^2 |  \\mathbf{Y} , \\boldsymbol{\\beta} \\sim IG\\left(a + \\frac{n}{2},b+\\frac{\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)}{2}\\right)\\]\n\nWhy inverse-Gamma (\\(IG\\)) distribution for \\(\\sigma^2\\)?"
  },
  {
    "objectID": "slides/03-mcmc.html#sampling-from-the-posterior",
    "href": "slides/03-mcmc.html#sampling-from-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Sampling from the posterior",
    "text": "Sampling from the posterior\nLet‚Äôs simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/03-mcmc.html#visualize-simulated-data",
    "href": "slides/03-mcmc.html#visualize-simulated-data",
    "title": "Markov chain Monte Carlo",
    "section": "Visualize simulated data",
    "text": "Visualize simulated data"
  },
  {
    "objectID": "slides/03-mcmc.html#inspecting-the-prior",
    "href": "slides/03-mcmc.html#inspecting-the-prior",
    "title": "Markov chain Monte Carlo",
    "section": "Inspecting the prior",
    "text": "Inspecting the prior\n\n###Define hyperparameters\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\na &lt;- 3\nb &lt;- 1"
  },
  {
    "objectID": "slides/03-mcmc.html#perform-gibbs-sampling",
    "href": "slides/03-mcmc.html#perform-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Perform Gibbs sampling",
    "text": "Perform Gibbs sampling\n\nsigma2 &lt;- exp(rnorm(1)) # initial value\nsamples &lt;- NULL\nfor (s in 1:5000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n  \n  ###Sample from full conditional for sigma2\n  quadratic &lt;- as.numeric(t(Y - X %*% beta) %*% (Y - X %*% beta))\n  sigma2 &lt;- 1 / rgamma(1, shape = a + n / 2, rate = b + quadratic / 2)\n  \n  ###Save samples after a burn-in\n  samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-mcmc.html#inspect-results",
    "href": "slides/03-mcmc.html#inspect-results",
    "title": "Markov chain Monte Carlo",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-mcmc.html#summary-of-gibbs-sampling",
    "href": "slides/03-mcmc.html#summary-of-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Summary of Gibbs sampling",
    "text": "Summary of Gibbs sampling\n\nGibbs sampling is great when we are able to sample from the full conditional distributions.\nIt has been the main inference machine for Bayesian inference since the early 1990s.\nComputing full conditionals and coding up a Gibbs sampler can be mathematically and computationally rigorous.\nNew classes of MCMC are becoming more common to make Bayesian inference less rigorous."
  },
  {
    "objectID": "slides/03-mcmc.html#intuition-behind-metropolis-sampling",
    "href": "slides/03-mcmc.html#intuition-behind-metropolis-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Intuition behind Metropolis sampling",
    "text": "Intuition behind Metropolis sampling\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let‚Äôs consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)‚Äôs in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)‚Äôs.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-acceptance-ratio",
    "href": "slides/03-mcmc.html#metropolis-acceptance-ratio",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis acceptance ratio",
    "text": "Metropolis acceptance ratio\n\nFortunately, the comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) can be made even if we cannot compute \\(f(\\boldsymbol{\\theta} | \\mathbf{Y})\\).\n\n\\[\\begin{aligned}\nr &= \\frac{f(\\boldsymbol{\\theta}^* | \\mathbf{Y})}{f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y})}\\frac{f(\\mathbf{Y})}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\n\\end{aligned}\\]\nHaving computed \\(r\\), how should we proceed?"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-intuition",
    "href": "slides/03-mcmc.html#metropolis-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis intuition",
    "text": "Metropolis intuition\nMetropolis ratio: \\(r = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\)\nIf \\(r &gt; 1:\\)\n\nIntuition: Since \\(\\boldsymbol{\\theta}^{(s)}\\) is already in our set, we should include \\(\\boldsymbol{\\theta}^*\\) as it has a higher probability than \\(\\boldsymbol{\\theta}^{(s)}\\)\nProcedure: Accept \\(\\boldsymbol{\\theta}^*\\) into our set (i.e., set \\(\\boldsymbol{\\theta}^{(s + 1)} = \\boldsymbol{\\theta}^*\\))\n\nIf \\(r &lt; 1:\\)\n\nIntuition: The relative frequency of \\(\\boldsymbol{\\theta}\\)-values in our set equal to \\(\\boldsymbol{\\theta}^*\\) compared to those equal to \\(\\boldsymbol{\\theta}^{(s)}\\) should be \\(r\\). This means that for every instance of \\(\\boldsymbol{\\theta}^{(s)}\\), we should have only a ‚Äúfraction‚Äù of an instance of a \\(\\boldsymbol{\\theta}^*\\) value.\nProcedure: Set \\(\\boldsymbol{\\theta}^{(s + 1)}\\) equal to either \\(\\boldsymbol{\\theta}^*\\) or \\(\\boldsymbol{\\theta}^{(s)}\\), with probability \\(r\\) and \\(1 ‚àí r\\) respectively."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-update",
    "href": "slides/03-mcmc.html#metropolis-update",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis update",
    "text": "Metropolis update\nGiven \\(\\boldsymbol{\\theta}^{(s)}\\), the Metropolis algorithm generates a value \\(\\boldsymbol{\\theta}^{(s + 1)}\\) as follows:\n\nSample \\(\\boldsymbol{\\theta}^*\\) from a proposal distribution, \\(\\boldsymbol{\\theta}^* ‚àº J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\)\nCompute the acceptance ratio \\(r\\)\nLet \\[\\boldsymbol{\\theta}^{(s + 1)} =\n\\left\\{\n  \\begin{array}{ll}\n\\boldsymbol{\\theta}^* & \\text{with probability }\\min(r, 1) \\\\\n\\boldsymbol{\\theta}^{(s)} & \\text{with probability }1 -\\min(r, 1)\n  \\end{array}\n\\right.\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-proposal-distribution",
    "href": "slides/03-mcmc.html#metropolis-proposal-distribution",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis proposal distribution",
    "text": "Metropolis proposal distribution\n\nThe proposal distribution is symmetric (i.e., \\(J(\\boldsymbol{\\theta}_a | \\boldsymbol{\\theta}_b) = J(\\boldsymbol{\\theta}_b | \\boldsymbol{\\theta}_a)\\)\nUsually \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) is very simple, with samples from \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) being near \\(\\boldsymbol{\\theta}\\) with high probability.\nThe most common proposal is a normal distribution\n\n\\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)}) = N(\\boldsymbol{\\theta}^{(s)}, \\boldsymbol{\\Delta})\\)\n\nThe value of the parameter \\(\\boldsymbol{\\Delta}\\) is generally chosen to make the approximation algorithm run efficiently"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-and-gibbs-combined",
    "href": "slides/03-mcmc.html#metropolis-and-gibbs-combined",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis and Gibbs combined",
    "text": "Metropolis and Gibbs combined\n\nThe Gibbs and Metropolis samplers are actually both algorithms within a larger class of Metropolis-Hastings algorithms\nWhen performing MCMC, one can actually choose to update a parameter using either a Gibbs or Metropolis update\nLet‚Äôs see this in action using our linear regression example"
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs",
    "href": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nIn this example, we will use a Metropolis update for \\(\\sigma^2\\), however we will actually focus on \\(\\log\\sigma^2\\).\n\nMetropolis requires a symmetric proposal, so it is often easier to transform parameters to be on the real line and use a normal proposal.\n\nWe will use the following proposal, \\(\\log\\sigma^{2*} \\sim N\\left(\\log\\sigma^{2(s)}, \\delta\\right)\\), where \\(\\delta = 1\\).\nWe will place the prior: \\(\\log\\sigma^2 \\sim N(0,1)\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs-1",
    "href": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs-1",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nsigma2 &lt;- exp(rnorm(1))\nsamples &lt;- NULL\ndelta &lt;- 1\nfor (s in 1:10000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n\n  ###Metropolis update for sigma2\n  # Sample a proposal value\n  log_sigma2_proposal &lt;- rnorm(1, log(sigma2), delta)\n  # Compute the ratio r on the log scale for numeric stability\n  # Also, I've decided to update log(sigma2) instead of sigma2, so I can use a normal proposal distribution\n  # I've placed a normal prior on log(sigma2)\n  likelihood_proposal &lt;- sum(dnorm(Y, X %*% beta, sqrt(exp(log_sigma2_proposal)), log = TRUE))\n  likelihood_current &lt;- sum(dnorm(Y, X %*% beta, sqrt(sigma2), log = TRUE))\n  prior_proposal &lt;- dnorm(log_sigma2_proposal, 0, 1, log = TRUE)\n  prior_current &lt;- dnorm(log(sigma2), 0, 1, log = TRUE)\n  log_r &lt;- (likelihood_proposal + prior_proposal) - (likelihood_current + prior_current)\n  # Update beta using Metropolis ratio\n  if (log(runif(1)) &lt; log_r) sigma2 &lt;- exp(log_sigma2_proposal)\n\n  ###Save samples after a burn-in\n  if (s &gt; 5000) samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-mcmc.html#inspect-results-1",
    "href": "slides/03-mcmc.html#inspect-results-1",
    "title": "Markov chain Monte Carlo",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-mcmc.html#summary-of-metropolis-algorithm",
    "href": "slides/03-mcmc.html#summary-of-metropolis-algorithm",
    "title": "Markov chain Monte Carlo",
    "section": "Summary of Metropolis algorithm",
    "text": "Summary of Metropolis algorithm\n\nMore flexible than Gibbs sampling, because we are no longer required to compute the full conditional distribution analytically.\nPosterior samples can be obtained, however the algorithm must be properly tuned (i.e., choosing \\(\\delta\\)) and the samples may take longer to converge.\nFurthermore, choosing a proper proposal distribution can be difficult in practice.\nIn more recent years, Hamiltonian Monte Carlo has emerged as an new MCMC approach that alleviates the aforementioned issues."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-hastings-mh-algorithm",
    "href": "slides/03-mcmc.html#metropolis-hastings-mh-algorithm",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis-Hastings (MH) algorithm",
    "text": "Metropolis-Hastings (MH) algorithm\nThe proposal distribution is no longer assumed to be symmetric, so the acceptance ratio is, \\[\\begin{aligned}\nr &= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}^*, \\sigma^{2(s)}) f(\\boldsymbol{\\beta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\beta}^{(s)},\\sigma^{2(s)})f(\\boldsymbol{\\beta}^{(s)})} \\frac{J(\\boldsymbol{\\beta}^* | \\boldsymbol{\\beta}^{(s)})}{J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*)}.\n\\end{aligned}\\]\nBoth Metropolis and Gibbs can easily be seen as subcases of Metropolis Hastings.\n\nMetropolis assumes a symmetric proposal, so the proposal terms cancel.\nGibbs assumes that \\(J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*) = f(\\boldsymbol{\\beta}^* | \\mathbf{Y}, \\sigma^{2(s)})\\), which implies that \\(r=1\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#hamiltonian-monte-carlo-hmc-intuition",
    "href": "slides/03-mcmc.html#hamiltonian-monte-carlo-hmc-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "Hamiltonian Monte Carlo (HMC) intuition",
    "text": "Hamiltonian Monte Carlo (HMC) intuition\n\nHMC is a new MCMC approach that has been shown to work better than the usual MH algorithm.\nIt is based on the idea of Hamiltonian dynamics (a physical concept)\n\n\n\n\n\n\n\nRollercoaster Metaphor\n\n\nImagine you‚Äôre on a roller coaster at an amusement park. As the roller coaster moves along the track, it goes up and down hills. When the roller coaster is at the top of a hill, it has a lot of potential energy (like stored energy). When it goes down the hill, that potential energy turns into kinetic energy (energy of motion), making the roller coaster go faster. Hamiltonian dynamics is like a set of rules that tells us how the roller coaster‚Äôs energy changes as it moves along the track."
  },
  {
    "objectID": "slides/03-mcmc.html#hmc-intuition",
    "href": "slides/03-mcmc.html#hmc-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "HMC intuition",
    "text": "HMC intuition\n\nHamiltonian dynamics is used to generate a proposal from a better proposal distribution, \\(J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*)\\), and modifies the acceptance part so the it has a higher acceptance rate.\nJust like the roller coaster follows the track smoothly, Hamiltonian Monte Carlo (HMC) helps us explore different possibilities smoothly and efficiently. This way, we can make more efficient samples from the posterior, just like how the roller coaster moves quickly and smoothly along its track.\nHMC requires evaluations of \\(\\log f(\\boldsymbol{\\theta} | \\mathbf{Y})\\) and \\(\\nabla_{\\boldsymbol{\\theta}} \\log f(\\boldsymbol{\\theta} | \\mathbf{Y})\\),\n\n\\[\\log f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto \\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) + \\log f(\\boldsymbol{\\theta})\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#prepare-for-next-class",
    "href": "slides/03-mcmc.html#prepare-for-next-class",
    "title": "Markov chain Monte Carlo",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nBegin HW 01 which is due January 29\nComplete reading to prepare for next Tuesday‚Äôs lecture\nTuesday‚Äôs lecture: Probabilistic Programming (Intro to Stan!)"
  },
  {
    "objectID": "prepare/prepare-jan13.html",
    "href": "prepare/prepare-jan13.html",
    "title": "Prepare for January 13 lecture",
    "section": "",
    "text": "Occasionally during the semester we will reference material from the text book A First Course in Bayesian Statistical Methods by Peter Hoff. As a Duke student, an electronic version of the book is freely available to you through the Duke Library. We will refer to this textbook as Hoff.\nüìñ Read about Bayesian inference and probability in Hoff Chapter 1 and 2\nüìñ Read about Monte Carlo sampling in Hoff Chapter 4, Sections 4.1 and 4.2\n‚úÖ Complete HW 00 tasks"
  },
  {
    "objectID": "prepare/prepare-jan15.html",
    "href": "prepare/prepare-jan15.html",
    "title": "Prepare for January 15 lecture",
    "section": "",
    "text": "üìñ Read Hoff Chapter 6, Sections 6.1-6.5 to learn about Gibbs sampling\nüìñ Read Hoff Chapter 10, Sections 10.2-10.4 to learn about Metropolis sampling\nüìñ Review simple linear regression\n‚úÖ Complete HW 00 tasks before class"
  },
  {
    "objectID": "hw/hw-01.html",
    "href": "hw/hw-01.html",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "",
    "text": "Due date\n\n\n\nThis assignment is due on Thursday, January 29 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "hw/hw-01.html#exercises-1-7",
    "href": "hw/hw-01.html#exercises-1-7",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercises 1-7",
    "text": "Exercises 1-7\nDefine a random variable \\(Y_i\\) that represents the number of hospital visits during pregnancy for each woman \\(i\\), for \\(i = 1,\\ldots,n\\). Assume that this random variable follows a Poisson distribution with rate \\(\\lambda\\), such that \\(Y_i \\stackrel{iid}{\\sim} \\text{Poisson}(\\lambda)\\). For a Poisson distribution, the mean and variance are equal to \\(\\lambda\\). We are interested in performing statistical inference on \\(\\lambda\\) using a Bayesian approach. A frequently used prior for \\(\\lambda\\) is \\(\\text{Gamma}(\\text{shape = }a, \\text{rate = }b)\\), where \\(\\mathbb{E}[\\lambda] = a/b\\) and \\(\\mathbb{V}(\\lambda) = a/b^2\\).\n\nExercise 1\nThe researchers have prior knowledge that leads them to believe that \\(\\lambda\\) should have mean 8 and variance 4. What values of \\(a\\) and \\(b\\) should they specify?\n\n\nExercise 2\nUsing the prior specified in Exercise 1, compute the probability that \\(\\lambda\\) is greater than 11? For this computation compute the exact probability using the pgamma function in R. This is equivalent to computing \\(P(\\lambda &gt; 11)\\).\n\n\nExercise 3\nCompute the same probability as in Exercise 2, this time using Monte Carlo sampling. Report your Monte Carlo standard error and make sure it is less than 0.01.\n\n\nExercise 4\nSuppose the researchers are interested in the quantity, \\(\\alpha = \\sqrt{\\lambda}\\). Compute the probability that \\(\\alpha\\) is greater than 2.5. Use the same number of Monte Carlo samples as in Exercise 3 and describe why Monte Carlo sampling makes this computation much more efficient than computing the exact probability.\n\n\nExercise 5\nUsing the prior specified in Exercise 1, compute the posterior distribution for \\(\\lambda\\), \\(f(\\lambda | \\mathbf{Y})\\), where \\(\\mathbf{Y}_i = (Y_1,\\ldots,Y_n).\\) Recall that the Gamma prior for \\(\\lambda\\) is a conjugate prior, so that the posterior is given by: \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). Visualize the posterior distribution and report the posterior mean and a 95% credible interval. Provide an interpretation of the posterior summaries within the context of the US births data.\n\n\nExercise 6\nWhat is the posterior probability that \\(\\lambda\\) is greater than 11? This is equivalent to computing \\(P(\\lambda &gt; 11 | \\mathbf{Y})\\). Again, use Monte Carlo sampling. Provide an interpretation for this probability in the context of hospital visits.\n\n\nExercise 7\nCreate a figure that includes both the prior and posterior distributions for \\(\\lambda\\). Also, include a figure of the observed data. Use these figures to make a comparison of the prior and posterior probabilities found in Exercise 3 and Exercise 6, respectively. Describe any changes in these two probabilities and how they relate to the observed data."
  },
  {
    "objectID": "hw/hw-01.html#exercise-8-10",
    "href": "hw/hw-01.html#exercise-8-10",
    "title": "HW 01: Inference using Bayesian statistics",
    "section": "Exercise 8-10",
    "text": "Exercise 8-10\nDefine a random variable \\(weight_i\\) that represents the weight of the baby at birth in ounces for pregnancy \\(i\\). We are interested in learning the association between birth weight and the smoking habit, \\(habit_i\\), of the mother. Fit the following Bayesian linear regression model using Gibbs sampling,\n\\[\\begin{align*}\nweight_i &= \\beta_0 + \\beta_1 \\times 1(habit_i = \\text{smoker}) + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2),\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\\\\n\\boldsymbol{\\beta} &\\sim N(\\mathbf{0}, 100 \\mathbf{I})\\\\\n\\sigma^2 &\\sim \\text{Inv-Gamma}(3,1).\n\\end{align*}\\]\n\nExercise 8\nObtain samples from the posterior distribution of \\((\\boldsymbol{\\beta},\\sigma^2)\\) given the observed data. Visualize the posterior distributions and provide justification that the Gibbs sampler has converged.\n\n\nExercise 9\nReport the posterior mean, standard deviation, and 95% credible intervals for each parameter.\n\n\nExercise 10\nIf someone were to fit the same regression using a frequentist approach the resulting model would look like the following.\n\nmod &lt;- lm(weight ~ habit, data = births14)\nres &lt;- summary(mod)\nprint(res)\n\n\nCall:\nlm(formula = weight ~ habit, data = births14)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4965 -0.6865  0.0635  0.8150  3.1135 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.30654    0.04586 159.317  &lt; 2e-16 ***\nhabitsmoker -0.75203    0.16412  -4.582 5.34e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.241 on 792 degrees of freedom\nMultiple R-squared:  0.02583,   Adjusted R-squared:  0.0246 \nF-statistic:    21 on 1 and 792 DF,  p-value: 5.345e-06\n\n\nSuppose researchers are interested in testing the following hypothesis test: \\(H_0: \\beta_1 = 0, H_1: \\beta_1 &lt; 0\\). We can compute this p-value from the frequentist model.\n\npvalue &lt;- pt(coef(res)[, 3], mod$df, lower = TRUE)[2]\n\nThe resulting p-value is &lt;0.001. Compute the Bayesian p-value that corresponds to the same hypothesis test, \\(P(\\beta_1 &lt; 0 | \\mathbf{Y})\\). Interpret both p-values at a Type-I error rate of 0.05 and compare and contrast their interpretations in the context of the association between smoking and low birth weight.\n\nYou‚Äôre done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message ‚ÄúDone with Homework 1!‚Äù, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html",
    "href": "ae/ae-01-monte-carlo.html",
    "title": "AE 01: Monte Carlo sampling",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE is a demonstration and you do not have to turn anything in!\n\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#learning-goals",
    "href": "ae/ae-01-monte-carlo.html#learning-goals",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will‚Ä¶\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nPerform some Monte Carlo estimation"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-01-monte-carlo.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp26 organization on GitHub.\nClick on the repo with the prefix ae-01-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select SSH (this might already be selected by default, and if it is, you‚Äôll see the text Use a password-protected SSH key.). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#r-and-r-studio",
    "href": "ae/ae-01-monte-carlo.html#r-and-r-studio",
    "title": "AE 01: Monte Carlo sampling",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for ‚ÄúYAML Ain‚Äôt Markup Language‚Äù. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you‚Äôre happy with these changes, we‚Äôll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don‚Äôt have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we may tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\n\n\nPush changes\nNow that you have made an update and committed this change, it‚Äôs time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. Click on Push.\nNow let‚Äôs make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#exercise-1",
    "href": "ae/ae-01-monte-carlo.html#exercise-1",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Exercise 1",
    "text": "Exercise 1\nCompute a one-sided Bayesian p-value for \\(\\mu\\): \\(P(\\mu &lt; 14 | \\mathbf{Y})\\) using Monte Carlo sampling. Interpret the results in plain English.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#exercise-2",
    "href": "ae/ae-01-monte-carlo.html#exercise-2",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe can also compute \\(P(\\mu &lt; 14 | \\mathbf{Y})\\) using pnorm, since we have the posterior in closed form. Compute the exact probability and compare it to the Monte Carlo estimate given in Exercise 1.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-monte-carlo.html#exercise-3",
    "href": "ae/ae-01-monte-carlo.html#exercise-3",
    "title": "AE 01: Monte Carlo sampling",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute a 95% confidence interval for \\(\\mu\\). Provide an interpretation for this confidence interval.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your AE repo on GitHub. You‚Äôre done! üéâ\n\nRecall: This AE is a demonstration and nothing needs to be turned in!"
  },
  {
    "objectID": "ae/ae-02-mcmc.html",
    "href": "ae/ae-02-mcmc.html",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE is a demonstration and you do not have to turn anything in!\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#learning-goals",
    "href": "ae/ae-02-mcmc.html#learning-goals",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will‚Ä¶\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nPerform Gibbs sampling for Bayesian linear regression and compute some basic summaries"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-02-mcmc.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp26 organization on GitHub.\nClick on the repo with the prefix ae-02-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select SSH (this might already be selected by default, and if it is, you‚Äôll see the text Use a password-protected SSH key.). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-02.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-02-mcmc.html#exercise-1",
    "href": "ae/ae-02-mcmc.html#exercise-1",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Exercise 1",
    "text": "Exercise 1\nCompute the posterior mean and standard deviation for the intercept, slope, and measurement error. Provide an interpretation for each of these parameter estimates.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#exercise-2",
    "href": "ae/ae-02-mcmc.html#exercise-2",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Exercise 2",
    "text": "Exercise 2\nCompute a 95% confidence interval for the regression slope. Provide an interpretation for this confidence interval.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-mcmc.html#exercise-3",
    "href": "ae/ae-02-mcmc.html#exercise-3",
    "title": "AE 02: Posterior estimation using Gibbs sampling",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute a one-sided Bayesian p-value for the regression slope: \\(P(\\beta_1 &lt; 0)\\). Interpret the results in plain English. Is intraocular pressure associated with disease progression?\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today‚Äôs class.\nPush all your work to your AE repo on GitHub. You‚Äôre done! üéâ\n\nThis AE is a demonstration and you do not have to turn anything in!"
  }
]