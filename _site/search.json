[
  {
    "objectID": "project-old.html",
    "href": "project-old.html",
    "title": "Final project",
    "section": "",
    "text": "Project proposal\n\ndue Friday, October 27 (Tuesday labs)\ndue Sunday, October 29 (Thursday labs)\n\nDraft report + peer review\n\ndue Tuesday, November 14 (Tuesday labs)\ndue Thursday, November 16 (Thursday labs)\n\nRound 1 submission (optional) due Friday, December 1\nPresentation + Presentation comments\n\nTuesday, December 5 (Tuesday labs)\nThursday, December 7 (Thursday labs)\n\nWritten report due Wednesday, December 13\nReproducibility + organization due Wednesday, December 13"
  },
  {
    "objectID": "project-old.html#timeline",
    "href": "project-old.html#timeline",
    "title": "Final project",
    "section": "",
    "text": "Project proposal\n\ndue Friday, October 27 (Tuesday labs)\ndue Sunday, October 29 (Thursday labs)\n\nDraft report + peer review\n\ndue Tuesday, November 14 (Tuesday labs)\ndue Thursday, November 16 (Thursday labs)\n\nRound 1 submission (optional) due Friday, December 1\nPresentation + Presentation comments\n\nTuesday, December 5 (Tuesday labs)\nThursday, December 7 (Thursday labs)\n\nWritten report due Wednesday, December 13\nReproducibility + organization due Wednesday, December 13"
  },
  {
    "objectID": "project-old.html#introduction",
    "href": "project-old.html#introduction",
    "title": "Final project",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group’s interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio using Quarto and GitHub, and your analysis and written report must be reproducible.\n\nLogistics\nYou will work on the project with your lab groups. The four primary deliverables for the final project are\n\na written, reproducible report detailing your analysis\na GitHub repository corresponding to your report\nslides and an in-person presentation\nformal peer review on another team’s work and presentation feedback"
  },
  {
    "objectID": "project-old.html#project-proposal",
    "href": "project-old.html#project-proposal",
    "title": "Final project",
    "section": "Project proposal",
    "text": "Project proposal\n\n\n\n\n\n\nDue dates\n\n\n\n\nFriday, October 27 (Tuesday labs)\nSunday, October 29 (Thursday labs)\n\n\n\nThe purpose of the project proposal is for your team to identify the data set you’re interested in analyzing for the project, do some preliminary exploratory data analysis, and begin to think about a modeling strategy . If you’re unsure where to find data, you can use the list of potential data sources on the Tips + resources page as a starting point. It may also help to think of topics you’re interested in investigating and find data sets on those topics.\nThe data set must meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns, such that at least 6 of the columns are useful and unique predictor variables.\n\ne.g., identifier variables such as “name”, “ID number”, etc. are not useful predictor variables.\ne.g., if you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nMay not be data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\n\n\n\n\n\n\nTypes of data sets to avoid\n\n\n\n\nData that are likely violate the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nData sets in which there is no information about how the data were originally collected\nData sets in which there are missing or unclear definitions about the observations and/or variables\n\n\n\nAsk a member of the teaching team if you’re unsure whether your data set meets the criteria.\nThe proposal will include the following sections:\n\nSection 1: Introduction\nThe introduction section includes\n\nan introduction to the subject matter you’re investigating (citing any relevant literature)\nthe motivation for your research question (citing any relevant literature)\nthe primary research question you are interested in exploring\nyour team’s hypotheses regarding the research question\n\nThis is a narrative about what you think regarding the research question, not formal statistical hypotheses.\n\n\n\n\nSection 2: Data description\nThe data description section includes\n\nthe source of the data set\na description of when and how the data were originally collected (by the original data curator, not necessarily how you found the data)\na description of the observations and general characteristics being measured\n\n\n\nSection 3: Initial exploratory data analysis\nIn this section, you will begin to explore the data. This includes using narrative, visualizations and summary statistics to describe the following:\n\ndistribution of the response variable\ndistributions of one potential quantitative predictor variable and one potential categorical predictor variable\nthe relationships between the response variable and each of the predictors from the previous step\na potential interaction effect you’re interested in exploring (it doesn’t have to be an interaction with the two predictors from above)\n\nThese steps are to help get you started on exploratory data analysis and will not be the complete EDA for the final report. The requirements above are minimum requirements, but your group is welcome to include more at this stage.\nIn this section, you will also describe any data cleaning you need to do to prepare for modeling, such as imputing missing values, collapsing levels for categorical predictors, creating new variables, summarizing data, etc.\n\n\nSection 4: Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes\n\na description of the response variable and list of all potential predictors\nregression model technique (multiple linear regression or logistic regression)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of the data folder. You do not need to include the data dictionary in the PDF document.\n\n\nSubmission\n\n\n\n\n\n\nImportant\n\n\n\nWrite your narrative and analysis for Sections 1 - 4 in the proposal.qmd file. Put the data set and the data dictionary in the data folder.\nSubmit the PDF of the proposal to Gradescope. Mark all pages of the document.\n\n\n\n\nGrading\nThe anticipated length, including all graphs, tables, narrative, etc., is 2 -4 pages; it may not exceed 5 pages.\nThe proposal is worth 15 points and will be graded based on accurately and comprehensively addressing the criteria stated above. Points will be assigned based on a holistic review of the project proposal.\n\nExcellent (14 - 15 points) : All required elements are completed and are accurate. There is a thorough exploration of the data as descrbied above, and the team has demonstrated a careful and thoughtful approach exploring the data and preparing it for analysis. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nStrong: (11 - 13 points): Requirements are mostly met, but there are some elements that are incomplete or inaccurate. Some minor revision of the work required before team is ready for modeling.\nSatisfactory (8 - 10 points): Requirements partially met, but there are some elements that are incomplete and/or inaccurate. Major revision of the work required before team is ready for modeling.\nNeeds Improvement (7 or fewer points points): Requirements are largely unmet, and there are large elements that are incomplete and/or inaccurate. Substantial revisions of the work required before team is ready for modeling."
  },
  {
    "objectID": "project-old.html#draft-report-peer-review",
    "href": "project-old.html#draft-report-peer-review",
    "title": "Final project",
    "section": "Draft report + peer review",
    "text": "Draft report + peer review\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\n\nDraft report\n\n\n\n\n\n\nDue dates\n\n\n\nDraft is due in your project GitHub repo at 9am on\n\nTuesday, November 14 (Tuesday labs)\nThursday, November 16 (Thursday labs)\n\n\n\nWrite the draft in the written-report.qmd file in your project repo. You do not need to submit the draft on Gradescope.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the body of the report, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.\nThis section also includes initial interpretations and conclusions drawn from the model.\n\n\nGrading\nThe draft will be graded based on whether there is demonstration of a reasonable attempt at each of the sections described below in the written-report.qmd file in our GitHub repo by the deadline.\n\n\n\nPeer review\n\n\n\n\n\n\nImportant\n\n\n\nPeer review comments are due in GitHub at 11:59pm on\n\nWednesday, November 15 (Tuesday labs)\nFriday, November 17 (Thursday labs)\n\n\n\nCritically reviewing others’ work is a crucial part of the scientific process, and STA 210 is no exception. Each lab team will be assigned two other teams’s projects to review. Each team should push their draft to their GitHub repo by the 9am on the day their lab’s draft is due. The lab that week will be dedicate to the peer review, so your team will have time to review and provide quality feedback to two other teams.\nDuring the peer review process, you will be provided read-only access to your partner teams’ GitHub repos. Provide your review in the form of GitHub issues to your partner team’s GitHub repo using the issue template provided in the repo.\n\nSteps for peer review\n\n\n\n\n\n\nPeer review assignments\n\n\n\nClick here to see which project your team is reviewing. You’ll spend about 30 minutes reviewing each project.\n\n\nWhen you get to lab, you should have access to the GitHub repos for the teams you’re reviewing. In GitHub, search the repositories for project, and you should see the repos for the projects you’re reviewing. You will be able to read the files in the repo and post issues, but you cannot push changes to the repo. You will have access to the repo until the deadline for the peer review.\nFor each team you’re reviewing:\n\nOpen that team’s repo, read the project draft, and browse the rest of the repo.\nGo to the Issues tab in that repo, click on New issue, and click on Get started for the Peer Review issue. Fill out this issue. You will answer the the following questions:\n\nDescribe the goal of the project.\nDescribe the data set used in the project. What are the observations in the data? What is the source of the data? How were the data originally collected?\nConsider the exploratory data analysis (EDA). Describe one aspect of the EDA that is effective in helping you understand the data. Provide constructive feedback on how the team might improve the EDA.\nDescribe the statistical methods and analysis approach used.\nProvide constructive feedback on how the team might improve their analysis. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but also feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation?\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?\n\n\n\n\nGrading\nThe peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team’s report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions."
  },
  {
    "objectID": "project-old.html#written-report",
    "href": "project-old.html#written-report",
    "title": "Final project",
    "section": "Written report",
    "text": "Written report\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\n\n\n\n\n\n\nNote\n\n\n\nBefore you finalize your write up, make sure the code chunks are not visible and all messages and warnings are suppressed.\n\n\nYou will submit the PDF of your final report on GitHub.\nThe PDF you submit must match the .qmd in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including tables and visualizations, must be no more than 10 pages long. There is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will overwhelmingly be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the variables in the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics, conditions, and diagnostics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. The model conditions and diagnostics are thoroughly and accurately assessed for their model. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the model’s predictive power is thoroughly assessed.\n\n\n\nDiscussion + Conclusion\nIn this section you’ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report.\n\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted and labeled. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages.\n\n\nSubmission\n\n\n\n\n\n\nImportant\n\n\n\nThe written report is due on Wednesday, December 13 at 11:59pm.\nTo submit your report, written-report.qmd and the rendered written-report.pdf to your team’s GitHub repo by the deadline. You will not submit the report on Gradescope.\nThe version of the report in the repo by Wednesday, December 13 will be the one that is graded."
  },
  {
    "objectID": "project-old.html#round1-submission",
    "href": "project-old.html#round1-submission",
    "title": "Final project",
    "section": "Round 1 submission (optional)",
    "text": "Round 1 submission (optional)\n\n\n\n\n\n\nDue date\n\n\n\nFriday, December 1 at 11:59pm on GitHub (all teams)\nReports submitted after this date will not receive preliminary feedback.\n\n\nThe Round 1 submission is an opportunity to receive detailed feedback on your analysis and written report before the final submission. Therefore, to make the feedback most useful, you must submit a complete written report to receive feedback. You will also be notified of the grade you would receive at that point. You will have the option to keep the grade (and thus you don’t need to turn in an updated report) or resubmit the written report by the final submission deadline to receive a new grade.\n\nTo submit the Round 1 submission:\n\nPush the updated written-report.qmd and written-report.pdf to your GitHub repo.\nOpen an issue with the title “Round 1 Submission”. You can use the template issue in the GitHub repo. Make sure I am tagged in the issue (@matackett), so I receive an email notification of your Round 1 submission. See Creating an issue from a repository for instructions on opening an issue. Please ask a member of the teaching team for assistance if you need help opening the issue.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that this is optional, so there is nograde penalty for not turning in a Round 1 submission. Due to time constraints at the end of the semester, only high-level feedback will be given for the reports submitted at the final written report deadline on December 13."
  },
  {
    "objectID": "project-old.html#presentation",
    "href": "project-old.html#presentation",
    "title": "Final project",
    "section": "Presentation",
    "text": "Presentation\n\n\n\n\n\n\nImportant\n\n\n\nPresentations will take place in class during labs December 5 & 7.\n\nClick here for the presentation order.\n\n\nIn addition to the written report, your team will also do an in-person presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. The presentation should be supported by slides that serve as a brief visual addition to the presentation. The presentation and slides will be graded for content and clarity.\nYou can create your slides with any software you like (Keynote, PowerPoint, Google Slides, etc.). We recommend choosing an option that’s easy to collaborate with, e.g., Google Slides.\n\n\n\n\n\n\nNote\n\n\n\nYou can also use Quarto to make your slides! While we won’t be covering making slides with Quarto in the class, we would be happy to help you with it in office hours. It’s no different than writing other documents with Quarto, so the learning curve will not be steep!\n\n\nThe presentation must be no longer than 6 minutes. It is fine if the presentation is shorter than 6 minutes, but it cannot exceed 6 minutes due to the limited time during lab.\nEvery team member is expected to speak in the presentation. Part of the grade will be whether every team member had a meaningful speaking role in the presentation.\n\nSlides\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\nGrading criteria\nThe presentation grade will be based on the following criteira:\n\nContent: The group told a unified story using the appropriate regression analysis.\nSlides: The presentation slides were organized, included clear and informative visualizations, and were easily readable.\nProfessionalism: The group’s communication style was clear and professional.\nTime Management: Team divided the time well and stayed within the 6 minute time limit, with each team member making a meaning contribution to the presentation. (assessed by the teaching team only).\n\n80% of the presentation grade will be the average of the teaching scores and 20% will be the average of the peer scores.\n\n\n\n\n\n\nImportant\n\n\n\nYou can submit the presentation slides in two ways:\n\nPut a PDF of the slides in the presentation folder in your team’s GitHub repo.\nPut the URL to your slides in the README of the presentation folder. If you share the URL, please make sure permissions are set so Prof. Tackett can view the slides.\n\nSlides must be submitted by the start of your lab on December 5 or 7. You will not submit the slides on Gradescope."
  },
  {
    "objectID": "project-old.html#presentation-comments",
    "href": "project-old.html#presentation-comments",
    "title": "Final project",
    "section": "Presentation comments",
    "text": "Presentation comments\n\n\n\n\n\n\nImportant\n\n\n\nClick here to find the teams you’re scoring and a link to the feedback form.\nThis portion of the project will be assessed individually.\n\n\n\nYou will provide feedback on two teams’ presentations. You can find your assigned teams and the link to the feedback from here. Please provide all scores and comments by the end of the lab session. There will be a few minutes between each presentation to submit scores.\nThe grade will be based on submitting the scores and comments for both of your assigned teams by the end of the presentation day (December 5 for Tuesday labs, December 7 for Thursday labs)."
  },
  {
    "objectID": "project-old.html#reproducibility-organization",
    "href": "project-old.html#reproducibility-organization",
    "title": "Final project",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\nproposal.qmd & proposal.pdf: Project proposal\n/data: Folder that contains the data set for the final project.\nproject.Rproj: File specifying the RStudio project\n/presentation: Folder with the presentation slides or link to slides.\n.gitignore: File that lists all files that are in the local RStudio project but not the GitHub repo\n/.github: Folder for peer review issue template\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable.\n\n\n\n\n\n\nImportant\n\n\n\nThe repo must be ready for grading by Wednesday, December 13 at 11:59pm."
  },
  {
    "objectID": "project-old.html#peer-teamwork-evaluation",
    "href": "project-old.html#peer-teamwork-evaluation",
    "title": "Final project",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly."
  },
  {
    "objectID": "project-old.html#overall-grading",
    "href": "project-old.html#overall-grading",
    "title": "Final project",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nProject proposal\n15 pts\n\n\nDraft report + peer review\n15 pts\n\n\nPresentation\n20 pts\n\n\nPresentation comments\n5 pts\n\n\nWritten report\n40 pts\n\n\nReproducibility + organization\n5 pts\n\n\n\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort."
  },
  {
    "objectID": "project-old.html#late-work-policy",
    "href": "project-old.html#late-work-policy",
    "title": "Final project",
    "section": "Late work policy",
    "text": "Late work policy\nThere is no late work accepted on the draft report or presentation. Other components of the project may be accepted up to 48 hours late. A 10% late deduction will apply for each 24-hour period late.\nBe sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final project",
    "section": "",
    "text": "Research questions due Thursday, September 26\nProject proposal due Thursday, October 3\nExploratory data analysis due Thursday, October 31\nPresentation + Presentation comments Monday, November 11 (in lab)\nAnalysis draft + peer review Monday, November 25 (peer review in lab)\nRound 1 submission (optional) due Friday, December 6\nWritten report due Thursday, December 12 at 9pm\nReproducibility + organization due Thursday, December 12 at 9pm",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#project-milestones",
    "href": "project.html#project-milestones",
    "title": "Final project",
    "section": "",
    "text": "Research questions due Thursday, September 26\nProject proposal due Thursday, October 3\nExploratory data analysis due Thursday, October 31\nPresentation + Presentation comments Monday, November 11 (in lab)\nAnalysis draft + peer review Monday, November 25 (peer review in lab)\nRound 1 submission (optional) due Friday, December 6\nWritten report due Thursday, December 12 at 9pm\nReproducibility + organization due Thursday, December 12 at 9pm",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#introduction",
    "href": "project.html#introduction",
    "title": "Final project",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group’s interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio using Quarto and GitHub, and your analysis and written report must be reproducible.\n\nLogistics\nYou will work on the project with your lab groups. The primary deliverables for the project are\n\nan in-person presentation about the exploratory data analysis and initial modeling\na written, reproducible final report detailing your analysis\na GitHub repository containing all work from the project\n\nThere are intermediate milestones and peer review assignments throughout the semester to help you work towards the final deliverables.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#research-questions",
    "href": "project.html#research-questions",
    "title": "Final project",
    "section": "Research questions",
    "text": "Research questions\nThe goal of this milestone is to discuss topics and develop potential research questions your team is interested in investigating for the project. You are only developing potential research questions; you do not need to have a data set identified at this point.\nDevelop three potential research questions. Include the following for each question:\n\nA statement of the research question.\nThe target population of interest for this question.\nA statement about your motivation for investigating this research question and why this question is important.\nIdeas about the type of data you might use to answer this question. Note: These are your ideas about the type of data you could use. You do not need to have a data set at this point.\n\n\nSubmission\nWrite your responses in research-questions.qmd in your team’s project GitHub repo. Push the qmd and rendered pdf documents to GitHub by the deadline, Thursday, September 26 at 11:59pm.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#project-proposal",
    "href": "project.html#project-proposal",
    "title": "Final project",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is for your team to identify the data set you’re interested in analyzing to investigate one of your potential research questions. You will also do some preliminary exploration of the response variable and begin thinking about the modeling strategy. If you’re unsure where to find data, you can use the list of potential data sources on the Tips + resources page as a starting point.\n\n\n\n\n\n\nImportant\n\n\n\nYou must the data set(s) in the proposal for the final project, unless instructed otherwise when given feedback.\n\n\nThe data set must meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns, such that at least 6 of the columns are useful and unique predictor variables.\n\ne.g., identifier variables such as “name”, “ID number”, etc. are not useful predictor variables.\ne.g., if you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nMay not be data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\n\n\n\n\n\n\nTypes of data sets to avoid\n\n\n\n\nData that are likely violate the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nData sets in which there is no information about how the data were originally collected\nData sets in which there are missing or unclear definitions about the observations and/or variables\n\n\n\nAsk a member of the teaching team if you’re unsure whether your data set meets the criteria.\nThe proposal will include the following sections:\n\nSection 1: Introduction\n\n\n\n\n\n\nTip\n\n\n\nReuse and iterate on the work from the Research Questions milestone.\n\n\n\nAn introduction to the subject matter you’re investigating (citing any relevant literature)\nStatement of a well-developed research question.\nThe motivation for your research question and why it is important\nYour team’s hypotheses regarding the research question\n\nThis is a narrative about what you think regarding the research question, not formal statistical hypotheses.\n\n\n\n\nSection 2: Data description\n\nThe source of the data set\nA description of when and how the data were originally collected (by the original data curator, not necessarily how you found the data)\nA description of the observations and general characteristics being measured\n\n\n\nSection 3: Initial exploratory data analysis\n\nDescription of data cleaning you need to do to prepare for analysis (can focus on the response variable for now), such as joining data sets, imputing missing values, variable transformation, creating a new variable, etc.\nVisualizations, summary statistics, and narrative to describe the distribution of the response variable.\n\n\n\nSection 4: Analysis approach\n\na description of the potential predictor variables of interest\nregression model technique (multiple linear regression for quantitative response variable or logistic regression for a categorical response variable)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of the data folder. You do not need to include the data dictionary in the PDF document.\n\n\nSubmission\nWrite your narrative and analysis for Sections 1 - 4 in the proposal.qmd file in your team’s GitHub repo. Put the data set and the data dictionary in the data folder in the repo. Push the qmd and rendered pdf documents to GitHub by the deadline, Thursday, October 3 at 11:59pm.\n\n\nGrading\nThe anticipated length, including all graphs, tables, narrative, etc., is 2 -4 pages.\nThe proposal is worth 10 points and will be graded based on accurately and comprehensively addressing the criteria stated above. Points will be assigned based on a holistic review of the project proposal.\n\nExcellent (9 - 10 points) : All required elements are completed and are accurate. The data set meets the requirements (or the team has otherwise discussed the data with Professor Tackett) and the data do not pose obvious violations to the modeling assumptions. There is a thoughtful and comprehensive description of the data and exploration of the response variable as descrbied above. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nStrong (7 - 8 points): Requirements are mostly met, but there are some elements that are incomplete or inaccurate. Some minor revision of the work required before team is ready for modeling.\nSatisfactory (5 - 6 points): Requirements partially met, but there are some elements that are incomplete and/or inaccurate. Major revision of the work required before team is ready for modeling.\nNeeds Improvement (4 or fewer points points): Requirements are largely unmet, and there are large elements that are incomplete and/or inaccurate. Substantial revisions of the work required before team is ready for modeling.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#eda",
    "href": "project.html#eda",
    "title": "Final project",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\n\n\n\n\nTip\n\n\n\nReuse and iterate on the work from the previous milestones.\n\n\nThe purpose of this milestone is begin exploring the data and get early feedback on your data and analysis. You will submit a draft of the beginning of your report that includes the introduction and exploratory data analysis, with an emphasis on the EDA. It will also help you prepare for the presentation of the exploratory data analysis results.\nBelow is a brief description of the sections to include in this step:\n\nIntroduction\nThis section includes an introduction to the project motivation, background, data, and research question.\n\n\nExploratory data analysis\nThis section includes the following:\n\nDescription of the data set and key variables.\nExploratory data analysis of the response variable and key predictor variables.\n\nUnivariate EDA of the response and key predictor variables.\nBivariate EDA of the response and key predictor variables\nPotential interaction effects.\n\n\n\n\nSubmission\nWrite your draft introduction and exploratory data analysis in the written-report.qmd file in your team’s GitHub repo. Push the qmd and rendered pdf documents to GitHub by the deadline, Thursday, October 31 at 11:59pm.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#presentation",
    "href": "project.html#presentation",
    "title": "Final project",
    "section": "Presentation",
    "text": "Presentation",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#draft-report-peer-review",
    "href": "project.html#draft-report-peer-review",
    "title": "Final project",
    "section": "Analysis + peer review",
    "text": "Analysis + peer review",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#round1-submission",
    "href": "project.html#round1-submission",
    "title": "Final project",
    "section": "Round 1 submission (optional)",
    "text": "Round 1 submission (optional)",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#written-report",
    "href": "project.html#written-report",
    "title": "Final project",
    "section": "Written report",
    "text": "Written report",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#reproducibility-organization",
    "href": "project.html#reproducibility-organization",
    "title": "Final project",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#peer-teamwork-evaluation",
    "href": "project.html#peer-teamwork-evaluation",
    "title": "Final project",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#overall-grading",
    "href": "project.html#overall-grading",
    "title": "Final project",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nResearch question\n3 pts\n\n\nProject proposal\n10 pts\n\n\nExploratory data analysis\n15 pts\n\n\nPresentation\n10 pts\n\n\nPresentation comments\n2 pts\n\n\nDraft report + peer review\n15 pts\n\n\nWritten report\n40 pts\n\n\nReproducibility + organization\n5 pts\n\n\n\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project.html#late-work-policy",
    "href": "project.html#late-work-policy",
    "title": "Final project",
    "section": "Late work policy",
    "text": "Late work policy\nThere is no late work accepted on the draft report or presentation. Other components of the project may be accepted up to 48 hours late. A 10% late deduction will apply for each 24-hour period late.\nBe sure to turn in your work early to avoid any technological mishaps.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-tips.html",
    "href": "project-tips.html",
    "title": "Final project tips + resources",
    "section": "",
    "text": "Data sources\n\nSome resources that may be helpful as you find data:\n\nFiveThirtyEight data\nTidyTuesday\nData Is Plural\nR Data Sources for Regression Analysis\n\n\n\nOther data repositories\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies\n\n\n\n\nTips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your Qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works.\n\n\n\nFormatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\nAn alternative approach is to add the following code to the YAML:\n\nexecute:\n  echo: false\n  warning: false\n  message: false\n\n\n\n\nHeaders\n\nUse headers to clearly label each section. Make sure there is a space between the last # and the title, so the header renders correctly. For example, ###Section Title will not render as header, but ### Section Title will.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\n\nResize plots and figures, so you have more space for the narrative.\n\nResize individual figures: Use the code chunk header {r plot1, fig.height = 3, fig.width = 5}, replacing plot1 with a meaningful label and the height and width with values appropriate for your write up.\nResize all figures: Include the fig_width and fig_height options in your YAML header as shown below:\n\n\n\n---\ntitle: \"Your title\"\nauthor: \"Your names\"\nformat:\n  pdf:\n    fig-width: 7\n    fig-height: 5\n---\nReplace the height and width values with values appropriate for your write up.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\n\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\nIf you’re using base R function, i.e. when using the emplogit functions, put the code par(mfrow = c(rows,columns)) before the code to make the plots. For example, par(mfrow = c(2,3)) will arrange plots in a grid with 2 rows and 3 columns.\n\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\nUse coord_flip() to flip the x and y axes on the plot. This is useful if you a bar plot with an x-axis that is difficult to read due to overlapping text.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg |&gt;\n  count(manufacturer) |&gt;\n  mutate(manufacturer = str_to_title(manufacturer)) |&gt;\n  ggplot(aes(x = fct_reorder(manufacturer,n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document.\n\n\n\n\nAdditional resources\n\nExploring RStudio’s Visual Markdown Editor\nR for Data Science\nQuarto documentation:\n\nQuarto PDF Basics\nPresentations in Quarto\n\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package",
    "crumbs": [
      "Project",
      "Tips + resources"
    ]
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#announcements",
    "href": "slides/10-prop-of-estimators.html#announcements",
    "title": "Properties of estimators",
    "section": "Announcements",
    "text": "Announcements\n\nProject\n\nResearch questions due TODAY\nProposal due Thursday, October 3 at 11:59pm\n\nLab 03 due Thursday, October 3 at 11:59pm\nHW 02 due Thursday, October 3 at 11:59pm (released after class)\nStatistics experience due Tue, Nov 26 at 11:59pm"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#topics",
    "href": "slides/10-prop-of-estimators.html#topics",
    "title": "Properties of estimators",
    "section": "Topics",
    "text": "Topics\n\nCompute and interpret confidence interval for a single coefficient\nProperties of \\(\\hat{\\boldsymbol{\\beta}}\\)\nDefine “linear” model"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#computing-setup",
    "href": "slides/10-prop-of-estimators.html#computing-setup",
    "title": "Properties of estimators",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(kableExtra)  \nlibrary(patchwork)   \n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#data-ncaa-football-expenditures",
    "href": "slides/10-prop-of-estimators.html#data-ncaa-football-expenditures",
    "title": "Properties of estimators",
    "section": "Data: NCAA Football expenditures",
    "text": "Data: NCAA Football expenditures\nToday’s data come from Equity in Athletics Data Analysis and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a March 2022 Tidy Tuesday.\nWe will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :\n\ntotal_exp_m: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\nenrollment_th: Total student enrollment in the 2019 - 2020 academic year (in thousands)\ntype: institution type (Public or Private)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#regression-model",
    "href": "slides/10-prop-of-estimators.html#regression-model",
    "title": "Properties of estimators",
    "section": "Regression model",
    "text": "Regression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type, data = football)\ntidy(exp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#inference-for-beta_j",
    "href": "slides/10-prop-of-estimators.html#inference-for-beta_j",
    "title": "Properties of estimators",
    "section": "Inference for \\(\\beta_j\\)",
    "text": "Inference for \\(\\beta_j\\)\nWe often want to conduct inference on individual model coefficients\n\nHypothesis test: Is there a linear relationship between the response and \\(x_j\\)?\nConfidence interval: What is a plausible range of values \\(\\beta_j\\) can take?"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#confidence-interval-for-beta_j-1",
    "href": "slides/10-prop-of-estimators.html#confidence-interval-for-beta_j-1",
    "title": "Properties of estimators",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\n\nA plausible range of values for a population parameter is called a confidence interval\nUsing only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nSimilarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#what-confidence-means",
    "href": "slides/10-prop-of-estimators.html#what-confidence-means",
    "title": "Properties of estimators",
    "section": "What “confidence” means",
    "text": "What “confidence” means\n\n\nWe will construct \\(C\\%\\) confidence intervals\n\nThe confidence level impacts the width of the interval\n\n“Confidence” means if we were to take repeated samples of the same size as our data, fit regression lines using the same predictors, and calculate \\(C\\%\\) CIs for the coefficient of \\(x_j\\), then \\(C\\%\\) of those intervals will contain the true value of the coefficient \\(\\beta_j\\)\nNeed to balance precision and accuracy when selecting a confidence level"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#confidence-interval-for-beta_j-2",
    "href": "slides/10-prop-of-estimators.html#confidence-interval-for-beta_j-2",
    "title": "Properties of estimators",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE({\\hat{\\beta}_j})\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-p-1\\) degrees of freedom"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#computing-t-in-r",
    "href": "slides/10-prop-of-estimators.html#computing-t-in-r",
    "title": "Properties of estimators",
    "section": "Computing \\(t^*\\) in R",
    "text": "Computing \\(t^*\\) in R\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(football) - 2 - 1)\n\n[1] 1.97928\n\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(football) - 2 - 1)\n\n[1] 1.657235\n\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(football) - 2 - 1)\n\n[1] 2.61606"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#ci-for-coefficient-of-enrollment",
    "href": "slides/10-prop-of-estimators.html#ci-for-coefficient-of-enrollment",
    "title": "Properties of estimators",
    "section": "95% CI for coefficient of enrollment",
    "text": "95% CI for coefficient of enrollment\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n\n\n\n\n\n\n\n\\[\n\\hat{\\beta}_j \\pm t^* \\times SE(\\hat{\\beta}_j)\n\\]\n\n\n\\[\n0.7804 \\pm 1.9793 \\times 0.1103\n\\]\n\n\n\\[\n[0.562, 0.999]\n\\]"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#interpreting-the-ci",
    "href": "slides/10-prop-of-estimators.html#interpreting-the-ci",
    "title": "Properties of estimators",
    "section": "Interpreting the CI",
    "text": "Interpreting the CI\n🔗 edstem.org/us/courses/62513/discussion/648045"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#computing-ci-in-r",
    "href": "slides/10-prop-of-estimators.html#computing-ci-in-r",
    "title": "Properties of estimators",
    "section": "Computing CI in R",
    "text": "Computing CI in R\n\ntidy(exp_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n13.426\n25.239\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n0.562\n0.999\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n-19.466\n-6.986"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#motivation",
    "href": "slides/10-prop-of-estimators.html#motivation",
    "title": "Properties of estimators",
    "section": "Motivation",
    "text": "Motivation\n\n\nWe have discussed how to use least squares to find an estimator of \\(\\hat{\\boldsymbol{\\beta}}\\)\nHow do we know whether our least squares estimator is a “good” estimator?\nWhen we consider what makes an estimator “good”, we’ll look at three criteria:\n\nBias\nVariance\nMean squared error\n\nWe’ll take a look at these over the course of a few lectures and motivate why we might prefer using least squares to compute \\(\\hat{\\boldsymbol{\\beta}}\\) versus other methods"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#bias-and-variance",
    "href": "slides/10-prop-of-estimators.html#bias-and-variance",
    "title": "Properties of estimators",
    "section": "Bias and variance",
    "text": "Bias and variance\nSuppose you are throwing darts at a target\n\n\n\n\n\n\nImage source: Analytics Vidhya\n\n\n\n\nUnbiased: Darts distributed around the target\nBiased: Darts systematically away from the target\nVariance: Darts could be widely spread (high variance) or generally clustered together (low variance)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#bias-and-variance-1",
    "href": "slides/10-prop-of-estimators.html#bias-and-variance-1",
    "title": "Properties of estimators",
    "section": "Bias and variance",
    "text": "Bias and variance\n\nIdeal scenario: Darts are clustered around the target (unbiased and low variance)\nWorst case scenario: Darts are widely spread out and systematically far from the target (high bias and high variance)\nAcceptable scenario: There’s some trade-off between the bias and variance. For example, it may be acceptable for the darts to be clustered around a point that is close to the target (low bias and low variance)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#bias-and-variance-2",
    "href": "slides/10-prop-of-estimators.html#bias-and-variance-2",
    "title": "Properties of estimators",
    "section": "Bias and variance",
    "text": "Bias and variance\n\n\nEach time we take a sample of size \\(n\\), we can find the least squares estimator (throw dart at target)\nSuppose we take many independent samples of size \\(n\\) and find the least squares estimator for each sample (throw many darts at the target). Ideally,\n\nThe estimators are centered at the true parameter (unbiased)\nThe estimators are clustered around the true parameter (unbiased with low variance)\n\n\n\n\nLet’s take a look at the mean and variance of the least squares estimator"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta",
    "href": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta",
    "title": "Properties of estimators",
    "section": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)\nThe bias of an estimator is the difference between the estimator’s expected value and the true value of the parameter\n\nLet \\(\\hat{\\theta}\\) be an estimator of the parameter \\(\\theta\\). Then\n\\[\nBias(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\n\\]\n\n\nAn estimator is unbiased if the bias is 0 and thus \\(E(\\hat{\\theta}) = \\theta\\)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#finding-expected-value-and-variance",
    "href": "slides/10-prop-of-estimators.html#finding-expected-value-and-variance",
    "title": "Properties of estimators",
    "section": "Finding expected value and variance",
    "text": "Finding expected value and variance\nLet \\(\\mathbf{A}\\) be a \\(n \\times p\\) matrix of constants and \\(\\mathbf{b}\\) a \\(p \\times 1\\) vector of random variables. Then\n\\[\nE(\\mathbf{Ab}) = \\mathbf{A}E(\\mathbf{b})\n\\]\n\\[\nVar(\\mathbf{Ab}) = \\mathbf{A}Var(\\mathbf{b})\\mathbf{A}^T\n\\]"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta-1",
    "href": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta-1",
    "title": "Properties of estimators",
    "section": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)\nLet’s take a look at the expected value of the least squares estimator. Given \\(E(\\boldsymbol{\\epsilon}) = \\mathbf{0}\\),\n\\[\n\\begin{aligned}\nE(\\hat{\\boldsymbol{\\beta}}) &= E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}] \\\\[8pt]\n& = \\class{fragment}{E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon})]} \\\\[8pt]\n& = \\class{fragment}{E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}] + E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}]}\\\\[8pt]\n& = \\class{fragment}{\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^TE(\\boldsymbol{\\epsilon})} \\\\[8pt]\n& = \\class{fragment}{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta-2",
    "href": "slides/10-prop-of-estimators.html#expected-value-of-hatboldsymbolbeta-2",
    "title": "Properties of estimators",
    "section": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Expected value of \\(\\hat{\\boldsymbol{\\beta}}\\)\nThe least squares estimator \\(\\hat{\\boldsymbol{\\beta}}\\) is an unbiased estimator of \\(\\boldsymbol{\\beta}\\)\n\\[\nE(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}\n\\]\n\n\nNow let’s take a look at the variance"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#variance-of-hatboldsymbolbeta",
    "href": "slides/10-prop-of-estimators.html#variance-of-hatboldsymbolbeta",
    "title": "Properties of estimators",
    "section": "Variance of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Variance of \\(\\hat{\\boldsymbol{\\beta}}\\)\n\\[\n\\begin{aligned}\nVar(\\hat{\\boldsymbol{\\beta}}) &= Var((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}) \\\\[8pt]\n& = \\class{fragment}{[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]Var(\\mathbf{y})[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]^T }\\\\[8pt]\n& = \\class{fragment}{[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]\\sigma^2_{\\epsilon}\\mathbf{I}[\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}]} \\\\[8pt]\n& = \\class{fragment}{\\sigma^2_{\\epsilon}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}]} \\\\[8pt]\n& = \\class{fragment}{\\sigma^2_{\\epsilon}(\\mathbf{X}^T\\mathbf{X})^{-1}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#variance-of-hatboldsymbolbeta-1",
    "href": "slides/10-prop-of-estimators.html#variance-of-hatboldsymbolbeta-1",
    "title": "Properties of estimators",
    "section": "Variance of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Variance of \\(\\hat{\\boldsymbol{\\beta}}\\)\n\\[\nVar(\\hat{\\boldsymbol{\\beta}}) =  \\sigma^2_{\\epsilon}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\\]\nWe will show that \\(\\hat{\\boldsymbol{\\beta}}\\) is the “best” estimator (has the lowest variance) among the class of linear unbiased estimators"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#linear-regression-model",
    "href": "slides/10-prop-of-estimators.html#linear-regression-model",
    "title": "Properties of estimators",
    "section": "“Linear” regression model",
    "text": "“Linear” regression model\nWhat does it mean for a model to be a “linear” regression model?\n\n\nLinear regression models are linear in the parameters, i.e. given an observation \\(y_i\\)\n\\[\ny_i = \\beta_0 + \\beta_1f_1(x_{i1}) +  \\dots + \\beta_pf_p(x_{ip}) + \\epsilon_i\n\\]\nThe functions \\(f_1, \\ldots, f_p\\) can be non-linear as long as \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are linear in \\(Y\\)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#identify-the-linear-regression-model",
    "href": "slides/10-prop-of-estimators.html#identify-the-linear-regression-model",
    "title": "Properties of estimators",
    "section": "Identify the linear regression model",
    "text": "Identify the linear regression model\n🔗 edstem.org/us/courses/62513/discussion/648051"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#identify-the-linear-regression-model-1",
    "href": "slides/10-prop-of-estimators.html#identify-the-linear-regression-model-1",
    "title": "Properties of estimators",
    "section": "Identify the linear regression model",
    "text": "Identify the linear regression model\n\n\n\\(y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i1}^2 + \\beta_3x_{i2}  + \\epsilon_i\\)\n\\(y_i = \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i1}x_{i2} + \\epsilon_i\\)\n\\(y_i = \\beta_0  + \\beta_1\\sin(x_{i1} + \\beta_2x_{i2}) + \\beta_3x_{i3} + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1e^{x_{i1}} + \\beta_2e^{x_{i2}} + \\epsilon_i\\)\n\\(y_i = \\exp(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3}) + \\epsilon_i\\)"
  },
  {
    "objectID": "slides/10-prop-of-estimators.html#recap",
    "href": "slides/10-prop-of-estimators.html#recap",
    "title": "Properties of estimators",
    "section": "Recap",
    "text": "Recap\n\nComputed and interpreted confidence interval for a single coefficient\nShowed some properties of \\(\\hat{\\boldsymbol{\\beta}}\\)\nDefined “linear” model\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#announcements",
    "href": "slides/06-mlr-pt2.html#announcements",
    "title": "Multiple linear regression (MLR)",
    "section": "Announcements",
    "text": "Announcements\n\nLab 01 due on TODAY at 11:59pm\n\nPush work to GitHub repo\nSubmit final PDF on Gradescope + mark pages for each question\n\nHW 01 due Thursday, September 19 at 11:59pm\n\nWill be released after class\n\nTeam labs start on Monday"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#homework",
    "href": "slides/06-mlr-pt2.html#homework",
    "title": "Multiple linear regression (MLR)",
    "section": "Homework",
    "text": "Homework\nHomework will generally be split into two sections:\n\n1️⃣ Conceptual exercises\nThe conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\n\nYou may write the answers and associated work for conceptual exercises by hand or type them in your Quarto document."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#homework-1",
    "href": "slides/06-mlr-pt2.html#homework-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Homework",
    "text": "Homework\n2️⃣ Applied exercises\nThe applied exercises are focused on applying the concepts to analyze data.\nAll work for the applied exercises must be typed in your Quarto document following a reproducible workflow.\nWrite all narrative using complete sentences and include informative axis labels / titles on visualizations."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#topics",
    "href": "slides/06-mlr-pt2.html#topics",
    "title": "Multiple linear regression (MLR)",
    "section": "Topics",
    "text": "Topics\n\nCategorical predictors and interaction terms\nAssess model fit using RSME and \\(R^2\\)\nCompare models using \\(Adj. R^2\\)\nIntroduce LaTex"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#computing-setup",
    "href": "slides/06-mlr-pt2.html#computing-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(viridis) #adjust color palette\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#data-peer-to-peer-lender",
    "href": "slides/06-mlr-pt2.html#data-peer-to-peer-lender",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 4\n   annual_income_th debt_to_income verified_income interest_rate\n              &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1             59           0.558  Not Verified            10.9 \n 2             60           1.31   Not Verified             9.92\n 3             75           1.06   Verified                26.3 \n 4             75           0.574  Not Verified             9.92\n 5            254           0.238  Not Verified             9.43\n 6             67           1.08   Source Verified          9.92\n 7             28.8         0.0997 Source Verified         17.1 \n 8             80           0.351  Not Verified             6.08\n 9             34           0.698  Not Verified             7.97\n10             80           0.167  Source Verified         12.6 \n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#variables",
    "href": "slides/06-mlr-pt2.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income_th: Annual income (in $1000s)\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nResponse: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#response-vs.-predictors",
    "href": "slides/06-mlr-pt2.html#response-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Response vs. predictors",
    "text": "Response vs. predictors\n\nGoal: Use these predictors in a single model to understand variability in interest rate."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#model-fit-in-r",
    "href": "slides/06-mlr-pt2.html#model-fit-in-r",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit in R",
    "text": "Model fit in R\n\nint_fit &lt;- lm(interest_rate ~ debt_to_income + verified_income  + annual_income_th,\n              data = loan50)\n\ntidy(int_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#matrix-form-of-multiple-linear-regression",
    "href": "slides/06-mlr-pt2.html#matrix-form-of-multiple-linear-regression",
    "title": "Multiple linear regression (MLR)",
    "section": "Matrix form of multiple linear regression",
    "text": "Matrix form of multiple linear regression\n\\[\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n=\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_{11} & \\dots & x_{1p}\\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n1 &  x_{n1} & \\dots &x_{np}\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n\\]\n\nHow might we include a categorical predictor with \\(k\\) levels in the design matrix, \\(\\mathbf{X}\\) ?"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#indicator-variables",
    "href": "slides/06-mlr-pt2.html#indicator-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables",
    "text": "Indicator variables\nSuppose we want to predict the amount of sleep a Duke student gets based on whether they are in Pratt (Pratt Yes/ No are the only two options). Consider the model\n\\[\nSleep_i = \\beta_0 + \\beta_1\\mathbf{1}(Pratt_i = \\texttt{Yes}) + \\beta_2\\mathbf{1}(Pratt_i = \\texttt{No})\n\\]\n\n\nWrite out the design matrix for this hypothesized linear model.\nDemonstrate that the design matrix is not of full column rank (that is, affirmatively provide one of the columns in terms of the others).\nUse this intuition to explain why when we include categorical predictors, we cannot include both indicators for every level of the variable and an intercept."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#indicator-variables-1",
    "href": "slides/06-mlr-pt2.html#indicator-variables-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(k\\) levels\nWe can make \\(k\\) indicator variables from the data - one indicator for each level\nAn indicator (dummy) variable takes values 1 or 0\n\n1 if the observation belongs to that level\n0 if the observation does not belong to that level"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#indicator-variables-for-verified_income",
    "href": "slides/06-mlr-pt2.html#indicator-variables-for-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables for verified_income",
    "text": "Indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\n\n\n\n# A tibble: 3 × 4\n  verified_income not_verified source_verified verified\n  &lt;fct&gt;                  &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified               1               0        0\n2 Verified                   0               0        1\n3 Source Verified            0               1        0"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#indicator-variables-in-the-model",
    "href": "slides/06-mlr-pt2.html#indicator-variables-in-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables in the model",
    "text": "Indicator variables in the model\n\nWe will use \\(k-1\\) of the indicator variables in the model.\nThe baseline is the category that doesn’t have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\n\n\n\nloan50 |&gt;\n  select(verified_income, source_verified, verified) |&gt;\n  slice(1, 3, 6)\n\n# A tibble: 3 × 3\n  verified_income source_verified verified\n  &lt;fct&gt;                     &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified                  0        0\n2 Verified                      0        1\n3 Source Verified               1        0\n\n\n\nTake a look at the design matrix in AE 02"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#interpreting-verified_income",
    "href": "slides/06-mlr-pt2.html#interpreting-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\n\n\nThe baseline level is Not verified.\nPeople with source verified income are expected to take a loan with an interest rate that is 2.211% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant.\n\n\n\n\n\nWhat is the expected interest rate for someone whose income is Verified, who has a debt-to-income ratio of 0 and annual income of $0?"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#interaction-terms-1",
    "href": "slides/06-mlr-pt2.html#interaction-terms-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#interest-rate-vs.-annual-income",
    "href": "slides/06-mlr-pt2.html#interest-rate-vs.-annual-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is a potential interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#interaction-term-in-model",
    "href": "slides/06-mlr-pt2.html#interaction-term-in-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nint_fit_2 &lt;- lm(interest_rate ~ debt_to_income + verified_income + annual_income_th + verified_income * annual_income_th,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.560\n2.034\n4.700\n0.000\n\n\ndebt_to_income\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n3.577\n2.539\n1.409\n0.166\n\n\nverified_incomeVerified\n9.923\n3.654\n2.716\n0.009\n\n\nannual_income_th\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_th\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_th\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#interpreting-interaction-terms",
    "href": "slides/06-mlr-pt2.html#interpreting-interaction-terms",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#rmse-r2",
    "href": "slides/06-mlr-pt2.html#rmse-r2",
    "title": "Multiple linear regression (MLR)",
    "section": "RMSE & \\(R^2\\)",
    "text": "RMSE & \\(R^2\\)\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#comparing-models",
    "href": "slides/06-mlr-pt2.html#comparing-models",
    "title": "Multiple linear regression (MLR)",
    "section": "Comparing models",
    "text": "Comparing models\n\n\nWhen comparing models, do we prefer the model with the lower or higher RMSE?\nThough we use \\(R^2\\) to assess the model fit, it is generally unreliable for comparing models with different number of predictors. Why?\n\n\\(R^2\\) will stay the same or increase as we add more variables to the model . Let’s show why this is true.\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#adjusted-r2",
    "href": "slides/06-mlr-pt2.html#adjusted-r2",
    "title": "Multiple linear regression (MLR)",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#r2-and-adjusted-r2",
    "href": "slides/06-mlr-pt2.html#r2-and-adjusted-r2",
    "title": "Multiple linear regression (MLR)",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}\\]\n\n\n\\[R^2_{adj} = 1 - \\frac{SSR/(n-p-1)}{SST/(n-1)}\\]\nwhere\n\n\\(n\\) is the number of observations used to fit the model\n\\(p\\) is the number of terms (not including the intercept) in the model"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#using-r2-and-adjusted-r2",
    "href": "slides/06-mlr-pt2.html#using-r2-and-adjusted-r2",
    "title": "Multiple linear regression (MLR)",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables\n\n\n\n📋 https://sta221-fa24.netlify.app/ae/ae-02-mlr"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#latex-in-this-class",
    "href": "slides/06-mlr-pt2.html#latex-in-this-class",
    "title": "Multiple linear regression (MLR)",
    "section": "Latex in this class",
    "text": "Latex in this class\nFor this class you will need to be able to…\n\nProperly write mathematical symbols, e.g., \\(\\beta_1\\) not B1, \\(R^2\\) not R2\nWrite basic regression equations, e.g., \\(\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\nWrite matrix equations: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\)\nWrite hypotheses (we’ll start this next week), e.g., \\(H_0: \\beta = 0\\)\n\nYou are welcome to but not required to write math proofs using LaTex."
  },
  {
    "objectID": "slides/06-mlr-pt2.html#recap",
    "href": "slides/06-mlr-pt2.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nInterpreted categorical predictors and interaction terms\nAssessed model fit using RSME and \\(R^2\\)\nCompared models using \\(Adj. R^2\\)\nIntroduced LaTex"
  },
  {
    "objectID": "slides/06-mlr-pt2.html#next-class",
    "href": "slides/06-mlr-pt2.html#next-class",
    "title": "Multiple linear regression (MLR)",
    "section": "Next class",
    "text": "Next class\n\nGeometric interpretation\nInference for regression\nSee Sep 17 prepare\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#announcements",
    "href": "slides/03-slr-model-assessment.html#announcements",
    "title": "SLR: Model Assessment",
    "section": "Announcements",
    "text": "Announcements\n\nOffice hours start this week. See schedule on Overview page of the course website or on Canvas."
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#topics",
    "href": "slides/03-slr-model-assessment.html#topics",
    "title": "SLR: Model Assessment",
    "section": "Topics",
    "text": "Topics\n\nUse R to conduct exploratory data analysis and fit a model\nEvaluate models using RMSE and \\(R^2\\)\nUse analysis of variance to partition variability in the response variable"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#computing-set-up",
    "href": "slides/03-slr-model-assessment.html#computing-set-up",
    "title": "SLR: Model Assessment",
    "section": "Computing set up",
    "text": "Computing set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling (includes broom, yardstick, and other packages)\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme for ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#data-houses-in-duke-forest",
    "href": "slides/03-slr-model-assessment.html#data-houses-in-duke-forest",
    "title": "SLR: Model Assessment",
    "section": "Data: Houses in Duke Forest",
    "text": "Data: Houses in Duke Forest\n\n\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\nSource: openintro::duke_forest\n\n\n\n\nGoal: Use the area (in square feet) to understand variability in the price of houses in Duke Forest."
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#clone-repo-start-new-rstudio-project",
    "href": "slides/03-slr-model-assessment.html#clone-repo-start-new-rstudio-project",
    "title": "SLR: Model Assessment",
    "section": "Clone repo + Start new RStudio project",
    "text": "Clone repo + Start new RStudio project\n\nGo to the course organization. Click on the repo with the prefix ae-01. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File → New Project → Version Control → Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#two-statistics",
    "href": "slides/03-slr-model-assessment.html#two-statistics",
    "title": "SLR: Model Assessment",
    "section": "Two statistics",
    "text": "Two statistics\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\n\n\nWhat indicates a good model fit? Higher or lower RMSE? Higher or lower \\(R^2\\)?"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#rmse",
    "href": "slides/03-slr-model-assessment.html#rmse",
    "title": "SLR: Model Assessment",
    "section": "RMSE",
    "text": "RMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}}\n\\]\n\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nThe value of RMSE is more useful for comparing across models than evaluating a single model (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#analysis-of-variance-anova",
    "href": "slides/03-slr-model-assessment.html#analysis-of-variance-anova",
    "title": "SLR: Model Assessment",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of Variance (ANOVA): Technique to partition variability in \\(Y\\) by the sources of variability"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#total-variability-response",
    "href": "slides/03-slr-model-assessment.html#total-variability-response",
    "title": "SLR: Model Assessment",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\n\n\n\n\nMin\nMedian\nMax\nMean\nStd.Dev\n\n\n\n\n95000\n540000\n1520000\n559898.7\n225448.1"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#partition-sources-of-variability-in-price",
    "href": "slides/03-slr-model-assessment.html#partition-sources-of-variability-in-price",
    "title": "SLR: Model Assessment",
    "section": "Partition sources of variability in price",
    "text": "Partition sources of variability in price"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#total-variability-response-1",
    "href": "slides/03-slr-model-assessment.html#total-variability-response-1",
    "title": "SLR: Model Assessment",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\\[\\text{Sum of Squares Total (SST)} = \\sum_{i=1}^n(y_i - \\bar{y})^2 = (n-1)s_y^2\\]"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#explained-variability-model",
    "href": "slides/03-slr-model-assessment.html#explained-variability-model",
    "title": "SLR: Model Assessment",
    "section": "Explained variability (Model)",
    "text": "Explained variability (Model)\n\n\\[\\text{Sum of Squares Model (SSM)} = \\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#unexplained-variability-residuals",
    "href": "slides/03-slr-model-assessment.html#unexplained-variability-residuals",
    "title": "SLR: Model Assessment",
    "section": "Unexplained variability (Residuals)",
    "text": "Unexplained variability (Residuals)\n\n\\[\\text{Sum of Squares Residuals (SSR)} = \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2\\]"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#sum-of-squares",
    "href": "slides/03-slr-model-assessment.html#sum-of-squares",
    "title": "SLR: Model Assessment",
    "section": "Sum of Squares",
    "text": "Sum of Squares\n\n\\[\n\\begin{aligned}\n\\color{#407E99}{SST} \\hspace{5mm}&= &\\color{#993399}{SSM} &\\hspace{5mm} +  &\\color{#8BB174}{SSR} \\\\[10pt]\n\\color{#407E99}{\\sum_{i=1}^n(y_i - \\bar{y})^2} \\hspace{5mm}&= &\\color{#993399}{\\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2} &\\hspace{5mm}+ &\\color{#8BB174}{\\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#r2",
    "href": "slides/03-slr-model-assessment.html#r2",
    "title": "SLR: Model Assessment",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nThe coefficient of determination \\(R^2\\) is the proportion of variation in the response, \\(Y\\), that is explained by the regression model\n\n\\[\\large{R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}}\\]\n\n\nWhat is the range of \\(R^2\\)? Does \\(R^2\\) have units?"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#interpreting-r2",
    "href": "slides/03-slr-model-assessment.html#interpreting-r2",
    "title": "SLR: Model Assessment",
    "section": "Interpreting $R^2$",
    "text": "Interpreting $R^2$\n\nQuestionSubmit\n\n\n\nSubmit your response to the following question on Ed Discussion.\n\nThe \\(R^2\\) of the model for price from area of houses in Duke Forest is 44.5%. Which of the following is the correct interpretation of this value?\n\nArea correctly predicts 44.5% of price for houses in Duke Forest.\n44.5% of the variability in price for houses in Duke Forest can be explained by area.\n44.5% of the variability in area for houses in Duke Forest can be explained by price.\n44.5% of the time price for houses in Duke Forest can be predicted by area.\n\nDo you think this model is useful for explaining variability in the price of Duke Forest houses?\n\n\n\n\n\n\n\n\n🔗 https://edstem.org/us/courses/62513/discussion/629888"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#augmented-data-frame",
    "href": "slides/03-slr-model-assessment.html#augmented-data-frame",
    "title": "SLR: Model Assessment",
    "section": "Augmented data frame",
    "text": "Augmented data frame\nUse the augment() function from the broom package to add columns for predicted values, residuals, and other observation-level model statistics\n\n\nduke_forest_aug &lt;- augment(duke_forest_fit)\nduke_forest_aug\n\n# A tibble: 98 × 8\n     price  area  .fitted  .resid   .hat  .sigma   .cooksd .std.resid\n     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 1520000  6040 1079931. 440069. 0.133  162605. 0.604         2.80  \n 2 1030000  4475  830340. 199660. 0.0435 168386. 0.0333        1.21  \n 3  420000  1745  394951.  25049. 0.0226 169664. 0.000260      0.150 \n 4  680000  2091  450132. 229868. 0.0157 168011. 0.0150        1.37  \n 5  428500  1772  399257.  29243. 0.0220 169657. 0.000345      0.175 \n 6  456000  1950  427645.  28355. 0.0182 169659. 0.000266      0.170 \n 7 1270000  3909  740072. 529928. 0.0250 160502. 0.130         3.18  \n 8  557450  2841  569744. -12294. 0.0102 169679. 0.0000277    -0.0732\n 9  697500  3924  742465. -44965. 0.0254 169620. 0.000948     -0.270 \n10  650000  2173  463209. 186791. 0.0145 168582. 0.00912       1.11  \n# ℹ 88 more rows"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#finding-rmse-in-r",
    "href": "slides/03-slr-model-assessment.html#finding-rmse-in-r",
    "title": "SLR: Model Assessment",
    "section": "Finding RMSE in R",
    "text": "Finding RMSE in R\nUse the rmse() function from the yardstick package (part of tidymodels)\n\nrmse(duke_forest_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     167067.\n\n\n\n\nDo you think this model is useful for predicting the price of Duke Forest houses?"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#finding-r2-in-r",
    "href": "slides/03-slr-model-assessment.html#finding-r2-in-r",
    "title": "SLR: Model Assessment",
    "section": "Finding \\(R^2\\) in R",
    "text": "Finding \\(R^2\\) in R\nUse the rsq() function from the yardstick package (part of tidymodels)\n\nrsq(duke_forest_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.445\n\n\n\n\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(duke_forest_fit)$r.squared\n\n[1] 0.4451945"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#recap",
    "href": "slides/03-slr-model-assessment.html#recap",
    "title": "SLR: Model Assessment",
    "section": "Recap",
    "text": "Recap\n\nUsed R to conduct exploratory data analysis and fit a model\nEvaluated models using RMSE and \\(R^2\\)\nUsed analysis of variance to partition variability in the response variable"
  },
  {
    "objectID": "slides/03-slr-model-assessment.html#next-class",
    "href": "slides/03-slr-model-assessment.html#next-class",
    "title": "SLR: Model Assessment",
    "section": "Next class",
    "text": "Next class\n\nMatrix representation of simple linear regression\n\nSee Sep 5 prepare\n\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#announcements",
    "href": "slides/07-mlr-pt3.html#announcements",
    "title": "ANOVA + Geometric interpretation",
    "section": "Announcements",
    "text": "Announcements\n\nLab 02 due on Thursday at 11:59pm\n\nPush work to GitHub repo\nSubmit final PDF on Gradescope + select all team members + mark pages for each question\n\nHW 01 due Thursday at 11:59pm\n\nNote submission instructions"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#homework-submission",
    "href": "slides/07-mlr-pt3.html#homework-submission",
    "title": "ANOVA + Geometric interpretation",
    "section": "Homework submission",
    "text": "Homework submission\nIf you write your responses to Exercises 1 - 4 by hand, you will need to combine your written work to the completed PDF for Exercises 5 - 10 before submitting on Gradescope.\nInstructions to combine PDFs:\n\nPreview (Mac): support.apple.com/guide/preview/combine-pdfs-prvw43696/mac\nAdobe (Mac or PC): helpx.adobe.com/acrobat/using/merging-files-single-pdf.html\n\nGet free access to Adobe Acrobat as a Duke student: oit.duke.edu/help/articles/kb0030141/"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#latex-in-this-class",
    "href": "slides/07-mlr-pt3.html#latex-in-this-class",
    "title": "ANOVA + Geometric interpretation",
    "section": "Latex in this class",
    "text": "Latex in this class\nFor this class you will need to be able to…\n\nProperly write mathematical symbols, e.g., \\(\\beta_1\\) not B1, \\(R^2\\) not R2\nWrite basic regression equations, e.g., \\(\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\nWrite matrix equations: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\)\nWrite hypotheses (we’ll start this next week), e.g., \\(H_0: \\beta = 0\\)\n\nYou are welcome to but not required to write math proofs using LaTex."
  },
  {
    "objectID": "slides/07-mlr-pt3.html#topics",
    "href": "slides/07-mlr-pt3.html#topics",
    "title": "ANOVA + Geometric interpretation",
    "section": "Topics",
    "text": "Topics\n\nCompare models using Adjusted \\(R^2\\)\nIntroduce the ANOVA table\nUse a geometric interpretation to find the least squares estimates"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#computing-setup",
    "href": "slides/07-mlr-pt3.html#computing-setup",
    "title": "ANOVA + Geometric interpretation",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(viridis) #adjust color palette\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#data-peer-to-peer-lender",
    "href": "slides/07-mlr-pt3.html#data-peer-to-peer-lender",
    "title": "ANOVA + Geometric interpretation",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 4\n   annual_income_th debt_to_income verified_income interest_rate\n              &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1             59           0.558  Not Verified            10.9 \n 2             60           1.31   Not Verified             9.92\n 3             75           1.06   Verified                26.3 \n 4             75           0.574  Not Verified             9.92\n 5            254           0.238  Not Verified             9.43\n 6             67           1.08   Source Verified          9.92\n 7             28.8         0.0997 Source Verified         17.1 \n 8             80           0.351  Not Verified             6.08\n 9             34           0.698  Not Verified             7.97\n10             80           0.167  Source Verified         12.6 \n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#variables",
    "href": "slides/07-mlr-pt3.html#variables",
    "title": "ANOVA + Geometric interpretation",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income_th: Annual income (in $1000s)\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nResponse: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#model-fit-in-r",
    "href": "slides/07-mlr-pt3.html#model-fit-in-r",
    "title": "ANOVA + Geometric interpretation",
    "section": "Model fit in R",
    "text": "Model fit in R\n\nint_fit &lt;- lm(interest_rate ~ debt_to_income + verified_income  + annual_income_th, data = loan50)\n\nint_fit2 &lt;- lm(interest_rate ~ debt_to_income + verified_income  + annual_income_th + verified_income * annual_income_th, data = loan50)"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#rmse-r2",
    "href": "slides/07-mlr-pt3.html#rmse-r2",
    "title": "ANOVA + Geometric interpretation",
    "section": "RMSE & \\(R^2\\)",
    "text": "RMSE & \\(R^2\\)\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#comparing-models",
    "href": "slides/07-mlr-pt3.html#comparing-models",
    "title": "ANOVA + Geometric interpretation",
    "section": "Comparing models",
    "text": "Comparing models\n\n\nThough we use \\(R^2\\) to assess the model fit, it is generally unreliable for comparing models with different number of predictors. Why?\n\n\\(R^2\\) will stay the same or increase as we add more variables to the model . Let’s show why this is true.\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables."
  },
  {
    "objectID": "slides/07-mlr-pt3.html#adjusted-r2",
    "href": "slides/07-mlr-pt3.html#adjusted-r2",
    "title": "ANOVA + Geometric interpretation",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#r2-and-adjusted-r2",
    "href": "slides/07-mlr-pt3.html#r2-and-adjusted-r2",
    "title": "ANOVA + Geometric interpretation",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}\\]\n\n\n\\[R^2_{adj} = 1 - \\frac{SSR/(n-p-1)}{SST/(n-1)}\\]\nwhere\n\n\\(n\\) is the number of observations used to fit the model\n\\(p\\) is the number of terms (not including the intercept) in the model"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#compare-models",
    "href": "slides/07-mlr-pt3.html#compare-models",
    "title": "ANOVA + Geometric interpretation",
    "section": "Compare models",
    "text": "Compare models\nWhich model would you select int_fit (main effects only) or int_fit2 (main effects + interaction) based on…\n\\(R^2\\)\n\nglance(int_fit)$r.squared\n\n[1] 0.279854\n\nglance(int_fit2)$r.squared\n\n[1] 0.2963437\n\n\n\n\\(Adj. R^2\\)\n\nglance(int_fit)$adj.r.squared\n\n[1] 0.215841\n\nglance(int_fit2)$adj.r.squared\n\n[1] 0.1981591"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#anova-table",
    "href": "slides/07-mlr-pt3.html#anova-table",
    "title": "ANOVA + Geometric interpretation",
    "section": "ANOVA table",
    "text": "ANOVA table\n\n\n\nSource\nSum of squares\nDF\nMean square\nF\n\n\n\n\nModel\n\\(\\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2\\)\n\\(p\\)\n\\(SSM / p\\)\n\\(MSM / MSR\\)\n\n\nResidual\n\\(\\sum_{i=1}^n(y_i- \\hat{y}_i)^2\\)\n\\(n - p - 1\\)\n\\(SSR / (n - p - 1)\\)\n\n\n\nTotal\n\\(\\sum_{i = 1}^n(y_i - \\bar{y})^2\\)\n\\(n - 1\\)\n\n\n\n\n\n\n\n\nThe degrees of freedom (df) are the number of independent pieces of information used to calculate a statistic.\nMean square (MS) is the sum of squares divided by the associated degrees of freedom."
  },
  {
    "objectID": "slides/07-mlr-pt3.html#using-r2-and-adjusted-r2",
    "href": "slides/07-mlr-pt3.html#using-r2-and-adjusted-r2",
    "title": "ANOVA + Geometric interpretation",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression",
    "href": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression",
    "title": "ANOVA + Geometric interpretation",
    "section": "Geometry of least squares regression",
    "text": "Geometry of least squares regression\n\n\nLet \\(\\text{Col}(\\mathbf{X})\\) be the column space of \\(\\mathbf{X}\\): the set all possible linear combinations (span) of the columns of \\(\\mathbf{X}\\)\nThe vector of responses \\(\\mathbf{y}\\) is not in \\(\\text{Col}(\\mathbf{X})\\).\nGoal: Find another vector \\(\\mathbf{z} = \\mathbf{Xb}\\) that is in \\(\\text{Col}(\\mathbf{X})\\) and is as close as possible to \\(\\mathbf{y}\\).\n\n\\(\\mathbf{z}\\) is called a projection of \\(\\mathbf{y}\\) onto \\(\\text{Col}(\\mathbf{X})\\) ."
  },
  {
    "objectID": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression-1",
    "href": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression-1",
    "title": "ANOVA + Geometric interpretation",
    "section": "Geometry of least squares regression",
    "text": "Geometry of least squares regression\n\n\nFor any \\(\\mathbf{z} = \\mathbf{Xb}\\) in \\(\\text{Col}(\\mathbf{X})\\), the vector \\(\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}\\) is the difference between \\(\\mathbf{y}\\) and \\(\\mathbf{Xb}\\).\n\nIn other words, we want to minimize \\(||\\mathbf{e}||^2 = ||\\mathbf{y} - \\mathbf{Xb}||^2\\)\n\nThis is minimized for the \\(\\mathbf{b}\\) ( we’ll call it \\(\\hat{\\boldsymbol{\\beta}}\\) ) that makes \\(\\mathbf{e}\\) orthogonal to \\(\\text{Col}(\\mathbf{X})\\)\nRecall: If \\(\\mathbf{e}\\) is orthogonal to \\(\\text{Col}(\\mathbf{X})\\), then the inner product of any vector in \\(\\text{Col}(\\mathbf{X})\\) and \\(\\mathbf{e}\\) is 0 \\(\\Rightarrow \\mathbf{X}^T\\mathbf{e} = \\mathbf{0}\\)"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression-2",
    "href": "slides/07-mlr-pt3.html#geometry-of-least-squares-regression-2",
    "title": "ANOVA + Geometric interpretation",
    "section": "Geometry of least squares regression",
    "text": "Geometry of least squares regression\n\nTherefore, we have\n\n\\[\n\\mathbf{X}^T(\\mathbf{y} - \\mathbf{Xb}) = \\mathbf{0}\n\\]\nLet’s solve for \\(\\mathbf{b}\\) to get the least squares estimate."
  },
  {
    "objectID": "slides/07-mlr-pt3.html#recap",
    "href": "slides/07-mlr-pt3.html#recap",
    "title": "ANOVA + Geometric interpretation",
    "section": "Recap",
    "text": "Recap\n\nCompared models using Adjusted \\(R^2\\)\nIntroduced the ANOVA table\nUsed a geometric interpretation to find the least squares estimates"
  },
  {
    "objectID": "slides/07-mlr-pt3.html#next-class",
    "href": "slides/07-mlr-pt3.html#next-class",
    "title": "ANOVA + Geometric interpretation",
    "section": "Next class",
    "text": "Next class\n\nInference for regression\nSee Sep 19 prepare\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/08-inference.html#announcements",
    "href": "slides/08-inference.html#announcements",
    "title": "Inference for regression",
    "section": "Announcements",
    "text": "Announcements\n\nLab 02 due on TODAY at 11:59pm\nHW 01 due TODAY at 11:59pm\nStatistics experience due Tue, Nov 26 at 11:59pm"
  },
  {
    "objectID": "slides/08-inference.html#statistics-experience",
    "href": "slides/08-inference.html#statistics-experience",
    "title": "Inference for regression",
    "section": "Statistics experience",
    "text": "Statistics experience\nGoal: Engage with statistics / data science outside the classroom and connect your experience with what you’re learning in the course.\nWhat: Have a statistics experience + create a slide reflecting on the experience. Counts as a homework grade.\nWhen: Must do the activity this semester. Reflection due Tuesday, November 26 at 11:59pm\nFor more info: sta221-fa24.netlify.app/hw/stats-experience"
  },
  {
    "objectID": "slides/08-inference.html#reminder-course-policies-about-assignments",
    "href": "slides/08-inference.html#reminder-course-policies-about-assignments",
    "title": "Inference for regression",
    "section": "Reminder: course policies about assignments",
    "text": "Reminder: course policies about assignments\n\nLate work\n\nHW and labs accepted up to 2 days late.\n5% deduction for each 24-hour period the assignment is late.\n\nOne time late waiver\n\nCan use on HW and individual labs\n\nLowest HW and lowest lab grade dropped at the end of the semester."
  },
  {
    "objectID": "slides/08-inference.html#reminder-course-policies-about-assignments-1",
    "href": "slides/08-inference.html#reminder-course-policies-about-assignments-1",
    "title": "Inference for regression",
    "section": "Reminder: course policies about assignments",
    "text": "Reminder: course policies about assignments\n\nRead the feedback on Gradescope carefully! If you have questions about the comments, ask a member of the teaching team during office hours or before/after class.\nRegrade requests\n\nOpened 1 day after assignment is returned and due within 1 week\nOnly submit regrade request if there is an error in the grading not to dispute points or ask questions about grading.\nProf. Tackett or Kat (Head TA) will regrade the entire exercise being disputed, which could potentially result in a lower grade."
  },
  {
    "objectID": "slides/08-inference.html#poll-office-hours-availability",
    "href": "slides/08-inference.html#poll-office-hours-availability",
    "title": "Inference for regression",
    "section": "Poll: Office hours availability",
    "text": "Poll: Office hours availability"
  },
  {
    "objectID": "slides/08-inference.html#topics",
    "href": "slides/08-inference.html#topics",
    "title": "Inference for regression",
    "section": "Topics",
    "text": "Topics\n\nUnderstand statistical inference in the context of regression\nDescribe the assumptions for regression\nUnderstand connection between distribution of residuals and inferential procedures\nConduct inference on a single coefficient"
  },
  {
    "objectID": "slides/08-inference.html#computing-setup",
    "href": "slides/08-inference.html#computing-setup",
    "title": "Inference for regression",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(kableExtra)  \nlibrary(patchwork)   \n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "slides/08-inference.html#data-ncaa-football-expenditures",
    "href": "slides/08-inference.html#data-ncaa-football-expenditures",
    "title": "Inference for regression",
    "section": "Data: NCAA Football expenditures",
    "text": "Data: NCAA Football expenditures\nToday’s data come from Equity in Athletics Data Analysis and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a March 2022 Tidy Tuesday.\nWe will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :\n\ntotal_exp_m: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\nenrollment_th: Total student enrollment in the 2019 - 2020 academic year (in thousands)\ntype: institution type (Public or Private)\n\n\nfootball &lt;- read_csv(\"data/ncaa-football-exp.csv\")"
  },
  {
    "objectID": "slides/08-inference.html#univariate-eda",
    "href": "slides/08-inference.html#univariate-eda",
    "title": "Inference for regression",
    "section": "Univariate EDA",
    "text": "Univariate EDA"
  },
  {
    "objectID": "slides/08-inference.html#bivariate-eda",
    "href": "slides/08-inference.html#bivariate-eda",
    "title": "Inference for regression",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA"
  },
  {
    "objectID": "slides/08-inference.html#regression-model",
    "href": "slides/08-inference.html#regression-model",
    "title": "Inference for regression",
    "section": "Regression model",
    "text": "Regression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type, data = football)\ntidy(exp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n\n\n\n\n\n\nFor every additional 1,000 students, we expect the institution’s total expenditures on football to increase by $780,000, on average, holding institution type constant."
  },
  {
    "objectID": "slides/08-inference.html#from-sample-to-population",
    "href": "slides/08-inference.html#from-sample-to-population",
    "title": "Inference for regression",
    "section": "From sample to population",
    "text": "From sample to population\n\nFor every additional 1,000 students, we expect the institution’s total expenditures on football to increase by $780,000, on average, holding institution type constant.\n\n\n\n\nThis estimate is valid for the single sample of 127 higher education institutions in the 2019 - 2020 academic year.\nBut what if we’re not interested quantifying the relationship between student enrollment, institution type, and football expenditures for this single sample?\nWhat if we want to say something about the relationship between these variables for all colleges and universities with football programs and across different years?"
  },
  {
    "objectID": "slides/08-inference.html#statistical-inference",
    "href": "slides/08-inference.html#statistical-inference",
    "title": "Inference for regression",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\n\nStatistical inference provides methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be representative (ideally random) of the population we’re interested in\n\n\n\n\n\nImage source: Eugene Morgan © Penn State"
  },
  {
    "objectID": "slides/08-inference.html#inference-for-linear-regression",
    "href": "slides/08-inference.html#inference-for-linear-regression",
    "title": "Inference for regression",
    "section": "Inference for linear regression",
    "text": "Inference for linear regression\n\nInference based on ANOVA\n\nHypothesis test for the statistical significance of the overall regression model\nHypothesis test for a subset of coefficients\n\nInference for a single coefficient \\(\\beta_j\\)\n\nHypothesis test for a coefficient \\(\\beta_j\\)\nConfidence interval for a coefficient \\(\\beta_j\\)"
  },
  {
    "objectID": "slides/08-inference.html#linear-regression-model",
    "href": "slides/08-inference.html#linear-regression-model",
    "title": "Inference for regression",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\n\\begin{aligned}\n\\mathbf{y} &= Model + Error \\\\[5pt]\n&= f(\\mathbf{X}) + \\boldsymbol{\\epsilon} \\\\[5pt]\n&= E(\\mathbf{y}|\\mathbf{X}) + \\mathbf{\\epsilon} \\\\[5pt]\n&= \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\n\\end{aligned}\n\\]\n\n\n\nWe have discussed multiple ways to find the least squares estimates of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\\\beta_1\\end{bmatrix}\\)\n\nNone of these approaches depend on the distribution of \\(\\boldsymbol{\\epsilon}\\)\n\nNow we will use statistical inference to draw conclusions about \\(\\boldsymbol{\\beta}\\) that depend on particular assumptions about the distribution of \\(\\boldsymbol{\\epsilon}\\)"
  },
  {
    "objectID": "slides/08-inference.html#linear-regression-model-1",
    "href": "slides/08-inference.html#linear-regression-model-1",
    "title": "Inference for regression",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\\begin{aligned}\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\hspace{8mm} \\boldsymbol{\\epsilon} \\sim N(0, \\sigma^2_{\\epsilon}\\mathbf{I})\n\\end{aligned}\n\\]\nsuch that the errors are independent and normally distributed.\n\n\nIndependent: Knowing the error term for one observation doesn’t tell you anything about the error term for another observation\nNormally distributed: Tell us the shape of the distribution of residuals\n\n\nWhat else do we know about the distribution of the residuals based on this equation?"
  },
  {
    "objectID": "slides/08-inference.html#describing-random-phenomena",
    "href": "slides/08-inference.html#describing-random-phenomena",
    "title": "Inference for regression",
    "section": "Describing random phenomena",
    "text": "Describing random phenomena\n\n\nThere is some uncertainty in the residuals (and the predicted responses), so we use mathematical models to describe that uncertainty.\nSome terminology:\n\nSample space: Set of all possible outcomes\nRandom variable: Function (mapping) from the sample space onto real numbers\nEvent: Subset of the sample space, i.e., a set of possible outcomes (possible values the random variable can take)\nProbability distribution function: Mathematical function that produces probability of occurrences for events in the sample space"
  },
  {
    "objectID": "slides/08-inference.html#example",
    "href": "slides/08-inference.html#example",
    "title": "Inference for regression",
    "section": "Example",
    "text": "Example\nSuppose we are tossing 2 fair coins with sides heads (H) and tails (T)\n\n\nSample space: {HH, HT, TH, TT}\nRandom variable: \\(X\\) : The number of heads in two coin tosses\nEvent: We flip two coins and get 1 head\nProbability distribution function: \\[P(X = x_i) = {2 \\choose x_i}0.5^{x_i}{0.5}^{2-x_i}\\]\nNow we can find \\[P(X = 1) = {2 \\choose 1}0.5^1{0.5}^{2-1} = 0.5\\]"
  },
  {
    "objectID": "slides/08-inference.html#mathematical-representation",
    "href": "slides/08-inference.html#mathematical-representation",
    "title": "Inference for regression",
    "section": "Mathematical representation",
    "text": "Mathematical representation\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/08-inference.html#expected-value-of-mathbfy",
    "href": "slides/08-inference.html#expected-value-of-mathbfy",
    "title": "Inference for regression",
    "section": "Expected value of \\(\\mathbf{y}\\)",
    "text": "Expected value of \\(\\mathbf{y}\\)\nLet \\(\\mathbf{b} = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\b_p\\end{bmatrix}\\) be a \\(p \\times 1\\) vector of random variables.\n\n\nThen \\(E(\\mathbf{b}) = E\\begin{bmatrix}b_1 \\\\ \\vdots \\\\ b_p\\end{bmatrix} = \\begin{bmatrix}E(b_1) \\\\ \\vdots \\\\ E(b_p)\\end{bmatrix}\\)\n\n\n\n\nUse this to find \\(E(\\mathbf{y}|\\mathbf{X})\\)."
  },
  {
    "objectID": "slides/08-inference.html#variance",
    "href": "slides/08-inference.html#variance",
    "title": "Inference for regression",
    "section": "Variance",
    "text": "Variance\nLet \\(\\mathbf{b} = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\b_p\\end{bmatrix}\\) be a \\(p \\times 1\\) vector of independent random variables.\n\n\nThen \\(Var(\\mathbf{b}) = \\begin{bmatrix}Var(b_1) & 0 & \\dots & 0 \\\\ 0 & Var(b_2) & \\dots & 0 \\\\ \\vdots & \\vdots & \\dots & \\cdot \\\\ 0 & 0 & \\dots & Var(b_p)\\end{bmatrix}\\)\n\n\n\n\nUse this to find \\(Var(\\mathbf{y}|\\mathbf{X})\\)."
  },
  {
    "objectID": "slides/08-inference.html#assumptions-of-regression",
    "href": "slides/08-inference.html#assumptions-of-regression",
    "title": "Inference for regression",
    "section": "Assumptions of regression",
    "text": "Assumptions of regression\n\n\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n\\]\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\n\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from one another."
  },
  {
    "objectID": "slides/08-inference.html#estimating-sigma2_epsilon",
    "href": "slides/08-inference.html#estimating-sigma2_epsilon",
    "title": "Inference for regression",
    "section": "Estimating \\(\\sigma^2_{\\epsilon}\\)",
    "text": "Estimating \\(\\sigma^2_{\\epsilon}\\)\n\nOnce we fit the model, we can use the residuals to estimate \\(\\sigma_{\\epsilon}^2\\)\n\\(\\hat{\\sigma}^2_{\\epsilon}\\) is needed for hypothesis testing and constructing confidence intervals for regression\n\n\\[\n\\hat{\\sigma}^2_\\epsilon = \\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-p-1} = \\frac{\\sum_\\limits{i=1}^ne_i^2}{n - p - 1} = \\frac{SSR}{n - p - 1}\n\\]\n\n\nThe regression standard error \\(\\hat{\\sigma}_{\\epsilon}\\) is a measure of the average distance between the observations and regression line\n\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{SSR}{n - p - 1}}\n\\]"
  },
  {
    "objectID": "slides/08-inference.html#inference-for-beta_j",
    "href": "slides/08-inference.html#inference-for-beta_j",
    "title": "Inference for regression",
    "section": "Inference for \\(\\beta_j\\)",
    "text": "Inference for \\(\\beta_j\\)\nWe often want to conduct inference on individual model coefficients\n\nHypothesis test: Is there a linear relationship between the response and \\(x_j\\)?\nConfidence interval: What is a plausible range of values \\(\\beta_j\\) can take?\n\n\nBut first we need to understand the distribution of \\(\\hat{\\beta}_j\\)"
  },
  {
    "objectID": "slides/08-inference.html#sampling-distribution-of-hatbeta",
    "href": "slides/08-inference.html#sampling-distribution-of-hatbeta",
    "title": "Inference for regression",
    "section": "Sampling distribution of \\(\\hat{\\beta}\\)",
    "text": "Sampling distribution of \\(\\hat{\\beta}\\)\n\nA sampling distribution is the probability distribution of a statistic based on a large number of random samples of size \\(n\\) from a population\nThe sampling distribution of \\(\\hat{\\boldsymbol{\\beta}}\\) is the probability distribution of the estimated coefficients if we repeatedly took samples of size \\(n\\) and fit the regression model\n\n\\[\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon(\\mathbf{X}^T\\mathbf{X})^{-1})\n\\]\n\nThe estimated coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) are normally distributed with\n\\[\nE(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta} \\hspace{10mm} Var(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2_{\\epsilon}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\n\\]"
  },
  {
    "objectID": "slides/08-inference.html#sampling-distribution-of-hatbeta_j",
    "href": "slides/08-inference.html#sampling-distribution-of-hatbeta_j",
    "title": "Inference for regression",
    "section": "Sampling distribution of \\(\\hat{\\beta}_j\\)",
    "text": "Sampling distribution of \\(\\hat{\\beta}_j\\)\n\\[\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon(\\mathbf{X}^T\\mathbf{X})^{-1})\n\\]\nLet \\(\\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\). Then, for each coefficient \\(\\hat{\\beta}_j\\),\n\n\n\\(E(\\hat{\\beta}_j) = \\boldsymbol{\\beta}_j\\), the \\(j^{th}\\) element of \\(\\boldsymbol{\\beta}\\)\n\\(Var(\\hat{\\beta}_j) = \\sigma^2_{\\epsilon}C_{jj}\\)\n\\(Cov(\\hat{\\beta}_i, \\hat{\\beta}_j) = \\sigma^2_{\\epsilon}C_{ij}\\)"
  },
  {
    "objectID": "slides/08-inference.html#steps-for-a-hypothesis-test",
    "href": "slides/08-inference.html#steps-for-a-hypothesis-test",
    "title": "Inference for regression",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState the null and alternative hypotheses.\nCalculate a test statistic.\nCalculate the p-value.\nState the conclusion."
  },
  {
    "objectID": "slides/08-inference.html#hypothesis-test-for-beta_j-hypotheses",
    "href": "slides/08-inference.html#hypothesis-test-for-beta_j-hypotheses",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Hypotheses",
    "text": "Hypothesis test for \\(\\beta_j\\): Hypotheses\nWe will generally test the hypotheses:\n\\[\n\\begin{aligned}\n&H_0: \\beta_j = 0 \\\\\n&H_a: \\beta_j \\neq 0\n\\end{aligned}\n\\]\n\nState these hypotheses in words."
  },
  {
    "objectID": "slides/08-inference.html#hypothesis-test-for-beta_j-test-statistic",
    "href": "slides/08-inference.html#hypothesis-test-for-beta_j-test-statistic",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Test statistic",
    "text": "Hypothesis test for \\(\\beta_j\\): Test statistic\nTest statistic: Number of standard errors the estimate is away from the null\n\\[\n\\text{Test Statstic} = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n\\]\n\nIf \\(\\sigma^2_{\\epsilon}\\) was known, the test statistic would be\n\\[Z = \\frac{\\hat{\\beta}_j - 0}{SE(\\hat{\\beta}_j)} ~ = ~\\frac{\\hat{\\beta}_j - 0}{\\sqrt{\\sigma^2_\\epsilon C_{jj}}} ~\\sim ~ N(0, 1)\n\\]\n\n\nIn general, \\(\\sigma^2_{\\epsilon}\\) is not known, so we use \\(\\hat{\\sigma}_{\\epsilon}^2\\) to calculate \\(SE(\\hat{\\beta}_j)\\)\n\\[T = \\frac{\\hat{\\beta}_j - 0}{SE(\\hat{\\beta}_j)} ~ = ~\\frac{\\hat{\\beta}_j - 0}{\\sqrt{\\hat{\\sigma}^2_\\epsilon C_{jj}}} ~\\sim ~ t_{n-p-1}\n\\]"
  },
  {
    "objectID": "slides/08-inference.html#hypothesis-test-for-beta_j-test-statistic-1",
    "href": "slides/08-inference.html#hypothesis-test-for-beta_j-test-statistic-1",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Test statistic",
    "text": "Hypothesis test for \\(\\beta_j\\): Test statistic\n\nThe test statistic \\(T\\) follows a \\(t\\) distribution with \\(n - p -1\\) degrees of freedom.\nWe need to account for the additional variability introduced by calculating \\(SE(\\hat{\\beta}_j)\\) using an estimated value instead of a constant"
  },
  {
    "objectID": "slides/08-inference.html#t-vs.-n01",
    "href": "slides/08-inference.html#t-vs.-n01",
    "title": "Inference for regression",
    "section": "t vs. N(0,1)",
    "text": "t vs. N(0,1)\n\n\nFigure 1: Standard normal vs. t distributions"
  },
  {
    "objectID": "slides/08-inference.html#hypothesis-test-for-beta_j-p-value",
    "href": "slides/08-inference.html#hypothesis-test-for-beta_j-p-value",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): P-value",
    "text": "Hypothesis test for \\(\\beta_j\\): P-value\nThe p-value is the probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\np-value = P(|t| &gt; |\\text{test statistic}|),\n\\]\ncalculated from a \\(t\\) distribution with \\(n- p - 1\\) degrees of freedom\n\n\nWhy do we take into account “extreme” on both the high and low ends?"
  },
  {
    "objectID": "slides/08-inference.html#understanding-the-p-value",
    "href": "slides/08-inference.html#understanding-the-p-value",
    "title": "Inference for regression",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/08-inference.html#hypothesis-test-for-beta_j-conclusion",
    "href": "slides/08-inference.html#hypothesis-test-for-beta_j-conclusion",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Conclusion",
    "text": "Hypothesis test for \\(\\beta_j\\): Conclusion\nThere are two parts to the conclusion\n\nMake a conclusion by comparing the p-value to a predetermined decision-making threshold called the significance level ( \\(\\alpha\\) level)\n\nIf \\(\\text{P-value} &lt; \\alpha\\): Reject \\(H_0\\)\nIf \\(\\text{P-value} \\geq \\alpha\\): Fail to reject \\(H_0\\)\n\nState the conclusion in the context of the data"
  },
  {
    "objectID": "slides/08-inference.html#recap",
    "href": "slides/08-inference.html#recap",
    "title": "Inference for regression",
    "section": "Recap",
    "text": "Recap\n\nIntroduced statistical inference in the context of regression\nDescribed the assumptions for regression\nConnected the distribution of residuals and inferential procedures\nConducted inference on a single coefficient\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/02-slr.html#announcements",
    "href": "slides/02-slr.html#announcements",
    "title": "Simple linear regression",
    "section": "",
    "text": "No labs on Mon, Sep 2 (Labor Day)\nApplication exercises start Tue, Sep 3\n\nBring fully-charged laptop or device with keyboard\nMake sure you have accepted invite to GitHub course organization\n\nSee website for resources to learn / review R\nOffice hours start Tue, Sep 3"
  },
  {
    "objectID": "slides/02-slr.html#topics",
    "href": "slides/02-slr.html#topics",
    "title": "Simple linear regression",
    "section": "Topics",
    "text": "Topics\n\nHow regression is used to understand the relationship between multiple variables\nLeast squares estimation for the slope and intercept\nInterpret the slope and intercept\nPredict the response given a value of the predictor"
  },
  {
    "objectID": "slides/02-slr.html#computing-set-up",
    "href": "slides/02-slr.html#computing-set-up",
    "title": "Simple linear regression",
    "section": "Computing set up",
    "text": "Computing set up\n\n# load packages\nlibrary(tidyverse)        # for data wrangling\nlibrary(broom)            # for formatting regression output\nlibrary(fivethirtyeight)  # for the fandango dataset\nlibrary(knitr)            # for formatting tables\nlibrary(patchwork)        # for arranging graphs\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)\n\n\n\n\n\nSource: R for Data Science with additions from The Art of Statistics: How to Learn from Data.\n\n\n\n\n\n\nSource:R for Data Science"
  },
  {
    "objectID": "slides/02-slr.html#movie-scores",
    "href": "slides/02-slr.html#movie-scores",
    "title": "Simple linear regression",
    "section": "Movie scores",
    "text": "Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/02-slr.html#data-prep",
    "href": "slides/02-slr.html#data-prep",
    "title": "Simple linear regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/02-slr.html#data-overview",
    "href": "slides/02-slr.html#data-overview",
    "title": "Simple linear regression",
    "section": "Data overview",
    "text": "Data overview\n\nglimpse(movie_scores)\n\nRows: 146\nColumns: 23\n$ film                       &lt;chr&gt; \"Avengers: Age of Ultron\", \"Cinderella\", \"A…\n$ year                       &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ critics                    &lt;int&gt; 74, 85, 80, 18, 14, 63, 42, 86, 99, 89, 84,…\n$ audience                   &lt;int&gt; 86, 80, 90, 84, 28, 62, 53, 64, 82, 87, 77,…\n$ metacritic                 &lt;int&gt; 66, 67, 64, 22, 29, 50, 53, 81, 81, 80, 71,…\n$ metacritic_user            &lt;dbl&gt; 7.1, 7.5, 8.1, 4.7, 3.4, 6.8, 7.6, 6.8, 8.8…\n$ imdb                       &lt;dbl&gt; 7.8, 7.1, 7.8, 5.4, 5.1, 7.2, 6.9, 6.5, 7.4…\n$ fandango_stars             &lt;dbl&gt; 5.0, 5.0, 5.0, 5.0, 3.5, 4.5, 4.0, 4.0, 4.5…\n$ fandango_ratingvalue       &lt;dbl&gt; 4.5, 4.5, 4.5, 4.5, 3.0, 4.0, 3.5, 3.5, 4.0…\n$ rt_norm                    &lt;dbl&gt; 3.70, 4.25, 4.00, 0.90, 0.70, 3.15, 2.10, 4…\n$ rt_user_norm               &lt;dbl&gt; 4.30, 4.00, 4.50, 4.20, 1.40, 3.10, 2.65, 3…\n$ metacritic_norm            &lt;dbl&gt; 3.30, 3.35, 3.20, 1.10, 1.45, 2.50, 2.65, 4…\n$ metacritic_user_nom        &lt;dbl&gt; 3.55, 3.75, 4.05, 2.35, 1.70, 3.40, 3.80, 3…\n$ imdb_norm                  &lt;dbl&gt; 3.90, 3.55, 3.90, 2.70, 2.55, 3.60, 3.45, 3…\n$ rt_norm_round              &lt;dbl&gt; 3.5, 4.5, 4.0, 1.0, 0.5, 3.0, 2.0, 4.5, 5.0…\n$ rt_user_norm_round         &lt;dbl&gt; 4.5, 4.0, 4.5, 4.0, 1.5, 3.0, 2.5, 3.0, 4.0…\n$ metacritic_norm_round      &lt;dbl&gt; 3.5, 3.5, 3.0, 1.0, 1.5, 2.5, 2.5, 4.0, 4.0…\n$ metacritic_user_norm_round &lt;dbl&gt; 3.5, 4.0, 4.0, 2.5, 1.5, 3.5, 4.0, 3.5, 4.5…\n$ imdb_norm_round            &lt;dbl&gt; 4.0, 3.5, 4.0, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5…\n$ metacritic_user_vote_count &lt;int&gt; 1330, 249, 627, 31, 88, 34, 17, 124, 62, 54…\n$ imdb_user_vote_count       &lt;int&gt; 271107, 65709, 103660, 3136, 19560, 39373, …\n$ fandango_votes             &lt;int&gt; 14846, 12640, 12055, 1793, 1021, 397, 252, …\n$ fandango_difference        &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…"
  },
  {
    "objectID": "slides/02-slr.html#univariate-exploratory-data-analysis-eda",
    "href": "slides/02-slr.html#univariate-exploratory-data-analysis-eda",
    "title": "Simple linear regression",
    "section": "Univariate exploratory data analysis (EDA)",
    "text": "Univariate exploratory data analysis (EDA)\nThe data set contains the “Tomatometer” score (critics) and audience score (audience) for 146 movies rated on rottentomatoes.com."
  },
  {
    "objectID": "slides/02-slr.html#bivariate-eda",
    "href": "slides/02-slr.html#bivariate-eda",
    "title": "Simple linear regression",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA"
  },
  {
    "objectID": "slides/02-slr.html#bivariate-eda-1",
    "href": "slides/02-slr.html#bivariate-eda-1",
    "title": "Simple linear regression",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA\nGoal: Fit a line to describe the relationship between the critics score and audience score."
  },
  {
    "objectID": "slides/02-slr.html#why-fit-a-line",
    "href": "slides/02-slr.html#why-fit-a-line",
    "title": "Simple linear regression",
    "section": "Why fit a line?",
    "text": "Why fit a line?\nWe fit a line to accomplish one or both of the following:\n\n. . .\n\nPrediction\n\n\nWhat is an example of a prediction question for this data set?\n\n\n. . .\n\nInference\n\n\nWhat is an example of an inference question for this data set?"
  },
  {
    "objectID": "slides/02-slr.html#terminology",
    "href": "slides/02-slr.html#terminology",
    "title": "Simple linear regression",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nResponse, \\(Y\\): variable describing the outcome of interest\nPredictor, \\(X\\): variable we use to help understand the variability in the response"
  },
  {
    "objectID": "slides/02-slr.html#regression-model",
    "href": "slides/02-slr.html#regression-model",
    "title": "Simple linear regression",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the response, \\(Y\\), and the predictor, \\(X\\).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{f(X)} + \\epsilon \\\\[8pt]\n& = \\color{black}{E(Y|X)} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\mu_{Y|X}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-slr.html#regression-model-1",
    "href": "slides/02-slr.html#regression-model-1",
    "title": "Simple linear regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{purple}{f(X)} + \\epsilon \\\\[8pt]\n&= \\color{purple}{E(Y|X)} + \\epsilon \\\\[8pt]\n&= \\color{purple}{\\mu_{Y|X}} + \\epsilon \\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\(E(Y|X) = \\mu_{Y|X}\\), the mean value of \\(Y\\) given a particular value of \\(X\\)."
  },
  {
    "objectID": "slides/02-slr.html#regression-model-2",
    "href": "slides/02-slr.html#regression-model-2",
    "title": "Simple linear regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\color{blue}{\\textbf{Error}} \\\\[8pt]\n&= \\color{purple}{f(X)} + \\color{blue}{\\epsilon}\\\\[8pt]\n&= \\color{purple}{E(Y|X)} + \\color{blue}{\\epsilon}\\\\[8pt]\n&= \\color{purple}{\\mu_{Y|X}} + \\color{blue}{\\epsilon} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-slr.html#determine-fx",
    "href": "slides/02-slr.html#determine-fx",
    "title": "Simple linear regression",
    "section": "Determine \\(f(X)\\)",
    "text": "Determine \\(f(X)\\)\n\nGoal: Determine \\(f(X)\\)\nHow do we determine \\(f(X)\\)\n\nMake an assumption about the functional form \\(f(X)\\) (parametric model)\nUse the data to fit a model based on that form"
  },
  {
    "objectID": "slides/02-slr.html#slr-statistical-model-population",
    "href": "slides/02-slr.html#slr-statistical-model-population",
    "title": "Simple linear regression",
    "section": "SLR: Statistical model (population)",
    "text": "SLR: Statistical model (population)\nWhen we have a quantitative response, \\(Y\\), and a single quantitative predictor, \\(X\\), we can use a simple linear regression model to describe the relationship between \\(Y\\) and \\(X\\). \\[\\large{Y = \\mathbf{\\beta_0 + \\beta_1 X} + \\epsilon}, \\hspace{8mm} \\epsilon \\sim N(0, \\sigma_{\\epsilon}^2)\\]\n. . .\n\n\\(\\beta_1\\): Population (true) slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): Population (true) intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "slides/02-slr.html#slr-regression-equation-sample",
    "href": "slides/02-slr.html#slr-regression-equation-sample",
    "title": "Simple linear regression",
    "section": "SLR: Regression equation (sample)",
    "text": "SLR: Regression equation (sample)\n\\[\\Large{\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X}\\]\n\n\\(\\hat{\\beta}_1\\): Estimated (sample) slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{\\beta}_0\\): Estimated (sample) intercept of the relationship between \\(X\\) and \\(Y\\)\nNo error term!"
  },
  {
    "objectID": "slides/02-slr.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "href": "slides/02-slr.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "title": "Simple linear regression",
    "section": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)",
    "text": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)"
  },
  {
    "objectID": "slides/02-slr.html#residuals",
    "href": "slides/02-slr.html#residuals",
    "title": "Simple linear regression",
    "section": "Residuals",
    "text": "Residuals\n\n\n\n\n\n\n\n\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]"
  },
  {
    "objectID": "slides/02-slr.html#least-squares-line",
    "href": "slides/02-slr.html#least-squares-line",
    "title": "Simple linear regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[e_i = \\text{observed} - \\text{predicted}\n= y_i - \\hat{y}_i\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals\n\nClick here for full calculations."
  },
  {
    "objectID": "slides/02-slr.html#properties-of-least-squares-regression",
    "href": "slides/02-slr.html#properties-of-least-squares-regression",
    "title": "Simple linear regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\)\nThe slope has the same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}\\)\nThe sum of the residuals is approximately zero: \\(\\sum_{i = 1}^n e_i \\approx 0\\)\nThe residuals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/02-slr.html#estimating-the-slope",
    "href": "slides/02-slr.html#estimating-the-slope",
    "title": "Simple linear regression",
    "section": "Estimating the slope",
    "text": "Estimating the slope\n\\[\\large{\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}}\\]\n\n. . .\n\\[\n\\begin{aligned} s_X = 30.1688  \\hspace{15mm} &s_Y =  20.0244 \\hspace{15mm} r  = 0.7814 \\\\[10pt]\\hat{\\beta}_1  &= 0.7814 \\times \\frac{20.0244}{30.1688} \\\\&= \\mathbf{0.5187}\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-slr.html#estimating-the-intercept",
    "href": "slides/02-slr.html#estimating-the-intercept",
    "title": "Simple linear regression",
    "section": "Estimating the intercept",
    "text": "Estimating the intercept\n\\[\\large{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}}\\]\n\n. . .\n\\[\n\\begin{aligned}\\bar{x} = 60.8493 & \\hspace{15mm} \\bar{y} = 63.8767 \\hspace{15mm} \\hat{\\beta}_1 = 0.5187 \\\\[10pt]\n\\hat{\\beta}_0 &= 63.8767 - 0.5187 \\times 60.8493 \\\\\n&= \\mathbf{32.3142}\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-slr.html#interpretation",
    "href": "slides/02-slr.html#interpretation",
    "title": "Simple linear regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nQuestionSubmit\n\n\n\n\nSubmit your answers to the following questions on Ed Discussion:\n\nThe slope of the model for predicting audience score from critics score is 0.5187 . Which of the following is the best interpretation of this value?\n32.3142 is the predicted mean audience score for what type of movies?\n\n\n\n\n\n\n\n\n\n🔗 https://edstem.org/us/courses/62513/discussion/5181157"
  },
  {
    "objectID": "slides/02-slr.html#does-it-make-sense-to-interpret-the-intercept",
    "href": "slides/02-slr.html#does-it-make-sense-to-interpret-the-intercept",
    "title": "Simple linear regression",
    "section": "Does it make sense to interpret the intercept?",
    "text": "Does it make sense to interpret the intercept?\n. . .\n✅ The intercept is meaningful in the context of the data if\n\nthe predictor can feasibly take values equal to or near zero, or\nthere are values near zero in the observed data.\n\n. . .\n🛑 Otherwise, the intercept may not be meaningful!"
  },
  {
    "objectID": "slides/02-slr.html#making-a-prediction",
    "href": "slides/02-slr.html#making-a-prediction",
    "title": "Simple linear regression",
    "section": "Making a prediction",
    "text": "Making a prediction\nSuppose that a movie has a critics score of 70. According to this model, what is the movie’s predicted audience score?\n\\[\\begin{aligned}\n\\widehat{\\text{audience}} &= 32.3142 + 0.5187 \\times \\text{critics} \\\\\n&= 32.3142 + 0.5187 \\times 70 \\\\\n&= \\mathbf{68.6232}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-slr.html#fit-the-model",
    "href": "slides/02-slr.html#fit-the-model",
    "title": "Simple linear regression",
    "section": "Fit the model",
    "text": "Fit the model\nUse the lm() function to fit a linear regression model\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\nmovie_fit\n\n\nCall:\nlm(formula = audience ~ critics, data = movie_scores)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187"
  },
  {
    "objectID": "slides/02-slr.html#tidy-results",
    "href": "slides/02-slr.html#tidy-results",
    "title": "Simple linear regression",
    "section": "Tidy results",
    "text": "Tidy results\nUse the tidy() function from the broom R package to “tidy” the data\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\ntidy(movie_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/02-slr.html#format-results",
    "href": "slides/02-slr.html#format-results",
    "title": "Simple linear regression",
    "section": "Format results",
    "text": "Format results\nUse the kable() function from the knitr package to neatly format the results\n\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\ntidy(movie_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n32.316\n2.343\n13.795\n0\n\n\ncritics\n0.519\n0.035\n15.028\n0"
  },
  {
    "objectID": "slides/02-slr.html#prediction-1",
    "href": "slides/02-slr.html#prediction-1",
    "title": "Simple linear regression",
    "section": "Prediction",
    "text": "Prediction\nUse the predict() function to calculate predictions for new observations\n\nSingle observation\n\nnew_movie &lt;- tibble(critics = 70)\npredict(movie_fit, new_movie)\n\n       1 \n68.62297 \n\n\n\n. . .\nMultiple observations\n\nmore_new_movies &lt;- tibble(critics = c(24,70, 85))\npredict(movie_fit, more_new_movies)\n\n       1        2        3 \n44.76379 68.62297 76.40313"
  },
  {
    "objectID": "slides/02-slr.html#recap",
    "href": "slides/02-slr.html#recap",
    "title": "Simple linear regression",
    "section": "Recap",
    "text": "Recap\n\nDescribed how regression is used to understand the relationship between multiple variables\nUsed least squares to estimate the slope and intercept\nInterpreted the slope and intercept for simple linear regression\nPredicted the response given a value of the predictor"
  },
  {
    "objectID": "slides/02-slr.html#next-time",
    "href": "slides/02-slr.html#next-time",
    "title": "Simple linear regression",
    "section": "Next time",
    "text": "Next time\n\nModel assessment for simple linear regression\n\nSee Sep 3 prepare\n\nBring fully-charged laptop or device with keyboard for in-class application exercise (AE)"
  },
  {
    "objectID": "slides/lab-01.html#getting-started",
    "href": "slides/lab-01.html#getting-started",
    "title": "Lab 01",
    "section": "Getting started",
    "text": "Getting started\nAsk your TA if\n\nYou do not have a lab-01 repo in the GitHub course organization: github.com/sta221-fa24\nYou need help cloning the repo and starting a new RStudio project"
  },
  {
    "objectID": "slides/lab-01.html#tips-for-working-on-lab",
    "href": "slides/lab-01.html#tips-for-working-on-lab",
    "title": "Lab 01",
    "section": "Tips for working on lab",
    "text": "Tips for working on lab\n\nYou do not have to finish the lab in class, they will always be due Thursdays at 11:59pm. One work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when a TA can help you on the spot and leave the narrative writing until later.\nDo not pressure each other to finish early (particularly once you start working on teams); use the time wisely to really learn the material and produce a quality report."
  },
  {
    "objectID": "slides/lab-01.html#workflow-and-formatting",
    "href": "slides/lab-01.html#workflow-and-formatting",
    "title": "Lab 01",
    "section": "Workflow and formatting",
    "text": "Workflow and formatting\nPart of the lab grade is for “workflow and formatting” assessing the reproducible workflow and document format. This includes\n\nHaving at least 3 informative commit messages (practicing version control)\n\nThere are markers in Lab 01 to help you incorporate version control in your workflow\n\nThe PDF is neatly organized document with clear exercise headings and readable code and narrative\nThe name (first and last) and date are updated at the top of the document."
  },
  {
    "objectID": "slides/lab-01.html#when-youre-done-with-lab",
    "href": "slides/lab-01.html#when-youre-done-with-lab",
    "title": "Lab 01",
    "section": "When you’re done with lab",
    "text": "When you’re done with lab\n\nMake sure all your final changes have been pushed to your GitHub repo\nSubmit your final PDF to Gradescope\n\nAccess Gradescope through the course Canvas site\nMark the pages associated with each exercise."
  },
  {
    "objectID": "slides/lab-01.html#lab-01-park-access",
    "href": "slides/lab-01.html#lab-01-park-access",
    "title": "Lab 01",
    "section": "Lab 01: Park access",
    "text": "Lab 01: Park access\nToday’s lab focuses on exploratory data analysis and simple linear regression, content from Weeks 01 and 02 in the course.\n🔗 sta221-fa24.netlify.app/labs/lab-01.html\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/lab-03.html#goals",
    "href": "slides/lab-03.html#goals",
    "title": "Lab 03",
    "section": "Goals",
    "text": "Goals\n\nTeam project\nLab 03: Palmer penguins"
  },
  {
    "objectID": "slides/lab-03.html#lab-03-palmer-penguins",
    "href": "slides/lab-03.html#lab-03-palmer-penguins",
    "title": "Lab 03",
    "section": "Lab 03: Palmer penguins",
    "text": "Lab 03: Palmer penguins\nLab 03 focuses on\n\nusing linear regression and statistical inference to draw conclusions about penguins living in Palmer Archipelago in Antarctica.\nuse the data to check conditions about the distribution of the model residuals.\n\nUse this week to get started on the lab. We will continue discussing statistical inference in this week’s lectures, so this lab will be due on Thursday, October 3, 2024.\n🔗 https://sta221-fa24.netlify.app/labs/lab-03"
  },
  {
    "objectID": "slides/lab-03.html#final-team-project",
    "href": "slides/lab-03.html#final-team-project",
    "title": "Lab 03",
    "section": "Final Team Project",
    "text": "Final Team Project\nGoal: Use the methods from STA 221 to analyze data and answer a research question developed by your team\nPrimary deliverables:\n\nan in-person presentation about the exploratory data analysis and initial modeling\na written, reproducible final report detailing your analysis\na GitHub repository containing all work from the project\n\nSubmission: All work for the project will be submitted in your team’s GitHub repo. You will receive feedback via an Issue on GitHub to model a workflow often used in practice."
  },
  {
    "objectID": "slides/lab-03.html#final-team-project-1",
    "href": "slides/lab-03.html#final-team-project-1",
    "title": "Lab 03",
    "section": "Final team project",
    "text": "Final team project\nMilestones: There are periodic project milestones throughout the semester to help you work towards the final deliverables:\n\nResearch questions (today’s lab)\nProject proposal (next week’s lab)\nExploratory data analysis draft\nPresentation + Presentation comments\nAnalysis draft + peer review\nRound 1 submission (optional)\nWritten report\nReproducibility + organization\n\nSee the Final Project Instructions for a timeline and details for each milestone."
  },
  {
    "objectID": "slides/lab-03.html#today-research-questions",
    "href": "slides/lab-03.html#today-research-questions",
    "title": "Lab 03",
    "section": "Today: Research questions",
    "text": "Today: Research questions\nGoal: Develop three potential research questions your team may be interested in investigating.\nYou do not need to have a data set at this point\nFull instructions here: sta221-fa24.netlify.app/project#research-questions"
  },
  {
    "objectID": "slides/lab-03.html#reminder-team-workflow",
    "href": "slides/lab-03.html#reminder-team-workflow",
    "title": "Lab 03",
    "section": "Reminder: Team workflow",
    "text": "Reminder: Team workflow\n\nOnly one team member should type at a time. There are markers in today’s lab to help you determine whose turn it is to type.\n\nEvery team member should still be engaged in discussion for all questions, even if it’s not your turn type.\n\nDon’t forget to pull to get your teammates’ updates before making changes to the .qmd file.\n\n\n\n\n\n\nImportant\n\n\nOnly one submission per team on Gradescope. Read the submission instructions carefully!"
  },
  {
    "objectID": "slides/lab-03.html#reminder-tips-for-working-on-a-team",
    "href": "slides/lab-03.html#reminder-tips-for-working-on-a-team",
    "title": "Lab 03",
    "section": "Reminder: Tips for working on a team",
    "text": "Reminder: Tips for working on a team\n\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report.\nThe labs are structured to help you learn the steps of a data analysis. Do not split up the lab among the team members; work on it together in its entirety.\nEveryone has something to contribute! Use the lab groups as an opportunity to share ideas and learn from each other.\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\ntopic\nprepare\nslides\nae\nhw\nnotes\n\n\n\n\n1\nTh\nJan 9\nWelcome + What Is Bayesian Health Data Science?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW00 assigned\n\n\n2\nTu\nJan 14\nMonte Carlo Sampling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 16\nMarkov Chain Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\nHW 01 assigned, HW00 due\n\n\n3\nTu\nJan 21\nProbabilistic Programming (Intro to Stan!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 23\nPriors, Posteriors, and PPDs!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\nTu\nJan 28\nModel Checking\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nJan 30\nModel Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nHW 02 assigned, HW 01 due\n\n\n5\nTu\nFeb 4\nBayesian Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 6\nNonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\nTu\nFeb 11\nRobust Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 13\nRegularization\n\n\n\n\n\n\n\n\n\n\n\n\nHW 03 assigned, HW 02 due\n\n\n7\nTu\nFeb 18\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 20\nMulticlass Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\nTu\nFeb 25\nMissing Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nFeb 27\nHierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\nExam 01 assigned, HW03 due\n\n\n9\nTu\nMar 4\nExam 1 Office Hours\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 6\nGaussian Processes\n\n\n\n\n\n\n\n\n\n\n\n\nHW04 assigned, Exam 01 due\n\n\n10\nTu\nMar 11\nNO CLASS: Spring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 13\nNO CLASS: Spring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\nTu\nMar 18\nLongitudinal Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nMar 20\nGeospatial Modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\nTu\nMar 25\nDisease Mapping\n\n\n\n\n\n\n\n\n\n\n\n\nHW 05 assigned, HW04 due\n\n\n\nTh\nMar 27\nBayesian Meta-Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\nTu\nApr 1\nScalable Gaussian Processes #1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nApr 3\nScalable Gaussian Processes #2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\nTu\nApr 8\nBayesian Clustering\n\n\n\n\n\n\n\n\n\n\n\n\nHW 05 due, Exam 02 assigned\n\n\n\nTh\nApr 10\nTBD Topic/Exam Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\nTu\nApr 15\nExam 02 Office Hours\n\n\n\n\n\n\n\n\n\n\n\n\nExam 02 due for feedback\n\n\nExam period\nTBD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExam 02 due",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "Below are freely available resources to learn or review the following in R: data wrangling, data visualization, Quarto basics.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#in-depth-introduction",
    "href": "computing-r-resources.html#in-depth-introduction",
    "title": "Resources for learning R",
    "section": "In-depth introduction",
    "text": "In-depth introduction\nCoursera: Data Visualization and Transformation with R by Mine Çetinkaya-Rundel and Elijah Meyer\n\nIncludes videos, readings, practice exercise, quizzes, and other resources\nYou can select content within the modules you want to complete.\nFocus on Modules 2 and 3. Review the content in Module 1 as needed.\nClick here for instructions to register for Coursera for free as a Duke student",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#in-depth-review",
    "href": "computing-r-resources.html#in-depth-review",
    "title": "Resources for learning R",
    "section": "In-depth review",
    "text": "In-depth review\nData Science with R videos by Mine Çetinkaya-Rundel and Elijah Meyer\n\nVideos from the data science Coursera course\nFocus on videos on visualizing and summarizing data\nYou need to join the Coursera course to access the files from the code along videos.\n\nLearn R: An interactive introduction to data analysis with R\n\nHands-on tutorial that can be completed within the site (no RStudio required)\nFocus on Chapters 4 - 6",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#shorter-review",
    "href": "computing-r-resources.html#shorter-review",
    "title": "Resources for learning R",
    "section": "Shorter review",
    "text": "Shorter review\nR for Data Science (2nd ed) by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund\n\nFocus on Chapters 1 - 3, 10",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nTidy Modeling with R by Max Kuhn & Julia Silge\nPosit Cheatsheets\nR workshops by Duke Center for Data and Visualization Sciences",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "ae/ae-02-mlr.html",
    "href": "ae/ae-02-mlr.html",
    "title": "AE 02: Multiple linear regression",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-02 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class to submit your AE.\n\n\n\nPackages\n\nlibrary(tidyverse)   \nlibrary(tidymodels)   \nlibrary(openintro)    \nlibrary(knitr)       \n\n\n\nData\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\nWe will focus on the following variables:\n\nannual_income_th: Annual income (in $1000s)\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\ninterest_rate: Interest rate for the loan\n\nThe goal of this analysis is to use the annual income, debt-to-income ratio, and income verification to understand variability in the interest rate on the loan.\nWe’ll start with data prep to rescale annual income to $1000’s and recode verified_income to fix an issue with the underlying data.\n\nloan50 &lt;- loan50 |&gt;\n   mutate(annual_income_th = annual_income / 1000, \n          verified_income = \n            case_when(verified_income == \"Not Verified\" ~ \"Not Verified\",\n                      verified_income == \"Source Verified\" ~ \"Source Verified\",\n                      verified_income == \"Verified\" ~ \"Verified\"),\n          verified_income = as_factor(verified_income)\n   )                    \n\n\nglimpse(loan50)\n\nRows: 50\nColumns: 19\n$ state                   &lt;fct&gt; NJ, CA, SC, CA, OH, IN, NY, MO, FL, FL, MD, HI…\n$ emp_length              &lt;dbl&gt; 3, 10, NA, 0, 4, 6, 2, 10, 6, 3, 8, 10, 10, 2,…\n$ term                    &lt;dbl&gt; 60, 36, 36, 36, 60, 36, 36, 36, 60, 60, 36, 36…\n$ homeownership           &lt;fct&gt; rent, rent, mortgage, rent, mortgage, mortgage…\n$ annual_income           &lt;dbl&gt; 59000, 60000, 75000, 75000, 254000, 67000, 288…\n$ verified_income         &lt;fct&gt; Not Verified, Not Verified, Verified, Not Veri…\n$ debt_to_income          &lt;dbl&gt; 0.55752542, 1.30568333, 1.05628000, 0.57434667…\n$ total_credit_limit      &lt;int&gt; 95131, 51929, 301373, 59890, 422619, 349825, 1…\n$ total_credit_utilized   &lt;int&gt; 32894, 78341, 79221, 43076, 60490, 72162, 2872…\n$ num_cc_carrying_balance &lt;int&gt; 8, 2, 14, 10, 2, 4, 1, 3, 10, 4, 3, 4, 3, 2, 3…\n$ loan_purpose            &lt;fct&gt; debt_consolidation, credit_card, debt_consolid…\n$ loan_amount             &lt;int&gt; 22000, 6000, 25000, 6000, 25000, 6400, 3000, 1…\n$ grade                   &lt;fct&gt; B, B, E, B, B, B, D, A, A, C, D, A, A, A, A, E…\n$ interest_rate           &lt;dbl&gt; 10.90, 9.92, 26.30, 9.92, 9.43, 9.92, 17.09, 6…\n$ public_record_bankrupt  &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ loan_status             &lt;fct&gt; Current, Current, Current, Current, Current, C…\n$ has_second_income       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ total_income            &lt;dbl&gt; 59000, 60000, 75000, 75000, 254000, 67000, 288…\n$ annual_income_th        &lt;dbl&gt; 59.0, 60.0, 75.0, 75.0, 254.0, 67.0, 28.8, 80.…\n\n\n\n\nCategorical predictors\n\n\n\n\n\n\nExercise 1\n\n\n\nLet’s take a look at the design matrix for the model with predictors debt_to_income, annual_income_th, and verified_income.\nHow does R choose the baseline level by default?\n\n\n\n## add code here\n\n[Add response here]\n\n\n\n\n\n\nExercise 2\n\n\n\nFit the model with the predictors debt_to_income, annual_income_th, verified_income , and the interaction between annual_income_th and verified_income.\nNeatly display the model results using 3 digits.\n\n\n\n# add code here\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nWrite the estimated regression equation for the people with Not Verified income.\nWrite the estimated regression equation for people with Verified income.\n\n\n\n[add response here]\n\n\n\n\n\n\nExercise 4\n\n\n\nIn general, how do\n\nindicators for categorical predictors impact the model equation?\ninteraction terms impact the model equation?\n\n\n\n[Add response here]\n\n\nModel assessment\n\n\n\n\n\n\nExercise 5\n\n\n\nLet’s compare the original model without interaction effects to the model you fit in Exercise 2.\nCalculate \\(R^2\\) and \\(Adj. R^2\\) for each model. You can find \\(Adj. R^2\\) from the glance function:\nglance(model_name)$adj.r.squared\n\n\n\nint_fit &lt;- lm(interest_rate ~ debt_to_income + annual_income_th +\n                verified_income, data = loan50)\n\n\n# add code here\n\n\n\n\n\n\n\nExercise 6\n\n\n\nWhich model would you choose based on\n\n\\(R^2\\)?\n\\(Adj. R^2\\)?\n\n\n\n[add response here]\n\n\nLaTex\nSometimes, you will need to include mathematical notation in your document. There are two ways you can display mathematics in your document:\nInline: Your mathematics will display within the line of text.\n\nUse $ to start and end your LaTex syntax. You can also use the menu: Insert -&gt; LaTex Math -&gt; Inline Math.\nExample: The text The simple linear regression model is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ produces\n\nThe simple linear regression model is \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\)\n\n\nDisplayed: Your mathematics will display outside the line of text\n\nUse a $$ to start and end your LaTex syntax. You can also use the menu: Insert -&gt; LaTex Math -&gt; Display Math.\nExample: The text The estimated regression equation is $$\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$$produces\n\nThe estimated regression equation is\n\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nClick here for a quick reference of LaTex code.\n\n\n\n\nSubmission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "ae/ae-03-inference.html",
    "href": "ae/ae-03-inference.html",
    "title": "AE 03: Inference",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-03 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class to submit your AE.\n\n\n\nSet up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\nfootball &lt;- read_csv(\"data/ncaa-football-exp.csv\")\n\n\n\nData\n\n\nRegression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type, data = football)\n\ntidy(exp_fit)|&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n\n\n\n\n\n\n\nHypothesis test\nWe want to conduct a hypothesis test to determine if there is a linear relationship between enrollment and football expenditures after accounting for institution type.\nWe’ll start by getting estimates for statistics we’ll need for inference.\n\n\n\n\n\n\nExercise 1\n\n\n\nWe will use the vector of responses \\(\\mathbf{y}\\) and the design matrix \\(\\mathbf{X}\\) to calculate the values needed for inference.\nGet \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) from the football data frame. What are their dimensions?\n\n\n\n# add code here\n\n\n\n\n\n\n\nExercise 2\n\n\n\nNext, let’s calculate \\(\\hat{\\sigma}_\\epsilon^2\\) the estimate. Use \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) from the previous exercise to calculate this value.\n\n\n\n## add code here\n\n\n\n\n\n\n\nExercise 3\n\n\n\nNow we’re ready to conduct the hypothesis test. State the null and alternative hypotheses in words and using mathematical notation.\n\n\n. . .\n\n\n\n\n\n\nExercise 4\n\n\n\nCalculate \\(SE(\\beta_j)\\), then use this value to calculate the test statistic for the hypothesis test.\n\n\n\n## add code here\n\n\n\n\n\n\n\nExercise 5\n\n\n\nNow we need to calculate p-value to help make our final conclusion.\n\nState the distribution used to calculate the p-value.\nFill in the code below to calculate the p-value. Remove #| eval: false once you’ve filled in the code.\n\n\n\n\npt([test-statistic], [df], lower.tail = FALSE)\n\n\n\n\n\n\n\nExercise 6\n\n\n\nState your conclusion in the context of the data. Use a threshold of \\(\\alpha = 0.05\\).\n\n\n. . .\n\n\n\n\n\n\nSubmission\n\n\n\nTo submit the AE:\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "prepare/prepare-sep5.html",
    "href": "prepare/prepare-sep5.html",
    "title": "Prepare for September 5 lecture",
    "section": "",
    "text": "Review linear algebra concepts (as needed)\n\nMatrices and vectors: [slides][video]\nMatrix-Vector products: [slides][video]\nVector geometry: [slides][video]\nMatrix multiplication: [slides][video]\n\n\n\n\n\n\n\nNote\n\n\n\nAll linear algebra review materials from Math 218: Matrices and Vectors (Summer 2024) taught by Dr. Brian Fitzpatrick at Duke University"
  },
  {
    "objectID": "prepare/prepare-sep12.html",
    "href": "prepare/prepare-sep12.html",
    "title": "Prepare for September 12 lecture",
    "section": "",
    "text": "📖 Read Multiple Linear Regression\n✅ Review Vector Geometry [slides][video]1\n🎥: Watch Geometric interpretation of least squares"
  },
  {
    "objectID": "prepare/prepare-sep12.html#footnotes",
    "href": "prepare/prepare-sep12.html#footnotes",
    "title": "Prepare for September 12 lecture",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom Math 218: Matrices and Vectors (Summer 2024) taught by Dr. Brian Fitzpatrick at Duke University↩︎"
  },
  {
    "objectID": "prepare/prepare-sep10.html",
    "href": "prepare/prepare-sep10.html",
    "title": "Prepare for September 10 lecture",
    "section": "",
    "text": "Review class notes and readings on simple linear regression.\nWe will extend what we’ve done thus far to multiple linear regression, with 2 or more predictors."
  },
  {
    "objectID": "hw/hw-02.html",
    "href": "hw/hw-02.html",
    "title": "HW 02",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#instructions",
    "href": "hw/hw-02.html#instructions",
    "title": "HW 02: Multiple linear regression",
    "section": "Instructions",
    "text": "Instructions\nThe conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\n\nYou may write the answers and associated work for conceptual exercises by hand or type them in your Quarto document.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-1",
    "href": "hw/hw-02.html#exercise-1",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet \\(Var(\\hat{\\mathbf{y}})\\) be the variance of the fitted (predicted) values of the response variable. Show that \\(Var(\\hat{\\mathbf{y}}) = \\sigma^2_\\epsilon \\mathbf{H}\\).",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-2",
    "href": "hw/hw-02.html#exercise-2",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nSuppose we fit the model \\(\\mathbf{y} = \\mathbf{X}_1\\boldsymbol{\\beta}_1 + \\boldsymbol{\\epsilon}\\) when the true model is actually given by \\(\\mathbf{y} = \\mathbf{X}_1\\boldsymbol{\\beta}_1 + \\mathbf{X}_2\\boldsymbol{\\beta}_2 + \\boldsymbol{\\epsilon}\\). Assume \\(E(\\boldsymbol{\\epsilon}) = \\mathbf{0}\\) for both models.\n\nFind the expected value of the least-squares estimate \\(\\hat{\\boldsymbol{\\beta}}_1\\).\nUnder what conditions is the estimate \\(\\hat{\\boldsymbol{\\beta}}_1\\) unbiased?",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#instructions-1",
    "href": "hw/hw-02.html#instructions-1",
    "title": "HW 02: Multiple linear regression",
    "section": "Instructions",
    "text": "Instructions\nThe applied exercises are focused on applying the concepts to analyze data.\nAll work for the applied exercises must be typed in your Quarto document following a reproducible workflow.\nWrite all narrative using complete sentences and include informative axis labels / titles on visualizations.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#data-lego-sets",
    "href": "hw/hw-02.html#data-lego-sets",
    "title": "HW 02: Multiple linear regression",
    "section": "Data: LEGO® sets",
    "text": "Data: LEGO® sets\nThe data for Exercises 3 - 5 includes information about LEGO® sets from themes produced January 1, 2018 and September 11, 2020. The data were originally scraped from Brickset.com, an online LEGO set guide and were obtained for this assignment from Peterson and Ziegler (2021).\nYou will work with data on about 400 randomly selected LEGO sets produced during this time period. The primary variables are interest in this analysis are\n\nPieces: Number of pieces in the set from brickset.com.\nMinifigures: Number of minifigures (LEGO® people) in the set scraped from brickset.com.\nAmazon_Price: Price of the set on Amazon.com (in U.S. dollars)\nSize: General size of the interlocking bricks (Large = LEGO Duplo® sets - which include large brick pieces safe for children ages 1 to 5, Small = LEGO® sets which- include the traditional smaller brick pieces created for age groups 5 and - older, e.g., City, Friends)\n\nThe data are contained in lego-sample.csv. Use the code below to read in the data and remove any observations that have missing values for the relevant variables.\n\nlegos &lt;- read_csv(\"data/lego-sample.csv\")|&gt;\n  drop_na(Pieces, Amazon_Price, Size, Minifigures)\n\n\n\n\n\n\n\nAnalysis goal\n\n\n\nWe want to fit a multiple linear regression model to predict the price of LEGO® sets on Amazon.com based on Pieces, Size, and Minifigures.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-3",
    "href": "hw/hw-02.html#exercise-3",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nIn this analysis, we dropped observations that have missing values for some of the relevant variables.\n\nWhat is a disadvantage of dropping observations that have missing values, instead of using a method to impute (fill in) the missing data?\nHow might dropping these observations impact the generalizability of conclusions?\n\nFit the regression model and neatly display the results using three digits.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-4",
    "href": "hw/hw-02.html#exercise-4",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nWe want to understand the relationship between Pieces and Amazon_Price based on this model that also takes into the size of the blocks and number of minifigures.\nYou are convinced from the model output that there is evidence of a linear relationship between the two variables. Now you want to be more specific and test whether the slope is actually different from 0.1 ($10 increase in the price for every 100 additional pieces).\n\nWrite the null and alternative hypotheses for this test in using words and mathematical notation.\nCalculate the test statistic for this test. You may use any relevant output from the model in the previous exercise.\nWhat is the distribution of the test statistic under the null hypothesis for this problem?\nCalculate the p-value and state your conclusion in the context of the data using a threshold of \\(\\alpha = 0.05\\).\nCalculate the 95% confidence interval. Is the confidence interval consistent with your conclusion from the hypothesis test? Briefly explain.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-5",
    "href": "hw/hw-02.html#exercise-5",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nInstead of using the number of minifigures in the model, you decide to create an indicator variable for whether or not there are any minifigures in the set.\n\nCreate an indicator variable that takes the value “No” if there are zero minifigures in the LEGO® set, and “Yes” if there is at least one minifigure.\nYou hypothesize that the relationship between the price and number of pieces may differ based on whether or not there are minifigures in the set.\nMake a plot to visualize this potential effect. Does the relationship between price and number of pieces seem to differ based on the inclusion of minifigures? Briefly explain.\nFit a model using the number pieces, size of the blocks, the indicator for minifigures, and the interaction between pieces and the presence of minifigures to predict the price on Amazon.com.\nBased on this model, is there evidence that the effect of pieces on the price differs based on the inclusion of minifigures? Briefly explain your response, referencing any statistics used to make your determination.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#data-world-bank",
    "href": "hw/hw-02.html#data-world-bank",
    "title": "HW 02: Multiple linear regression",
    "section": "Data: World Bank",
    "text": "Data: World Bank\nThe World Bank collects “world development indicators” about the past and current development of countries. These data are made available on the World Bank’s website. It can be used to understand the relationships between these various factors and trends over time.\n\nThis analysis focuses on indicators from 2011 on 165 countries. The variables of interest are:\n\ngdp.per.capita: gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.\nsanit.access.factor: Population access to sanitation facilities (Low, High)\nedu.expend: Government expenditure on education, total (% of government expenditure)\nlife.expect: Life expectancy at birth (in years)",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-6",
    "href": "hw/hw-02.html#exercise-6",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nYou fit a model using sanitation access, education expenditures, and life expectancy to understand variability in GDP. The model takes the form\n\\[\n\\begin{aligned}\\widehat{\\log(GDP)} = \\hat{\\beta}_0 &+ \\hat{\\beta}_1 ~ sanit.access.factor + \\hat{\\beta}_2 ~ edu.expend + \\hat{\\beta}_3 ~life.expect \\\\ &+ \\hat{\\beta}_4 ~ sanit.access.factor \\times life.expect\\end{aligned}\n\\]\nwhere \\(\\log(GDP)\\) is the natural log of gdp.per.capita.\nThe F output for this model is shown below.\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.491\n1.638\n2.742\n0.007\n\n\nsanit.access.factorhigh\n-6.993\n1.971\n-3.548\n0.001\n\n\nedu.expend\n0.097\n0.038\n2.550\n0.012\n\n\nlife.expect\n0.030\n0.029\n1.061\n0.291\n\n\nsanit.access.factorhigh:life.expect\n0.122\n0.032\n3.853\n0.000\n\n\n\n\n\n\nInterpret the coefficient of edu.expend in the context of the data. You can interpret the coefficient in terms of \\(log(GDP)\\) .\nInterpret the coefficient of sanit.access.factorhigh in the context of the data.You can interpret the coefficient in terms of \\(log(GDP)\\) .",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-7",
    "href": "hw/hw-02.html#exercise-7",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nWrite the estimated regression equation for countries with high sanitation access.\nInterpret the effect of life.expect for countries with high sanitation access in the context of the data. You can interpret the coefficient in terms of \\(log(GDP)\\) .",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#exercise-8",
    "href": "hw/hw-02.html#exercise-8",
    "title": "HW 02: Multiple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nBelow are plots from the exploratory data analysis of the relationships between the predictor variables. Based on these plots, what appears to be a potential issue with the model from Exercise 6? Briefly explain your response.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-02.html#bonus-optional",
    "href": "hw/hw-02.html#bonus-optional",
    "title": "HW 02: Multiple linear regression",
    "section": "Bonus (optional)",
    "text": "Bonus (optional)\nUse the model from Exercise 6 to interpret the coefficients of edu.expend and sanit.access.factorhigh in term of GDP (not log(GDP)). Write your interpretations in the context of the data.\nEach interpretation is worth 1 point out of 50. The interpretation must be exactly correct to receive credit.",
    "crumbs": [
      "Homework",
      "HW 02"
    ]
  },
  {
    "objectID": "hw/hw-01.html",
    "href": "hw/hw-01.html",
    "title": "HW 01",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#instructions",
    "href": "hw/hw-01.html#instructions",
    "title": "HW 01: Simple linear regression",
    "section": "Instructions",
    "text": "Instructions\nThe conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\n\nYou may write the answers and associated work for conceptual exercises by hand or type them in your Quarto document.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-1",
    "href": "hw/hw-01.html#exercise-1",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\na. Show that the hat matrix \\(\\mathbf{H}\\) is symmetric \\((\\mathbf{H}^T = \\mathbf{H})\\) and idempotent \\((\\mathbf{H}^2 = \\mathbf{H})\\).\nb. Show that \\((\\mathbf{I} - \\mathbf{H})\\) is symmetric and idempotent.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-2",
    "href": "hw/hw-01.html#exercise-2",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet \\(\\mathbf{x}\\) be a \\(k \\times 1\\) vector and \\(\\mathbf{A}\\) be a symmetric \\(k \\times k\\) matrix, such that \\(\\mathbf{A}\\) is not a function of \\(\\mathbf{x}\\).\nShow that the gradient of \\(\\boldsymbol{x}^T\\mathbf{A}\\mathbf{x}\\) with respect to \\(\\mathbf{x}\\) is\n\\[\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^T\\mathbf{A}\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}\n\\]\n(Proposition 2 from class)",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-3",
    "href": "hw/hw-01.html#exercise-3",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nIn class we used the sum of squared residuals (SSR) to estimate the regression coefficients, \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) . To show this is the least squares estimate, we now need to show that we have, in fact, found the estimate of \\(\\boldsymbol{\\beta}\\) that minimizes the SSR (rather than maximize).\nIf the Hessian matrix \\(\\nabla_{\\boldsymbol{\\beta}}^2 SSR\\) is positive definite, then we know we have found the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes SSR, i.e., the least squares estimator. Additionally, we have the following proposition:",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#proposition",
    "href": "hw/hw-01.html#proposition",
    "title": "HW 01: Simple linear regression",
    "section": "Proposition",
    "text": "Proposition\nA matrix \\(\\mathbf{A}\\) is positive definite if \\(\\mathbf{z}^T\\mathbf{A}\\mathbf{z} &gt; 0\\) , given \\(\\mathbf{z}\\) is a non-zero vector.\nShow that \\(\\nabla_{\\boldsymbol{\\beta}}^2 SSR\\) is positive definite.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-4",
    "href": "hw/hw-01.html#exercise-4",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nProve that the maximum value of \\(R^2\\) must be less than 1 if the data set contains observations such that there are different observed values of the response for the same value of the predictor (e.g., the dataset contains observations \\((x_i, y_i)\\) and \\((x_j, y_j)\\) such that \\(x_i = x_j\\) and \\(y_i \\neq y_j\\) ).",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#instructions-1",
    "href": "hw/hw-01.html#instructions-1",
    "title": "HW 01: Simple linear regression",
    "section": "Instructions",
    "text": "Instructions\nThe applied exercises are focused on applying the concepts to analyze data.\nAll work for the applied exercises must be typed in your Quarto document following a reproducible workflow.\nWrite all narrative using complete sentences and include informative axis labels / titles on visualizations.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#data",
    "href": "hw/hw-01.html#data",
    "title": "HW 01: Simple linear regression",
    "section": "Data",
    "text": "Data\nThe datasets wi-icecover.csv and wi-air-temperature.csv contain information about ice cover and air temperature, respectively, at Lake Monona and Lake Mendota (both in Madison, Wisonsin) for days in 1886 through 2019. The data were obtained from the ntl_icecover and ntl_airtemp data frames in the lterdatasampler R package. They were originally collected by the US Long Term Ecological Research program (LTER) Network.\n\nicecover &lt;- read_csv(\"data/wi-icecover.csv\")\nairtemp &lt;- read_csv(\"data/wi-air-temperature.csv\")\n\nThe analysis will focus on the following variables:\n\nyear: year of observation\nlakeid: lake name\nice_duration: number of days between the freeze and ice breakup dates of each lake\nair_temp_avg: yearly average air temperature in Madison, WI (degrees Celsius)",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#analysis-goal",
    "href": "hw/hw-01.html#analysis-goal",
    "title": "HW 01: Simple linear regression",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goal of this analysis is to use linear regression explain variability in ice duration for lakes in Madison, WI based on air temperature. Because ice cover is impacted by various environmental factors, researchers are interested in examining the association between these two factors to better understand the changing climate.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-5",
    "href": "hw/hw-01.html#exercise-5",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nLet’s start by looking at the response variable ice_duration.\n\nCreate side-by-side boxplots to visualize the distribution of ice_duration for each lake.\nVisualize the distribution of ice duration over time for each lake.\nThere are separate measurements for each lake in the icecover data frame. In this analysis, we will combine the data from both lakes and use the average ice duration each year.\nEvaluate the analysis choice to use the average per year rather than the individual lake measurements. Some things to consider in your evaluation: Does the average accurately reflects the ice duration for lakes in Madison, WI for that year? Will there be information loss? How might that impact (or not) the analysis conclusions? Etc.\n\n\n\n\n\n\n\nTip\n\n\n\nSee the ggplot2 reference for example code and plots.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-6",
    "href": "hw/hw-01.html#exercise-6",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nNext, let’s combine the ice duration and air temperature data into a single analysis data frame.\n\nFill in the code below to create a new data frame, icecover_avg, of the average ice duration by year.\nThen join icecover_avg and airtemp to create a new data frame. The new data frame should have 134 observations.\n\nicecover_avg &lt;- icecover |&gt;\n  group_by(_____) |&gt;\n  summarise(_____) |&gt;\n  ungroup()\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the new data frame with average ice duration and average air temperature for the remainder of the assignment.\n\n\n\nVisualize the relationship between the air temperature and average ice duration. Do you think a linear model would be a good fit to capture the relationship between the two variables? \n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-7",
    "href": "hw/hw-01.html#exercise-7",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nWe will fit a model using the average air temperature to explain variability in ice duration that takes the form\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]\n\nState the dimensions of \\(\\mathbf{y}\\), \\(\\mathbf{X}\\), \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\epsilon}\\) for this analysis. Your answer should have exact values given this data set.\nFind the estimated regression coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) using the matrix representation of the model. Show the code used to get the answer.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-8",
    "href": "hw/hw-01.html#exercise-8",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\n\nFit the model from the previous exercise using the lm function. Neatly display the results using 3 digits.\nInterpret the slope in the context of the data.\n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-9",
    "href": "hw/hw-01.html#exercise-9",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCalculate \\(R^2\\) for the model in the previous exercise and interpret it in the context of the data.\nBriefly comment on the model fit based on \\(R^2\\).",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "hw/hw-01.html#exercise-10",
    "href": "hw/hw-01.html#exercise-10",
    "title": "HW 01: Simple linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nYou are asked to use a reproducible workflow for all of your work in the class, and the goal of this question to is better understand potential real-world implications of doing (or not) so. Below are some real-life examples in which having a non-reproducible workflow resulted in errors that impacted research or public records.\n\nSource: Ostblom and Timbers (2022)\n\n\nReproducibility error\nConsequence\nSource(s)\n\n\n\n\nLimitations in Excel data formats\nLoss of 16,000 COVID case records in the UK\n(Kelion 2020)\n\n\nAutomatic formatting in Excel\nImportant genes disregarded in scientific studies\n(Ziemann, Eren, and El-Osta 2016)\n\n\nDeletion of a cell caused rows to shift\nMix-up of which patient group received the treatment\n(Wallensteen et al. 2018)\n\n\nUsing binary instead of explanatory labels\nMix-up of the intervention with the control group\n(Aboumatar and Wise 2019)\n\n\nUsing the same notation for missing data and zero values\nPaper retraction\n(Whitehouse et al. 2021)\n\n\nIncorrectly copying data in a spreadsheet\nDelay in the opening of a hospital\n(Picken 2020)\n\n\n\nChoose one of the scenarios from the table and read the linked article discussing what went wrong. Then,\n\nBriefly describe what went wrong, i.e., what part of the process of was not reproducible and what error or impact that had.\nDescribe one way the researchers could have made the process reproducible.",
    "crumbs": [
      "Homework",
      "HW 01"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "This course will teach students how to analyze biomedical data from a Bayesian inference perspective with a strong emphasis on using real-world data, including electronic health records, wearables, and imaging data. The course will begin by introducing the machinery of Bayesian statistics through the lens of linear regression, giving enough context for students with no prior experience with Bayesian statistics. A history of computational approaches used in Bayesian statistics will be given before ultimately landing on Stan, a state-of-the-art probabilistic programming language that makes Bayesian inference accessible as a viable data science tool. The course will then branch out from regression and introduce Bayesian versions of machine learning tools, including regularization and classification. The course will then emphasize Bayesian hierarchical models, including Gaussian process models for temporal and spatial data; and clustering. Additional topics may be discussed from the Bayesian perspective, including causal inference, and meta-analysis. While an applied course, the methods will be introduced from a mathematical perspective, allowing students to obtain a fundamental understanding of the introduced models. Students will learn computational skills for implementing Bayesian models using R and Stan. By the end of this course, students will be well-equipped to tackle complex problems in biomedical research using Bayesian inference.\n\n\nBIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#pre-requisites",
    "href": "overview.html#pre-requisites",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "",
    "text": "BIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#teaching-assistants",
    "href": "overview.html#teaching-assistants",
    "title": "BIOSTAT 725 - Bayesian Health Data Science",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\nName\nRole\nOffice Hours\nLocation\n\n\nDr. Youngsoo Baek\nTA\nWed 2 - 4pm\nHock 10090\n\n\nBraden Scherting\nTA\nTBD\nTBD",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "labs/lab-07.html",
    "href": "labs/lab-07.html",
    "title": "Lab 07",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "labs/lab-04.html",
    "href": "labs/lab-04.html",
    "title": "Lab 04",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "labs/lab-01.html",
    "href": "labs/lab-01.html",
    "title": "Lab 01: Simple linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Thursday, September 12 at 11:59pm. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "labs/lab-01.html#learning-goals",
    "href": "labs/lab-01.html#learning-goals",
    "title": "Lab 01: Simple linear regression",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to produce visualizations and summary statistics to describe distributions\nBe able to fit, interpret, and evaluate simple linear regression models"
  },
  {
    "objectID": "labs/lab-01.html#clone-the-repo-start-new-rstudio-project",
    "href": "labs/lab-01.html#clone-the-repo-start-new-rstudio-project",
    "title": "Lab 01: Simple linear regression",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta221-fa24 organization on GitHub.\nClick on the repo with the prefix lab-01-. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the Lab 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab-01.html#r-and-r-studio",
    "href": "labs/lab-01.html#r-and-r-studio",
    "title": "Lab 01: Simple linear regression",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of an Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "labs/lab-01.html#exercise-1",
    "href": "labs/lab-01.html#exercise-1",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWe begin with some exploratory data analysis (EDA). As a first step, let’s get a quick summary look at the data using the glimpse function.\nViewing a summary of the data is a useful starting point for analysis, especially if there are a large number of observations or variables.\n\nglimpse(parks)\n\n\nHow many observations are in the parks data frame?\nWhat information is provided in the data about the time and location of the measurements?"
  },
  {
    "objectID": "labs/lab-01.html#exercise-2",
    "href": "labs/lab-01.html#exercise-2",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe predictor variable for this analysis, spend_per_resident_data, is quantitative; however, from the glimpse of the data in Exercise 1, we see its data type is chr (character) in R. We would expect it to be dbl (double), the data type for numeric data.\nWhy did spend_per_resident_data get read by R as a character data type instead of a double? Be specific."
  },
  {
    "objectID": "labs/lab-01.html#exercise-3",
    "href": "labs/lab-01.html#exercise-3",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nUse the code below to transform spend_per_resident_data , so that it is correctly treated as quantitative data in R. Write a brief explanation of what each numbered line of code does.\n\n1parks &lt;-\n  parks |&gt;  \n  mutate(spend_per_resident_data = \n2           str_replace(spend_per_resident_data,\"\\\\$\", \"\")) |&gt;\n  mutate(spend_per_resident_data = \n3           as.numeric(spend_per_resident_data))\n\n\n1\n\n______\n\n2\n\n______\n\n3\n\n______\n\n\n\n\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., “Completed exercises 1 - 3”), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab-01.html#exercise-4",
    "href": "labs/lab-01.html#exercise-4",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nNow we’ll examine the distributions of the variables of interest.\n\nMake a histogram of spend_per_resident_data and calculate summary statistics for this variable.\nComment on the features of the distribution of this variable by describing the shape, center, spread, and presence of potential outliers.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen performing data visualization, make sure that all your plots have clear and informative titles and axis labels. When investigating more complex relationships with many variables, this simple tip will save you and your readers a lot of time and confusion.\n\nSee AE 01 for example code."
  },
  {
    "objectID": "labs/lab-01.html#exercise-5",
    "href": "labs/lab-01.html#exercise-5",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNext let’s explore the response variable, pct_near_park_points. Visualize the distribution of the variable and calculate summary statistics. Describe the distribution pct_near_park_points."
  },
  {
    "objectID": "labs/lab-01.html#exercise-6",
    "href": "labs/lab-01.html#exercise-6",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nNow let’s use bivariate exploratory data analysis to look at the relationship between spend_per_resident_data and pct_near_park_points.\n\nMake a scatterplot to visualize the relationship between the two variables.\nDoes there seem to be a relationship between spending and park access? If so, what is the shape and direction of the relationship?\n\n\nThis is a another good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g. “Completed exercises 4 - 6”), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab-01.html#exercise-7",
    "href": "labs/lab-01.html#exercise-7",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nWe have seen the mathematical formulation for simple linear regression. In particular, given a response variable \\(Y\\) and predictor variable \\(X\\), the simple linear regression model is \\[Y = \\beta_0 + \\beta_1 X + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\]\nfor some unknown regression coefficients for slope and intercept\\((\\beta_0, \\beta_1)\\). This means that the expected value of each observation lies on the regression line\n\\[ E(Y|X) = \\beta_0 + \\beta_1 X\\]\nAnswer the following questions about simple linear regression. Your response should be in general terms about simple linear, not be specific to the parks data.\n\nWhat does \\(E(Y|X) = \\beta_0 + \\beta_1X\\) mean in terms of a given value of \\(X\\)?\nWhat is the interpretation of the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in terms of the expected value of \\(Y\\)?"
  },
  {
    "objectID": "labs/lab-01.html#exercise-8",
    "href": "labs/lab-01.html#exercise-8",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nIn class we’ve seen how matrices can be used to represent the simple linear model from the previous exercise. In particular\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\n\\]\nRecall that the goal is to fit a model that uses spend_per_resident_data to explain variability in park_near_pct_points.Estimate regression coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) for the model using the matrix representation. Show any work and/or code used to get the answer."
  },
  {
    "objectID": "labs/lab-01.html#exercise-9",
    "href": "labs/lab-01.html#exercise-9",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nNow let’s fit the model using the lm() function in R.\n\nFit the model and neatly display the output using 4 digits.\nInterpret the slope in the context of the data.\nDoes it make sense of the interpret the intercept? If so, interpret the intercept in the context of the data. Otherwise, explain why not."
  },
  {
    "objectID": "labs/lab-01.html#exercise-10",
    "href": "labs/lab-01.html#exercise-10",
    "title": "Lab 01: Simple linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nDo you think that city expenditure on residents is a useful predictor of park access? Briefly explain your response, reporting any statistics used to make your assessment.\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Lab 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "labs/lab-02.html",
    "href": "labs/lab-02.html",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Thursday, September 19 at 11:59pm. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your team’s GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "labs/lab-02.html#exercise-1",
    "href": "labs/lab-02.html#exercise-1",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet’s start with some exploratory data analysis. Visualize the distribution of the response variable mcsa and calculate summary statistics. Describe the distribution of this variable, including the shape, center, spread, and presence of potential outliers.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the childcare_train for all analysis in Exercises 1 - 7."
  },
  {
    "objectID": "labs/lab-02.html#exercise-2",
    "href": "labs/lab-02.html#exercise-2",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nAs you can see from the data dictionary in the README of the data folder, there are many interesting potential variables that could be included in the model to predict median childcare cost for school-age children. Therefore, we will do some feature selection and feature design to choose potential predictors and construct new ones.\nAs a team, select four variables you want to use as predictors for the model. For each variable, state the variable name, definition, and a brief explanation about why your team hypothesizes this will be a relevant predictor of median childcare costs. The explanation may (but is not required to) include some short exploratory analysis.\n\nTeam Member 1: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 1- 2.\nTeam Member 2: It’s your turn! Type the team’s response to exercises 3 - 4."
  },
  {
    "objectID": "labs/lab-02.html#exercise-3",
    "href": "labs/lab-02.html#exercise-3",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nOnce we’ve identified potential predictor variables, we often need to transform some variables (e.g., change raw counts into proportions) or create new ones (e.g., create a categorical variable out of quantitative data) before fitting the regression model. This process is particularly useful when putting a variable in the model “as-is” may result in interpretation issues.\nChoose one of the variables selected in the previous exercise. For this variable,\n\nTransform the variable or use it to create a new variable. Be sure to save the variable to the childcare_train data frame.\nBriefly explain your reasoning for the transformation or new variable.\nUse visualizations and/or summary statistics to display the distribution of the original variable and the transformed / newly created variable. Note: This is to help ensure the transformation / new variable is what you expect.\n\nAn example using h_6to17_both_work is below. Note you cannot use this variable for your transformation / new variable.\n\n\n\n\n\n\nExample\n\n\n\nSay we believe that the amount of households with two working parents increases demand for childcare services, and hence their price. We have the column h_6to17_both_work encoding the number of such households per county. The raw population count differs across county, so having a larger value of h_6to17_both_work may reflect population size and not necessarily imply the type of such household is more prevalent in the county.\nA reasonable thing would be creating a variable encoding the proportion of households with both parents working. We could do this by creating a variable p_6to17_both_work:\n\np_6to17_both_work = h_6to17_both_work / households\n\nIn this case, we would then use p_6to17_both_work not h_6to17_both_work as a predictor in the model.\n\n\nYou may decide to transform and/or create multiple new variables; however, you will only be graded on the one of them.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the transformed / new variable (not the original variable) in the model!"
  },
  {
    "objectID": "labs/lab-02.html#exercise-4",
    "href": "labs/lab-02.html#exercise-4",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nNow let’s conduct some bivariate exploratory data analysis. Visualize the relationship between the response variable and one of your predictor variables.\nWrite two distinct observations from the visualization.\n\nTeam Member 2: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 3 - 4.\nTeam Member 3: It’s your turn! Type the team’s response to exercises 5 - 6."
  },
  {
    "objectID": "labs/lab-02.html#exercise-5",
    "href": "labs/lab-02.html#exercise-5",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse the matrix form of the model to represent the regression model with the variables you selected and transformed/created in exercises 2 and 3 as the predictors. For each symbol in the model\n\ndescribe what it represents, and\nstate the dimensions.\n\nThe description and dimensions should be in the context of these data, not in general."
  },
  {
    "objectID": "labs/lab-02.html#exercise-6",
    "href": "labs/lab-02.html#exercise-6",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse lm to fit the regression model you described in the previous exercise.\n\nNeatly display the model using a reasonable number of digits.\n\n\n\nInterpret the coefficient for one predictor in the model.\n\n\nTeam Member 3: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 5 - 6.\nTeam Member 4: It’s your turn! Type the team’s response to exercises 7 - 9."
  },
  {
    "objectID": "labs/lab-02.html#exercise-7",
    "href": "labs/lab-02.html#exercise-7",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow let’s assess the fit of the model.\n\nHow much of the variability in the childcare costs is explained by your chosen predictor variables?\n\n\n\nBased on this, do you think the model explains a significant portion of the variability in childcare costs for school-age children in North Carolina? Briefly explain."
  },
  {
    "objectID": "labs/lab-02.html#exercise-8",
    "href": "labs/lab-02.html#exercise-8",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let’s use the testing data to explore the predictive power of the model.\n\nAdd the variable you created in Exercise 3 to the testing data.\nThen, use the code below to compute the predicted childcare costs for the observations in the testing data using the predict function.\n\n\n# compute predictions\npred &lt;- predict(childcare_fit, childcare_test)\n\n# add predictions to testing data set\nchildcare_test &lt;- childcare_test |&gt;\n  mutate(pred = pred)"
  },
  {
    "objectID": "labs/lab-02.html#exercise-9",
    "href": "labs/lab-02.html#exercise-9",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCompute the RMSE for the test set, and compare it to the standard deviation of the response variable mcsa.\n\n\n\nHow do these values compare?\nBased on this, how would assess the predictive power of the model?\n\n\nTeam Member 4: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and the rest of the team can see the completed lab.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the team’s completed lab!"
  },
  {
    "objectID": "labs/lab-02.html#exercise-10",
    "href": "labs/lab-02.html#exercise-10",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nIf you haven’t already, make sure you have completed the team agreement (see the instructions in [Meet your team!])."
  },
  {
    "objectID": "labs/lab-02.html#footnotes",
    "href": "labs/lab-02.html#footnotes",
    "title": "Lab 02: Multiple Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDon’t trust yourself to keep your hands off the keyboard? Put them in your pocket or cross your arms. No matter how silly it might feel, resist the urge to touch your keyboard until otherwise instructed!↩︎"
  },
  {
    "objectID": "labs/lab-03.html",
    "href": "labs/lab-03.html",
    "title": "Lab 03: Inference for regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Thursday, October 3 at 11:59pm. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your team’s GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "labs/lab-03.html#exercise-0",
    "href": "labs/lab-03.html#exercise-0",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 0",
    "text": "Exercise 0\nThere are two penguins in the data frame that do not have reported values for flipper length or body mass and thus will not be included in any analysis. Remove these observations from the data frame, so that we have an accurate count of the number of observations used for the analysis.\n\n\n\n\n\n\nNote\n\n\n\nExericse 0 is not graded."
  },
  {
    "objectID": "labs/lab-03.html#exercise-1",
    "href": "labs/lab-03.html#exercise-1",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet’s begin by exploring the relationship between between flipper length and body mass, while accounting for species.\n\nVisualize the relationship between flipper length and body mass. Then describe the relationship.\nFit the main effects linear regression model (no interaction terms) between these three variables. Neatly display the results using three digits.\nInterpret the coefficient of flipper length in the context of the data."
  },
  {
    "objectID": "labs/lab-03.html#exercise-2",
    "href": "labs/lab-03.html#exercise-2",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nNow let’s look at the assumptions underlying the regression model. Consider the linear regression model\n\\[ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I}_n)  \\tag{1}\\]\nand let \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\) be the least squares estimator. This model relies on four assumptions:\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from one another.\n\nFor each condition, state the components of Equation 1 that are used to represent it."
  },
  {
    "objectID": "labs/lab-03.html#exercise-3",
    "href": "labs/lab-03.html#exercise-3",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe can visually assess the linearity and constant variance assumptions by examining a scatterplot of the residuals versus fitted (predicted) values.\n\nCreate a scatterplot of the residuals (y-axis) versus fitted values (x-axis) for the model fit in Exercise 1.\nIf there is a linear relationship between the response and predictor variables, no discernible pattern should be present between fitted values and residuals. Does the linearity assumption appear to be satisfied?\nBriefly explain why no discernible pattern in the plot of residuals versus fitted values would indicate the linearity condition is satisfied."
  },
  {
    "objectID": "labs/lab-03.html#exercise-4",
    "href": "labs/lab-03.html#exercise-4",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nIf errors have constant variance, we would expect the variability the of residuals about their mean to be approximately equal as the fitted value increases. Does the constant variance assumption appear to be satisfied? Briefly explain."
  },
  {
    "objectID": "labs/lab-03.html#exercise-5",
    "href": "labs/lab-03.html#exercise-5",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNext, let’s assess the assumptions about the distribution fo the residuals. Under the normality assumption, the residuals are expected to be normally distributed. Visualize the distribution of the residuals. Does the normality assumption appear to be satisfied? Briefly explain."
  },
  {
    "objectID": "labs/lab-03.html#exercise-6",
    "href": "labs/lab-03.html#exercise-6",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nThe last assumption is that the residuals are independent of one another. Do you think it’s reasonable to assume the independence of residuals in this analysis? Briefly explain."
  },
  {
    "objectID": "labs/lab-03.html#exercise-7",
    "href": "labs/lab-03.html#exercise-7",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow let’s set up the test of whether the flipper length has a statistically significant effect on body mass in this model.\n\nWrite the null and alternative hypotheses in words and in mathematical notation.\nShow how the test statistic is computed specifically for this problem. In your response, show the code to obtain each relevant quantity in the formula for the test statistic using the matrix form of the model. Do not merely refer to the values in the lm output. You must show how each value is computed using the matrix / vector calculations.\nState the distribution of the test statistic under the null hypothesis for this problem."
  },
  {
    "objectID": "labs/lab-03.html#exercise-8",
    "href": "labs/lab-03.html#exercise-8",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nIn the regression output from Exercise 1, you are provided the \\(p\\)-value for the test of significance of each individual coefficient.\n\nInterpret the p-value for the coefficient of flipper length in the context of the data.\nThen, use the p-value and a decision-making threshold of \\(\\alpha = 0.05\\) to draw a conclusion about the relationship between flipper length and body mass in this model. State your conclusion in the context of the data."
  },
  {
    "objectID": "labs/lab-03.html#exercise-9",
    "href": "labs/lab-03.html#exercise-9",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nNow let’s construct the 95% confidence interval for the coefficient of flipper length.\n\nWrite the general formula for the 95% confidence interval.\nUse R functions to compute all the quantities you need for the interval, then compute the interval."
  },
  {
    "objectID": "labs/lab-03.html#exercise-10",
    "href": "labs/lab-03.html#exercise-10",
    "title": "Lab 03: Inference for regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nInterpret the interval from the previous exercise in the context of the data."
  },
  {
    "objectID": "labs/lab-00.html",
    "href": "labs/lab-00.html",
    "title": "Lab 00: Welcome + Getting Started",
    "section": "",
    "text": "Important\n\n\n\nPlease complete all today’s lab tasks before leaving lab today."
  },
  {
    "objectID": "labs/lab-00.html#rstudio",
    "href": "labs/lab-00.html#rstudio",
    "title": "Lab 00: Welcome + Getting Started",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\n\nReserve RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers. You will log in using your NetID credentials.\nClick “Reserve STA 210” to reserve an RStudio container. Be sure you reserve the container labeled STA 210 to ensure you have the computing set up you need for the class.\n\nYou only need to reserve a container once per semester.\n\n\nOpen RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA210 to log into the Docker container. You should now see the RStudio environment."
  },
  {
    "objectID": "labs/lab-00.html#git-and-github",
    "href": "labs/lab-00.html#git-and-github",
    "title": "Lab 00: Welcome + Getting Started",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nIn addition to R and RStudio, we will use git and GitHub for version control and collaboration.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, project, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username.\n\n\n\nIf you already have a GitHub account, you can move on to the next step."
  },
  {
    "objectID": "labs/lab-00.html#connect-rstudio-and-github",
    "href": "labs/lab-00.html#connect-rstudio-and-github",
    "title": "Lab 00: Welcome + Getting Started",
    "section": "Connect RStudio and GitHub",
    "text": "Connect RStudio and GitHub\nNow that you have RStudio and a GitHub account, we will configure git so that RStudio and GitHub communicate with one another.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps; you are encouraged to follow along as your TA demonstrates the steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system.\n\n\n\nStep 0: Open your STA 210 RStudio container.\nStep 1: Type credentials::ssh_setup_github() into the console on the bottom left of the RStudio environment.\nStep 2: R will ask “No SSH key found. Generate one now?” Click 1 for yes.\nStep 3: You will generate a key. It will begin with “ssh-rsa….” R will then ask “Would you like to open a browser now?” Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used, e.g., sta221)\n\n\n\nConfigure git\nThe last thing we need to do is configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"Maria Tackett\",\n  user.email = \"maria.tackett@duke.edu\")\n\nIt may look like nothing happened but you are now ready interact between GitHub and RStudio! We will begin working with RStudio and GitHub in lecture this week."
  },
  {
    "objectID": "labs/lab-05.html",
    "href": "labs/lab-05.html",
    "title": "Lab 05",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "labs/lab-06.html",
    "href": "labs/lab-06.html",
    "title": "Lab 06",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "hw/hw-04.html",
    "href": "hw/hw-04.html",
    "title": "HW 04",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 04"
    ]
  },
  {
    "objectID": "hw/hw-03.html",
    "href": "hw/hw-03.html",
    "title": "HW 03",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 03"
    ]
  },
  {
    "objectID": "hw/stats-experience.html",
    "href": "hw/stats-experience.html",
    "title": "Statistics Experience",
    "section": "",
    "text": "Important\n\n\n\nThis assignment is due on Tuesday, November 26 at 11:59pm on Gradescope.\nThe world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1️⃣ Have a statistics experience.\n2️⃣ Make a slide reflecting on your experience.\nYou must complete both parts to receive credit. The statistics experience will count as a homework grade.",
    "crumbs": [
      "Homework",
      "Statistics experience"
    ]
  },
  {
    "objectID": "hw/stats-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/stats-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you’d like to do but you’re not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::conf talks\n\n2022 conference\n2021 conference\n2020 conference\n\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask Professor Tackett if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn’t on this list, ask Professor Tackett to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nWeapons of Math Destruction by Cathy O’Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don’t by Nate Silver\nList of books about data science ethics\n\nThis list is not exhaustive.\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n✅ Create a GitHub repo for your TidyTuesday submission. Your repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The visualization should include features or customization that are beyond what we’ve done in class .\n✅ Include the link to your GitHub repo in the slide summarizing your experience.\n\n\nCategory 7: CURV - connecting, uplifting, and recognizing voices\nCURV is a project by Dr. Jo Hardin at Pomona College to highlight statisticians and data scientists from groups who have been historically marginalized in the discipline. \n\n\n\n\n\n\nFor this statistics experience, you can contribute to the CURV data base. If there is a scholar you would like to suggest for the data base, submit your suggestion as an issue or pull request on the CURV GitHub repo and create a sample CURV page.\nA few guidelines:\n✅ Create a draft of the CURV page for your suggested scholar. For reference, click here for the CURV page for W.E.B. Du Bois. The page must be created in a Quarto document.\n\n\n\n\n\n\nTip\n\n\n\nYou can find the Quarto documents for current scholars in the data base in the CURV GitHub repo. You can use one of these as a template to format your page.\n\n\n✅ Make a pull request to the CURV GitHub repo to add the .qmd file for your suggested scholar, OR open an issue with a link to the .qmd file for your suggested scholar. You can ask a member of the teaching team if you have questions about how to do this.\n✅ Include the URL to your pull request or issue in your one-slide reflection.",
    "crumbs": [
      "Homework",
      "Statistics experience"
    ]
  },
  {
    "objectID": "hw/stats-experience.html#part-2-reflect-on-your-experience",
    "href": "hw/stats-experience.html#part-2-reflect-on-your-experience",
    "title": "Statistics Experience",
    "section": "Part 2: Reflect on your experience",
    "text": "Part 2: Reflect on your experience\nMake one slide summarizing and reflecting on your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nDescription of the experience\n\nName and brief description of the event/podcast/competition/etc.\n\nSomething you learned\n\nWrite 2 - 4 sentences about something you learned or found particularly interesting or unexpected.\n\nConnection to STA 221\n\nWrite 2 - 4 sentences about how the experience connects to what we’ve done in the course.\n\nCitation or link to web page for event/competition/etc.\n\nNo citation needed if you do an interview.\n\n\nMake sure the slide includes the information mentioned above and is easily readable (i.e. use a reasonable font size!). Creativity on the experience and slide design is encouraged!",
    "crumbs": [
      "Homework",
      "Statistics experience"
    ]
  },
  {
    "objectID": "hw/stats-experience.html#submission",
    "href": "hw/stats-experience.html#submission",
    "title": "Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the Statistics Experience assignment on Gradescope by Tuesday, November 26 at 11:59pm. Standard homework late policy applies.",
    "crumbs": [
      "Homework",
      "Statistics experience"
    ]
  },
  {
    "objectID": "prepare/prepare-sep3.html",
    "href": "prepare/prepare-sep3.html",
    "title": "Prepare for September 3 lecture",
    "section": "",
    "text": "📖 Read Section 4.7: Model Assessment\n✅ Go through R resource (optional)"
  },
  {
    "objectID": "prepare/prepare-aug29.html",
    "href": "prepare/prepare-aug29.html",
    "title": "Prepare for August 29 lecture",
    "section": "",
    "text": "📖 Read Simple Linear Regression\n✅ Complete Lab 00 tasks"
  },
  {
    "objectID": "prepare/prepare-sep19.html",
    "href": "prepare/prepare-sep19.html",
    "title": "Prepare for September 19 lecture",
    "section": "",
    "text": "📖 Read Inference for Simple Linear Regression:\n\nSections 5.1 - 5.3\nSection 5.6\nSection 5.8\nSection 5.9"
  },
  {
    "objectID": "ae/ae-01-slr.html",
    "href": "ae/ae-01-slr.html",
    "title": "AE 01: Simple linear regression",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-01 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class to submit your AE.\n\nThis AE will not count towards your participation grade.\nlibrary(tidyverse)    # data wrangling and visualization\nlibrary(tidymodels)   # broom and yardstick package\nlibrary(openintro)    # duke_forest dataset\nlibrary(knitr)        # format output\nlibrary(scales)       # format plot axes\nlibrary(skimr)        # quickly calculate summary statistics"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-1",
    "href": "ae/ae-01-slr.html#exercise-1",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWhat are 1 - 2 observations about the distribution of price?"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-2",
    "href": "ae/ae-01-slr.html#exercise-2",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nVisualize the distribution of area and calculate summary statistics.\n\n# add code here\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-3",
    "href": "ae/ae-01-slr.html#exercise-3",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat are 1 - 2 observations about the distribution of area?"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-4",
    "href": "ae/ae-01-slr.html#exercise-4",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nFill in the code to visualize the relationship between price and area. What are 1 - 2 observations about the relationship between these two variables?\n\n\n\n\n\n\nImportant\n\n\n\nRemove #|eval: false after you have filled in the code!\n\n\n\nggplot(duke_forest, aes(x = ____, y = ____)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    x = \"_______\",\n    y = \"_________\",\n    title = \"Price and area of houses in Duke Forest\"\n  ) +\n  scale_y_continuous(labels = label_dollar())"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-5",
    "href": "ae/ae-01-slr.html#exercise-5",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nYou want to fit a model of the form\n\\[\nprice = \\beta_0 + \\beta_1 ~ area + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2_\\epsilon)\n\\]\nWould a model of this form be a reasonable fit for the data? Why or why not?"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-6",
    "href": "ae/ae-01-slr.html#exercise-6",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nFit the linear model described in the previous exercise and neatly display the output.\nSee notes for example code.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-7",
    "href": "ae/ae-01-slr.html#exercise-7",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nInterpret the slope in the context of the data.\nInterpret the slope in terms of area increasing by 100 sqft.\nWhich interpretation do you think is more meaningful in practice?"
  },
  {
    "objectID": "ae/ae-01-slr.html#exercise-8",
    "href": "ae/ae-01-slr.html#exercise-8",
    "title": "AE 01: Simple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nDoes it make sense to interpret the intercept? If so, interpret it in the context of the data. Otherwise, explain why not."
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#lectures-and-labs",
    "href": "support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#office-hours",
    "href": "support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!\nMake a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#ed-discussion",
    "href": "support.html#ed-discussion",
    "title": "Course support",
    "section": "Ed Discussion",
    "text": "Ed Discussion\nOutside of class and office hours, any general questions about course content or assignments should be posted on Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters that are not appropriate for the class discussion forum (e.g. illness, accommodations, etc.), you may email me at sib2@duke.edu. If you email me, please include “BIOSTAT 725” in the subject line. Barring extenuating circumstances, I will respond to BIOSTAT 725 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#academic-support",
    "href": "support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact ARC@duke.edu, 919-684-5917.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#mental-health-and-wellness",
    "href": "support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\n\nDukeReach: Provides comprehensive outreach services to identify and support students in managing all aspects of well being. If you have concerns about a student’s behavior or health visit the website for resources and assistance. Go to studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS): CAPS services include individual, group, and couples counseling services, health coaching, psychiatric services, and workshops and discussions. (919) 660-1000 or students.duke.edu/wellness/caps\nTimelyCare (formerly known as Blue Devils Care): An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#technology-accommodations",
    "href": "support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\n\nWhile we encourage students to use their own laptops for work in this course, we will provide the opportunity to use Duke’s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#course-materials-costs",
    "href": "support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#assistance-with-zoom-or-canvas",
    "href": "support.html#assistance-with-zoom-or-canvas",
    "title": "Course support",
    "section": "Assistance with Zoom or Canvas",
    "text": "Assistance with Zoom or Canvas\nFor technical help with Canvas or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Canvas here.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA725 on the Reservations available menu on the right. You only need to do this once, and when you do, you’ll see this container moved to the My reservations menu on the left.\nNext, click on STA725 under My reservations to access the RStudio instance you’ll use for the course.",
    "crumbs": [
      "Computing",
      "Access"
    ]
  },
  {
    "objectID": "slides/lab-02.html#goals",
    "href": "slides/lab-02.html#goals",
    "title": "Lab 02",
    "section": "Goals",
    "text": "Goals\n\nLaTex in this course\nMeet your team!\nTeam agreement\nLab 02: Childcare costs"
  },
  {
    "objectID": "slides/lab-02.html#latex-in-this-class",
    "href": "slides/lab-02.html#latex-in-this-class",
    "title": "Lab 02",
    "section": "LaTex in this class",
    "text": "LaTex in this class\nFor this class you will need to be able to…\n\nProperly write mathematical symbols, e.g., \\(\\beta_1\\) not B1, \\(R^2\\) not R2\nWrite basic regression equations, e.g., \\(\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\nWrite matrix equations: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\)\nWrite hypotheses (we’ll start this next week), e.g., \\(H_0: \\beta = 0\\)\n\nYou are welcome to but not required to write math proofs using LaTex."
  },
  {
    "objectID": "slides/lab-02.html#writing-latex-from-ae-02",
    "href": "slides/lab-02.html#writing-latex-from-ae-02",
    "title": "Lab 02",
    "section": "Writing LaTex (from AE 02)",
    "text": "Writing LaTex (from AE 02)\nInline: Your mathematics will display within the line of text.\n\nUse $ to start and end your LaTex syntax. You can also use the menu: Insert -&gt; LaTex Math -&gt; Inline Math.\nExample: The text The simple linear regression model is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ produces\nThe simple linear regression model is \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\)"
  },
  {
    "objectID": "slides/lab-02.html#writing-latex-from-ae-02-1",
    "href": "slides/lab-02.html#writing-latex-from-ae-02-1",
    "title": "Lab 02",
    "section": "Writing LaTex (from AE 02)",
    "text": "Writing LaTex (from AE 02)\nDisplay: Your mathematics will display outside the line of text\n\nUse a $$ to start and end your LaTex syntax. You can also use the menu: Insert -&gt; LaTex Math -&gt; Display Math.\nExample: The text The estimated regression equation is $$\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$$ produces\nThe estimated regression equation is\n\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\]\n\n\n\n\n\n\nTip\n\n\nClick here for a quick reference of LaTex code."
  },
  {
    "objectID": "slides/lab-02.html#meet-your-team",
    "href": "slides/lab-02.html#meet-your-team",
    "title": "Lab 02",
    "section": "Meet your team!",
    "text": "Meet your team!\n\nClick here to find your team.\nSit with your team."
  },
  {
    "objectID": "slides/lab-02.html#team-name-agreement",
    "href": "slides/lab-02.html#team-name-agreement",
    "title": "Lab 02",
    "section": "Team name + agreement",
    "text": "Team name + agreement\n\nCome up with a team name. You can’t have the same name as another group in the class, so be creative!\n\nYour TA will get your team name by the end of lab.\n\nFill out the team agreement. The goals of the agreement are to…\n\nGain a common understanding of the team’s goals and expectations for collaboration\nMake a plan for team communication\nMake a plan for working outside of lab"
  },
  {
    "objectID": "slides/lab-02.html#team-workflow",
    "href": "slides/lab-02.html#team-workflow",
    "title": "Lab 02",
    "section": "Team workflow",
    "text": "Team workflow\n\nOnly one team member should type at a time. There are markers in today’s lab to help you determine whose turn it is to type.\n\nEvery team member should still be engaged in discussion for all questions, even if it’s not your turn type.\n\nDon’t forget to pull to get your teammates’ updates before making changes to the .qmd file.\n\n\n\n\n\n\nImportant\n\n\nOnly one submission per team on Gradescope. Read the submission instructions carefully!"
  },
  {
    "objectID": "slides/lab-02.html#team-workflow-in-action",
    "href": "slides/lab-02.html#team-workflow-in-action",
    "title": "Lab 02",
    "section": "Team workflow, in action",
    "text": "Team workflow, in action\n\nComplete the “Workflow: Using Git and GitHub as a team” section of the lab in your teams.\nRaise your hand if you have any questions about the workflow.\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/lab-02.html#tips-for-working-on-a-team",
    "href": "slides/lab-02.html#tips-for-working-on-a-team",
    "title": "Lab 02",
    "section": "Tips for working on a team",
    "text": "Tips for working on a team\n\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report.\nThe labs are structured to help you learn the steps of a data analysis. Do not split up the lab among the team members; work on it together in its entirety.\nEveryone has something to contribute! Use the lab groups as an opportunity to share ideas and learn from each other."
  },
  {
    "objectID": "slides/lab-02.html#lab-02-childcare-costs",
    "href": "slides/lab-02.html#lab-02-childcare-costs",
    "title": "Lab 02",
    "section": "Lab 02: Childcare costs",
    "text": "Lab 02: Childcare costs\nToday’s lab focuses on using multiple linear regression (Week 03 content) to predict childcare costs for school-aged children in North Carolina.\n🔗 sta221-fa24.netlify.app/labs/lab-02.html\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#announcements",
    "href": "slides/04-slr-model-assessment-contd.html#announcements",
    "title": "SLR: Model assessment cont’d",
    "section": "Announcements",
    "text": "Announcements\n\nOffice hours start this week. See schedule on Overview page of the course website or on Canvas.\nLabs resume Monday, September 09"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#topics",
    "href": "slides/04-slr-model-assessment-contd.html#topics",
    "title": "SLR: Model assessment cont’d",
    "section": "Topics",
    "text": "Topics\n\nEvaluate models using RMSE and \\(R^2\\)\nUse analysis of variance to partition variability in the response variable"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#computing-set-up",
    "href": "slides/04-slr-model-assessment-contd.html#computing-set-up",
    "title": "SLR: Model assessment cont’d",
    "section": "Computing set up",
    "text": "Computing set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling (includes broom, yardstick, and other packages)\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme for ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#data-houses-in-duke-forest",
    "href": "slides/04-slr-model-assessment-contd.html#data-houses-in-duke-forest",
    "title": "SLR: Model assessment cont’d",
    "section": "Data: Houses in Duke Forest",
    "text": "Data: Houses in Duke Forest\n\n\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\nSource: openintro::duke_forest\n\n\n\n\nGoal: Use the area (in square feet) to understand variability in the price of houses in Duke Forest."
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#regression-model",
    "href": "slides/04-slr-model-assessment-contd.html#regression-model",
    "title": "SLR: Model assessment cont’d",
    "section": "Regression model",
    "text": "Regression model\n\nduke_forest_fit &lt;- lm(price ~ area, data = duke_forest)\n\ntidy(duke_forest_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000\n\n\n\n\n\n\n\nWe fit a model but is it any good?"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#two-statistics",
    "href": "slides/04-slr-model-assessment-contd.html#two-statistics",
    "title": "SLR: Model assessment cont’d",
    "section": "Two statistics",
    "text": "Two statistics\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#rmse",
    "href": "slides/04-slr-model-assessment-contd.html#rmse",
    "title": "SLR: Model assessment cont’d",
    "section": "RMSE",
    "text": "RMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}}\n\\]\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nThe value of RMSE is more useful for comparing across models than evaluating a single model (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#analysis-of-variance-anova",
    "href": "slides/04-slr-model-assessment-contd.html#analysis-of-variance-anova",
    "title": "SLR: Model assessment cont’d",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of Variance (ANOVA): Technique to partition variability in \\(Y\\) by the sources of variability"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#total-variability-response",
    "href": "slides/04-slr-model-assessment-contd.html#total-variability-response",
    "title": "SLR: Model assessment cont’d",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\nGoal: Quantify how much variability in price is accounted for by the model (area) and how much accounted for by factors not included in the model."
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#partition-variability-in-price",
    "href": "slides/04-slr-model-assessment-contd.html#partition-variability-in-price",
    "title": "SLR: Model assessment cont’d",
    "section": "Partition variability in price",
    "text": "Partition variability in price\nFor now, let’s focus on two observations"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#total-variability-response-1",
    "href": "slides/04-slr-model-assessment-contd.html#total-variability-response-1",
    "title": "SLR: Model assessment cont’d",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\\[\\text{Sum of Squares Total (SST)} = \\sum_{i=1}^n(y_i - \\bar{y})^2 = (n-1)s_y^2\\]"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#explained-variability-model",
    "href": "slides/04-slr-model-assessment-contd.html#explained-variability-model",
    "title": "SLR: Model assessment cont’d",
    "section": "Explained variability (Model)",
    "text": "Explained variability (Model)\n\n\\[\\text{Sum of Squares Model (SSM)} = \\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#unexplained-variability-residuals",
    "href": "slides/04-slr-model-assessment-contd.html#unexplained-variability-residuals",
    "title": "SLR: Model assessment cont’d",
    "section": "Unexplained variability (Residuals)",
    "text": "Unexplained variability (Residuals)\n\n\\[\\text{Sum of Squares Residuals (SSR)} = \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2\\]"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#sum-of-squares",
    "href": "slides/04-slr-model-assessment-contd.html#sum-of-squares",
    "title": "SLR: Model assessment cont’d",
    "section": "Sum of Squares",
    "text": "Sum of Squares\n\n\\[\n\\begin{aligned}\n\\color{#407E99}{SST} \\hspace{5mm}&= &\\color{#993399}{SSM} &\\hspace{5mm} +  &\\color{#8BB174}{SSR} \\\\[10pt]\n\\color{#407E99}{\\sum_{i=1}^n(y_i - \\bar{y})^2} \\hspace{5mm}&= &\\color{#993399}{\\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2} &\\hspace{5mm}+ &\\color{#8BB174}{\\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nNote\n\n\nSee Sum of Squares for mathematical details showing \\(SST = SSM + SSR\\)."
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#r2",
    "href": "slides/04-slr-model-assessment-contd.html#r2",
    "title": "SLR: Model assessment cont’d",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nThe coefficient of determination \\(R^2\\) is the proportion of variation in the response, \\(Y\\), that is explained by the regression model\n\n\\[\\large{R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}}\\]\n\n\nWhat is the range of \\(R^2\\)? Does \\(R^2\\) have units?"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#interpreting-r2",
    "href": "slides/04-slr-model-assessment-contd.html#interpreting-r2",
    "title": "SLR: Model assessment cont’d",
    "section": "Interpreting $R^2$",
    "text": "Interpreting $R^2$\n\nQuestionSubmit\n\n\n\nSubmit your response to the following question on Ed Discussion.\n\nThe \\(R^2\\) of the model for price from area of houses in Duke Forest is 44.5%. Which of the following is the correct interpretation of this value?\n\nArea correctly predicts 44.5% of price for houses in Duke Forest.\n44.5% of the variability in price for houses in Duke Forest can be explained by area.\n44.5% of the variability in area for houses in Duke Forest can be explained by price.\n44.5% of the time price for houses in Duke Forest can be predicted by area.\n\nDo you think this model is useful for explaining variability in the price of Duke Forest houses?\n\n\n\n\n\n\n\n\n🔗 https://edstem.org/us/courses/62513/discussion/629888"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#augmented-data-frame",
    "href": "slides/04-slr-model-assessment-contd.html#augmented-data-frame",
    "title": "SLR: Model assessment cont’d",
    "section": "Augmented data frame",
    "text": "Augmented data frame\nUse the augment() function from the broom package (part of tidymodels) to add columns for predicted values, residuals, and other observation-level model statistics\n\n\nduke_forest_aug &lt;- augment(duke_forest_fit)\nduke_forest_aug\n\n# A tibble: 98 × 8\n     price  area  .fitted  .resid   .hat  .sigma   .cooksd .std.resid\n     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 1520000  6040 1079931. 440069. 0.133  162605. 0.604         2.80  \n 2 1030000  4475  830340. 199660. 0.0435 168386. 0.0333        1.21  \n 3  420000  1745  394951.  25049. 0.0226 169664. 0.000260      0.150 \n 4  680000  2091  450132. 229868. 0.0157 168011. 0.0150        1.37  \n 5  428500  1772  399257.  29243. 0.0220 169657. 0.000345      0.175 \n 6  456000  1950  427645.  28355. 0.0182 169659. 0.000266      0.170 \n 7 1270000  3909  740072. 529928. 0.0250 160502. 0.130         3.18  \n 8  557450  2841  569744. -12294. 0.0102 169679. 0.0000277    -0.0732\n 9  697500  3924  742465. -44965. 0.0254 169620. 0.000948     -0.270 \n10  650000  2173  463209. 186791. 0.0145 168582. 0.00912       1.11  \n# ℹ 88 more rows"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#finding-rmse-in-r",
    "href": "slides/04-slr-model-assessment-contd.html#finding-rmse-in-r",
    "title": "SLR: Model assessment cont’d",
    "section": "Finding RMSE in R",
    "text": "Finding RMSE in R\nUse the rmse() function from the yardstick package (part of tidymodels)\n\nrmse(duke_forest_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     167067.\n\n\n\n\nWhat does this value mean?"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#finding-r2-in-r",
    "href": "slides/04-slr-model-assessment-contd.html#finding-r2-in-r",
    "title": "SLR: Model assessment cont’d",
    "section": "Finding \\(R^2\\) in R",
    "text": "Finding \\(R^2\\) in R\nUse the rsq() function from the yardstick package (part of tidymodels)\n\nrsq(duke_forest_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.445\n\n\n\n\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\)\n\nglance(duke_forest_fit)$r.squared\n\n[1] 0.4451945"
  },
  {
    "objectID": "slides/04-slr-model-assessment-contd.html#recap",
    "href": "slides/04-slr-model-assessment-contd.html#recap",
    "title": "SLR: Model assessment cont’d",
    "section": "Recap",
    "text": "Recap\n\nEvaluated models using RMSE and \\(R^2\\)\nUsed analysis of variance to partition variability in the response variable\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/lab-00.html#meet-your-tas",
    "href": "slides/lab-00.html#meet-your-tas",
    "title": "Welcome to STA 221 labs!",
    "section": "Meet your TAs!",
    "text": "Meet your TAs!"
  },
  {
    "objectID": "slides/lab-00.html#meet-each-other",
    "href": "slides/lab-00.html#meet-each-other",
    "title": "Welcome to STA 221 labs!",
    "section": "Meet each other!",
    "text": "Meet each other!\n\n\nGet into groups of 2 or 3\nIntroduce yourself: Name, year, major (or academic interest), a highlight from the summer\nIntroduce your partner to the class\n\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/lab-00.html#what-to-expect-in-lab",
    "href": "slides/lab-00.html#what-to-expect-in-lab",
    "title": "Welcome to STA 221 labs!",
    "section": "What to expect in lab",
    "text": "What to expect in lab\n\nIntroduction to the lab assignment (~ 5 - 10 minutes)\nReview lecture content, as needed (~ 10 minutes)\nWork on the lab assignment (individual for Lab 01 and in teams for the remainder of the semester)\nStarting with Lab 01, you will find the starter materials for lab in your repo in the course GitHub organization."
  },
  {
    "objectID": "slides/lab-00.html#todays-lab",
    "href": "slides/lab-00.html#todays-lab",
    "title": "Welcome to STA 221 labs!",
    "section": "Today’s lab",
    "text": "Today’s lab\nThe rest of the today’s lab is focused on setting up the computing for the course and completing the class survey. Click the link below for the Lab 00 instructions. The instructions are available on the course website.\n\n🔗 sta221-fa24.netlify.app/labs/lab-00.html\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-tackett",
    "href": "slides/01-welcome.html#meet-prof.-tackett",
    "title": "Welcome to BIOSTAT 725",
    "section": "Meet Prof. Tackett!",
    "text": "Meet Prof. Tackett!\n\n\nEducation and career journey\n\nBS in Math and MS in Statistics from University of Tennessee\nStatistician at Capital One\nPhD in Statistics from University of Virginia\nAssistant Professor of the Practice, Department of Statistical Science at Duke\n\nWork focuses on statistics education and sense of belonging in introductory math and statistics classes\nCo-leader of the Bass Connections team Mental Health and the Justice System in Durham County\nMom of 19-month-old twins 🙂"
  },
  {
    "objectID": "slides/01-welcome.html#teaching-assistants-tas",
    "href": "slides/01-welcome.html#teaching-assistants-tas",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Teaching Assistants (TAs)",
    "text": "Teaching Assistants (TAs)\n\nDr. Youngsoo Baek (PhD)\n\nPhd in Statistical Science from Duke University\nPostdoc in Biostatistics & Bioinformatics\n\nBraden Scherting\n\nPhd candidate in Statistical Science"
  },
  {
    "objectID": "slides/01-welcome.html#check-in-on-ed-discussion",
    "href": "slides/01-welcome.html#check-in-on-ed-discussion",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Check-in on Ed Discussion!",
    "text": "Check-in on Ed Discussion!\n\nClick on the link or scan the QR code to answer the Ed Discussion poll\nhttps://edstem.org/us/courses/68995/discussion/5942168"
  },
  {
    "objectID": "slides/01-welcome.html#topics",
    "href": "slides/01-welcome.html#topics",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Topics",
    "text": "Topics\n\nIntroduction to the course\nSyllabus activity\nReview of probability"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-regression-analysis",
    "href": "slides/01-welcome.html#what-is-regression-analysis",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\n\nRegression analysis is a statistical method used to examine the relationship between a response variable and one or more predictor variables. It is used for predicting future values, understanding relationships between variables, and identifying key predictors. It also helps in modeling trends, assessing the impact of changes, and detecting outliers in data.\n\nSource: ChatGPT (with modification)"
  },
  {
    "objectID": "slides/01-welcome.html#example-rent-vs.-commute-time",
    "href": "slides/01-welcome.html#example-rent-vs.-commute-time",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Example: Rent vs. commute time",
    "text": "Example: Rent vs. commute time\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\n\n\\[\n\\text{rent} = \\beta_0 + \\beta_1 ~ \\text{commute_time} + \\epsilon\n\\]\n\n\n\n\\[\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\end{bmatrix} +  \\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-sta-221",
    "href": "slides/01-welcome.html#what-is-sta-221",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is STA 221?",
    "text": "What is STA 221?\n\n\n\n\n\n STA 210 \n\nApplication\n\n\n\n\n+\n\n\n\n\n\nSTA 211\n\nTheory\n\n\n\n\nPrerequisites: Introductory statistics or probability course and linear algebra\nRecommended corequisite: Probability course at Duke"
  },
  {
    "objectID": "slides/01-welcome.html#course-learning-objectives",
    "href": "slides/01-welcome.html#course-learning-objectives",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to…\n\nunderstand fundamental concepts of Bayesian statistics, including prior and posterior, and predictive distributions,\nimplement the Bayesian workflow, including model building, checking, and refinement,\nuse probabilistic programming software for Bayesian analysis (e.g., Stan),\napply Bayesian techniques to real-world health data,\ncommunicate Bayesian analysis results effectively to both technical and non-technical audiences, and\nidentify opportunities for using Bayesian statistics in your research and/or job."
  },
  {
    "objectID": "slides/01-welcome.html#course-topics",
    "href": "slides/01-welcome.html#course-topics",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course topics",
    "text": "Course topics\n\n\n\n\nLinear regression\n\nMethods for inference\nPrior elicitation\nPosterior estimation\nUncertainty quantification\nModel assessment\nBayesian workflow\nPrediction\n\n\n\n\nHealth Datasets\n\n\n\n\nExtensions\n\nRobust regression\nRegularization\nClassification\nMissing data\n\n\n\n\nHierarchical Model\n\nGaussian processes\nLongitudinal data\nSpatial data"
  },
  {
    "objectID": "slides/01-welcome.html#course-toolkit",
    "href": "slides/01-welcome.html#course-toolkit",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nWebsite: https://biostat725-sp25.netlify.app/\n\nCentral hub for the course!\nTour of the website\n\nCanvas: https://canvas.duke.edu/courses/53305\n\nGradebook\nAnnouncements\nGradescope\nEd Discussion\n\nGitHub: github.com/biostat725-sp25\n\nDistribute assignments\nPlatform for version control and collaboration"
  },
  {
    "objectID": "slides/01-welcome.html#computing-toolkit",
    "href": "slides/01-welcome.html#computing-toolkit",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nInference using Stan, a probabilistic programming language (rstan)\nWrite reproducible reports in Quarto\nAccess RStudio through STA725 Docker Containers\n\n\n\n\n\n\n\n\n\n\n\n\nAccess assignments\nFacilitates version control and collaboration\nAll work in BIOSTAT 725 course organization"
  },
  {
    "objectID": "slides/01-welcome.html#classroom-community",
    "href": "slides/01-welcome.html#classroom-community",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Classroom community",
    "text": "Classroom community\n\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know.\nPlease let me know your preferred pronouns, if you are comfortable sharing.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said or done in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/01-welcome.html#accessibility",
    "href": "slides/01-welcome.html#accessibility",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nIf you have documented accommodations from SDAO, please send the documentation as soon as possible.\nI am committed to making all course activities and materials accessible. If any course component is not accessible to you in any way, please don’t hesitate to let me know."
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity",
    "href": "slides/01-welcome.html#syllabus-activity",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity",
    "text": "Syllabus activity\n\n\nIntroduce yourself to your group members.\nChoose a reporter. This person will share the group’s summary with the class.\nRead the portion of the syllabus assigned to your group.\nDiscuss the key points and questions you my have.\nThe reporter will share a summary with the class."
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity-assignments",
    "href": "slides/01-welcome.html#syllabus-activity-assignments",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity assignments",
    "text": "Syllabus activity assignments\n\nGroup 1: What to expect in the course\nGroup 2: Homework\nGroup 3: Exams\nGroup 4: Live Coding\nGroup 5: Application Exercises\nGroup 6: Academic honesty (except AI policy)\nGroup 7: Artificial intelligence policy\nGroup 8: Late work policy and waiver for extenuating circumstances\nGroup 9: Attendance and lecture recording request\nGroup 10: Getting help in the course"
  },
  {
    "objectID": "slides/01-welcome.html#syllabus-activity-report-out",
    "href": "slides/01-welcome.html#syllabus-activity-report-out",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Syllabus activity report out",
    "text": "Syllabus activity report out\n\n\nGroup 1: What to expect in the course\nGroup 2: Homework\nGroup 3: Exams\nGroup 4: Live Coding\nGroup 5: Application Exercises\nGroup 6: Academic honesty (except AI policy)\nGroup 7: Artificial intelligence policy\nGroup 8: Late work policy and waiver for extenuating circumstances\nGroup 9: Attendance and lecture recording request\nGroup 10: Getting help in the course"
  },
  {
    "objectID": "slides/01-welcome.html#grading",
    "href": "slides/01-welcome.html#grading",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n40%\n\n\nExam 01\n20%\n\n\nExam 02\n20%\n\n\nLive Coding\n10%\n\n\nApplication Exercises\n10%\n\n\nTotal\n100%"
  },
  {
    "objectID": "slides/01-welcome.html#five-tips-for-success-in-sta-221",
    "href": "slides/01-welcome.html#five-tips-for-success-in-sta-221",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Five tips for success in STA 221",
    "text": "Five tips for success in STA 221\n\nComplete all the preparation work before class.\nAsk questions in class, office hours, and on Ed Discussion.\nDo the homework and labs; get started on homework early when possible.\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStay up-to-date on announcements on Ed Discussion and sent via email."
  },
  {
    "objectID": "slides/01-welcome.html#reproducibility-checklist",
    "href": "slides/01-welcome.html#reproducibility-checklist",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n✔️ Can the tables and figures be exactly reproduced from the code and data?\n✔️ Does the code actually do what you think it does?\n✔️ In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n✔️ Can the code be used for other data?\n✔️ Can you extend the code to do other things?"
  },
  {
    "objectID": "slides/01-welcome.html#why-is-reproducibility-important",
    "href": "slides/01-welcome.html#why-is-reproducibility-important",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)"
  },
  {
    "objectID": "slides/01-welcome.html#toolkit",
    "href": "slides/01-welcome.html#toolkit",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub"
  },
  {
    "objectID": "slides/01-welcome.html#r-and-rstudio",
    "href": "slides/01-welcome.html#r-and-rstudio",
    "title": "Welcome to BIOSTAT 725!",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\nSource: Statistical Inference via Data Science"
  },
  {
    "objectID": "slides/01-welcome.html#rstudio-ide",
    "href": "slides/01-welcome.html#rstudio-ide",
    "title": "Welcome to BIOSTAT 725!",
    "section": "RStudio IDE",
    "text": "RStudio IDE"
  },
  {
    "objectID": "slides/01-welcome.html#quarto",
    "href": "slides/01-welcome.html#quarto",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports – the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)"
  },
  {
    "objectID": "slides/01-welcome.html#quarto-1",
    "href": "slides/01-welcome.html#quarto-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "slides/01-welcome.html#how-will-we-use-quarto",
    "href": "slides/01-welcome.html#how-will-we-use-quarto",
    "title": "Welcome to BIOSTAT 725!",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou’ll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-versioning",
    "href": "slides/01-welcome.html#what-is-versioning",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is versioning?",
    "text": "What is versioning?"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-versioning-1",
    "href": "slides/01-welcome.html#what-is-versioning-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is versioning?",
    "text": "What is versioning?\nwith human readable messages"
  },
  {
    "objectID": "slides/01-welcome.html#why-do-we-need-version-control",
    "href": "slides/01-welcome.html#why-do-we-need-version-control",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\n\n\n\n\n\n\n\nProvides a clear record of how the analysis methods evolved. This makes analysis auditable and thus more trustworthy and reliable. (Ostblom and Timbers 2022)"
  },
  {
    "objectID": "slides/01-welcome.html#git-and-github",
    "href": "slides/01-welcome.html#git-and-github",
    "title": "Welcome to BIOSTAT 725!",
    "section": "git and GitHub",
    "text": "git and GitHub\n\n\ngit is a version control system – like “Track Changes” features from Microsoft Word.\nGitHub is the home for your git-based projects on the internet (like DropBox but much better).\nThere are a lot of git commands and very few people know them all. 99% of the time you will use git to add, commit, push, and pull."
  },
  {
    "objectID": "slides/01-welcome.html#this-week",
    "href": "slides/01-welcome.html#this-week",
    "title": "Welcome to BIOSTAT 725!",
    "section": "This week",
    "text": "This week\n\nComplete Lab 00 tasks\nReview syllabus\nComplete reading to prepare for Thursday’s lecture\nThursday’s lecture: Simple linear regression"
  },
  {
    "objectID": "slides/01-welcome.html#references",
    "href": "slides/01-welcome.html#references",
    "title": "Welcome to BIOSTAT 725!",
    "section": "References",
    "text": "References\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/09-inference-pt2.html#announcements",
    "href": "slides/09-inference-pt2.html#announcements",
    "title": "Inference for regression",
    "section": "Announcements",
    "text": "Announcements\n\nProject\n\nResearch questions due Thursday at 11:59pm\nProposal due Thursday, October 3 at 11:59pm\n\nLab 03 due Thursday, October 3 at 11:59pm\nStatistics experience due Tue, Nov 26 at 11:59pm"
  },
  {
    "objectID": "slides/09-inference-pt2.html#topics",
    "href": "slides/09-inference-pt2.html#topics",
    "title": "Inference for regression",
    "section": "Topics",
    "text": "Topics\n\nUnderstand statistical inference in the context of regression\nDescribe the assumptions for regression\nUnderstand connection between distribution of residuals and inferential procedures\nConduct inference on a single coefficient\nConduct inference on the overall regression model"
  },
  {
    "objectID": "slides/09-inference-pt2.html#computing-setup",
    "href": "slides/09-inference-pt2.html#computing-setup",
    "title": "Inference for regression",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(kableExtra)  \nlibrary(patchwork)   \n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "slides/09-inference-pt2.html#data-ncaa-football-expenditures",
    "href": "slides/09-inference-pt2.html#data-ncaa-football-expenditures",
    "title": "Inference for regression",
    "section": "Data: NCAA Football expenditures",
    "text": "Data: NCAA Football expenditures\nToday’s data come from Equity in Athletics Data Analysis and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a March 2022 Tidy Tuesday.\nWe will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :\n\ntotal_exp_m: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\nenrollment_th: Total student enrollment in the 2019 - 2020 academic year (in thousands)\ntype: institution type (Public or Private)\n\n\nfootball &lt;- read_csv(\"data/ncaa-football-exp.csv\")"
  },
  {
    "objectID": "slides/09-inference-pt2.html#univariate-eda",
    "href": "slides/09-inference-pt2.html#univariate-eda",
    "title": "Inference for regression",
    "section": "Univariate EDA",
    "text": "Univariate EDA"
  },
  {
    "objectID": "slides/09-inference-pt2.html#bivariate-eda",
    "href": "slides/09-inference-pt2.html#bivariate-eda",
    "title": "Inference for regression",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA"
  },
  {
    "objectID": "slides/09-inference-pt2.html#regression-model",
    "href": "slides/09-inference-pt2.html#regression-model",
    "title": "Inference for regression",
    "section": "Regression model",
    "text": "Regression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type, data = football)\ntidy(exp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n\n\n\n\n\n\nFor every additional 1,000 students, we expect the institution’s total expenditures on football to increase by $780,000, on average, holding institution type constant."
  },
  {
    "objectID": "slides/09-inference-pt2.html#statistical-inference",
    "href": "slides/09-inference-pt2.html#statistical-inference",
    "title": "Inference for regression",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\n\nStatistical inference provides methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be representative (ideally random) of the population we’re interested in\n\n\n\n\n\nImage source: Eugene Morgan © Penn State"
  },
  {
    "objectID": "slides/09-inference-pt2.html#inference-for-linear-regression",
    "href": "slides/09-inference-pt2.html#inference-for-linear-regression",
    "title": "Inference for regression",
    "section": "Inference for linear regression",
    "text": "Inference for linear regression\n\nInference based on ANOVA\n\nHypothesis test for the statistical significance of the overall regression model\nHypothesis test for a subset of coefficients\n\nInference for a single coefficient \\(\\beta_j\\)\n\nHypothesis test for a coefficient \\(\\beta_j\\)\nConfidence interval for a coefficient \\(\\beta_j\\)"
  },
  {
    "objectID": "slides/09-inference-pt2.html#linear-regression-model",
    "href": "slides/09-inference-pt2.html#linear-regression-model",
    "title": "Inference for regression",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\n\\begin{aligned}\n\\mathbf{y} &= Model + Error \\\\[5pt]\n&= f(\\mathbf{X}) + \\boldsymbol{\\epsilon} \\\\[5pt]\n&= E(\\mathbf{y}|\\mathbf{X}) + \\mathbf{\\epsilon} \\\\[5pt]\n&= \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\n\\end{aligned}\n\\]\n\n\n\nWe have discussed multiple ways to find the least squares estimates of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\\\beta_1\\end{bmatrix}\\)\n\nNone of these approaches depend on the distribution of \\(\\boldsymbol{\\epsilon}\\)\n\nNow we will use statistical inference to draw conclusions about \\(\\boldsymbol{\\beta}\\) that depend on particular assumptions about the distribution of \\(\\boldsymbol{\\epsilon}\\)"
  },
  {
    "objectID": "slides/09-inference-pt2.html#linear-regression-model-1",
    "href": "slides/09-inference-pt2.html#linear-regression-model-1",
    "title": "Inference for regression",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/09-inference-pt2.html#expected-value-of-mathbfy",
    "href": "slides/09-inference-pt2.html#expected-value-of-mathbfy",
    "title": "Inference for regression",
    "section": "Expected value of \\(\\mathbf{y}\\)",
    "text": "Expected value of \\(\\mathbf{y}\\)\nLet \\(\\mathbf{b} = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\b_p\\end{bmatrix}\\) be a \\(p \\times 1\\) vector of random variables.\n\n\nThen \\(E(\\mathbf{b}) = E\\begin{bmatrix}b_1 \\\\ \\vdots \\\\ b_p\\end{bmatrix} = \\begin{bmatrix}E(b_1) \\\\ \\vdots \\\\ E(b_p)\\end{bmatrix}\\)\n\n\n\n\nUse this to find \\(E(\\mathbf{y}|\\mathbf{X})\\)."
  },
  {
    "objectID": "slides/09-inference-pt2.html#variance",
    "href": "slides/09-inference-pt2.html#variance",
    "title": "Inference for regression",
    "section": "Variance",
    "text": "Variance\nLet \\(\\mathbf{b} = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\b_p\\end{bmatrix}\\) be a \\(p \\times 1\\) vector of independent random variables.\n\n\nThen \\(Var(\\mathbf{b}) = \\begin{bmatrix}Var(b_1) & 0 & \\dots & 0 \\\\ 0 & Var(b_2) & \\dots & 0 \\\\ \\vdots & \\vdots & \\dots & \\cdot \\\\ 0 & 0 & \\dots & Var(b_p)\\end{bmatrix}\\)\n\n\n\n\nUse this to find \\(Var(\\mathbf{y}|\\mathbf{X})\\)."
  },
  {
    "objectID": "slides/09-inference-pt2.html#assumptions-of-regression",
    "href": "slides/09-inference-pt2.html#assumptions-of-regression",
    "title": "Inference for regression",
    "section": "Assumptions of regression",
    "text": "Assumptions of regression\n\n\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n\\]\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\n\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from one another."
  },
  {
    "objectID": "slides/09-inference-pt2.html#estimating-sigma2_epsilon",
    "href": "slides/09-inference-pt2.html#estimating-sigma2_epsilon",
    "title": "Inference for regression",
    "section": "Estimating \\(\\sigma^2_{\\epsilon}\\)",
    "text": "Estimating \\(\\sigma^2_{\\epsilon}\\)\n\nOnce we fit the model, we can use the residuals to estimate \\(\\sigma_{\\epsilon}^2\\)\n\\(\\hat{\\sigma}^2_{\\epsilon}\\) is needed for hypothesis testing and constructing confidence intervals for regression\n\n\\[\n\\hat{\\sigma}^2_\\epsilon = \\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-p-1} = \\frac{\\sum_\\limits{i=1}^ne_i^2}{n - p - 1} = \\frac{SSR}{n - p - 1}\n\\]\n\nThe regression standard error \\(\\hat{\\sigma}_{\\epsilon}\\) is a measure of the average distance between the observations and regression line\n\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{SSR}{n - p - 1}}\n\\]"
  },
  {
    "objectID": "slides/09-inference-pt2.html#inference-for-beta_j",
    "href": "slides/09-inference-pt2.html#inference-for-beta_j",
    "title": "Inference for regression",
    "section": "Inference for \\(\\beta_j\\)",
    "text": "Inference for \\(\\beta_j\\)\nWe often want to conduct inference on individual model coefficients\n\nHypothesis test: Is there a linear relationship between the response and \\(x_j\\)?\nConfidence interval: What is a plausible range of values \\(\\beta_j\\) can take?\n\nBut first we need to understand the distribution of \\(\\hat{\\beta}_j\\)"
  },
  {
    "objectID": "slides/09-inference-pt2.html#sampling-distribution-of-hatbeta_j",
    "href": "slides/09-inference-pt2.html#sampling-distribution-of-hatbeta_j",
    "title": "Inference for regression",
    "section": "Sampling distribution of \\(\\hat{\\beta}_j\\)",
    "text": "Sampling distribution of \\(\\hat{\\beta}_j\\)\n\\[\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon(\\mathbf{X}^T\\mathbf{X})^{-1})\n\\]\nLet \\(\\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\). Then, for each coefficient \\(\\hat{\\beta}_j\\),\n\n\n\\(E(\\hat{\\beta}_j) = \\boldsymbol{\\beta}_j\\), the \\(j^{th}\\) element of \\(\\boldsymbol{\\beta}\\)\n\\(Var(\\hat{\\beta}_j) = \\sigma^2_{\\epsilon}C_{jj}\\)\n\\(Cov(\\hat{\\beta}_i, \\hat{\\beta}_j) = \\sigma^2_{\\epsilon}C_{ij}\\)"
  },
  {
    "objectID": "slides/09-inference-pt2.html#steps-for-a-hypothesis-test",
    "href": "slides/09-inference-pt2.html#steps-for-a-hypothesis-test",
    "title": "Inference for regression",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState the null and alternative hypotheses.\nCalculate a test statistic.\nCalculate the p-value.\nState the conclusion."
  },
  {
    "objectID": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-hypotheses",
    "href": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-hypotheses",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Hypotheses",
    "text": "Hypothesis test for \\(\\beta_j\\): Hypotheses\nWe will generally test the hypotheses:\n\nNull Hypothesis: \\(H_0: \\beta_j = 0\\)\n\nThere is no linear relationship between \\(\\beta_j\\) and \\(y\\) after accounting for the other variables in the model\n\nAlternative hypothesis: \\(H_a: \\beta_j \\neq 0\\)\n\nThere is a linear relationship between \\(\\beta_j\\) and \\(y\\) after accounting for the other variables in the model"
  },
  {
    "objectID": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-test-statistic",
    "href": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-test-statistic",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Test statistic",
    "text": "Hypothesis test for \\(\\beta_j\\): Test statistic\nTest statistic: Number of standard errors the estimate is away from the null hypothesized value\n\\[\n\\text{Test Statstic} = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n\\]\n\n\n\\[T = \\frac{\\hat{\\beta}_j - 0}{SE(\\hat{\\beta}_j)} ~ = ~\\frac{\\hat{\\beta}_j - 0}{\\sqrt{\\hat{\\sigma}^2_\\epsilon C_{jj}}} ~\\sim ~ t_{n-p-1}\n\\]"
  },
  {
    "objectID": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-p-value",
    "href": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-p-value",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): P-value",
    "text": "Hypothesis test for \\(\\beta_j\\): P-value\nThe p-value is the probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\np-value = P(|t| &gt; |\\text{test statistic}|),\n\\]\ncalculated from a \\(t\\) distribution with \\(n- p - 1\\) degrees of freedom"
  },
  {
    "objectID": "slides/09-inference-pt2.html#understanding-the-p-value",
    "href": "slides/09-inference-pt2.html#understanding-the-p-value",
    "title": "Inference for regression",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-conclusion",
    "href": "slides/09-inference-pt2.html#hypothesis-test-for-beta_j-conclusion",
    "title": "Inference for regression",
    "section": "Hypothesis test for \\(\\beta_j\\): Conclusion",
    "text": "Hypothesis test for \\(\\beta_j\\): Conclusion\nThere are two parts to the conclusion\n\nMake a conclusion by comparing the p-value to a predetermined decision-making threshold called the significance level ( \\(\\alpha\\) level)\n\nIf \\(\\text{p-value} &lt; \\alpha\\): Reject \\(H_0\\)\nIf \\(\\text{p-value} \\geq \\alpha\\): Fail to reject \\(H_0\\)\n\nState the conclusion in the context of the data"
  },
  {
    "objectID": "slides/09-inference-pt2.html#confidence-interval-for-beta_j-1",
    "href": "slides/09-inference-pt2.html#confidence-interval-for-beta_j-1",
    "title": "Inference for regression",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\n\nA plausible range of values for a population parameter is called a confidence interval\nUsing only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nSimilarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter"
  },
  {
    "objectID": "slides/09-inference-pt2.html#what-confidence-means",
    "href": "slides/09-inference-pt2.html#what-confidence-means",
    "title": "Inference for regression",
    "section": "What “confidence” means",
    "text": "What “confidence” means\n\n\nWe will construct \\(C\\%\\) confidence intervals.\n\nThe confidence level impacts the width of the interval\n\n\n\n\n“Confident” means if we were to take repeated samples of the same size as our data, fit regression lines using the same predictors, and calculate \\(C\\%\\) CIs for the coefficient of \\(x_j\\), then \\(C\\%\\) of those intervals will contain the true value of the coefficient \\(\\beta_j\\)\n\n\n\nBalance precision and accuracy when selecting a confidence level"
  },
  {
    "objectID": "slides/09-inference-pt2.html#confidence-interval-for-beta_j-2",
    "href": "slides/09-inference-pt2.html#confidence-interval-for-beta_j-2",
    "title": "Inference for regression",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE({\\hat{\\beta}_j})\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-p-1\\) degrees of freedom"
  },
  {
    "objectID": "slides/09-inference-pt2.html#confidence-interval-critical-value",
    "href": "slides/09-inference-pt2.html#confidence-interval-critical-value",
    "title": "Inference for regression",
    "section": "Confidence interval: Critical value",
    "text": "Confidence interval: Critical value\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(football) - 2 - 1)\n\n[1] 1.97928\n\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(football) - 2 - 1)\n\n[1] 1.657235\n\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(football) - 2 - 1)\n\n[1] 2.61606"
  },
  {
    "objectID": "slides/09-inference-pt2.html#ci-for-beta_j-calculation",
    "href": "slides/09-inference-pt2.html#ci-for-beta_j-calculation",
    "title": "Inference for regression",
    "section": "95% CI for \\(\\beta_j\\): Calculation",
    "text": "95% CI for \\(\\beta_j\\): Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0"
  },
  {
    "objectID": "slides/09-inference-pt2.html#ci-for-beta_j-in-r",
    "href": "slides/09-inference-pt2.html#ci-for-beta_j-in-r",
    "title": "Inference for regression",
    "section": "95% CI for \\(\\beta_j\\) in R",
    "text": "95% CI for \\(\\beta_j\\) in R\n\ntidy(exp_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n19.332\n2.984\n6.478\n0\n13.426\n25.239\n\n\nenrollment_th\n0.780\n0.110\n7.074\n0\n0.562\n0.999\n\n\ntypePublic\n-13.226\n3.153\n-4.195\n0\n-19.466\n-6.986\n\n\n\n\n\n\nInterpretation: We are 95% confident that for each additional 1,000 students enrolled, the institution’s expenditures on football will be greater by $562,000 to $999,000, on average, holding institution type constant."
  },
  {
    "objectID": "slides/09-inference-pt2.html#test-for-overall-significance-hypotheses",
    "href": "slides/09-inference-pt2.html#test-for-overall-significance-hypotheses",
    "title": "Inference for regression",
    "section": "Test for overall significance: Hypotheses",
    "text": "Test for overall significance: Hypotheses\nWe can conduct a hypothesis test using the ANOVA table to determine if there is at least one non-zero coefficient in the model\n\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\dots = \\beta_p = 0\\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one }j\n\\end{aligned}\n\\]\n\nFor the football data\n\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\beta_2 = 0\\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one }j\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-inference-pt2.html#test-for-overall-significance-test-statistic",
    "href": "slides/09-inference-pt2.html#test-for-overall-significance-test-statistic",
    "title": "Inference for regression",
    "section": "Test for overall significance: Test statistic",
    "text": "Test for overall significance: Test statistic\n\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF Stat\nPr(&gt; F)\n\n\n\n\nModel\n2\n7138.591\n3569.296\n26.628\n0\n\n\nResiduals\n124\n16621.344\n134.043\n\n\n\n\nTotal\n126\n23759.935\n\n\n\n\n\n\n\n\n\nTest statistic: Ratio of explained to unexplained variability\n\\[\nF = \\frac{\\text{Mean Square Model}}{\\text{Mean Square Residuals}}\n\\]\nThe test statistic follows an \\(F\\) distribution with \\(p\\) and \\(n -  p - 1\\) degrees of freedom"
  },
  {
    "objectID": "slides/09-inference-pt2.html#test-for-overall-significance-p-value",
    "href": "slides/09-inference-pt2.html#test-for-overall-significance-p-value",
    "title": "Inference for regression",
    "section": "Test for overall significance: P-value",
    "text": "Test for overall significance: P-value\n\n\\[\n\\text{P-value} = \\text{Pr}(F &gt; \\text{F Stat})\n\\]"
  },
  {
    "objectID": "slides/09-inference-pt2.html#test-for-overall-significance-conclusion",
    "href": "slides/09-inference-pt2.html#test-for-overall-significance-conclusion",
    "title": "Inference for regression",
    "section": "Test for overall significance: Conclusion",
    "text": "Test for overall significance: Conclusion\n\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\beta_2 = 0\\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one }j\n\\end{aligned}\n\\]\n\nfootball_anova |&gt;\n  kable(digits = 3)\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF Stat\nPr(&gt; F)\n\n\n\n\nModel\n2\n7138.591\n3569.296\n26.628\n0\n\n\nResiduals\n124\n16621.344\n134.043\n\n\n\n\nTotal\n126\n23759.935\n\n\n\n\n\n\n\n\n\nWhat is the conclusion from this hypothesis test?"
  },
  {
    "objectID": "slides/09-inference-pt2.html#recap",
    "href": "slides/09-inference-pt2.html#recap",
    "title": "Inference for regression",
    "section": "Recap",
    "text": "Recap\n\nIntroduced statistical inference in the context of regression\nDescribed the assumptions for regression\nConnected the distribution of residuals and inferential procedures\nConducted inference on a single coefficient\nConducted inference on the overall regression model\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/04-slr-matrix.html#topics",
    "href": "slides/04-slr-matrix.html#topics",
    "title": "SLR: Matrix representation",
    "section": "Topics",
    "text": "Topics\n\nMatrix representation for simple linear regression\n\nModel form\nLeast square estimate\nPredicted (fitted) values\nResiduals\n\nMatrix representation in R"
  },
  {
    "objectID": "slides/04-slr-matrix.html#slr-statistical-model-population",
    "href": "slides/04-slr-matrix.html#slr-statistical-model-population",
    "title": "SLR: Matrix representation",
    "section": "SLR: Statistical model (population)",
    "text": "SLR: Statistical model (population)\nWhen we have a quantitative response, \\(Y\\), and a single quantitative predictor, \\(X\\), we can use a simple linear regression model to describe the relationship between \\(Y\\) and \\(X\\). \\[\\large{Y = \\mathbf{\\beta_0 + \\beta_1 X} + \\epsilon}, \\hspace{8mm} \\epsilon \\sim N(0, \\sigma_{\\epsilon}^2)\\]\n\n\n\\(\\beta_1\\): Population (true) slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): Population (true) intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "slides/04-slr-matrix.html#slr-in-matrix-form",
    "href": "slides/04-slr-matrix.html#slr-in-matrix-form",
    "title": "SLR: Matrix representation",
    "section": "SLR in matrix form",
    "text": "SLR in matrix form\nSuppose we have \\(n\\) observations.\n\\[\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n=\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_1 \\\\\n\\vdots &  \\vdots \\\\\n1 &  x_n\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n\\]\n\n\nWhat are the dimensions of \\(\\mathbf{y}\\), \\(\\mathbf{X}\\), \\(\\boldsymbol{\\beta}\\), and \\(\\boldsymbol{\\epsilon}\\)?"
  },
  {
    "objectID": "slides/04-slr-matrix.html#sum-of-squared-residuals",
    "href": "slides/04-slr-matrix.html#sum-of-squared-residuals",
    "title": "SLR: Matrix representation",
    "section": "Sum of squared residuals",
    "text": "Sum of squared residuals\nWe use the sum of squared residuals (also called “sum of squared error”) to find the least squares line:\n\\[\nSSR = \\sum_{i=1}^ne_i^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - \\hat{\\mathbf{y}})^T(\\mathbf{y} - \\hat{\\mathbf{y}})\n\\]\n\n\n\nWhat is the dimension of SSR?\nWhat is \\(\\hat{\\mathbf{y}}\\) in terms of \\(\\mathbf{y}\\), \\(\\mathbf{X}\\), and/or \\(\\boldsymbol{\\beta}\\) ?"
  },
  {
    "objectID": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals",
    "href": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nWe want to find values of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimize the sum of squared residuals \\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-1",
    "href": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-1",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nWe want to find values of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimize the sum of squared residuals \\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-2",
    "href": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-2",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nWe want to find values of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimize the sum of squared residuals \\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-3",
    "href": "slides/04-slr-matrix.html#minimize-sum-of-squared-residuals-3",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nWe want to find values of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimize the sum of squared residuals \\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#least-squares-estimators",
    "href": "slides/04-slr-matrix.html#least-squares-estimators",
    "title": "SLR: Matrix representation",
    "section": "Least squares estimators",
    "text": "Least squares estimators\n\\[\nSSR = \\mathbf{e}^T\\mathbf{e} =\\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\\]\n\n\nThe least squares estimators must satisfy\n\\[\n\\nabla_{\\boldsymbol{\\beta}} SSR = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = 0\n\\]\n\n\n\n\\[\n\\color{#993399}{\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#did-we-find-a-minimum",
    "href": "slides/04-slr-matrix.html#did-we-find-a-minimum",
    "title": "SLR: Matrix representation",
    "section": "Did we find a minimum?",
    "text": "Did we find a minimum?\n\\[\n\\nabla^2_{\\beta} SSR \\propto  2\\mathbf{X}^T\\mathbf{X} = 0\n\\]\n\n\n\\(\\mathbf{X}\\) is full rank \\(\\Rightarrow\\) \\(\\mathbf{X}^T\\mathbf{X}\\) is positive definite\nTherefore we have found the minimizing point"
  },
  {
    "objectID": "slides/04-slr-matrix.html#obtain-mathbfy-vector",
    "href": "slides/04-slr-matrix.html#obtain-mathbfy-vector",
    "title": "SLR: Matrix representation",
    "section": "Obtain \\(\\mathbf{y}\\) vector",
    "text": "Obtain \\(\\mathbf{y}\\) vector\nLet’s go back to the Duke Forest data. We want to use the matrix representation to fit a model of the form:\n\\[\nprice = \\beta_0 + \\beta_1 ~ area + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2_\\epsilon)\n\\]\n\nGet \\(\\mathbf{y}\\), the vector of responses\n\ny &lt;- duke_forest$price\n\n\n\n\nLet’s look at the first 10 observations of \\(y\\)\n\ny[1:10]\n\n [1] 1520000 1030000  420000  680000  428500  456000 1270000  557450  697500\n[10]  650000"
  },
  {
    "objectID": "slides/04-slr-matrix.html#obtain-mathbfx-matrix",
    "href": "slides/04-slr-matrix.html#obtain-mathbfx-matrix",
    "title": "SLR: Matrix representation",
    "section": "Obtain \\(\\mathbf{X}\\) matrix",
    "text": "Obtain \\(\\mathbf{X}\\) matrix\nUse the model.matrix() function to get \\(\\mathbf{X}\\)\n\nX &lt;- model.matrix(price ~ area, data = duke_forest)\n\n\n\nLet’s look at the first 10 rows of \\(\\mathbf{X}\\)\n\nX[1:10,]\n\n   (Intercept) area\n1            1 6040\n2            1 4475\n3            1 1745\n4            1 2091\n5            1 1772\n6            1 1950\n7            1 3909\n8            1 2841\n9            1 3924\n10           1 2173"
  },
  {
    "objectID": "slides/04-slr-matrix.html#calculate-hatboldsymbolbeta",
    "href": "slides/04-slr-matrix.html#calculate-hatboldsymbolbeta",
    "title": "SLR: Matrix representation",
    "section": "Calculate \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Calculate \\(\\hat{\\boldsymbol{\\beta}}\\)\nMatrix functions in R. Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be matrices\n\nt(A): transpose \\(\\mathbf{A}\\)\nsolve(A): inverse of \\(\\mathbf{A}\\)\nA %*% B: multiply \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)\n\n\nNow let’s calculate \\(\\hat{\\boldsymbol{\\beta}}\\)\n\nbeta_hat &lt;- solve(t(X)%*%X)%*%t(X)%*%y\nbeta_hat\n\n                   [,1]\n(Intercept) 116652.3251\narea           159.4833"
  },
  {
    "objectID": "slides/04-slr-matrix.html#compare-to-result-from-lm",
    "href": "slides/04-slr-matrix.html#compare-to-result-from-lm",
    "title": "SLR: Matrix representation",
    "section": "Compare to result from lm",
    "text": "Compare to result from lm\n\nduke_forest_model &lt;- lm(price ~ area, data = duke_forest)\ntidy(duke_forest_model) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000\n\n\n\n\n\n\n\n\nbeta_hat \n\n                   [,1]\n(Intercept) 116652.3251\narea           159.4833"
  },
  {
    "objectID": "slides/04-slr-matrix.html#predicted-fitted-values",
    "href": "slides/04-slr-matrix.html#predicted-fitted-values",
    "title": "SLR: Matrix representation",
    "section": "Predicted (fitted) values",
    "text": "Predicted (fitted) values\nNow that we have \\(\\hat{\\boldsymbol{\\beta}}\\), let’s predict values of \\(\\mathbf{y}\\) using the model\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\underbrace{\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T}_{\\mathbf{H}}\\mathbf{y} = \\mathbf{H}\\mathbf{y}\n\\]\n\nHat matrix: \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\n\n\\(\\mathbf{H}\\) is an \\(n\\times n\\) matrix\nMaps vector of observed values \\(\\mathbf{y}\\) to a vector of fitted values \\(\\hat{\\mathbf{y}}\\)"
  },
  {
    "objectID": "slides/04-slr-matrix.html#residuals",
    "href": "slides/04-slr-matrix.html#residuals",
    "title": "SLR: Matrix representation",
    "section": "Residuals",
    "text": "Residuals\nRecall that the residuals are the difference between the observed and predicted values\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{y} - \\hat{\\mathbf{y}}\\\\[10pt]\n& = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n& = \\mathbf{y} - \\mathbf{H}\\mathbf{y} \\\\[10pt]\n& = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}\n\\end{aligned}\n\\]\n\n\\[\n\\color{#993399}{\\mathbf{e} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}}\n\\]"
  },
  {
    "objectID": "slides/04-slr-matrix.html#recap",
    "href": "slides/04-slr-matrix.html#recap",
    "title": "SLR: Matrix representation",
    "section": "Recap",
    "text": "Recap\n\nIntroduced matrix representation for simple linear regression\n\nModel from\nLeast square estimate\nPredicted (fitted) values\nResiduals\n\nUsed R for matrix calculations"
  },
  {
    "objectID": "slides/04-slr-matrix.html#next-class",
    "href": "slides/04-slr-matrix.html#next-class",
    "title": "SLR: Matrix representation",
    "section": "Next class",
    "text": "Next class\n\nMultiple linear regression\nSee Sep 10 prepare\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/05-mlr.html#topics",
    "href": "slides/05-mlr.html#topics",
    "title": "Multiple linear regression (MLR)",
    "section": "Topics",
    "text": "Topics\n\nExploratory data analysis for multiple linear regression\nFitting the least squares line\nInterpreting coefficients for quantitative predictors\nPrediction"
  },
  {
    "objectID": "slides/05-mlr.html#computing-setup",
    "href": "slides/05-mlr.html#computing-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computing setup",
    "text": "Computing setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/05-mlr.html#data-peer-to-peer-lender",
    "href": "slides/05-mlr.html#data-peer-to-peer-lender",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 4\n   annual_income debt_to_income verified_income interest_rate\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/05-mlr.html#variables",
    "href": "slides/05-mlr.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nOutcome: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/05-mlr.html#outcome-interest_rate",
    "href": "slides/05-mlr.html#outcome-interest_rate",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\n\n\n\n\nMin\nMedian\nMax\nIQR\n\n\n\n\n5.31\n9.93\n26.3\n5.755"
  },
  {
    "objectID": "slides/05-mlr.html#predictors",
    "href": "slides/05-mlr.html#predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/05-mlr.html#data-manipulation-1-rescale-income",
    "href": "slides/05-mlr.html#data-manipulation-1-rescale-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_th = annual_income / 1000)\n\n\n\n\nWhy did we rescale income?"
  },
  {
    "objectID": "slides/05-mlr.html#outcome-vs.-predictors",
    "href": "slides/05-mlr.html#outcome-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors\n\n\nGoal: Use these predictors in a single model to understand variability in interest rate.\n\n\n\nWhy do we want to use a single model versus 3 separate simple linear regression models?"
  },
  {
    "objectID": "slides/05-mlr.html#multiple-linear-regression-mlr",
    "href": "slides/05-mlr.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\text{interest_rate} ~ =\n\\beta_0 & + \\beta_1 ~ \\text{debt_to_income} \\\\ & + \\beta_2 ~ \\text{verified_income} \\\\ &+ \\beta_3~ \\text{annual_income_th} \\\\\n& +\\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-mlr.html#multiple-linear-regression-1",
    "href": "slides/05-mlr.html#multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nRecall: The simple linear regression model\n\\[\nY = \\beta_0 + \\beta_1~ X + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\]\n\nThe form of the multiple linear regression model is\n\\[\nY = \\beta_0 + \\beta_1X_1 +  \\dots + \\beta_pX_p + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\]\n\n\n\nTherefore,\n\\[\nE(Y|X_1, \\ldots, X_p) = \\beta_0 + \\beta_1X_1 +  \\dots + \\beta_pX_p\n\\]"
  },
  {
    "objectID": "slides/05-mlr.html#fitting-the-least-squares-line",
    "href": "slides/05-mlr.html#fitting-the-least-squares-line",
    "title": "Multiple linear regression (MLR)",
    "section": "Fitting the least squares line",
    "text": "Fitting the least squares line\nSimilar to simple linear regression, we want to find estimates for \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) that minimize\n\\[\n\\sum_{i=1}^{n}e_i^2 = \\sum_{i=1}^n[y_i - \\hat{y}_i]^2 = \\sum_{i=1}^n[y_i - (\\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_px_{ip})]^2\n\\]\n\n\nThe calculations can be very tedious, especially if \\(p\\) is large"
  },
  {
    "objectID": "slides/05-mlr.html#matrix-form-of-multiple-linear-regression",
    "href": "slides/05-mlr.html#matrix-form-of-multiple-linear-regression",
    "title": "Multiple linear regression (MLR)",
    "section": "Matrix form of multiple linear regression",
    "text": "Matrix form of multiple linear regression\nSuppose we have \\(n\\) observations, a quantitative response variable, and \\(p\\) &gt; 1 predictors \\[\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n=\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_{11} & \\dots & x_{1p}\\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n1 &  x_{n1} & \\dots &x_{np}\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n\\]\n\nWhat are the dimensions of \\(\\mathbf{y}\\), \\(\\mathbf{X}\\), \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\epsilon}\\)?"
  },
  {
    "objectID": "slides/05-mlr.html#matrix-form-of-multiple-linear-regression-1",
    "href": "slides/05-mlr.html#matrix-form-of-multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Matrix form of multiple linear regression",
    "text": "Matrix form of multiple linear regression\nAs with simple linear regression, we have\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]\n\nGeneralizing the derivations from SLR to \\(p &gt; 2\\), we have\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\\]\nas before."
  },
  {
    "objectID": "slides/05-mlr.html#model-fit-in-r",
    "href": "slides/05-mlr.html#model-fit-in-r",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit in R",
    "text": "Model fit in R\n\nint_fit &lt;- lm(interest_rate ~ debt_to_income + verified_income  + annual_income_th,\n              data = loan50)\n\ntidy(int_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078"
  },
  {
    "objectID": "slides/05-mlr.html#model-equation",
    "href": "slides/05-mlr.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{interest_rate}} =  10.726 &+0.671 \\times \\text{debt_to_income}\\\\\n&+ 2.211 \\times \\text{source_verified}\\\\  \n&+ 6.880 \\times \\text{verified}\\\\\n& -0.021 \\times \\text{annual_income_th}\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\nWe will talk about why there are two terms in the model for verified_income soon!"
  },
  {
    "objectID": "slides/05-mlr.html#interpreting-hatbeta_j",
    "href": "slides/05-mlr.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(Y\\) when \\(X_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n\n\nExample: The estimated coefficient for debt_to_income is 0.671. This means for each point in an borrower’s debt to income ratio, the interest rate on the loan is expected to be greater by 0.671%, holding annual income and income verification constant."
  },
  {
    "objectID": "slides/05-mlr.html#interpreting-hatbeta_j-1",
    "href": "slides/05-mlr.html#interpreting-hatbeta_j-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient for annual_income_th is -0.021. Interpret this coefficient in the context of the data.\n\n\n\n\nWhy do we need to include a statement about holding all other predictors constant?"
  },
  {
    "objectID": "slides/05-mlr.html#interpreting-hatbeta_0",
    "href": "slides/05-mlr.html#interpreting-hatbeta_0",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_0\\)",
    "text": "Interpreting \\(\\hat{\\beta}_0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\nDescribe the subset of borrowers who are expected to get an interest rate of 10.726% based on our model. Is this interpretation meaningful? Why or why not?"
  },
  {
    "objectID": "slides/05-mlr.html#prediction",
    "href": "slides/05-mlr.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted interest rate for an borrower with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000?\n\n\n\n10.726 + 0.671 * 0.558 + 2.211 * 0 + 6.880 * 0 - 0.021 * 59\n\n[1] 9.861418\n\n\n\nThe predicted interest rate for an borrower with with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000 is 9.86%."
  },
  {
    "objectID": "slides/05-mlr.html#prediction-in-r",
    "href": "slides/05-mlr.html#prediction-in-r",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction in R",
    "text": "Prediction in R\nJust like with simple linear regression, we can use the predict() function in R to calculate the appropriate intervals for our predicted values:\n\nnew_borrower &lt;- tibble(\n  debt_to_income  = 0.558, \n  verified_income = \"Not Verified\", \n  annual_income_th = 59\n)\n\npredict(int_fit, new_borrower)\n\n       1 \n9.890888 \n\n\n\n\n\n\n\n\nNote\n\n\nDifference in predicted value due to rounding the coefficients on the previous slide."
  },
  {
    "objectID": "slides/05-mlr.html#cautions",
    "href": "slides/05-mlr.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/05-mlr.html#recap",
    "href": "slides/05-mlr.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nShowed exploratory data analysis for multiple linear regression\nUsed least squares to fit the regression line\nInterpreted the coefficients for quantitative predictors\nPredicted the response for new observations"
  },
  {
    "objectID": "slides/05-mlr.html#next-class",
    "href": "slides/05-mlr.html#next-class",
    "title": "Multiple linear regression (MLR)",
    "section": "Next class",
    "text": "Next class\n\nMore on multiple linear regression\n\nCategorical predictors\nModel assessment\nGeometric interpretation (as time permits)\n\nSee Sep 12 prepare\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#announcements",
    "href": "slides/05-slr-matrix-contd.html#announcements",
    "title": "SLR: Matrix representation",
    "section": "Announcements",
    "text": "Announcements\n\nLab 01 due on Thursday, September 12 at 11:59pm\n\nPush work to GitHub repo\nSubmit final PDF on Gradescope + mark pages for each question\n\nHW 01 will be assigned on Thursday"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#topics",
    "href": "slides/05-slr-matrix-contd.html#topics",
    "title": "SLR: Matrix representation",
    "section": "Topics",
    "text": "Topics\n\nMatrix representation of simple linear regression\n\nModel form\nLeast square estimate\nPredicted (fitted) values\nResiduals"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#slr-in-matrix-form",
    "href": "slides/05-slr-matrix-contd.html#slr-in-matrix-form",
    "title": "SLR: Matrix representation",
    "section": "SLR in matrix form",
    "text": "SLR in matrix form\nSuppose we have \\(n\\) observations, a quantitative response variable, and a single predictor\\[\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n=\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_1 \\\\\n\\vdots &  \\vdots \\\\\n1 &  x_n\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n\\]\n\n\n\\(\\mathbf{y}\\): \\(n\\times 1\\) vector of responses\n\\(\\mathbf{X}\\): \\(n \\times 2\\) design matrix\n\\(\\boldsymbol{\\beta}\\): \\(2 \\times 1\\) vector of coefficients\n\\(\\boldsymbol{\\epsilon}\\): \\(n \\times 1\\) vector of error terms"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals",
    "href": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nGoal: Find \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimizes the sum of squared residuals \\[\n\\begin{aligned}\nSSR = \\sum_{i=1}^n e_i^2 = \\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-1",
    "href": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-1",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\n\\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&\\class{fragment}{= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})}\\\\[10pt]\n&\\class{fragment}{=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}}\\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-2",
    "href": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-2",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\n\\[\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-3",
    "href": "slides/05-slr-matrix-contd.html#minimize-sum-of-squared-residuals-3",
    "title": "SLR: Matrix representation",
    "section": "Minimize sum of squared residuals",
    "text": "Minimize sum of squared residuals\nThe estimate of \\(\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) that minimizes SSR is the one such that\n\\[\n\\nabla_{\\boldsymbol{\\beta}} SSR = \\nabla_{\\boldsymbol{\\beta}}( \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}) = 0\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-vector-operations",
    "href": "slides/05-slr-matrix-contd.html#side-note-vector-operations",
    "title": "SLR: Matrix representation",
    "section": "Side note: Vector operations",
    "text": "Side note: Vector operations\nLet \\(\\mathbf{x} = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\x_k\\end{bmatrix}\\)be a \\(k \\times 1\\) vector and \\(f(\\mathbf{x})\\) be a function of \\(\\mathbf{x}\\).\n\nThen \\(\\nabla_\\mathbf{x}f\\), the gradient of \\(f\\) with respect to \\(\\mathbf{x}\\) is\n\\[\n\\nabla_\\mathbf{x}f = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_k}\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-vector-operations-1",
    "href": "slides/05-slr-matrix-contd.html#side-note-vector-operations-1",
    "title": "SLR: Matrix representation",
    "section": "Side note: Vector operations",
    "text": "Side note: Vector operations\nTh Hessian matrix, \\(\\nabla_\\mathbf{x}^2f\\) is a \\(k \\times k\\) matrix of partial second derivatives\n\\[\n\\nabla_{\\mathbf{x}}^2f = \\begin{bmatrix} \\frac{\\partial^2f}{\\partial x_1^2} & \\frac{\\partial^2f}{\\partial x_1 \\partial x_2} & \\dots & \\frac{\\partial^2f}{\\partial x_1\\partial x_k} \\\\\n\\frac{\\partial^2f}{\\partial\\ x_2 \\partial x_1} & \\frac{\\partial^2f}{\\partial x_2^2} & \\dots & \\frac{\\partial^2f}{\\partial x_2 \\partial x_k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2f}{\\partial x_k\\partial x_1} & \\frac{\\partial^2f}{\\partial x_k\\partial x_2} & \\dots & \\frac{\\partial^2f}{\\partial x_k^2} \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-vector-operations-2",
    "href": "slides/05-slr-matrix-contd.html#side-note-vector-operations-2",
    "title": "SLR: Matrix representation",
    "section": "Side note: Vector operations",
    "text": "Side note: Vector operations\n\n\n\nProposition 1\n\n\nLet \\(\\mathbf{x}\\) be a \\(k \\times 1\\) vector and \\(\\mathbf{z}\\) be a \\(k \\times 1\\) vector, such that \\(\\mathbf{z}\\) is not a function of \\(\\mathbf{x}\\) .\nThe gradient of \\(\\mathbf{x}^T\\mathbf{z}\\) with respect to \\(\\mathbf{x}\\) is\n\\[\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^T\\mathbf{z} = \\mathbf{z}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-proposition-1",
    "href": "slides/05-slr-matrix-contd.html#side-note-proposition-1",
    "title": "SLR: Matrix representation",
    "section": "Side note: Proposition 1",
    "text": "Side note: Proposition 1\n\\[\n\\begin{aligned}\n\\mathbf{x}^T\\mathbf{z} &= \\class{fragment}{\\begin{bmatrix}x_1 & x_2 & \\dots &x_k\\end{bmatrix}\n\\begin{bmatrix}z_1 \\\\ z_2 \\\\ \\vdots \\\\z_k\\end{bmatrix}} \\\\[10pt]\n&\\class{fragment}{= x_1z_1 + x_2z_2 + \\dots + x_kz_k} \\\\\n&\\class{fragment}{= \\sum_{i=1}^k x_iz_i}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-proposition-1-1",
    "href": "slides/05-slr-matrix-contd.html#side-note-proposition-1-1",
    "title": "SLR: Matrix representation",
    "section": "Side note: Proposition 1",
    "text": "Side note: Proposition 1\n\\[\n\\nabla_\\mathbf{x}\\hspace{1mm}\\mathbf{x}^T\\mathbf{z} = \\class{fragment}{\\begin{bmatrix}\\frac{\\partial \\mathbf{x}^T\\mathbf{z}}{\\partial x_1} \\\\ \\frac{\\partial \\mathbf{x}^T\\mathbf{z}}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathbf{x}^T\\mathbf{z}}{\\partial x_k}\\end{bmatrix}}  \n= \\class{fragment}{\\begin{bmatrix}\\frac{\\partial}{\\partial x_1} (x_1z_1 + x_2z_2 + \\dots + x_kz_k) \\\\ \\frac{\\partial}{\\partial x_2} (x_1z_1 + x_2z_2 + \\dots + x_kz_k)\\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_k} (x_1z_1 + x_2z_2 + \\dots + x_kz_k)\\end{bmatrix}}\n= \\class{fragment}{\\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_k\\end{bmatrix} = \\mathbf{z}}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#side-note-vector-matrix-operations",
    "href": "slides/05-slr-matrix-contd.html#side-note-vector-matrix-operations",
    "title": "SLR: Matrix representation",
    "section": "Side note: Vector + matrix operations",
    "text": "Side note: Vector + matrix operations\n\n\n\nProposition 2\n\n\nLet \\(\\mathbf{x}\\) be a \\(k \\times 1\\) vector and \\(\\mathbf{A}\\) be a \\(k \\times k\\) matrix, such that \\(\\mathbf{A}\\) is not a function of \\(\\mathbf{x}\\) .\nThen the gradient of \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\) with respect to \\(\\mathbf{x}\\) is\n\\[\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^T\\mathbf{A}\\mathbf{x} = (\\mathbf{A}\\mathbf{x} + \\mathbf{A}^T \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\n\\]\nIf \\(\\mathbf{A}\\) is symmetric, then\n\\[\n(\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}\n\\]\n\n\n\n\nProof in HW 01"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#find-the-least-squares-estimators",
    "href": "slides/05-slr-matrix-contd.html#find-the-least-squares-estimators",
    "title": "SLR: Matrix representation",
    "section": "Find the least squares estimators",
    "text": "Find the least squares estimators\n\\[\n\\begin{aligned}\n\\nabla_{\\boldsymbol{\\beta}} SSR &= \\nabla_{\\boldsymbol{\\beta}}( \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta})  \\\\[10pt]\n& \\class{fragment}{= \\nabla_\\boldsymbol{\\beta} \\hspace{1mm} \\mathbf{y}^T\\mathbf{y} - 2\\nabla_\\boldsymbol{\\beta} \\hspace{1mm} \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\nabla_\\boldsymbol{\\beta} \\hspace{1mm} \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}} \\\\[10pt]\n&\\class{fragment}{= 0 - 2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}}\\class{fragment}{=0} \\\\[10pt]\n&\\class{fragment}{\\Rightarrow \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}} \\\\[10pt]\n&\\class{fragment}{\\Rightarrow   (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}} \\\\[10pt]\n&\\class{fragment}{\\color{#993399}{\\Rightarrow \\hat{\\boldsymbol{\\beta}} =  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#did-we-find-a-minimum",
    "href": "slides/05-slr-matrix-contd.html#did-we-find-a-minimum",
    "title": "SLR: Matrix representation",
    "section": "Did we find a minimum?",
    "text": "Did we find a minimum?\n\\[\n\\begin{aligned}\n\\nabla^2_{\\boldsymbol{\\beta}} SSR &= \\nabla_{\\boldsymbol{\\beta}} (-2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&\\class{fragment}{=-2\\nabla_{\\boldsymbol{\\beta}}\\mathbf{X}^T\\mathbf{y} + 2\\nabla_{\\boldsymbol{\\beta}}(\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta})} \\\\[10pt]\n&\\class{fragment}{\\propto \\mathbf{X}^T\\mathbf{X}}\\class{fragment}{ &gt; 0}\n\\end{aligned}\n\\]\n\nShow the details in HW 01"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#predicted-fitted-values",
    "href": "slides/05-slr-matrix-contd.html#predicted-fitted-values",
    "title": "SLR: Matrix representation",
    "section": "Predicted (fitted) values",
    "text": "Predicted (fitted) values\nNow that we have \\(\\hat{\\boldsymbol{\\beta}}\\), let’s predict values of \\(\\mathbf{y}\\) using the model\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\underbrace{\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T}_{\\mathbf{H}}\\mathbf{y} = \\mathbf{H}\\mathbf{y}\n\\]\n\nHat matrix: \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\n\n\n\n\\(\\mathbf{H}\\) is an \\(n\\times n\\) matrix\nMaps vector of observed values \\(\\mathbf{y}\\) to a vector of fitted values \\(\\hat{\\mathbf{y}}\\)\nIt is only a function of \\(\\mathbf{X}\\) not \\(\\mathbf{y}\\)"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#residuals",
    "href": "slides/05-slr-matrix-contd.html#residuals",
    "title": "SLR: Matrix representation",
    "section": "Residuals",
    "text": "Residuals\nRecall that the residuals are the difference between the observed and predicted values\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{y} - \\hat{\\mathbf{y}}\\\\[10pt]\n&\\class{fragment}{ = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}} \\\\[10pt]\n&\\class{fragment}{ = \\mathbf{y} - \\mathbf{H}\\mathbf{y}} \\\\[20pt]\n\\class{fragment}{\\color{#993399}{\\mathbf{e}}} &\\class{fragment}{\\color{#993399}{=(\\mathbf{I} - \\mathbf{H})\\mathbf{y}}} \\\\[10pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slr-matrix-contd.html#recap",
    "href": "slides/05-slr-matrix-contd.html#recap",
    "title": "SLR: Matrix representation",
    "section": "Recap",
    "text": "Recap\n\nIntroduced matrix representation for simple linear regression\n\nModel from\nLeast square estimate\nPredicted (fitted) values\nResiduals\n\n\n\n\n\n\n🔗 STA 221 - Fall 2024"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\n🔗 for Duke Container Manager\n\n\nCourse GitHub organization\n🔗 for GitHub\n\n\nCourse Canvas site\n🔗 for Canvas\n\n\nDiscussion forum\n🔗 to Ed Discussion\n\n\nAssignment submission\n🔗 to Gradescope\n\n\nZoom links\n🔗 on Canvas",
    "crumbs": [
      "Useful links"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "BIOSTAT 725 Syllabus",
    "section": "",
    "text": "Lecture\nTue & Thu 11:45am - 1pm\nHock 10089\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nRole\nOffice Hours\n\n\n\n\nProf. Sam Berchuck\nInstructor\nTBD\nTBD\nor by appointment\n\n\nDr. Youngsoo Baek\nTA\n\nWed 2 - 4pm, Hock 10090\n\n\nBraden Scherting\nTA\n\nTBD",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "BIOSTAT 725 Syllabus",
    "section": "",
    "text": "Lecture\nTue & Thu 11:45am - 1pm\nHock 10089\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nRole\nOffice Hours\n\n\n\n\nProf. Sam Berchuck\nInstructor\nTBD\nTBD\nor by appointment\n\n\nDr. Youngsoo Baek\nTA\n\nWed 2 - 4pm, Hock 10090\n\n\nBraden Scherting\nTA\n\nTBD",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course description",
    "text": "Course description\nThis course will teach students how to analyze biomedical data from a Bayesian inference perspective with a strong emphasis on using real-world data, including electronic health records, wearables, and imaging data. The course will begin by introducing the machinery of Bayesian statistics through the lens of linear regression, giving enough context for students with no prior experience with Bayesian statistics. A history of computational approaches used in Bayesian statistics will be given before ultimately landing on Stan, a state-of-the-art probabilistic programming language that makes Bayesian inference accessible as a viable data science tool. The course will then branch out from regression and introduce Bayesian versions of machine learning tools, including regularization and classification. The course will then emphasize Bayesian hierarchical models, including Gaussian process models for temporal and spatial data; and clustering. Additional topics may be discussed from the Bayesian perspective, including causal inference, and meta-analysis. While an applied course, the methods will be introduced from a mathematical perspective, allowing students to obtain a fundamental understanding of the introduced models. Students will learn computational skills for implementing Bayesian models using R and Stan. By the end of this course, students will be well-equipped to tackle complex problems in biomedical research using Bayesian inference.\n\nPrerequisites\nBIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission. Interested students with different backgrounds should seek instructor consent.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to…\n\nunderstand fundamental concepts of Bayesian statistics, including prior and posterior, and predictive distributions,\nimplement the Bayesian workflow, including model building, checking, and refinement,\nuse probabilistic programming software for Bayesian analysis (e.g., Stan),\napply Bayesian techniques to real-world health data,\ncommunicate Bayesian analysis results effectively to both technical and non-technical audiences, and\nidentify opportunities for using Bayesian statistics in your research and/or job.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course materials",
    "text": "Course materials\nWhile there is no official textbook for the course; readings will primarily be made available as they are assigned. We will use the statistical software R. Students will be encourage to download the required software on their own laptops. As a courtesy, students will also be able to access R and the required software through Docker containers provided by Duke Office of Information Technology. See the computing page for more information.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course community",
    "text": "Course community\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke’s Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean or director of graduate studies (DGS) are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nPronouns\nPronouns are meaningful tools to communicate identities and experiences, and using pronouns supports a campus environment where all community members can thrive. Please update your gender pronouns in Duke Hub. You can find instructions to do so here. You can learn more at the Center for Sexual and Gender Diversity’s website.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, biostat725-sp25.netlify.app.\nLinks to Zoom meetings may be found in Canvas. Periodic announcements will be sent via email and will also be available through Ed Discussion and Canvas Announcements. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nEmail\nIf you have questions about assignment extensions, accommodations, or any other matter not appropriate for the class discussion forum, please email me directly at sib2@duke.edu. If you email me, please include “BIOSTAT 725” in the subject line. Barring extenuating circumstances, I will respond to BIOSTAT 725 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#five-tips-for-success",
    "href": "syllabus.html#five-tips-for-success",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. Your TA(s) and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TA, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you’re not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings and other preparation work.\nDo the homeworks. The earlier you start, the better. It’s not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example).\nDon’t procrastinate. The content builds upon what was taught in previous weeks, so if something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, etc. Don’t let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours and work with a member of the teaching team to help you identify a good (re)starting point.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#getting-help-in-the-course",
    "href": "syllabus.html#getting-help-in-the-course",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Getting help in the course",
    "text": "Getting help in the course\n\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours1 to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the class discussion forum Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts in Ed Discussion before adding a new question. If you know the answer to a question posted in the discussion forum, you are encouraged to respond!\n\nCheck out the Support page for more resources.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#what-to-expect-in-the-course",
    "href": "syllabus.html#what-to-expect-in-the-course",
    "title": "BIOSTAT 725 Syllabus",
    "section": "What to expect in the course",
    "text": "What to expect in the course\n\nLectures\nLectures are designed to be interactive, so you gain experience applying new concepts and learning from each other. My role as instructor is to introduce you to new methods, tools, and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities during the lectures. You are expected to prepare for class by completing assigned readings, attend all lecture sessions, and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded based on completing what we do in class.\nYou are expected to bring a laptop, tablet, or any device with internet and a keyboard to each class so that you can participate in the in-class exercises. Please make sure your device is fully charged before you come to class, as the number of outlets in the classroom will not be sufficient to accommodate everyone.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\nYou will be assessed based on four components: homework, exams, live coding, and application exercises.\n\nHomework\nIn homework, you will apply what you’ve learned during lecture to complete data analysis tasks and explain the underlying mathematics, with a focus on the computation and communication. Homework assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and submitted as a PDF for grading in Gradescope. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. HW0 will not be graded for credit.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams\nThere will be two exams in this course. Each exam will be an open-note take-home assessment. Through these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. The exams will focus on both conceptual understanding of the applied and mathematical content and application through analysis and computational tasks. The exams will be based on content in reading assignments, lectures, application exercises, and homework assignments. More detail about the exams will be given during the semester.\n\n\nLive Coding\nThere will be a live coding evaluation in this course. Students will meet with Prof. Berchuck and/or a TA in a closed session to demonstrate their ability to code in Stan. The session will be 15-30 minutes. During the session, they will be asked to implement a Bayesian model and interpret the results. This exercise aims to assess their understanding of Bayesian methods and their proficiency in using Stan. Students will be required to schedule and complete the session after returning from spring break and before the last day of classes. The evaluation will surround one of the course assignments (e.g., a particular homework) which the student will choose. More information about the project will be provided during the semester.\n\n\nApplication exercises\nYou will get the most out of the course if you actively participate in class. Parts of some lectures will be dedicated to working on Application Exercises (AEs). AEs are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59p ET, and AEs from Thursday lectures are should be submitted by Sunday at 11:59p ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nAEs will be graded based on making a good-faith effort to attempt all questions covered in class. You are welcome to, but not required, to work on AEs beyond lecture.\nSuccessful on-time effort on at least 80% of AEs will result in full credit for AEs in the final course grade.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n40%\n\n\nExam 01\n20%\n\n\nExam 02\n20%\n\n\nLive Coding\n10%\n\n\nApplication exercises\n10%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nDuke Community Standard\nAll students must adhere to the Duke Community Standard (DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard, students agree:\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;and\nI will act if the Standard is compromised.\n\n\n\n\n\n\nAcademic honesty\nTL;DR: Don’t cheat!\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nYou may not discuss or otherwise work with others on the exams. Unauthorized collaboration or using unauthorized materials will be considered a violation for all students involved. More details will be given closer to the exam date.\n\n\n\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g. StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:2 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\nAI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\nNo AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments. In general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask a member of the teaching team.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including academic integrity (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects,and more). Ignoring these requirements is a violation of the Duke Community Standard. Any questions and/or concerns regarding academic integrity can be directed to the Office of Student Conduct and Community Standards at conduct@duke.edu.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback in a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework assignment will be dropped to accommodate such circumstances.\n\nHomework may be submitted up to 2 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThe late work policy for exams will be provided with the exam instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email me at sib2@duke.edu before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your director of graduate studies (DGS) know, as they can be a resource. Please let me know if you need help contacting your DGS.\n\n\nRegrade Requests\nRegrade requests must be submitted on Gradescope within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the last day of classes.\n\n\nAttendance policy\nEvery student is expected to attend and participate in lecture. There may be times, however, when you cannot attend class. Lecture recordings will be are available upon request for students who have an excused absence. If you miss a lecture, make sure to review the material and complete the application exercise, if applicable, before the next lecture.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\n\nAcademic accommodations\nIf you need accommodations for this class, you will need to register with the Student Disability Access Office (SDAO) and provide them with documentation related to your needs. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#academic-and-wellness-support",
    "href": "syllabus.html#academic-and-wellness-support",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Academic and wellness support",
    "text": "Academic and wellness support\n\n\n\nCAPS\nDuke Counseling & Psychological Services (CAPS) helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJanuary 8: Classes begin\nJanuary 20: MLK Jr Day. No classes\nJanuary 22: Drop/Add ends\nMarch 8 - 16: Spring Break. No classes\nApril 16: Graduate classes end.\nApril 17: Reading period begins (no classes or projects are due during the reading period).\nApril 27: Reading period ends.\nApril 28 - May 3: Final exam period\n\nClick here for the full Duke academic calendar.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "BIOSTAT 725 Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOffice hours are times the teaching team set aside each week to meet with students. Click here to learn more about how to effectively use office hours.↩︎\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you’re having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you’ll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it’s been resolved. If there’s a deadline coming up soon, post on the course forum to let us know that there’s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don’t anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you’ve tried and the errors you see (including verbatim errors and/or screenshots).",
    "crumbs": [
      "Computing",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "hw/hw-05.html",
    "href": "hw/hw-05.html",
    "title": "HW 05",
    "section": "",
    "text": "To be posted.",
    "crumbs": [
      "Homework",
      "HW 05"
    ]
  },
  {
    "objectID": "slides/02-slr.html",
    "href": "slides/02-slr.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "No labs on Mon, Sep 2 (Labor Day)\nApplication exercises start Tue, Sep 3\n\nBring fully-charged laptop or device with keyboard\nMake sure you have accepted invite to GitHub course organization\n\nSee website for resources to learn / review R\nOffice hours start Tue, Sep 3"
  },
  {
    "objectID": "slides/02-prob-bayes.html#announcements",
    "href": "slides/02-prob-bayes.html#announcements",
    "title": "Probability and Bayesian Statistics",
    "section": "Announcements",
    "text": "Announcements\n\nNo labs on Mon, Sep 2 (Labor Day)\nApplication exercises start Tue, Sep 3\n\nBring fully-charged laptop or device with keyboard\nMake sure you have accepted invite to GitHub course organization\n\nSee website for resources to learn / review R\nOffice hours start Tue, Sep 3\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "support.html#lectures",
    "href": "support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "computing-r-resources.html#stan-resources",
    "href": "computing-r-resources.html#stan-resources",
    "title": "Resources for learning R",
    "section": "Stan resources",
    "text": "Stan resources\n\nRStan Getting Started by Stan Development Team\nBrief Introduction to Stan by Mark Lai",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "hw/hw-00.html",
    "href": "hw/hw-00.html",
    "title": "HW 00",
    "section": "",
    "text": "Important\n\n\n\nThis first homework will not be graded, however it will be critical that you complete this on time. All of the computing tools we’ll use in the class will be introduced during this assignemnt.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-berchuck",
    "href": "slides/01-welcome.html#meet-prof.-berchuck",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Meet Prof. Berchuck!",
    "text": "Meet Prof. Berchuck!\n\n\nEducation and career journey\n\nBS in Statistical Science from Duke University\nPhD in Biostatistics from University of North Carolina - Chapel Hill\nPostdoc in Duke Forge: Duke’s Center for Actionable Health Data Science\nNIH/NEI Pathway to Independence Fellow (K99/R00)\nAssistant Professor, Department of Biostatistics & Bioinformatics and Statistical Science at Duke; Faculty Affiliate of Duke AI Health\n\nWork focuses on developing data science tools to improve patient experience using biomedical data (including EHR)\nDad of 4 and 6 year old daughters 🙂"
  },
  {
    "objectID": "slides/01-welcome.html#early-years",
    "href": "slides/01-welcome.html#early-years",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Early Years",
    "text": "Early Years\n\n\n\nThomas Bayes, 1701-1761\n\n\nPierre-Simon Laplace, 1749-1827"
  },
  {
    "objectID": "slides/01-welcome.html#bayes-theorem",
    "href": "slides/01-welcome.html#bayes-theorem",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nFirst introduced in its’ modern form in the 1920’s.\n\nSuppose we have events \\(A\\) and \\(B\\) with probabilities \\(P(A)\\) and \\(P(B)\\).\nThe basic form of Bayes theorem is given by, \\[P(A|B)=\\frac{P(A, B)}{P(B)}=\\frac{P(B|A)P(A)}{P(B)}.\\]\nBayes rule gives the relationship between the marginal probabilities of \\(A\\) and \\(B\\) and the conditional probabilities"
  },
  {
    "objectID": "slides/01-welcome.html#what-did-bayes-say",
    "href": "slides/01-welcome.html#what-did-bayes-say",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What did Bayes say?",
    "text": "What did Bayes say?"
  },
  {
    "objectID": "slides/01-welcome.html#translation-the-table-game",
    "href": "slides/01-welcome.html#translation-the-table-game",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Translation: The Table Game",
    "text": "Translation: The Table Game\nWe will illuminate a version of the example explored by Bayes in his original paper Originally from Eddy 2004.\n\n\nAlice and Bob are playing a game in which the first person to get 6 points wins. The way each point is decided is a little strange. The Casino has a pool table that Alice and Bob can’t see. Before the game begins, the Casino rolls an initial ball onto the table, which comes to rest at a completely random position, which the Casino marks. Then, each point is decided by the Casino rolling another ball onto the table randomly. If it comes to rest to the left of the initial mark, Alice wins the point; to the right of the mark, Bob wins the point. The Casino reveals nothing to Alice and Bob except who won each point."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-biostat-725",
    "href": "slides/01-welcome.html#what-is-biostat-725",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is BIOSTAT 725?",
    "text": "What is BIOSTAT 725?\n\n\n\n\n\n Bayes \n\nModeling\n\n\n\n\n+\n\n\n\n\n\n Stan\n\nProbabilistic Programming\n\n\n\nPrerequisites: BIOSTAT 724 (Introduction to Applied Bayesian Analysis) or equivalent course with instructor permission."
  },
  {
    "objectID": "slides/01-welcome.html#exploring-the-game",
    "href": "slides/01-welcome.html#exploring-the-game",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Exploring the Game",
    "text": "Exploring the Game\n\nThe probability that Alice wins a point is the fraction of the table to the left of the mark, call this probability \\(\\pi\\), and for Bob, \\(1-\\pi.\\)\nNote: Because the Casino rolled the initial ball to a random position, before any points were decided every value of \\(\\pi\\) was equally probable.\nThe mark is only set once per game, so \\(\\pi\\) is the same for every point.\n\n\nThe Question: Imagine Alice is already winning 5 points to 3, and now she bets Bob that she’s going to win. What are fair betting odds for Alice to offer Bob? That is, what is the expected probability that Alice will win?"
  },
  {
    "objectID": "slides/01-welcome.html#different-ways-to-approach-the-question",
    "href": "slides/01-welcome.html#different-ways-to-approach-the-question",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Different Ways to Approach The Question",
    "text": "Different Ways to Approach The Question\n\n\nIf \\(\\pi\\) were known this would be easy!\nInferring \\(\\pi\\) from the data, classical inference.\nInferring \\(\\pi\\) from the data, Bayesian inference."
  },
  {
    "objectID": "slides/01-welcome.html#if-pi-were-known-this-would-be-easy.",
    "href": "slides/01-welcome.html#if-pi-were-known-this-would-be-easy.",
    "title": "Welcome to BIOSTAT 725!",
    "section": "If \\(\\pi\\) were known this would be easy.",
    "text": "If \\(\\pi\\) were known this would be easy.\n\nBecause Alice just needs one more point to win, Bob only wins the game if he takes the next three points in a row. The probability of this is \\((1-\\pi)^3\\) ; Alice will win on any other outcome, so the probability of her winning is \\([1-(1-\\pi)^3]\\). Example: If we were flipping a coin, \\(\\pi=\\frac{1}{2}\\).\n\n\n\\(\\implies\\)Alice will win with probability \\(\\frac{7}{8}\\) and the odds will be \\(7:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#if-pi-were-known-this-would-be-easy",
    "href": "slides/01-welcome.html#if-pi-were-known-this-would-be-easy",
    "title": "Welcome to BIOSTAT 725!",
    "section": "If \\(\\pi\\) were known this would be easy!",
    "text": "If \\(\\pi\\) were known this would be easy!\n\nBecause Alice just needs one more point to win, Bob only wins the game if he takes the next three points in a row. The probability of this is \\((1-\\pi)^3\\).\nAlice will win on any other outcome, so the probability of her winning is \\(1-(1-\\pi)^3\\).\n\n\nExample: If we were flipping a coin, \\(\\pi=\\frac{1}{2}\\).\n\n\\(\\implies\\)Alice will win with probability \\(\\frac{7}{8}\\) and the odds will be \\(7:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-classical-inference.",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-classical-inference.",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, classical inference.",
    "text": "Inferring \\(\\pi\\) from the data, classical inference.\n\n\n\nDefine the random variable \\(A_i\\) as the event that Alice wins a point on throw \\(i\\). Then, assuming conditional independence, \\[P(data|model)=\\prod_{i=1}^8 p(A_i|\\pi) \\sim Binomial(8,\\pi)\\]\nThe interpretation of \\(\\hat{\\pi}\\) is intuitive: the frequency at which Alice has won so far (\\(\\hat{\\pi} = \\frac{5}{8}\\)).\nWhat comes with this? The usual, asymptotic normality, confidence intervals, p-values, etc…"
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-classical-inference.-1",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-classical-inference.-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, classical inference.",
    "text": "Inferring \\(\\pi\\) from the data, classical inference.\nSuppose our interest is in: \\(H_0: \\pi = 0.5, H_1: \\pi &gt; 0.5\\).\n\n\nAsymptotic Theory: \\(\\pi \\sim N\\left(\\hat{\\pi}, \\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}\\right) = N\\left(\\frac{5}{8}, \\frac{15}{512}\\right)\\). \nConfidence Intervals: \\(\\pi: \\hat{\\pi} \\pm 1.96 \\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}=(0.29,0.96)\\).\np-values: \\(z=\\frac{(\\hat{\\pi}-0.5)}{\\sqrt{\\frac{0.5\\left(1-0.5\\right)}{8}}}=0.71\\sim N(0,1)\\implies p=0.24\\).\n\n\nThe expected value that Alice wins is \\(1-(1-\\hat{\\pi})^3=\\frac{485}{512}\\) and the odds are \\(\\frac{485/512}{27/512}=18:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#bayesian-framework",
    "href": "slides/01-welcome.html#bayesian-framework",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Bayesian Framework",
    "text": "Bayesian Framework\n\n\nFundamental assumption: unknown parameters are random variables. (i.e., no longer interested in assuming that unknown parameters are fixed).\nProbability statements can be assigned to parameters, since each parameters is assumed to have a distribution.\nPrior information is incorporated into our estimates (do not want to ignore large body of prior research).\nThis is a huge step from the Frequentist framework where theory is based on asymptotics and unknown parameters are assumed to have a true value."
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\n\n\nUnder Bayes’ prior, he was able to compute a posterior distribution, \\(\\text{Beta}(6,4)\\).\n\n\n\nAsymptotic Theory: Not needed\nExact inference always exists.\n95% Credible Interval:\n\n\\(\\pi \\in (0.30,0.86)\\)\n\np-values: \\(P(\\pi&gt;0.5|A^8)=0.75\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the uniform prior, the posterior probability that Alice wins is \\(\\frac{10}{11}\\), with odds \\(10:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-1",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\n\nLet’s return to Alice and Bob: \\[\\mathbb{E}[\\text{Alice wins}|A^8]=1-\\int_{0}^1 (1-\\pi)^3 f(\\pi |A^8) d\\pi,\\]\nwhere \\[\\begin{aligned}\n\\int_{0}^1 (1-\\pi)^3 f(\\pi |A^8) d\\pi&=\\int_{0}^1 (1-\\pi)^3  \\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)}\\pi^{(a+5)-1}(1-\\pi)^{(b+3)-1} d\\pi\\\\\n&=\\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)} \\int_{0}^1 \\pi^{(a+5)-1} (1-\\pi)^{(b+6)-1} d\\pi\\\\\n&= \\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)} \\frac{\\Gamma(a+5)\\Gamma(b+6)} {\\Gamma(a+b+11)}.\n\\end{aligned}\\]\nFor the uniform prior, \\(\\mathbb{E}[\\text{Alice wins}|A^8]=\\frac{10}{11}\\), with odds \\(10:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#bringing-it-back-to-thomas-bayes",
    "href": "slides/01-welcome.html#bringing-it-back-to-thomas-bayes",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Bringing it back to Thomas Bayes",
    "text": "Bringing it back to Thomas Bayes\n\n\n 1763: An Essay towards solving a Problem in the Doctrine of Chances\n\n\nHe understood that the underlying probability of a win was random\n\nNot just random, but clearly uniform between 0 and 1\n\\(f(\\pi)\\sim \\text{Uniform}(0,1)\\)\n\\([\\text{Uniform}(0,1)=\\text{Beta}(1,1)]\\)\n\n\n\\(f(\\pi)=\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\pi^{a-1}(1-\\pi)^{b-1}\\)"
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-2",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-2",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\nComputing the posterior distribution: \\[\\begin{aligned}\nf(\\pi|A^8)&=\\frac{\\frac{8!}{5!3!} \\pi^5 (1-\\pi)^3f(\\pi)}{\\int_0^1 \\frac{8!}{5!3!} \\pi^5 (1-\\pi)^3f(\\pi) d\\pi}\\\\\n&=\\frac{\\frac{8!}{5!3!} \\pi^5 (1-\\pi)^3 \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\pi^{a-1}(1-\\pi)^{b-1}}{\\int_0^1 \\frac{8!}{5!3!} \\pi^5 (1-\\pi)^3 \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\pi^{a-1}(1-\\pi)^{b-1} d\\pi}\\\\\n&=\\frac{\\pi^{(a+5)-1} (1-\\pi)^{(3+b)-1}}{\\int_0^1 \\pi^{(a+5)-1} (1-\\pi)^{(3+b)-1} d\\pi}\\\\\n&=\\frac{\\Gamma(a+5+3+b)}{\\Gamma(a+5)\\Gamma(3+b)}\\pi^{(a+5)-1} (1-\\pi)^{(3+b)-1}\\\\\n&\\sim \\text{Beta}(a+5,b+3).\n\\end{aligned}\\] That seemed HARD! Let’s do this another way."
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-3",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-3",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\n\nComputing the posterior distribution: \\[\\begin{aligned}\nf(\\pi|A^8)&=\\frac{f(A^8|\\pi)f(\\pi)}{\\int_0^1 f(A^8|\\pi) f(\\pi) d\\pi}\\\\\n&\\propto f(A^8|\\pi)f(\\pi)\\\\\n&=\\frac{8!}{5!3!} \\pi^5 (1-\\pi)^3 \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\pi^{a-1}(1-\\pi)^{b-1}\\\\\n&\\propto \\pi^{(a+5)-1} (1-\\pi)^{(3+b)-1}\\\\\n&\\sim \\text{Beta}(a+5,b+3).\n\\end{aligned}\\] Under uniform prior, the posterior is \\(\\text{Beta}(6,4)\\).\nKey concepts: Kernel tricks, conjugacy"
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-4",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-4",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\nSuppose our interest is in: \\(H_0: \\pi =0.5, H_1: \\pi &gt; 0.5.\\) We will use our posterior distribution: \\(\\text{Beta}(6,4)\\).\n\n\n\nAsymptotic Theory: Not needed\nExact inference always exists.\n95% Credible Interval:\n\n\\(\\pi \\in (0.30,0.86)\\)\n\np-values: \\(P(\\pi&gt;0.5|A^8)=0.75\\)."
  },
  {
    "objectID": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-5",
    "href": "slides/01-welcome.html#inferring-pi-from-the-data-bayesian-inference.-5",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Inferring \\(\\pi\\) from the data, Bayesian inference.",
    "text": "Inferring \\(\\pi\\) from the data, Bayesian inference.\n\nLet’s return to Alice and Bob: \\[\\mathbb{E}[\\text{Alice wins}|A^8]=1-\\int_{0}^1 (1-\\pi)^3 f(\\pi |A^8) d\\pi,\\]\nwhere \\[\\begin{aligned}\n\\int_{0}^1 (1-\\pi)^3 f(\\pi |A^8) d\\pi&=\\int_{0}^1 (1-\\pi)^3  \\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)}\\pi^{(a+5)-1}(1-\\pi)^{(b+3)-1} d\\pi\\\\\n&=\\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)} \\int_{0}^1 \\pi^{(a+5)-1} (1-\\pi)^{(b+6)-1} d\\pi\\\\\n&= \\frac{\\Gamma(a+b+8)}{\\Gamma(a+5)\\Gamma(b+3)} \\frac{\\Gamma(a+5)\\Gamma(b+6)} {\\Gamma(a+b+11)}.\n\\end{aligned}\\]\nFor the uniform prior, \\(\\mathbb{E}[\\text{Alice wins}|A^8]=\\frac{10}{11}\\), with odds \\(10:1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#why-do-frequentist-and-bayesian-approaches-differ",
    "href": "slides/01-welcome.html#why-do-frequentist-and-bayesian-approaches-differ",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Why do Frequentist and Bayesian Approaches Differ?",
    "text": "Why do Frequentist and Bayesian Approaches Differ?\n\nFrequentist could lose a lot of money!\nFrequentist approach can be improved by estimating \\(\\pi\\) after each throw.\nWhat if we assume the probability of Alice winning each throw are not independent, \\[\\begin{aligned}\n\\mathbb{E}[\\text{Alice wins}]&=1-\\mathbb{E}[A_9=0,A_{10}=0,A_{11}=0]\\\\\n&=1-\\mathbb{E}[A_9=0]\\mathbb{E}[A_{10}=0|A_9=0] \\\\\n&\\times \\mathbb{E}[A_{11}=0|A_9=0,A_{10}=0]\\\\\n&=1-\\left(\\frac{3}{8}\\right)\\left(\\frac{4}{9}\\right)\\left(\\frac{5}{10}\\right)=0.92.\n\\end{aligned}\\] \\(\\implies\\) Odds=11:1."
  },
  {
    "objectID": "slides/01-welcome.html#whats-going-on",
    "href": "slides/01-welcome.html#whats-going-on",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What’s going on?",
    "text": "What’s going on?\n\n\n\nThe Frequentist method can be seen as a subset of the Bayesian method, with a Beta(0,0) prior.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Methods\n\n\n\n\n\nMethod\n\\(\\hat{\\pi}\\)\n95% CI\np-value\n\\(\\mathbb{E}\\)[Alice wins]\nOdds\n\n\n\n\nMLE: Naive\n0.625\n(0.29, 0.96)\n0.24\n0.95\n18:1\n\n\nMLE: Dependent\n0.625\n(0.29, 0.96)\n0.24\n0.92\n11:1\n\n\nBayes: Beta(0,0)\n0.625\n(0.29, 0.90)\n0.77\n0.92\n11:1\n\n\nBayes: Beta(1,1)\n0.600\n(0.30, 0.86)\n0.75\n0.91\n10:1"
  },
  {
    "objectID": "slides/01-welcome.html#pros-and-cons-of-bayesian-inference",
    "href": "slides/01-welcome.html#pros-and-cons-of-bayesian-inference",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Pros and Cons of Bayesian Inference",
    "text": "Pros and Cons of Bayesian Inference\nCons:\n\nComputations can be difficult.\nBayesian methods require specifying prior probability distributions, which are often themselves unknown.\nIt is not clear that parameters or hypotheses should be treated as random variables.\n\n\nPros:\n\nOften not possible to get good estimates in complex problems without taking a Bayesian or approximately Bayesian approach.\nPrior distributions can incorporate prior knowledge.\nProbability statements can be made about parameters."
  },
  {
    "objectID": "slides/01-welcome.html#what-did-the-table-game-teach-us",
    "href": "slides/01-welcome.html#what-did-the-table-game-teach-us",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What did the Table Game teach us?",
    "text": "What did the Table Game teach us?\nThe beauty of Bayes’ Table Game analogy is that it circumvented all three cons in one stroke…\n\nThe resulting integrals have analytic solutions.\nIt provided a physical mechanism for drawing a probability from a uniform prior.\nRepresenting the unknown parameter as random is logical.\n\nIt is easy to verify that the correct answer to the table game problem is 10:1.\nDoes this mean that Frequentist methods can’t be used in this problem?"
  },
  {
    "objectID": "slides/01-welcome.html#when-to-use-bayesian-inference",
    "href": "slides/01-welcome.html#when-to-use-bayesian-inference",
    "title": "Welcome to BIOSTAT 725!",
    "section": "When to use Bayesian inference?",
    "text": "When to use Bayesian inference?\nThe choice is up to the statistician!\n\nBayesian methods can be a tool for statisticians.\nDon’t be stubborn (i.e., only use Frequentist or Bayesian methods)\nThere are times that Frequentist methods are preferred…and times that Bayesian methods are preferred.\nIt is good to know about both!"
  },
  {
    "objectID": "slides/01-welcome.html#data-science",
    "href": "slides/01-welcome.html#data-science",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Data Science",
    "text": "Data Science\nRoses are Data Science, violets are blue."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-bayesian-health-data-science-1",
    "href": "slides/01-welcome.html#what-is-bayesian-health-data-science-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is Bayesian Health Data Science?",
    "text": "What is Bayesian Health Data Science?\n\nPeople seem Distinguishing between Data Science,"
  },
  {
    "objectID": "slides/01-welcome.html#why-data-science",
    "href": "slides/01-welcome.html#why-data-science",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Why Data Science?",
    "text": "Why Data Science?\n\n\nStatistics versus Data Science?\nIntroductory Bayesian statistics courses are often very mathematical and involve intense computation; thus Bayesian methods are not as frequently used in applied settings.\nModern software now exists to lower the mathematical burden and computational intensity of Bayesian statistics, but courses do not reflect this.\nThis course focuses on teaching students Bayesian statistics as a tool for research; or anywhere data science is practiced."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-bayesian-health-data-science",
    "href": "slides/01-welcome.html#what-is-bayesian-health-data-science",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is Bayesian Health Data Science?",
    "text": "What is Bayesian Health Data Science?\n\n\nBayesian Health Data Science involves using Bayesian methods to analyze health data, which can include electronic health records (EHR), clinical trial data, and other health-related datasets. These methods are model-based and can appropriately quantify and propagate uncertainty, making them suitable for tackling challenges in health research.\n\nSource: ChatGPT"
  },
  {
    "objectID": "slides/01-welcome.html#five-tips-for-success-in-biostat-725",
    "href": "slides/01-welcome.html#five-tips-for-success-in-biostat-725",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Five tips for success in BIOSTAT 725",
    "text": "Five tips for success in BIOSTAT 725\n\nComplete all the preparation work before class.\nAsk questions in class, office hours, and on Ed Discussion.\nDo the homework; get started on homework early when possible.\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStay up-to-date on announcements on Ed Discussion and sent via email."
  },
  {
    "objectID": "slides/02-probability.html#defining-the-model",
    "href": "slides/02-probability.html#defining-the-model",
    "title": "Probability and Bayesian Statistics",
    "section": "Defining the model",
    "text": "Defining the model\nSuppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\).\nFor the purpose of today’s lecture, we assume \\(\\sigma\\) is known."
  },
  {
    "objectID": "slides/02-probability.html#defining-the-likelihood",
    "href": "slides/02-probability.html#defining-the-likelihood",
    "title": "Probability and Bayesian Statistics",
    "section": "Defining the likelihood",
    "text": "Defining the likelihood\nThe individual likelihood contribution for subject \\(i\\) is given by, \\[Y_i|\\boldsymbol{\\beta} \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta}) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta}),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/02-probability.html#defining-a-posterior",
    "href": "slides/02-probability.html#defining-a-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Defining a posterior",
    "text": "Defining a posterior\nJust like in a Frequentist approach to linear regression, our goal is inference for the regression parameters, \\(\\boldsymbol{\\beta}\\). In a Bayesian setting we are interested in the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "slides/02-probability.html#axioms-of-probability-in-words",
    "href": "slides/02-probability.html#axioms-of-probability-in-words",
    "title": "Probability and Bayesian Statistics",
    "section": "Axioms of probability (in words)",
    "text": "Axioms of probability (in words)\nP1. Probabilities are between 0 and 1, importantly \\(P(\\neg H | H) = 0\\) and \\(P(H | H) = 1\\).\nP2. If two events \\(A\\) and \\(B\\) are disjoint, then \\(P(A\\text{ or }B) = P(A) + P(B)\\).\nP3. The joint probability of two events may be broken down stepwise: \\(P(A,B) = P(A | B)P(B)\\).\n\nIt follows that,\n\nfor any partition \\(\\{H_i\\}_{i = 1}^n\\), \\(\\sum_{i=1}^n P(H_i) = 1\\) (rule of total probability)\n\nNote: simplest partition \\(P(A) + P(\\neg A) = 1\\)\n\n\\(P(A) = \\sum_{i=1}^n P(A, H_i)\\) (rule of marginal probability)\n\nNote: P3 implies that equivalently, \\(P(A) = \\sum_{i=1}^n P(A | H_i) P(H_i)\\)\n\n\\(P(A | B) = P(A,B) / P(B)\\) when \\(P(B) \\neq 0\\)\n\nNote: these statements can also be made where each term is additionally conditioned on another event \\(C\\)"
  },
  {
    "objectID": "slides/02-probability.html#defining-the-likelihood-matrix-version",
    "href": "slides/02-probability.html#defining-the-likelihood-matrix-version",
    "title": "Probability and Bayesian Statistics",
    "section": "Defining the likelihood (matrix version)",
    "text": "Defining the likelihood (matrix version)\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta} ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "hw/hw-00.html#rstudio",
    "href": "hw/hw-00.html#rstudio",
    "title": "HW 00",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\nIn this class, you have the option to use RStudio on your laptop (i.e., locally) or on through a container hosted by Duke OIT. My suggestion is for everyone to be comfortable using both options, for the following reasons:\n\nFlexibility: If you’re laptop has problems right before a due date, it will be helpful to be setup in the container.\nIndependence: It is important to be able to compute on your laptop, because when you graduate you will no longer have access to the Duke containers.\n\nThe container is offered as a convenience and you should take advantage of it when needed. We will now give instructions for using both.\n\nInstalling RStudio on your laptop\n\nMost of you probably already have RStudio installed on your laptop. In case you do not, please follow these instructions to install both R and RStudio, Installation instruction.\nWhen given the option, choose the most recent stable version of both.\n\n\n\nReserve RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers. You will log in using your NetID credentials.\nClick “Reserve STA725” to reserve an RStudio container. Be sure you reserve the container labeled STA725 to ensure you have the computing set up you need for the class.\n\nYou only need to reserve a container once per semester.\n\n\nOpen RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA725 to log into the Docker container. You should now see the RStudio environment.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#git-and-github",
    "href": "hw/hw-00.html#git-and-github",
    "title": "HW 00",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nIn addition to R and RStudio, we will use git and GitHub for version control and collaboration.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better). Git is important because:\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)\n\n\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, exams, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username.\n\n\n\nIf you already have a GitHub account, you can move on to the next step.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#connect-rstudio-and-github",
    "href": "hw/hw-00.html#connect-rstudio-and-github",
    "title": "HW 00",
    "section": "Connect RStudio and GitHub",
    "text": "Connect RStudio and GitHub\nNow that you have RStudio and a GitHub account, we will configure git so that RStudio and GitHub communicate with one another.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system. So, if you are using both your laptop and the container, you will need to do this process twice.\n\n\n\nStep 0: Open your RStudio (either the STA725 RStudio container or your laptop).\nStep 1: Type credentials::ssh_setup_github() into the console on the bottom left of the RStudio environment.\nStep 2: R will ask “No SSH key found. Generate one now?” Click 1 for yes.\nStep 3: You will generate a key. It will begin with “ssh-rsa….” R will then ask “Would you like to open a browser now?” Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used (e.g., biostat725).\n\n\n\nConfigure git\nThe last thing we need to do is configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"berchuck\",\n  user.email = \"sib2@duke.edu\")\n\nIt may look like nothing happened but you are now ready interact between GitHub and RStudio! We will begin working with RStudio and GitHub in lecture this week.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#stan-hello-world",
    "href": "hw/hw-00.html#stan-hello-world",
    "title": "HW 00",
    "section": "Stan Hello World!",
    "text": "Stan Hello World!\nIn this course, we will use the package rstan as our primary tool for conducting Bayesian inference. The container already has rstan installed, so these steps need to be used for installation on your laptop.\n\nFollow the installation guide here: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n\nPrior to installing RStan, you need to configure your R installation to be able to compile C++ code. Follow\nWe will start by making sure we can load the rstan R package.\n\nlibrary(rstan)\n\nNow we will simulate some data that we can fit with linear regression. We will also define a Stan data object (to be defined in detail in future lectures).\n\n###Set a seed for reproducibility\nset.seed(54)\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- 3 # number of covariates\n\n###True parameter values\nbeta &lt;- matrix(c(rnorm(p + 1)), ncol = 1)\nsigma &lt;- 1.5\n\n###Simulate covariates and outcome\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n###Create a Stan data object\nstan_data &lt;- list(\n  n = n,\n  p = p,\n  Y = Y,  \n  X = X\n)\n\nNow, we can define a Stan model (you do not need to understand this yet, we are just testing!)\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p + 1] X;\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  Y ~ normal(X * beta, sigma);\n}\n\n\n\n\n\nOK, great. We will now obtain posterior samples.\n\nfit &lt;- sampling(compiled_model, data = stan_data)\nprint(fit)",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#stan",
    "href": "hw/hw-00.html#stan",
    "title": "HW 00",
    "section": "Stan",
    "text": "Stan\nIn this course, we will use the package rstan as our primary tool for conducting Bayesian inference. The container already has rstan installed, so these steps need to be used for installation on your laptop.\n\nFollow the installation guide here: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n\nMake sure to follow these instructions closely, since prior to installing rstan, you need to configure your R installation to be able to compile C++ code.\n\nStan Hello World!\nOnce rstan has been installed, test to make sure we can load the R package.\n\nlibrary(rstan)\n\nNow we will simulate some data that we can fit with linear regression. We will also define a Stan data object (no need to understand this now, we will go into this in detail in future lectures).\n\n###Set a seed for reproducibility\nset.seed(54)\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- 3 # number of covariates\n\n###True parameter values\nbeta &lt;- matrix(c(rnorm(p + 1)), ncol = 1)\nsigma &lt;- 1.5\n\n###Simulate covariates and outcome\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n###Create a Stan data object\nstan_data &lt;- list(\n  n = n,\n  p = p,\n  Y = Y,  \n  X = X\n)\n\nNow, we can define a Stan model (you do not need to understand this yet, we are just testing!). In RStudio, create a new .stan file called test.stan and then copy and paste the following Stan code. To create a .stan file from RStudio, File -&gt; New File -&gt; Stan File.\n\n// Saved in test.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p + 1] X;\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  Y ~ normal(X * beta, sigma);\n}\n\nNext, we test to see if the model can compile. Note compilation can sometimes take a bit of time.\n\nstan_model &lt;- stan_model(file = \"test.stan\")\n\nOK, great. We will now obtain posterior samples, using default specifications for inference.\n\nfit &lt;- sampling(stan_model, data = stan_data, refresh = 0)\n\nFinally, print some summary estimates for the model parameters.\n\nprint(fit)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n           mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\nbeta[1]    1.89    0.00 0.18    1.54    1.77    1.89    2.01    2.23  4666    1\nbeta[2]    0.20    0.00 0.18   -0.15    0.08    0.20    0.32    0.55  4391    1\nbeta[3]   -0.30    0.00 0.16   -0.62   -0.41   -0.31   -0.19    0.01  5189    1\nbeta[4]    1.58    0.00 0.19    1.22    1.46    1.58    1.71    1.95  4983    1\nsigma      1.71    0.00 0.13    1.49    1.63    1.71    1.79    1.98  4479    1\nlp__    -102.49    0.04 1.61 -106.30 -103.33 -102.13 -101.31 -100.34  1907    1\n\nSamples were drawn using NUTS(diag_e) at Mon Jan  6 15:15:47 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "slides/02-probability.html#review-set-theory",
    "href": "slides/02-probability.html#review-set-theory",
    "title": "Probability and Bayesian Statistics",
    "section": "Review: Set theory",
    "text": "Review: Set theory\n\n\n\n\n\n\nDefinition\n\n\nset: a collection of elements, denoted by {}\nExamples\n\n\\(\\phi\\) = {} “the empty set”\n\\(A\\) = {1, 2, 3}\n\\(B\\) = {taken BIOSTAT 724, has not taken BIOSTAT 724}\n\\(C\\) = {{1,2,3}, {4, 5}}\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nsubset: denoted by \\(\\subset\\), \\(A \\subset B\\) iff \\(a \\in A \\implies a \\in B\\)\nExamples\nUsing the previously examples of \\(A\\), \\(B\\) and \\(C\\) above,\n\n\\(A \\subset C\\)\n\\(A \\not\\subset B\\)\n\n\n\n\nRecall: \\(\\cup\\) means “union”, “or”; \\(\\cap\\) means “intersection”, “and”"
  },
  {
    "objectID": "slides/02-probability.html#independence",
    "href": "slides/02-probability.html#independence",
    "title": "Probability and Bayesian Statistics",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\nDefinition\n\n\nTwo events \\(F\\) and \\(G\\) are conditionally independent given \\(H\\) if \\(p(F, G | H) = p(F | H) p(G | H)\\)\nThis implies that \\(p(F | H, G) = p(F | H).\\)\n\n\n\nThis means that if we know \\(H\\), then \\(G\\) does not supply any additional information about \\(F\\)."
  },
  {
    "objectID": "slides/02-probability.html#random-variables",
    "href": "slides/02-probability.html#random-variables",
    "title": "Probability and Bayesian Statistics",
    "section": "Random variables",
    "text": "Random variables\n\n\\(X\\) (capital) is a random variable\nWe want to compute the probability that \\(X\\) takes on a specific value \\(x\\) (lowercase)\n\nThis is denoted \\(P(X = x)\\)\n\nWe also might want to compute the probability of \\(X\\) being in a set \\(\\mathcal A\\)\n\nThis is denoted \\(P(X \\in \\mathcal A)\\)\n\nThe set of possible values that \\(X\\) can take on is called its support, \\(\\mathcal S\\)"
  },
  {
    "objectID": "slides/02-probability.html#moments",
    "href": "slides/02-probability.html#moments",
    "title": "Probability and Bayesian Statistics",
    "section": "Moments",
    "text": "Moments\nFor a random variable \\(X\\), the \\(n\\)th moment is defined as \\(\\mathbb{E}[X^n]\\).\nRecall, the expected value is defined for discrete random variable \\(X\\), \\[\n\\mathbb{E}[X] = \\sum_{x \\in \\mathcal{X}} x p(x)\n\\] and for continuous random variable \\(Y\\), \\[\n\\mathbb{E}[Y] = \\int_{-\\infty}^{\\infty} y f(y) dy\n\\] The variance of a random variable, is also known as the second central moment and is defined \\[\n\\mathbb{E}[(X - \\mathbb{E}[X])^2]\n\\] or equivalently, \\(\\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\). More generally, the covariance between two random variables \\(X\\) and \\(Y\\) is defined as, \\[\n\\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])].\n\\]"
  },
  {
    "objectID": "slides/02-probability.html#exchangeability",
    "href": "slides/02-probability.html#exchangeability",
    "title": "Probability and Bayesian Statistics",
    "section": "Exchangeability",
    "text": "Exchangeability\n\noffline notes"
  },
  {
    "objectID": "slides/02-probability.html#axioms-of-probability-in-words-1",
    "href": "slides/02-probability.html#axioms-of-probability-in-words-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Axioms of probability (in words)",
    "text": "Axioms of probability (in words)\n\nProbabilities are between 0 and 1, importantly for an event \\(A\\), \\(P(\\neg A|A) = 0\\) and \\(P(A|A) = 1\\).\nIf two events \\(A\\) and \\(B\\) are disjoint, then \\(P(A\\text{ or }B)\\) = \\(P(A) + P(B)\\).\nThe joint probability of two events may be broken down stepwise: \\(P(A,B) = P(A|B)P(B)\\)."
  },
  {
    "objectID": "slides/02-probability.html#review-set-theory-1",
    "href": "slides/02-probability.html#review-set-theory-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Review: Set theory",
    "text": "Review: Set theory\n\n\n\n\n\n\nDefinition\n\n\npartition: {\\(H_1, H_2, ... H_n\\)} = \\(\\{H_i\\}_{i = 1}^n\\) is a partition of \\(\\mathcal{H}\\) if\n\nthe union of sets is \\(\\mathcal{H}\\) i.e., \\(\\cup_{i = 1}^n H_i = \\mathcal{H}\\)\nthe sets are disjoint i.e., \\(H_i \\cap H_j = \\phi\\) for all \\(i \\neq j\\)\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nsample space: \\(\\mathcal{H}\\), the set of all possible data sets (outcomes)\nevent: a set of one or more outcomes\nNote: p(\\(\\mathcal{H}\\)) = 1\nExamples\n\nRoll a six-sided die once. The sample space \\(\\mathcal{H} = \\{1, 2, 3, 4, 5, 6\\}\\).\nLet \\(A\\) be the event that the die lands on an even number. \\(A = \\{2, 4, 6 \\}\\)"
  },
  {
    "objectID": "slides/02-probability.html#bayes-rule",
    "href": "slides/02-probability.html#bayes-rule",
    "title": "Probability and Bayesian Statistics",
    "section": "Bayes rule",
    "text": "Bayes rule\nOur course will focus on Bayes rule,\n\\[f(\\boldsymbol{\\theta}|\\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{\\int f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}.\\]\nBayes’ rule tells us how to update beliefs about \\(\\boldsymbol{\\theta}\\) given data \\(\\mathbf{Y}\\)."
  },
  {
    "objectID": "slides/02-probability.html#discrete-random-variables",
    "href": "slides/02-probability.html#discrete-random-variables",
    "title": "Probability and Bayesian Statistics",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe joint pmf is \\(f(x, y) = P(X = x, Y = y)\\)\n\n\\(\\sum_x \\sum_y f(x, y) = 1\\)\n\nThe marginal pmf for \\(X\\) is \\(f_X(x) = P(X = x) = \\sum_y f(x, y)\\)\nThe marginal pmf for \\(Y\\) is \\(f_Y(y) = P(Y = y) = \\sum_x\nf(x, y)\\)\nThe marginal distribution is the same as univariate distribution as if we ignored the other variable"
  },
  {
    "objectID": "slides/02-probability.html#discrete-random-variables-1",
    "href": "slides/02-probability.html#discrete-random-variables-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe conditional pmf of \\(Y\\) given \\(X\\) is \\(f(y|x) = P(Y = y|X = x) = \\frac{P(X = x, Y = y)}{P(X = x)} = \\frac{f(x, y)}{f_X (x)}.\\)\n\\(X\\) and \\(Y\\) are independent if \\(f(x, y) = f_X(x)f_Y(y)\\) for all \\(x\\) and \\(y\\)\n\nVariables are dependent if they are not independent\n\nEquivalently, \\(X\\) and \\(Y\\) are independent if \\(f(x|y) = f_X(x)\\) for all \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "slides/02-probability.html#example-discrete-random-variables",
    "href": "slides/02-probability.html#example-discrete-random-variables",
    "title": "Probability and Bayesian Statistics",
    "section": "Example discrete random variables",
    "text": "Example discrete random variables\n\nBinomial pmf: the probability of \\(y\\) successes in \\(n\\) trials, where each trial has an individual probability of success \\(\\theta\\). \\[p(y | \\theta) = {n \\choose y} \\theta ^y (1-\\theta)^{n-y} \\text{ for } y \\in \\{0, 1, \\ldots n \\}\\]\n\nsupport: \\(y \\in \\{0, 1, 2, \\ldots n\\}\\)\nsuccess probability \\(\\theta \\in [0, 1]\\)\ndbinom(y, n, theta) computes this pmf in R\n\nPoisson pmf: probability of \\(y\\) events occurring during a fixed interval at a mean rate \\(\\theta\\) \\[p(y | \\theta) = \\frac{\\theta^y e^{-\\theta}}{y!}\\]\n\nsupport: \\(y \\in \\{0, 1, 2, \\ldots \\}\\)\nrate \\(\\theta \\in \\mathbb{R}^+\\)\ndpois(y, theta) computes this pmf in R"
  },
  {
    "objectID": "slides/02-probability.html#continuous-random-variables",
    "href": "slides/02-probability.html#continuous-random-variables",
    "title": "Probability and Bayesian Statistics",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nManipulating joint pdfs is similar to joint pmfs but sums are replaced by integrals\nThe joint pdf is denoted \\(f(x, y)\\)\nProbabilities are computed as volume under the pdf: \\[P((X, Y) ∈ A) = \\int_A f(x, y)dxdy\\] where \\(A \\subset \\mathbb{R}^2\\)"
  },
  {
    "objectID": "slides/02-probability.html#example-continuous-random-variables",
    "href": "slides/02-probability.html#example-continuous-random-variables",
    "title": "Probability and Bayesian Statistics",
    "section": "Example continuous random variables",
    "text": "Example continuous random variables\n\nNormal pdf: \\[\nf(x | \\mu, \\sigma) = (2\\pi \\sigma^2)^{-\\frac{1}{2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\n\\]\nUniform pdf: \\[f(x|a,b) =\n\\begin{cases}\n\\frac{1}{b - a} \\hspace{.6cm}\\text{ for } x \\in [a, b]\\\\\n0 \\hspace{1cm}\\text{ otherwise }\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/02-probability.html#other-definitions",
    "href": "slides/02-probability.html#other-definitions",
    "title": "Probability and Bayesian Statistics",
    "section": "Other definitions",
    "text": "Other definitions\n\n\n\n\n\n\nDefinition\n\n\nThe part of the density/mass function that depends on the variable is called the kernel.\nExample\n\nthe kernel of the normal pdf is \\(e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\nWhat’s the kernel of a gamma random variable X?\nRecall: the pdf of a gamma distribution:\n\\[\np(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}\n\\]"
  },
  {
    "objectID": "slides/02-probability.html#kernel",
    "href": "slides/02-probability.html#kernel",
    "title": "Probability and Bayesian Statistics",
    "section": "Kernel",
    "text": "Kernel\n\n\n\n\n\n\nDefinition\n\n\nThe part of the density/mass function that depends on the variable is called the kernel.\nExample:\n\nthe kernel of the normal pdf is \\(e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)"
  },
  {
    "objectID": "slides/02-probability.html#prior-definition",
    "href": "slides/02-probability.html#prior-definition",
    "title": "Probability and Bayesian Statistics",
    "section": "Prior definition",
    "text": "Prior definition\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/02-probability.html#computing-the-posterior",
    "href": "slides/02-probability.html#computing-the-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Computing the posterior",
    "text": "Computing the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-probability.html#how-can-we-use-the-posterior",
    "href": "slides/02-probability.html#how-can-we-use-the-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nLet’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/02-probability.html#how-can-we-use-the-posterior-1",
    "href": "slides/02-probability.html#how-can-we-use-the-posterior-1",
    "title": "Probability and Bayesian Statistics",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nFirst, we define hyperparameteters.\n\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\n\nWe then compute the posterior moments and compare to OLS/MLE.\n\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\n\n\n\n\n\nparameter\ntrue\nbayes\nols\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984\n\n\n\n\n\n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/02-probability.html#review-of-probability",
    "href": "slides/02-probability.html#review-of-probability",
    "title": "Probability and Bayesian Statistics",
    "section": "Review of probability",
    "text": "Review of probability\n\nThe goal of Bayesian statistics is to compute the posterior distribution (i.e., the uncertainty distribution of the parameters, \\(\\boldsymbol{\\theta}\\), after observing the data, \\(\\mathbf{Y}\\))\nThis is the conditional distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{Y}\\)\nTherefore, we need to review the probability concepts that lead to the conditional distribution of one variable conditioned on another\n\nProbability mass (pmf) and density (pdf) functions\nJoint distributions\nMarginal and conditional distributions"
  },
  {
    "objectID": "slides/02-probability.html#random-variables-1",
    "href": "slides/02-probability.html#random-variables-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Random variables",
    "text": "Random variables\n\n\n\n\n\n\nDefinition\n\n\nIn Bayesian inference, a random variable is an unknown numerical quantity about which we make probability statements.\nThe support of a random variable is the set of values a random variable can take.\nExamples,\n\nData. E.g., the amount of a wheat a field will yield later this year. Since this data has not yet been generated, the quantity is unknown.\nA population parameter. E.g., the true mean resting heart rate of Duke students. Note: this is a fixed (non-random) quantity, but it is also unknown. We use probability to describe our uncertainty in this quantity."
  },
  {
    "objectID": "slides/02-probability.html#random-variables---example",
    "href": "slides/02-probability.html#random-variables---example",
    "title": "Probability and Bayesian Statistics",
    "section": "Random variables - example",
    "text": "Random variables - example\n\nExample 1: \\(X\\) is the roll of a die\n\nThe support is \\(\\mathcal S = \\{1, 2, 3, 4, 5, 6\\}\\)\n\\(P(X = 1) = 1/6\\)\n\nExample 2: \\(X\\) is a newborn baby’s weight\n\nThe support is \\(\\mathcal S = (0, \\infty)\\)\n\\(P(X \\in [0, \\infty]) = 1\\)"
  },
  {
    "objectID": "slides/02-probability.html#what-is-probability",
    "href": "slides/02-probability.html#what-is-probability",
    "title": "Probability and Bayesian Statistics",
    "section": "What is probability?",
    "text": "What is probability?\nObjective (associated with frequentist)\n\n\\(P(X = x)\\) as a purely mathematical statement\nIf we repeatedly sampled \\(X\\), the the proportion of draws equal to \\(x\\) converges to \\(P(X = x)\\)\n\nSubjective (associated with Bayesian)\n\n\\(P(X = x)\\) represents an individual’s degree of belief\nOften quantified as the amount an individual would be willing to wager that \\(X\\) will be \\(x\\)\n\nA Bayesian analysis makes use of both of these concepts"
  },
  {
    "objectID": "slides/02-probability.html#what-is-uncertainty",
    "href": "slides/02-probability.html#what-is-uncertainty",
    "title": "Probability and Bayesian Statistics",
    "section": "What is uncertainty?",
    "text": "What is uncertainty?\nAleatoric uncertainty (likelihood)\n\nUncontrollable randomness in the experiment\nFor example, the results of a fair coin flip can never be predicted with certainty\n\nEpistemic uncertainty (prior/posterior)\n\nUncertainty about a quantity that could theoretically be known\nFor example, if we flipped a coin infinitely-many times we could know the true probability of a head\n\nA Bayesian analysis makes use of both of these concepts"
  },
  {
    "objectID": "slides/02-probability.html#probability-versus-statistics",
    "href": "slides/02-probability.html#probability-versus-statistics",
    "title": "Probability and Bayesian Statistics",
    "section": "Probability versus statistics",
    "text": "Probability versus statistics\nProbability is the forward problem\n\nWe assume we know how the data are being generated and compute the probability of events\nFor example, what is the probability of flipping 5 straight heads if the coin is fair?\n\nStatistics is the inverse problem\n\nWe use data to learn about the data-generating mechanism\nFor example, if we flipped five straight head, can we conclude the coin is biased?\n\nAny statistical analysis obviously relies on probability"
  },
  {
    "objectID": "slides/02-probability.html#univariate-distributions",
    "href": "slides/02-probability.html#univariate-distributions",
    "title": "Probability and Bayesian Statistics",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables\nThe random variable \\(X\\) is discrete if its support \\(\\mathcal S\\) is countable\nExamples:\n\n\\(X \\in \\{0, 1, 2, 3\\}\\) is the number of successes in 3 trials\n\n\\(X \\in \\{0, 1, 2, \\ldots\\}\\) is the number of patients with COVID in Durham County"
  },
  {
    "objectID": "slides/02-probability.html#univariate-distributions-1",
    "href": "slides/02-probability.html#univariate-distributions-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables\nThe random variable \\(X\\) is continuous if its support \\(\\mathcal S\\) is uncountable\nExamples with \\(\\mathcal S = (0, \\infty)\\):\n\n\\(X &gt; 0\\) is systolic blood pressure\n\n\\(X &gt; 0\\) is a patient’s BMI"
  },
  {
    "objectID": "slides/02-probability.html#discrete-univariate-distributions",
    "href": "slides/02-probability.html#discrete-univariate-distributions",
    "title": "Probability and Bayesian Statistics",
    "section": "Discrete univariate distributions",
    "text": "Discrete univariate distributions\n\nIf \\(X\\) is discrete we describe its distribution with its probability mass function (pmf)\nThe pmf is \\(f(x) = P(X = x)\\)\nThe domain of \\(X\\) is the set of \\(x\\) with \\(f(x) &gt; 0\\)\nWe must have \\(f(x) \\geq 0\\) and \\(\\sum_x f(x) = 1\\)\nThe mean is \\(\\mathbb E[X] = \\sum_x x f(x)\\)\nThe variance is \\(\\mathbb V(X) = \\sum_x(x − \\mathbb E[X])^2f(x)\\)\nThe last three sums are over \\(X\\)’s domain"
  },
  {
    "objectID": "slides/02-probability.html#parametric-families-of-distributions",
    "href": "slides/02-probability.html#parametric-families-of-distributions",
    "title": "Probability and Bayesian Statistics",
    "section": "Parametric families of distributions",
    "text": "Parametric families of distributions\n\nA statistical analysis typically proceeds by selecting a pmf that seems to match the distribution of a sample\nWe rarely know the pmf exactly, but we assume it is from a parametric family of distributions\nFor example, Binomial(10, 0.5) and Binomial(4, 0.1) are different but both from the binomial family\nA family of distributions have the same equation for the pmf but differ by some unknown parameters \\(\\boldsymbol{\\theta}\\)\nWe must estimate these parameters"
  },
  {
    "objectID": "slides/02-probability.html#continuous-univariate-distributions",
    "href": "slides/02-probability.html#continuous-univariate-distributions",
    "title": "Probability and Bayesian Statistics",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nIf \\(X\\) is continuous we describe its distribution with the probability density function (pdf) \\(f(x) \\geq 0\\)\nSince there are uncountably many possible values, \\(P(X = x) = 0\\) for all \\(x\\),\nProbabilities are computed as areas under the pdf curve \\[P(a &lt; X &lt; b) = \\int_a^b f(x)dx\\]\nTherefore, to be valid \\(f(x)\\) must satisfy \\(f(x) \\geq 0\\) and \\[P(−\\infty &lt; X &lt; \\infty) = \\int_{-\\infty}^{\\infty} f(x)dx = 1\\]"
  },
  {
    "objectID": "slides/02-probability.html#continuous-univariate-distributions-1",
    "href": "slides/02-probability.html#continuous-univariate-distributions-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nThe domain is the set of \\(x\\) values with \\(f(x) &gt; 0\\)\nThe mean and the variance are defined similarly to the discrete case but with the sums replaced by integrals\nThe mean is \\(\\mathbb E[X] = \\int x f(x)dx\\)\nThe variance is \\(\\mathbb V(X) = \\int (x − \\mathbb E[X])^2 f(x)dx\\)"
  },
  {
    "objectID": "slides/02-probability.html#joint-distributions",
    "href": "slides/02-probability.html#joint-distributions",
    "title": "Probability and Bayesian Statistics",
    "section": "Joint distributions",
    "text": "Joint distributions\n\n\\(X = (X_1, \\ldots, X_p)\\) is a random vector (vectors and matrices should be in bold).\nFor notational convenience, let’s consider only \\(p = 2\\) random variables \\(X\\) and \\(Y\\).\n\\((X, Y)\\) is discrete if it can take on a countable number of values, such as\n\\(X\\) = number of hearts and \\(Y\\) = number of face cards.\n\\((X, Y)\\) is continuous if it can take on an uncountable number of values, such as\n\\(X\\) = birthweight and \\(Y\\) = gestational age."
  },
  {
    "objectID": "slides/02-probability.html#discrete-random-variables-2",
    "href": "slides/02-probability.html#discrete-random-variables-2",
    "title": "Probability and Bayesian Statistics",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nNotation: \\(X_1, \\dots, X_n \\overset{\\mathrm{iid}}{\\sim} f(x)\\) means that \\(X_1, \\ldots, X_n\\) are independent and identically distributed\nThis implies the joint pmf is \\[P(X_1 = x_1, \\ldots, X_n = x_n) = \\prod_{i=1}^n f(x_i)\\]\nThe same notation and definitions of independence apply to continuous random variables\nIn this class, assume independence unless otherwise noted"
  },
  {
    "objectID": "slides/02-probability.html#discrete-random-variables-3",
    "href": "slides/02-probability.html#discrete-random-variables-3",
    "title": "Probability and Bayesian Statistics",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\n\n\n\n\n\nDefinition\n\n\ndiscrete random variable: a random variable that takes countably many values. \\(Y\\) is discrete if its possible outcomes can be enumerated \\(\\mathcal{Y} = \\{y_1, y_2, \\ldots \\}\\).\nNote: discrete does not mean finite. There may be infinitely many outcomes!\nExamples,\n\n\\(Y\\) = the number of children of a randomly sampled person\n\\(Y\\) = the number of visible stars in the sky on a randomly sampled night of the year\n\nFor each \\(y \\in \\mathcal{Y}\\), let \\(p(Y) = probability(Y = y)\\). Then \\(p\\) is the probability mass function (pmf) of the random variable \\(Y\\). Finally, we have that,\n\\[\n\\begin{aligned}\n0 \\leq p(y) \\leq 1,\\\\\n\\sum_{y \\in \\mathcal{Y}} p(y) = 1.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-probability.html#continuous-random-variables-1",
    "href": "slides/02-probability.html#continuous-random-variables-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nThe marginal pdf of \\(X\\) is \\(f_X(x) = \\int f(x, y)dy\\)\n\\(f_X\\) is the univariate pdf for \\(X\\) as if we never considered \\(Y\\)\nThe conditional pdf of \\(Y\\) given \\(X\\) is \\[f(y|x) = \\frac{f(x, y)}{f_X (x)}\\]\nProper: \\(\\int f(y|x)dy = \\int \\frac{f(x,y)}{f_X(x)}dy = \\int \\frac{f(x,y)dy}{f_X(x)} = 1\\)"
  },
  {
    "objectID": "slides/02-probability.html#continuous-random-variables-2",
    "href": "slides/02-probability.html#continuous-random-variables-2",
    "title": "Probability and Bayesian Statistics",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\n\n\n\n\n\nDefinition\n\n\ncontinuous random variable: a random variable that takes uncountably many values.\nThe probability density function (pdf) of a continuous random variable, \\(X\\) is defined\n\\(f(x) = pdf(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{p(x &lt; X &lt; x + \\Delta x)}{\\Delta x}\\)\nand the probability \\(X\\) is in some interval,\n\\(P(x_1 &lt; X &lt; x_2) = \\int_{x_1}^{x_2} f(x) dx\\).\nFinally, we have that, \\[\n\\begin{aligned}\n0 \\leq p(y) \\ \\text{and}, \\\\\n\\int_{y \\in \\mathcal{Y}} p(y) = 1.\n\\end{aligned}\n\\] Note: For a continuous random variable \\(Y\\), \\(f(y)\\) can be larger than 1 and \\(f(y)\\) is not \\(P(Y = y)\\), which equals 0."
  },
  {
    "objectID": "slides/02-probability.html#defining-joint-distributions-conditionally",
    "href": "slides/02-probability.html#defining-joint-distributions-conditionally",
    "title": "Probability and Bayesian Statistics",
    "section": "Defining joint distributions conditionally",
    "text": "Defining joint distributions conditionally\n\nSpecifying joint distributions is hard\nEvery joint distribution can be written \\(f(x, y) = f(y|x)f(x)\\)\nTherefore, any joint distribution can be defined by\n\n\\(X\\)’s marginal distribution\nThe conditional distribution of \\(Y|X\\)\n\nThe joint problem reduces to two univariate problems\nThis idea forms the basis of hierarchical modeling"
  },
  {
    "objectID": "slides/02-probability.html#axioms-of-probability",
    "href": "slides/02-probability.html#axioms-of-probability",
    "title": "Probability and Bayesian Statistics",
    "section": "Axioms of probability",
    "text": "Axioms of probability\nFor two events \\(A\\) and \\(B\\),\nA1. Probabilities are between 0 and 1, importantly \\(P(A^c | A) = 0\\) and \\(P(A | A) = 1\\).\nA2. If \\(A\\) and \\(B\\) are disjoint, then \\(P(A\\text{ or }B) = P(A) + P(B)\\).\nA3. The joint probability of two events may be broken down stepwise: \\(P(A,B) = P(A | B)P(B)\\)."
  },
  {
    "objectID": "slides/02-probability.html#consequences-of-probability-axioms",
    "href": "slides/02-probability.html#consequences-of-probability-axioms",
    "title": "Probability and Bayesian Statistics",
    "section": "Consequences of probability axioms",
    "text": "Consequences of probability axioms\n\nFor any partition \\(\\{H_i\\}_{i = 1}^n\\), \\(\\sum_{i=1}^n P(H_i) = 1\\) (rule of total probability)\n\nNote: simplest partition \\(P(A) + P(A^c) = 1\\)\n\n\\(P(A) = \\sum_{i=1}^n P(A, H_i)\\) (rule of marginal probability)\n\nNote: A3 implies that equivalently, \\(P(A) = \\sum_{i=1}^n P(A | H_i) P(H_i)\\)\n\n\\(P(A | B) = P(A,B) / P(B)\\) when \\(P(B) \\neq 0\\)\n\nNote: these statements can also be made where each term is additionally conditioned on another event \\(C\\)"
  },
  {
    "objectID": "slides/02-probability.html#visualize-simulated-data",
    "href": "slides/02-probability.html#visualize-simulated-data",
    "title": "Probability and Bayesian Statistics",
    "section": "Visualize simulated data",
    "text": "Visualize simulated data"
  },
  {
    "objectID": "slides/02-probability.html#where-did-this-come-from",
    "href": "slides/02-probability.html#where-did-this-come-from",
    "title": "Probability and Bayesian Statistics",
    "section": "Where did this come from?",
    "text": "Where did this come from?\n\n\n\n\n\n\nKernel of a multivariate normal distribution: \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)\n\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]\n\n\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-probability.html#kernel-of-a-multivariate-normal-distribution",
    "href": "slides/02-probability.html#kernel-of-a-multivariate-normal-distribution",
    "title": "Probability and Bayesian Statistics",
    "section": "Kernel of a multivariate normal distribution:",
    "text": "Kernel of a multivariate normal distribution:\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\beta}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\beta}\\right)\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-probability.html#techniques-to-find-this-posterior",
    "href": "slides/02-probability.html#techniques-to-find-this-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Techniques to find this posterior",
    "text": "Techniques to find this posterior\n\nBrute force: complete the square\nEasy: kernel recognition\n\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the multivariate normal pdf, \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-probability.html#using-the-kernel-to-find-the-posterior",
    "href": "slides/02-probability.html#using-the-kernel-to-find-the-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Using the kernel to find the posterior",
    "text": "Using the kernel to find the posterior\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{aligned}\\]\nThis is the kernel of a multivariate normal for \\(\\boldsymbol{\\beta}\\), with \\(\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}\\) and \\(\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\). It’s easy to see then that, \\(f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\\)\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/02-probability.html#back-to-the-posterior",
    "href": "slides/02-probability.html#back-to-the-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Back to the posterior",
    "text": "Back to the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/02-probability.html#linear-regression-estimation-techniques",
    "href": "slides/02-probability.html#linear-regression-estimation-techniques",
    "title": "Probability and Bayesian Statistics",
    "section": "Linear regression estimation techniques",
    "text": "Linear regression estimation techniques\n\nOrdinary least squares (OLS)\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\nMaximum likelihood estimation (MLE)\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)"
  },
  {
    "objectID": "slides/02-probability.html#linear-regression-estimation",
    "href": "slides/02-probability.html#linear-regression-estimation",
    "title": "Probability and Bayesian Statistics",
    "section": "Linear regression estimation",
    "text": "Linear regression estimation\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)"
  },
  {
    "objectID": "slides/02-probability.html#bayesian-estimation",
    "href": "slides/02-probability.html#bayesian-estimation",
    "title": "Probability and Bayesian Statistics",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "slides/02-probability.html#comparing-to-olsmle",
    "href": "slides/02-probability.html#comparing-to-olsmle",
    "title": "Probability and Bayesian Statistics",
    "section": "Comparing to OLS/MLE",
    "text": "Comparing to OLS/MLE\nFirst, we define hyperparameteters.\n\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10"
  },
  {
    "objectID": "slides/02-probability.html#how-can-we-use-the-posterior-for-inference",
    "href": "slides/02-probability.html#how-can-we-use-the-posterior-for-inference",
    "title": "Probability and Bayesian Statistics",
    "section": "How can we use the posterior for inference?",
    "text": "How can we use the posterior for inference?\n\nThe posterior distribution has been computed in closed form, so we can use it to compute any summary we would like.\n\n\nJoint probabilities: \\(P(\\boldsymbol{\\beta} \\in \\mathbf{A} | \\mathbf{Y})\\)\nConditional probabilities: \\(P(\\beta_j \\in A | \\boldsymbol{\\beta}_{-j} , \\mathbf{Y})\\)\nMarginal probabilities: \\(P(\\beta_j \\in A | \\mathbf{Y})\\)\n\\((100\\alpha)\\%\\) quantiles: \\(\\{q_{\\alpha} : P(\\beta_j &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n…"
  },
  {
    "objectID": "slides/02-probability.html#making-posterior-inference-easy",
    "href": "slides/02-probability.html#making-posterior-inference-easy",
    "title": "Probability and Bayesian Statistics",
    "section": "Making posterior inference easy!",
    "text": "Making posterior inference easy!\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\beta_j &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/mathematical software packages\n\n\nThese methods work well for standard posterior quantities and distributions\n\nSolution: Monte Carlo approximation\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/02-probability.html#summarizing-the-posterior",
    "href": "slides/02-probability.html#summarizing-the-posterior",
    "title": "Probability and Bayesian Statistics",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\beta_j &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pnorm}\\))\n\n\nThese methods work well for standard posterior quantities and distributions"
  },
  {
    "objectID": "slides/02-probability.html#asdf",
    "href": "slides/02-probability.html#asdf",
    "title": "Probability and Bayesian Statistics",
    "section": "asdf",
    "text": "asdf\n\nSummarizing a posterior (with known parametric form) is straightforward for basic quantities of interest\n\n\nPosterior means, medians, modes, and variances\nJoint probabilities: \\(P(\\boldsymbol{\\beta} \\in \\mathbf{A} | \\mathbf{Y})\\)\nConditional probabilities: \\(P(\\beta_j \\in A | \\boldsymbol{\\beta}_{-j} , \\mathbf{Y})\\)\nMarginal probabilities: \\(P(\\beta_j \\in A | \\mathbf{Y})\\)\n\\((100\\alpha)\\%\\) quantiles: \\(\\{q_{\\alpha} : P(\\beta_j &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n…"
  },
  {
    "objectID": "slides/02-probability.html#summarizing-the-posterior-1",
    "href": "slides/02-probability.html#summarizing-the-posterior-1",
    "title": "Probability and Bayesian Statistics",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?"
  },
  {
    "objectID": "slides/02-probability.html#summarizing-the-posterior-2",
    "href": "slides/02-probability.html#summarizing-the-posterior-2",
    "title": "Probability and Bayesian Statistics",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?"
  },
  {
    "objectID": "slides/02-probability.html#monte-carlo-approximation",
    "href": "slides/02-probability.html#monte-carlo-approximation",
    "title": "Probability and Bayesian Statistics",
    "section": "Monte Carlo approximation",
    "text": "Monte Carlo approximation\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/02-probability.html#monte-carlo-mc-approximation",
    "href": "slides/02-probability.html#monte-carlo-mc-approximation",
    "title": "Probability and Bayesian Statistics",
    "section": "Monte Carlo (MC) approximation",
    "text": "Monte Carlo (MC) approximation\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/02-probability.html#mc-approximation",
    "href": "slides/02-probability.html#mc-approximation",
    "title": "Probability and Bayesian Statistics",
    "section": "MC approximation",
    "text": "MC approximation\nLet \\(g\\left(\\boldsymbol{\\beta}\\right)\\) be (just about) any function of \\(\\boldsymbol{\\beta}\\). The law of large numbers says that if,\n\\[\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right),\\] then, \\[\\frac{1}{S}\\sum_{s=1}^S g\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\boldsymbol{\\beta}\\right)f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)d\\boldsymbol{\\beta},\\] as \\(S\\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/02-probability.html#mc-approximation-1",
    "href": "slides/02-probability.html#mc-approximation-1",
    "title": "Probability and Bayesian Statistics",
    "section": "MC approximation",
    "text": "MC approximation\n\nImplications (as \\(S\\rightarrow \\infty\\)):\n\n\\(\\overline{\\boldsymbol{\\beta}}=\\frac{1}{S}\\sum_{s=1}^S \\boldsymbol{\\beta}^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\boldsymbol{\\beta}|\\mathbf{Y}\\right]\\)\n\\(\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\boldsymbol{\\beta}^{\\left(s\\right)}-\\overline{\\boldsymbol{\\beta}}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\n\\(\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\leq \\mathbf{c}\\right) \\rightarrow P\\left(\\boldsymbol{\\beta}\\leq \\mathbf{c} | \\mathbf{Y}\\right)\\)\n\\(\\alpha\\)-quantile of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}\\)\n\nJust about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample"
  },
  {
    "objectID": "slides/02-probability.html#posterior-inference-for-arbitrary-functions",
    "href": "slides/02-probability.html#posterior-inference-for-arbitrary-functions",
    "title": "Probability and Bayesian Statistics",
    "section": "Posterior inference for arbitrary functions",
    "text": "Posterior inference for arbitrary functions\nInterest in the posterior distribution of a function of \\(\\boldsymbol{\\beta}\\), \\(g\\left(\\boldsymbol{\\beta}\\right)\\)\n\nMC sampling plan:\n\nDraw \\(\\boldsymbol{\\beta}^{\\left(1\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right)\\)\n\\(\\ldots\\)\nDraw \\(\\boldsymbol{\\beta}^{\\left(S\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\)\n\n\\(\\Rightarrow g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right),\\ldots,g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right)\\)\nTherefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)"
  },
  {
    "objectID": "slides/02-probability.html#comparison-with-olsmle",
    "href": "slides/02-probability.html#comparison-with-olsmle",
    "title": "Probability and Bayesian Statistics",
    "section": "Comparison with OLS/MLE",
    "text": "Comparison with OLS/MLE\nCompute posterior moments\n\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\n\n\n\n\n\nparameter\ntrue\nbayes\nols/mle\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984"
  },
  {
    "objectID": "slides/02-probability.html#inspecting-the-prior",
    "href": "slides/02-probability.html#inspecting-the-prior",
    "title": "Probability and Bayesian Statistics",
    "section": "Inspecting the prior",
    "text": "Inspecting the prior\nFirst, we define hyperparameteters.\n\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html",
    "href": "ae/ae-01-linear-regression.html",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "",
    "text": "Due date\n\n\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\nThis AE is due on Sunday, January 19 at 11:59pm. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nNote: For homeworks and exams, you will also be required to submit your final .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-1",
    "href": "ae/ae-01-linear-regression.html#exercise-1",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "Exercise 1",
    "text": "Exercise 1\nCompute the posterior mean and standard deviation for the intercept, slope, and measurement error. Provide an interpretation for each of these parameter estimates.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-2",
    "href": "ae/ae-01-linear-regression.html#exercise-2",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "Exercise 2",
    "text": "Exercise 2\nCompute a 95% confidence interval for the regression slope. Provide an interpretation for this confidence interval.\nAnswer:\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-3",
    "href": "ae/ae-01-linear-regression.html#exercise-3",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "Exercise 3",
    "text": "Exercise 3\nIn real estate, the price per square foot is an indicator of how expensive it is to buy living space in that area, with a higher price per square foot generally signifying a more expensive city due to factors like high demand for limited land, desirable location, and often, a high cost of living.\nLet’s assume that a city is considered to have a high living cost if the price per square foot is higher than 150. Using the slope as an estimate for price per square foot, estimate the probability that the Duke Forest area has a price per square foot greater than 150, \\(P(\\beta_1 &gt; 150)\\). Interpret the results in plain English.\nAnswer:\n\n# add code here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your AE repo on GitHub. You’re done! 🎉"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-4",
    "href": "ae/ae-01-linear-regression.html#exercise-4",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "Exercise 4",
    "text": "Exercise 4\nFill in the code to visualize the relationship between price and area. What are 1 - 2 observations about the relationship between these two variables?"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-5",
    "href": "ae/ae-01-linear-regression.html#exercise-5",
    "title": "AE 01: Monte Carlo samping",
    "section": "Exercise 5",
    "text": "Exercise 5\nYou want to fit a model of the form\n\\[\nprice = \\beta_0 + \\beta_1 ~ area + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2_\\epsilon)\n\\]\nWould a model of this form be a reasonable fit for the data? Why or why not?"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-6",
    "href": "ae/ae-01-linear-regression.html#exercise-6",
    "title": "AE 01: Monte Carlo samping",
    "section": "Exercise 6",
    "text": "Exercise 6\nFit the linear model described in the previous exercise and neatly display the output.\nSee notes for example code.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-7",
    "href": "ae/ae-01-linear-regression.html#exercise-7",
    "title": "AE 01: Monte Carlo samping",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nInterpret the slope in the context of the data.\nInterpret the slope in terms of area increasing by 100 sqft.\nWhich interpretation do you think is more meaningful in practice?"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#exercise-8",
    "href": "ae/ae-01-linear-regression.html#exercise-8",
    "title": "AE 01: Monte Carlo samping",
    "section": "Exercise 8",
    "text": "Exercise 8\nDoes it make sense to interpret the intercept? If so, interpret it in the context of the data. Otherwise, explain why not."
  },
  {
    "objectID": "slides/03-linear-regression.html",
    "href": "slides/03-linear-regression.html",
    "title": "Probability and Bayesian Statistics",
    "section": "",
    "text": "Suppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\).\nFor the purpose of today’s lecture, we assume \\(\\sigma\\) is known.\n\n\n\n\nThe individual likelihood contribution for subject \\(i\\) is given by, \\[Y_i|\\boldsymbol{\\beta} \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta}) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta}),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\).\n\n\n\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ Y | X ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_1\\\\\n    \\beta_2\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]\n\n\n\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)\n\n\n\n\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\).\n\n\n\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)\n\n\n\n\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\n\nBrute force: complete the square\nEasy: kernel recognition\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the multivariate normal pdf, \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]\n\n\n\n\n\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\\\\\n&\\sim N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\n\\end{aligned}\\]\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)\n\n\n\n\n\n\n\n\n\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n. . .\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n. . .\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0\\)\n\n\n\n\nLet’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, we define hyperparameteters.\n\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\n\nWe then compute the posterior moments and compare to OLS/MLE.\n\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\n\n\n\n\n\nparameter\ntrue\nbayes\nols\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984\n\n\n\n\n\n\n\n\n\n\n\nSummarizing a posterior (with known parametric form) is straightforward for basic quantities of interest\n\n\nPosterior means, medians, modes, and variances\nJoint, conditional, and marginal probabilities, for example: \\(P(\\beta_j &lt; c | \\mathbf{Y})\\)\n\\(\\alpha\\)-quantiles: \\(\\{q_{\\alpha} : P(\\beta_j &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n\\(\\ldots\\)\n\n\n\n\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\beta_j &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pnorm}\\))\n\n\nThese methods work well for standard posterior quantities and distributions\n\n\n\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?\n\n\n\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\n\n\n\n\nLet \\(g\\left(\\boldsymbol{\\beta}\\right)\\) be (just about) any function of \\(\\boldsymbol{\\beta}\\). The law of large numbers says that if,\n\\[\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right),\\] then, \\[\\frac{1}{S}\\sum_{s=1}^S g\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\boldsymbol{\\beta}\\right)f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)d\\boldsymbol{\\beta},\\] as \\(S\\rightarrow \\infty\\).\n\n\n\n\nImplications (as \\(S\\rightarrow \\infty\\)):\n\n\\(\\overline{\\boldsymbol{\\beta}}=\\frac{1}{S}\\sum_{s=1}^S \\boldsymbol{\\beta}^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\boldsymbol{\\beta}|\\mathbf{Y}\\right]\\)\n\\(\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\boldsymbol{\\beta}^{\\left(s\\right)}-\\overline{\\boldsymbol{\\beta}}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\n\\(\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\leq \\mathbf{c}\\right) \\rightarrow P\\left(\\boldsymbol{\\beta}\\leq \\mathbf{c} | \\mathbf{Y}\\right)\\)\n\\(\\alpha\\)-quantile of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}\\)\n\nJust about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample\n\n\n\n\nInterest in the posterior distribution of a function of \\(\\boldsymbol{\\beta}\\), \\(g\\left(\\boldsymbol{\\beta}\\right)\\)\n\nMC sampling plan:\n\nDraw \\(\\boldsymbol{\\beta}^{\\left(1\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right)\\)\n\\(\\ldots\\)\nDraw \\(\\boldsymbol{\\beta}^{\\left(S\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\)\n\n\\(\\Rightarrow g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right),\\ldots,g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right)\\)\nTherefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)"
  },
  {
    "objectID": "slides/03-linear-regression.html#defining-the-model",
    "href": "slides/03-linear-regression.html#defining-the-model",
    "title": "Bayesian Linear Regression",
    "section": "Defining the model",
    "text": "Defining the model\nSuppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\).\nFor the purpose of today’s lecture, we assume \\(\\sigma\\) is known."
  },
  {
    "objectID": "slides/03-linear-regression.html#defining-the-likelihood",
    "href": "slides/03-linear-regression.html#defining-the-likelihood",
    "title": "Bayesian Linear Regression",
    "section": "Defining the likelihood",
    "text": "Defining the likelihood\nThe individual likelihood contribution for subject \\(i\\) is given by, \\[Y_i|\\boldsymbol{\\beta} \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta}) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta}),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/03-linear-regression.html#defining-the-likelihood-matrix-version",
    "href": "slides/03-linear-regression.html#defining-the-likelihood-matrix-version",
    "title": "Bayesian Linear Regression",
    "section": "Defining the likelihood (matrix version)",
    "text": "Defining the likelihood (matrix version)\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ Y | X ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_1\\\\\n    \\beta_2\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-estimation",
    "href": "slides/03-linear-regression.html#linear-regression-estimation",
    "title": "Bayesian Linear Regression",
    "section": "Linear regression estimation",
    "text": "Linear regression estimation\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#bayesian-estimation",
    "href": "slides/03-linear-regression.html#bayesian-estimation",
    "title": "Bayesian Linear Regression",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "slides/03-linear-regression.html#prior-definition",
    "href": "slides/03-linear-regression.html#prior-definition",
    "title": "Bayesian Linear Regression",
    "section": "Prior definition",
    "text": "Prior definition\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/03-linear-regression.html#computing-the-posterior",
    "href": "slides/03-linear-regression.html#computing-the-posterior",
    "title": "Bayesian Linear Regression",
    "section": "Computing the posterior",
    "text": "Computing the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#techniques-to-find-this-posterior",
    "href": "slides/03-linear-regression.html#techniques-to-find-this-posterior",
    "title": "Bayesian Linear Regression",
    "section": "Techniques to find this posterior",
    "text": "Techniques to find this posterior\n\nBrute force: complete the square\nEasy: kernel recognition\n\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the multivariate normal pdf, \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#using-the-kernel-to-find-the-posterior",
    "href": "slides/03-linear-regression.html#using-the-kernel-to-find-the-posterior",
    "title": "Bayesian Linear Regression",
    "section": "Using the kernel to find the posterior",
    "text": "Using the kernel to find the posterior\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\\\\\n&\\sim N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\n\\end{aligned}\\]\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#back-to-the-posterior",
    "href": "slides/03-linear-regression.html#back-to-the-posterior",
    "title": "Bayesian Linear Regression",
    "section": "Back to the posterior",
    "text": "Back to the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#how-can-we-use-the-posterior",
    "href": "slides/03-linear-regression.html#how-can-we-use-the-posterior",
    "title": "Bayesian Linear Regression",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nLet’s simulate some data again:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n###Define hyperparameteters\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\na &lt;- 3\nb &lt;- 1"
  },
  {
    "objectID": "slides/03-linear-regression.html#visualize-simulated-data",
    "href": "slides/03-linear-regression.html#visualize-simulated-data",
    "title": "Bayesian Linear Regression",
    "section": "Visualize simulated data",
    "text": "Visualize simulated data"
  },
  {
    "objectID": "slides/03-linear-regression.html#comparing-to-olsmle",
    "href": "slides/03-linear-regression.html#comparing-to-olsmle",
    "title": "Bayesian Linear Regression",
    "section": "Comparing to OLS/MLE",
    "text": "Comparing to OLS/MLE\nFirst, we define hyperparameteters.\n\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\n\nWe then compute the posterior moments and compare to OLS/MLE.\n\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\n\n\n\n\n\nparameter\ntrue\nbayes\nols\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984"
  },
  {
    "objectID": "slides/03-linear-regression.html#summarizing-the-posterior",
    "href": "slides/03-linear-regression.html#summarizing-the-posterior",
    "title": "Bayesian Linear Regression",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\n\nSummarizing a posterior (with known parametric form) is straightforward for basic quantities of interest\n\n\nPosterior means, medians, modes, and variances\nJoint, conditional, and marginal probabilities, for example: \\(P(\\beta_j &lt; c | \\mathbf{Y})\\)\n\\(\\alpha\\)-quantiles: \\(\\{q_{\\alpha} : P(\\beta_j &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n\\(\\ldots\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#summarizing-the-posterior-1",
    "href": "slides/03-linear-regression.html#summarizing-the-posterior-1",
    "title": "Bayesian Linear Regression",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\beta_j &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pnorm}\\))\n\n\nThese methods work well for standard posterior quantities and distributions"
  },
  {
    "objectID": "slides/03-linear-regression.html#summarizing-the-posterior-2",
    "href": "slides/03-linear-regression.html#summarizing-the-posterior-2",
    "title": "Bayesian Linear Regression",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-linear-regression.html#monte-carlo-mc-approximation",
    "href": "slides/03-linear-regression.html#monte-carlo-mc-approximation",
    "title": "Probability and Bayesian Statistics",
    "section": "",
    "text": "Integration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#mc-approximation",
    "href": "slides/03-linear-regression.html#mc-approximation",
    "title": "Probability and Bayesian Statistics",
    "section": "",
    "text": "Let \\(g\\left(\\boldsymbol{\\beta}\\right)\\) be (just about) any function of \\(\\boldsymbol{\\beta}\\). The law of large numbers says that if,\n\\[\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right),\\] then, \\[\\frac{1}{S}\\sum_{s=1}^S g\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\boldsymbol{\\beta}\\right)f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)d\\boldsymbol{\\beta},\\] as \\(S\\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/03-linear-regression.html#mc-approximation-1",
    "href": "slides/03-linear-regression.html#mc-approximation-1",
    "title": "Probability and Bayesian Statistics",
    "section": "",
    "text": "Implications (as \\(S\\rightarrow \\infty\\)):\n\n\\(\\overline{\\boldsymbol{\\beta}}=\\frac{1}{S}\\sum_{s=1}^S \\boldsymbol{\\beta}^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\boldsymbol{\\beta}|\\mathbf{Y}\\right]\\)\n\\(\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\boldsymbol{\\beta}^{\\left(s\\right)}-\\overline{\\boldsymbol{\\beta}}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\n\\(\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\leq \\mathbf{c}\\right) \\rightarrow P\\left(\\boldsymbol{\\beta}\\leq \\mathbf{c} | \\mathbf{Y}\\right)\\)\n\\(\\alpha\\)-quantile of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}\\)\n\nJust about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample"
  },
  {
    "objectID": "slides/03-linear-regression.html#posterior-inference-for-arbitrary-functions",
    "href": "slides/03-linear-regression.html#posterior-inference-for-arbitrary-functions",
    "title": "Probability and Bayesian Statistics",
    "section": "",
    "text": "Interest in the posterior distribution of a function of \\(\\boldsymbol{\\beta}\\), \\(g\\left(\\boldsymbol{\\beta}\\right)\\)\n\nMC sampling plan:\n\nDraw \\(\\boldsymbol{\\beta}^{\\left(1\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right)\\)\n\\(\\ldots\\)\nDraw \\(\\boldsymbol{\\beta}^{\\left(S\\right)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\)\n\n\\(\\Rightarrow g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right),\\ldots,g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right)\\)\nTherefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)"
  },
  {
    "objectID": "slides/02-probability.html#returning-to-linear-regression",
    "href": "slides/02-probability.html#returning-to-linear-regression",
    "title": "Probability and Bayesian Statistics",
    "section": "Returning to linear regression",
    "text": "Returning to linear regression\nLet’s obtain \\(S = 1,000\\) samples from our posterior.\n\nlibrary(mvtnorm) # multivariate rng\nbeta_samples &lt;- rmvnorm(1000, mean_beta, var_beta)\n\nWe can compute the posterior mean and variance.\n\nprint(apply(beta_samples, 2, mean))\n\n[1] -1.474821  3.298521\n\nprint(apply(beta_samples, 2, var))\n\n[1] 0.02316256 0.02189517"
  },
  {
    "objectID": "slides/02-probability.html#assessing-accuracy",
    "href": "slides/02-probability.html#assessing-accuracy",
    "title": "Probability and Bayesian Statistics",
    "section": "Assessing accuracy",
    "text": "Assessing accuracy"
  },
  {
    "objectID": "slides/02-probability.html#how-do-we-know-how-many-samples-to-take",
    "href": "slides/02-probability.html#how-do-we-know-how-many-samples-to-take",
    "title": "Probability and Bayesian Statistics",
    "section": "How do we know how many samples to take?",
    "text": "How do we know how many samples to take?\n\nWe can use a central limit theorem: \\(\\frac{\\sqrt{S}\\left(\\overline{\\boldsymbol{\\beta}}-\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}]\\right)}{\\sqrt{\\mathbb{V}\\left(\\overline{\\boldsymbol{\\beta}}\\right)}} \\stackrel{d}{\\rightarrow} \\text{N}\\left(0,1\\right)\\)\n\nwhere \\(\\mathbb{V}\\left(\\overline{\\boldsymbol{\\beta}}\\right) = \\mathbb{V}\\left(\\frac{1}{S}\\sum_{s=1}^S \\boldsymbol{\\beta}^{(s)}\\right) = \\frac{1}{S^2}\\sum_{s=1}^S\\mathbb{V}\\left(\\boldsymbol{\\beta}^{(s)}\\right) = \\frac{1}{S}\\mathbb{V}\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)\\).\n\\(\\theta=\\text{E}\\left[\\theta|\\boldsymbol{Y}=\\boldsymbol{y}\\right]\\),\\(\\sigma=\\sqrt{\\text{Var}\\left[\\theta|\\boldsymbol{Y}=\\boldsymbol{y}\\right]}\\) - \\(\\Rightarrow \\overline{\\theta}\\approx \\text{N}\\left(\\theta,\\sigma^2/S\\right)\\)\n\nMC standard error: \\(\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\), \\(\\widehat{\\sigma}^2=\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\theta^{\\left(s\\right)}-\\overline{\\theta}\\right)^2\\)\nApproximate 95% MC confidence interval for the posterior mean: \\(\\overline{\\theta} \\pm 2\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\)\nChoose \\(S\\) large enough to report the posterior mean with your desired precision\nReporting MC standard errors for the posterior mean is a good way to indicate that \\(S\\) is large enough"
  },
  {
    "objectID": "slides/02-probability.html#how-many-samples-to-take",
    "href": "slides/02-probability.html#how-many-samples-to-take",
    "title": "Probability and Bayesian Statistics",
    "section": "How many samples to take?",
    "text": "How many samples to take?\nWe can use a central limit theorem: \\(\\sqrt{S}\\left(\\overline{\\boldsymbol{\\beta}}-\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}]\\right)/\\sigma \\stackrel{d}{\\rightarrow} \\text{N}\\left(0,1\\right)\\),\nwhere \\(\\sigma^2 = \\mathbb{V}\\left(\\overline{\\boldsymbol{\\beta}}\\right) = \\frac{1}{S^2}\\sum_{s=1}^S\\mathbb{V}\\left(\\boldsymbol{\\beta}^{(s)}\\right) = \\frac{1}{S}\\mathbb{V}\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)\\).\n\\(\\implies \\overline{\\boldsymbol{\\beta}}\\approx N\\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}],\\sigma^2/S\\right)\\)\n\nMC standard error: \\(\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\), \\(\\widehat{\\sigma}^2=\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}-\\overline{\\boldsymbol{\\beta}}\\right)^2\\)\nApproximate 95% MC confidence interval for the posterior mean: \\(\\overline{\\boldsymbol{\\beta}} \\pm 2\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\)\nChoose \\(S\\) large enough to report the posterior mean with your desired precision\nReporting MC standard errors for the posterior mean is a good way to indicate that \\(S\\) is large enough"
  },
  {
    "objectID": "slides/02-probability.html#additional-posterior-summaries",
    "href": "slides/02-probability.html#additional-posterior-summaries",
    "title": "Probability and Bayesian Statistics",
    "section": "Additional posterior summaries",
    "text": "Additional posterior summaries\n\n# median\napply(beta_samples, 2, median)\n\n[1] -1.475117  3.292516\n\n# 95% credible intervals\napply(beta_samples, 2, function(x) quantile(x, probs = c(0.025, 0.975)))\n\n           [,1]     [,2]\n2.5%  -1.763541 3.017254\n97.5% -1.188284 3.590548\n\n# evaluating probability\nmean(beta_samples[, 1] &lt; -1.5)\n\n[1] 0.4358\n\n# summarizing arbitrary functions of the parameters\nbeta_new &lt;- beta_samples[, 1] * beta_samples[, 2]^3\nc(mean(beta_new), quantile(beta_new, probs = c(0.025, 0.975)))\n\n               2.5%     97.5% \n-53.15221 -71.43863 -38.00378 \n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/01-welcome.html#prepare-for-next-week",
    "href": "slides/01-welcome.html#prepare-for-next-week",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Prepare for next week",
    "text": "Prepare for next week\n\nComplete HW 00 tasks\nReview syllabus\nComplete reading to prepare for Tuesday’s lecture\nTuesday’s lecture: Monte Carlo Sampling\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-linear-regression.html#review-of-last-lecture",
    "href": "slides/03-linear-regression.html#review-of-last-lecture",
    "title": "Bayesian Linear Regression",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Tuesday, we performed posterior inference for Bayesian linear regression, but we assumed the measurement error (\\(\\sigma\\)) was known!\n\nWe did this so we could sample from a closed form posterior.\nMonte Carlo approximation.\n\nWe will consider the same model, \\[\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n)\\]\n\nIn today’s lecture we will estimate both \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\). This will require a new algorithm called Gibbs sampling."
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampling-1",
    "href": "slides/03-linear-regression.html#gibbs-sampling-1",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampling",
    "text": "Gibbs sampling\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampling-for-linear-regression",
    "href": "slides/03-linear-regression.html#gibbs-sampling-for-linear-regression",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampling for linear regression",
    "text": "Gibbs sampling for linear regression\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta, \\sigma^2})d\\boldsymbol{\\beta}d\\sigma^2}.\n\\end{aligned}\\]\nNo closed form exists for the posterior. \\[f(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)\\]\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-linear-regression.html#posterior-for-linear-regression",
    "href": "slides/03-linear-regression.html#posterior-for-linear-regression",
    "title": "Bayesian Linear Regression",
    "section": "Posterior for linear regression",
    "text": "Posterior for linear regression\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)d\\boldsymbol{\\beta}d\\sigma^2}.\n\\end{aligned}\\]\nNo closed form exists for the posterior. \\[f(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression",
    "href": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\nSuppose we can sample from the following two distribution,\n\n\\(f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta})\\)\n\\(f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\sigma^2)\\)\n\nThese are called full conditional distributions.\nSet initial values for \\(\\boldsymbol{\\theta}^{(0)} = (\\boldsymbol{\\beta}^{(0)}, \\sigma^{2(0)})\\). Then, given a current state of parameters \\(\\boldsymbol{\\theta}^{(s)}\\), we can generate a new state as follows:\n\nSample \\(\\boldsymbol{\\beta}^{(s + 1)} \\sim f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^{2(s)})\\)\nSample \\(\\sigma^{2(s + 1)} \\sim f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}^{(s + 1)})\\)\nLet \\(\\boldsymbol{\\theta}^{(s+1)} = (\\boldsymbol{\\beta}^{(s + 1)}, \\sigma^{2(s + 1)})\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#why-does-this-work",
    "href": "slides/03-linear-regression.html#why-does-this-work",
    "title": "Bayesian Linear Regression",
    "section": "Why does this work?",
    "text": "Why does this work?\n\n\\(\\boldsymbol{\\theta}^{(0)}\\) isn’t a sample from the posterior, it is an arbitrarily chosen initial value\n\\(\\boldsymbol{\\theta}^{(1)}\\) likely isn’t from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\)\n\\(\\boldsymbol{\\theta}^{(2)}\\) likely isn’t from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\) and \\(\\boldsymbol{\\theta}^{(1)}\\)\nTheorem: For any initial values, the chain will eventually converge to the posterior\nTheorem: If \\(\\boldsymbol{\\theta}^{(s)}\\) is a sample from the posterior, then \\(\\boldsymbol{\\theta}^{(s+1)}\\) is too"
  },
  {
    "objectID": "slides/03-linear-regression.html#motivation-for-gibbs-sampling",
    "href": "slides/03-linear-regression.html#motivation-for-gibbs-sampling",
    "title": "Bayesian Linear Regression",
    "section": "Motivation for Gibbs sampling",
    "text": "Motivation for Gibbs sampling\n\nSuppose we were given \\(\\sigma^{2(1)}\\), a single sample from the marginal posterior distribution \\(f\\left(\\sigma^2|\\mathbf{Y}\\right)\\) (from where, who knows?)\nUse the sample to generate \\(\\boldsymbol{\\beta}^{(1)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y},\\sigma^{2(1)}\\right)\\)\n\\(\\left(\\boldsymbol{\\beta}^{(1)},\\sigma^{2(1)}\\right)\\) is a sample from \\(f\\left(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}\\right)\\)\n\\(\\boldsymbol{\\beta}^{(1)}\\) is a sample from \\(f\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)\\)\n\n\n\n\n\n\n\n\nRecall\n\n\n\\(f\\left(\\boldsymbol{\\beta}, \\sigma^{2}|\\mathbf{Y}\\right) = f\\left(\\boldsymbol{\\beta} | \\sigma^{2},\\mathbf{Y}\\right)f\\left(\\sigma^{2}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampler",
    "href": "slides/03-linear-regression.html#gibbs-sampler",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nUnder mild regulatory conditions that are generally satisfied for most statistical models, one can show that the iteration \\(\\boldsymbol{\\theta}^{(s)}\\) converges in distribution to a draw from the true joint posterior distribution\nSo for \\(s\\) sufficiently large (say, bigger than \\(s_0\\)), \\(\\left\\{\\boldsymbol{\\theta}^{(s)}, s=s_0+1,\\ldots,S\\right\\}\\) is a correlated sample from the true joint posterior (and \\(\\boldsymbol{\\beta}^{(s)}\\) and \\(\\sigma^{2(s)}\\) are samples from the marginals)\nSimilar to Monte Carlo approximation, we can use these samples to estimate posterior quantities of interest"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampler-1",
    "href": "slides/03-linear-regression.html#gibbs-sampler-1",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\n\\(\\boldsymbol{\\theta}^{(t)}\\) depends on \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-1)}\\) only through \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\boldsymbol{\\theta}^{(t)}\\) is conditionally independent of \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-2)}\\) given \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\implies\\) Markov property, so the sequence is called a Markov chain\nWe use the samples similar to MC approximation; therefore, Gibbs sampling is a form of Markov chain Monte Carlo (MCMC)\nWe will cover diagnostics for MCMC in another lecture!"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression-1",
    "href": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression-1",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nComputing the full conditionals.\n\nWe already have the full conditional for \\(\\boldsymbol{\\beta}\\):\n\n\\(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2 \\sim N \\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}], \\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y})\\right)\\)\n\n\\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression-2",
    "href": "slides/03-linear-regression.html#gibbs-sampler-for-linear-regression-2",
    "title": "Bayesian Linear Regression",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nComputing the full conditionals.\n\n\\(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2 \\sim N \\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}], \\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y})\\right)\\)\nFull conditional for \\(\\sigma^2\\), assuming \\(f(\\sigma^2) \\sim IG(a, b)\\):\n\n\\[\\sigma^2 |  \\mathbf{Y} , \\boldsymbol{\\beta} \\sim IG\\left(a + \\frac{n}{2},b+\\frac{\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)}{2}\\right)\\]\n\nWhy inverse-Gamma (\\(IG\\)) distribution for \\(\\sigma^2\\)?"
  },
  {
    "objectID": "slides/03-linear-regression.html#perform-gibbs-sampling",
    "href": "slides/03-linear-regression.html#perform-gibbs-sampling",
    "title": "Bayesian Linear Regression",
    "section": "Perform Gibbs sampling",
    "text": "Perform Gibbs sampling\n\nsigma2 &lt;- exp(rnorm(1)) # initial value\nsamples &lt;- NULL\nfor (s in 1:5000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n  \n  ###Sample from full conditional for sigma2\n  quadratic &lt;- as.numeric(t(Y - X %*% beta) %*% (Y - X %*% beta))\n  sigma2 &lt;- 1 / rgamma(1, shape = a + n / 2, rate = b + quadratic / 2)\n  \n  ###Save samples after a burn-in\n  samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-linear-regression.html#inspect-results",
    "href": "slides/03-linear-regression.html#inspect-results",
    "title": "Bayesian Linear Regression",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-linear-regression.html#parameter-estimation-vs.-posterior-approximation",
    "href": "slides/03-linear-regression.html#parameter-estimation-vs.-posterior-approximation",
    "title": "Bayesian Linear Regression",
    "section": "Parameter estimation vs. posterior approximation",
    "text": "Parameter estimation vs. posterior approximation\n\nModel specification: Choice of likelihood and introduction of model parameters\nPrior specification\nCalculation of the posterior\nSummarizing the posterior using MC or MCMC methods:\n\nThese are not models!\nThey do not generate more information than is in \\(\\mathbf{Y}\\) or \\(f\\left(\\boldsymbol{\\theta}\\right)\\)\nThey are simply ways of looking at \\(f\\left(\\boldsymbol{\\theta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#additional-topic-metropolis-sampling",
    "href": "slides/03-linear-regression.html#additional-topic-metropolis-sampling",
    "title": "Bayesian Linear Regression",
    "section": "Additional topic: Metropolis sampling",
    "text": "Additional topic: Metropolis sampling\n\nBefore we start using Stan for probabilistic programming, we need to understand the MCMC algorithm that is the engine for Stan’s inference, Hamiltonian Monte Carlo.\nTo get us one step closer we will quickly review the concept of Metropolis sampling, another MCMC variant"
  },
  {
    "objectID": "slides/03-linear-regression.html#intuition-behind-metropolis-samping",
    "href": "slides/03-linear-regression.html#intuition-behind-metropolis-samping",
    "title": "Bayesian Linear Regression",
    "section": "Intuition behind Metropolis samping",
    "text": "Intuition behind Metropolis samping\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let’s consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)’s in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)’s.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-linear-regression.html#metropolis-acceptance-ratio",
    "href": "slides/03-linear-regression.html#metropolis-acceptance-ratio",
    "title": "Bayesian Linear Regression",
    "section": "Metropolis acceptance ratio",
    "text": "Metropolis acceptance ratio\n\nFortunately, the comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) can be made even if we cannot compute \\(f(\\boldsymbol{\\theta} | \\mathbf{Y})\\).\n\n\\[\\begin{aligned}\nr &= \\frac{f(\\boldsymbol{\\theta}^* | \\mathbf{Y})}{f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y})}\\frac{f(\\mathbf{Y})}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\n\\end{aligned}\\]\nHaving computed \\(r\\), how should we proceed?"
  },
  {
    "objectID": "slides/03-linear-regression.html#metropolis-update",
    "href": "slides/03-linear-regression.html#metropolis-update",
    "title": "Bayesian Linear Regression",
    "section": "Metropolis update",
    "text": "Metropolis update\nGiven \\(\\boldsymbol{\\theta}^{(s)}\\), the Metropolis algorithm generates a value \\(\\boldsymbol{\\theta}^{(s + 1)}\\) as follows:\n\nSample \\(\\boldsymbol{\\theta}^*\\) from a proposal distribution, \\(\\boldsymbol{\\theta}^* ∼ J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\)\nCompute the acceptance ratio \\(r\\)\nLet \\[\\boldsymbol{\\theta}^{(s + 1)} =\n\\left\\{\n  \\begin{array}{ll}\n\\boldsymbol{\\theta}^* & \\text{with probability }\\min(r, 1) \\\\\n\\boldsymbol{\\theta}^{(s)} & \\text{with probability }1 -\\min(r, 1)\n  \\end{array}\n\\right.\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#metropolis-intuition",
    "href": "slides/03-linear-regression.html#metropolis-intuition",
    "title": "Bayesian Linear Regression",
    "section": "Metropolis intuition",
    "text": "Metropolis intuition\nMetropolis ratio: \\(r = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\)\nIf \\(r &gt; 1:\\)\n\nIntuition: Since \\(\\boldsymbol{\\theta}^{(s)}\\) is already in our set, we should include \\(\\boldsymbol{\\theta}^*\\) as it has a higher probability than \\(\\boldsymbol{\\theta}^{(s)}\\)\nProcedure: Accept \\(\\boldsymbol{\\theta}^*\\) into our set (i.e., set \\(\\boldsymbol{\\theta}^{(s + 1)} = \\boldsymbol{\\theta}^*\\))\n\nIf \\(r &lt; 1:\\)\n\nIntuition: The relative frequency of \\(\\boldsymbol{\\theta}\\)-values in our set equal to \\(\\boldsymbol{\\theta}^*\\) compared to those equal to \\(\\boldsymbol{\\theta}^{(s)}\\) should be \\(r\\). This means that for every instance of \\(\\boldsymbol{\\theta}^{(s)}\\), we should have only a “fraction” of an instance of a \\(\\boldsymbol{\\theta}^*\\) value.\nProcedure: Set \\(\\boldsymbol{\\theta}^{(s + 1)}\\) equal to either \\(\\boldsymbol{\\theta}^*\\) or \\(\\boldsymbol{\\theta}^{(s)}\\), with probability \\(r\\) and \\(1 − r\\) respectively."
  },
  {
    "objectID": "slides/03-linear-regression.html#metropolis-proposal-distribution",
    "href": "slides/03-linear-regression.html#metropolis-proposal-distribution",
    "title": "Bayesian Linear Regression",
    "section": "Metropolis proposal distribution",
    "text": "Metropolis proposal distribution\n\nThe proposal distribution is symmetric (i.e., \\(J(\\boldsymbol{\\theta}_a | \\boldsymbol{\\theta}_b) = J(\\boldsymbol{\\theta}_b | \\boldsymbol{\\theta}_a)\\)\nUsually \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) is very simple, with samples from \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) being near \\(\\boldsymbol{\\theta}\\) with high probability.\nThe most common proposal is a normal distribution\n\n\\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)}) = N(\\boldsymbol{\\theta}^{(s)}, \\boldsymbol{\\Delta})\\)\n\nThe value of the parameter \\(\\boldsymbol{\\Delta}\\) is generally chosen to make the approximation algorithm run efficiently"
  },
  {
    "objectID": "slides/03-linear-regression.html#metropolis-and-gibbs-combined",
    "href": "slides/03-linear-regression.html#metropolis-and-gibbs-combined",
    "title": "Bayesian Linear Regression",
    "section": "Metropolis and Gibbs combined",
    "text": "Metropolis and Gibbs combined\n\nThe Gibbs and Metropolis samplers are actually both algorithms within a larger class of Metropolis-Hastings algorithms\nWhen performing MCMC, one can actually choose to update a parameter using either a Gibbs or Metropolis update\nLet’s see this in action using our linear regression example"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-using-metropolisgibbs",
    "href": "slides/03-linear-regression.html#linear-regression-using-metropolisgibbs",
    "title": "Bayesian Linear Regression",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nIn this example, we will use a Metropolis update for \\(\\sigma^2\\), however we will actually focus on \\(\\log\\sigma^2\\).\n\nMetropolis requires a symmetric proposal, so it is often easier to transform parameters to be on the real line and use a normal proposal.\n\nWe will use the following proposal, \\(\\log\\sigma^{2*} \\sim N\\left(\\log\\sigma^{2(s)}, \\delta\\right)\\), where \\(\\delta = 1\\).\nWe will place \\(\\log\\sigma^2 \\sim N(0,1)\\)."
  },
  {
    "objectID": "slides/03-linear-regression.html#inspect-results-1",
    "href": "slides/03-linear-regression.html#inspect-results-1",
    "title": "Bayesian Linear Regression",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-linear-regression.html#looking-towards-stan",
    "href": "slides/03-linear-regression.html#looking-towards-stan",
    "title": "Bayesian Linear Regression",
    "section": "Looking towards Stan",
    "text": "Looking towards Stan\n\nWe can use Monte Carlo approximation, when the posterior is available in closed form.\nWe can use Gibbs sampling when the full conditional distributions are available in closed form.\nWe can always use Metropolis (or its more general form Metropolis Hastings) regardless of the form of the posterior, we only need to be able to compute \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})\\)\n\nThis amounts to specifying a likelihood and a prior (which is the fun modeling part!)\nMetropolis can be difficult to tune (i.e., finding \\(\\boldsymbol{\\Delta}\\))"
  },
  {
    "objectID": "hw/hw-01-creation.html",
    "href": "hw/hw-01-creation.html",
    "title": "HW 01: Bayesian linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis assignment is due on Thursday, January 30 at 11:45am. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#learning-goals",
    "href": "ae/ae-01-linear-regression.html#learning-goals",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the AE, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nPerform Gibbs sampling for Bayesian linear regression and compute some basic sumaries"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#clone-the-repo-start-new-rstudio-project",
    "href": "ae/ae-01-linear-regression.html#clone-the-repo-start-new-rstudio-project",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/biostat725-sp25 organization on GitHub.\nClick on the repo with the prefix ae-01-. It contains the starter documents you need to complete the AE.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the HW 00 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick ae-01-linear-regression.qmd to open the template Quarto file. This is where you will write up your code and narrative for the AE."
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#r-and-r-studio",
    "href": "ae/ae-01-linear-regression.html#r-and-r-studio",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of an Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we may tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "ae/ae-01-test.html",
    "href": "ae/ae-01-test.html",
    "title": "Welcome to BIOSTAT 725!",
    "section": "",
    "text": "\\(1+1=2 = \\int 2 dx\\)"
  },
  {
    "objectID": "hw/hw-00.html#reproducibility-checklist",
    "href": "hw/hw-00.html#reproducibility-checklist",
    "title": "HW 00",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n. . .\nNear term goals:\n✔️ Can the tables and figures be exactly reproduced from the code and data?\n✔️ Does the code actually do what you think it does?\n✔️ In addition to what was done, is it clear why it was done?\n. . .\nLong term goals:\n✔️ Can the code be used for other data?\n✔️ Can you extend the code to do other things?",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#why-is-reproducibility-important",
    "href": "hw/hw-00.html#why-is-reproducibility-important",
    "title": "HW 00",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#toolkit",
    "href": "hw/hw-00.html#toolkit",
    "title": "HW 00",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#r-and-rstudio",
    "href": "hw/hw-00.html#r-and-rstudio",
    "title": "HW 00",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\n\n\nSource: Statistical Inference via Data Science",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#rstudio-ide",
    "href": "hw/hw-00.html#rstudio-ide",
    "title": "HW 00",
    "section": "RStudio IDE",
    "text": "RStudio IDE",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#quarto",
    "href": "hw/hw-00.html#quarto",
    "title": "HW 00",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports – the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#quarto-1",
    "href": "hw/hw-00.html#quarto-1",
    "title": "HW 00",
    "section": "Quarto",
    "text": "Quarto",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#how-will-we-use-quarto",
    "href": "hw/hw-00.html#how-will-we-use-quarto",
    "title": "HW 00",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou’ll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#what-is-versioning",
    "href": "hw/hw-00.html#what-is-versioning",
    "title": "HW 00",
    "section": "What is versioning?",
    "text": "What is versioning?",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#what-is-versioning-1",
    "href": "hw/hw-00.html#what-is-versioning-1",
    "title": "HW 00",
    "section": "What is versioning?",
    "text": "What is versioning?\nwith human readable messages",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#why-do-we-need-version-control",
    "href": "hw/hw-00.html#why-do-we-need-version-control",
    "title": "HW 00",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\n\n\n\n\n\n\n\nProvides a clear record of how the analysis methods evolved. This makes analysis auditable and thus more trustworthy and reliable. [@ostblom2022]",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "hw/hw-00.html#git-and-github-1",
    "href": "hw/hw-00.html#git-and-github-1",
    "title": "HW 00",
    "section": "git and GitHub",
    "text": "git and GitHub\n\n\n\n\n\n\ngit is a version control system – like “Track Changes” features from Microsoft Word.\nGitHub is the home for your git-based projects on the internet (like DropBox but much better).\nThere are a lot of git commands and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, project, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username.\n\n\n\nIf you already have a GitHub account, you can move on to the next step.",
    "crumbs": [
      "Homework",
      "HW 00"
    ]
  },
  {
    "objectID": "prepare/prepare-jan14.html",
    "href": "prepare/prepare-jan14.html",
    "title": "Prepare for January 14 lecture",
    "section": "",
    "text": "Occasionally during the semester we will reference material from the text book A First Course in Bayesian Statistical Methods by Peter Hoff. As a Duke student, an electronic version of the book is freely available to you through the Duke Library. We will refer to this textbook as Hoff.\n📖 Read about Bayesian inference and the probability in Hoff Chapter 1 and 2\n📖 Read about Monte Carlo sampling in Hoff Chapter 4, Sections 4.1 and 4.2\n📖 Review simple linear regression\n✅ Complete HW 00 tasks"
  },
  {
    "objectID": "slides/03-linear-regression.html#recall",
    "href": "slides/03-linear-regression.html#recall",
    "title": "Bayesian Linear Regression",
    "section": "Recall",
    "text": "Recall\n\\(f\\left(\\boldsymbol{\\beta}, \\sigma^{2}|\\mathbf{Y}\\right) = f\\left(\\boldsymbol{\\beta} | \\sigma^{2},\\mathbf{Y}\\right)f\\left(\\sigma^{2}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-using-metropolisgibbs-1",
    "href": "slides/03-linear-regression.html#linear-regression-using-metropolisgibbs-1",
    "title": "Bayesian Linear Regression",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nsigma2 &lt;- exp(rnorm(1))\nsamples &lt;- NULL\ndelta &lt;- 1\nfor (s in 1:10000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n\n  ###Metropolis update for sigma2\n  # Sample a proposal value\n  log_sigma2_proposal &lt;- rnorm(1, log(sigma2), delta)\n  # Compute the ratio r on the log scale for numeric stability\n  # Also, I've decided to update log(sigma2) instead of sigma2, so I can use a normal proposal distribution\n  # I've placed a normal prior on log(sigma2)\n  likelihood_proposal &lt;- sum(dnorm(Y, X %*% beta, sqrt(exp(log_sigma2_proposal)), log = TRUE))\n  likelihood_current &lt;- sum(dnorm(Y, X %*% beta, sqrt(sigma2), log = TRUE))\n  prior_proposal &lt;- dnorm(log_sigma2_proposal, 0, 1, log = TRUE)\n  prior_current &lt;- dnorm(log(sigma2), 0, 1, log = TRUE)\n  log_r &lt;- (likelihood_proposal + prior_proposal) - (likelihood_current + prior_current)\n  # Update beta using Metropolis ratio\n  if (log(runif(1)) &lt; log_r) sigma2 &lt;- exp(log_sigma2_proposal)\n\n  ###Save samples after a burn-in\n  if (s &gt; 5000) samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-linear-regression.html#looking-towards-stan-1",
    "href": "slides/03-linear-regression.html#looking-towards-stan-1",
    "title": "Bayesian Linear Regression",
    "section": "Looking towards Stan",
    "text": "Looking towards Stan\n\nStan uses an algorithm called Hamiltonian Monte Carlo, which is a form of MCMC that uses a Metropolis update\nStan does all of the MCMC tuning, allowing us to only focus on the modeling!\nThis means that our job moving forward will be to focus on specifying the\n\nlikelihood: \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})\\)\nprior: \\(f(\\boldsymbol{\\theta})\\)\n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "ae/ae-01-linear-regression.html#insepction-of-gibbs-sampler",
    "href": "ae/ae-01-linear-regression.html#insepction-of-gibbs-sampler",
    "title": "AE 01: Posterior estimation using sampling",
    "section": "Insepction of Gibbs sampler",
    "text": "Insepction of Gibbs sampler\nWe can begin by inspecting the posterior density and traceplots.\ndat.fig &lt;- data.frame(\n  parameter = rep(c(\"beta[0]\", \"beta[1]\", \"sigma^2\"), each = 5000),\n  index = rep(1:5000, 3),\n  value = as.numeric(samples)\n)\nggplot(dat.fig, aes(x = value)) +\n  geom_density(lwd = 1.5) +\n  facet_grid(. ~ parameter, labeller = label_parsed, scales = \"free_x\") +\n  ylab(\"Density\") +\n  xlab(\"Parameter value\")\nggplot(dat.fig, aes(x = index, y = value)) + \n  geom_line(lwd = 0.5) + \n  facet_grid(. ~ parameter, labeller = label_parsed, scales = \"free_x\") + \n  ylab(\"Parameter value\") +\n  xlab(\"Sample index\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe traceplots do not exhibit much autocorrelation, indicating we are looking at samples from the posterior.\nTo recover the regression parameters on their original scale, we can use the following transformation, where \\(\\mu_Y\\), \\(\\sigma_Y\\) are the mean and standard deviation for our outcome, price, \\(\\mu_X\\), \\(\\sigma_X\\) are the mean and standard deviation for out the predictor, area, and \\(\\beta_0^*\\), \\(\\beta_1^*\\) are the regression parameters obtained from the Gibbs sampler above (i.e., on the transformed data).\n\\[\\begin{align*}\n\\beta_0 &= \\mu_Y + \\sigma_Y \\beta_0^* - \\frac{\\sigma_Y}{\\sigma_X} \\mu_X \\beta_1^*\\\\\n\\beta_1 &= \\frac{\\sigma_Y}{\\sigma_X} \\beta_1^*\n\\end{align*}\\]\nUsing this transformation, \\(\\beta_0\\) and \\(\\beta_1\\) are on the original scale.\n\nmean_y &lt;- mean(duke_forest$price)\nsd_y &lt;- sd(duke_forest$price)\nmean_x &lt;- mean(duke_forest$area)\nsd_x &lt;- sd(duke_forest$area)\nintercept &lt;- mean_y + sd_y * samples[, 1] - (sd_y / sd_x) * mean_x * samples[, 2]\nslope &lt;- (sd_y / sd_x) * samples[, 2]\n\nA histogram of the posterior parameters can be examined as follows."
  },
  {
    "objectID": "prepare/prepare-jan16.html",
    "href": "prepare/prepare-jan16.html",
    "title": "Prepare for January 16 lecture",
    "section": "",
    "text": "📖 Read Hoff Chapter 6, Sections 6.1-6.5 to learn about Gibbs sampling\n📖 Read Hoff Chapter 10, Sections 10.2-10.4 to learn about Metropolis sampling\n✅ Complete HW 00 tasks"
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-10",
    "href": "hw/hw-01-creation.html#exercise-10",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nDo you think that city expenditure on residents is a useful predictor of park access? Briefly explain your response, reporting any statistics used to make your assessment.\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-1",
    "href": "hw/hw-01-creation.html#exercise-1",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nSetup a multivariable regression to estimate the association between access to recreational facilities and exercise, making sure to allow for this relationship to change based on crime. Be sure to control for the following confounders: age, marital status, and race. Assume that \\(exercise_i \\stackrel{iid}{\\sim} N(\\mu_i, \\sigma^2)\\) for \\(i = 1,\\ldots,n\\) where \\[\\begin{align*}\n\\mu_i &= \\alpha + recreation_i \\beta_1 + crime_i \\beta_2+(recreation_i \\times crime_i) \\beta_3\\\\\n&+ age_i \\beta_4 + black_i \\beta_5 + asian_i \\beta_6 + married_i \\beta_7.\n\\end{align*}\\]\nPresent the results from your regression.\n\nAnswer: We will perform a multivariable linear regression where the dependent variable is exercise, and the predictors are: recreation, crime, age, married, and race. Plus, to allow for the association between recreation and exercise to vary across crime, we include an interaction between crime and recreation. Note that we must be sure race is treated as a dummy variable, so we use the \\(\\{as.factor}\\) function.\n\n\nexercise$race &lt;- as.factor(exercise$race)\nreg &lt;- lm(exercise ~ recreation + crime + recreation * crime \n          + age + race + married, data = exercise)\nsummary(reg)\n\n\nCall:\nlm(formula = exercise ~ recreation + crime + recreation * crime + \n    age + race + married, data = exercise)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8386 -1.4686  0.2276  1.6052  4.1927 \n\nCoefficients:\n                  Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)      469.82486    1.52939  307.197  &lt; 2e-16 ***\nrecreation         2.96943    0.04534   65.489  &lt; 2e-16 ***\ncrime             -0.05616    0.03491   -1.609    0.112    \nage               -1.48267    0.03957  -37.464  &lt; 2e-16 ***\nrace1             -9.29598    0.61490  -15.118  &lt; 2e-16 ***\nrace2              5.01344    0.82803    6.055 4.48e-08 ***\nmarried            0.46918    0.61921    0.758    0.451    \nrecreation:crime  -0.29750    0.00277 -107.410  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.538 on 79 degrees of freedom\nMultiple R-squared:  0.9975,    Adjusted R-squared:  0.9973 \nF-statistic:  4457 on 7 and 79 DF,  p-value: &lt; 2.2e-16\n\n\n\n// Saved in linear_regression.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p + 1] X;\n  real&lt;lower = 0&gt; sigma_beta;\n  real&lt;lower = 0&gt; a;\n  real&lt;lower = 0&gt; b;\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  Y ~ normal(X * beta, sigma);\n  beta ~ normal(0, sigma_beta);\n  sigma ~ \n}"
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-2",
    "href": "hw/hw-01-creation.html#exercise-2",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nAssess the assumptions required for validity of the regression in Exercise 1.\n\nAnswer: We require independence of observations, linearity, normality of the residuals, and homeskedastic variance. We can create a figure to assess the last three assumptions.\n\n\n\n\n\n\n\n\n\n\nThe figure on the left is of the fitted values versus the residuals. From this figure we see that there is no evidence of heteroskedasticity, so our variance assumption is OK. This figure also indicates that linearity is satisfied, as there is no pattern in the residuals across the fitted values. Then, in the figure on the right we see that the distribution of the residuals is normally distributed, so our distributional assumptions is OK. Finally, independence is satisfied, because we are randomly sampling pregnant women from across Cook County, Chicago. Linearity could have also been assessed by looking at all pairwise scatterplots as follows:\n\n\n\n\n\n\n\n\n\nOur outcome variable, exercise appears to be linearly related to all of our continous predictors."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-3",
    "href": "hw/hw-01-creation.html#exercise-3",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat is interpretation of the slope main effect corresponding to recreation in your model at the 0.05 significance level? Is this interpretation appropriate?\n\nAnswer: Assuming a crime level of zero, we can interpret the slope corresponding to recreation, assuming all other variables are fixed. The interpretation is that for a one unit increase in recreation (i.e., number of recreational centers within a one-mile radius), exercise MET per week will increase by 2.97 MET. The p-value for this effect is significant at the 0.05 level, therefore we can conclude that for those people who live in areas of zero crime, there is an association between access to recreational facilities and exercise. This interpretation is likely not appropriate, because there are no areas with zero crime."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-4",
    "href": "hw/hw-01-creation.html#exercise-4",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the association between access to recreational facilities and exercise, for a pregnant women living in an area with 5 annual crimes/1,000 people? What about for 15 annual crimes/1,000 people?\n\nAnswer: The association between access to recreational facilities and exercise is a function of crime. In particular, this effect is given by the following: \\(\\beta_{Rec} + \\beta_{Rec \\times Crime} \\times \\texttt{crime}_i = 2.969 - 0.298 \\times  \\texttt{crime}_i\\). Plugging in for a crime value of 5, we get: 1.479. For a crime value of 15, we get a slope of -1.501."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-5",
    "href": "hw/hw-01-creation.html#exercise-5",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret the slopes from (d) and compare and contrast them. What do these slopes say about the impact of crime on the relationship between access to recreational facilities and exercise.\n\nAnswer: The interpretation of the slopes are as follows: At an annual level of crime of 5/1,000 people and keeping all of the variables constant, the association between recreational facilities and exercise has a slope of 1.479. This indicates that a one-unit increase in recreational facilities will result in a 1.5 MET increase in exercise. When crime increases to an annual level of 15/1,000 people, this slope becomes negative, -1.5. This indicates that crime is an effect modifier for the association of interest. In particular, for low levels of crime, the association is positive, however for larger values of crime the slope becomes negative."
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-6",
    "href": "hw/hw-01-creation.html#exercise-6",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nAt what level of crime does the association between recreational facilities and exercise disappear?\n\nAnswer: We know that the effect of recreational facilities on exercies is a function of crime, because of the interaction term. In particular, this effect is given by the followiing: \\(\\beta_{Rec} + \\beta_{Rec \\times Crime} \\times \\texttt{crime}_i = 2.969 - 0.298 \\times  \\texttt{crime}_i\\). Therefore, we would like to set this effect to zero and solve for \\(\\texttt{crime}_i\\). Doing this, we find that \\(\\texttt{crime}_i = 2.969 / 0.298 = 9.963.\\)"
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-7",
    "href": "hw/hw-01-creation.html#exercise-7",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nTo confirm your conclusions from (f), create a visualization that demontrates the relationship between recreation and exercise, across crime levels below and above the value you found in (f). An effective figure provides a title, axis labels, and a legible legend.\n\nAnswer: We will create a scatterplot that plots recreation vs. exercise. To visualize the association across crime, we will create a new variable that is an indicator of a crime value below and above the value we found in (f), 9.963. Then, we will color code the points based on this variable. This visualization confirms our conclusions about the association between recreation and exercise, as the slope is positive for low crime and negative for high crime.\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nexercise %&gt;% \n  mutate(crime2 = 1 * (crime &gt;= 9.963)) %&gt;%\n  ggplot(aes(x = recreation, y = exercise, col = as.factor(crime2))) + \n  geom_point() + \n  scale_color_discrete(name = \"Crime &gt;= 9.963\") +\n  labs(title = \"Scatter plot of recreational facilities vs. exercise (MET)\") + \n  ylab(\"Exercises (MET/week)\") + \n  xlab(\"Number of recreational facilities within 1-mile radius\")"
  },
  {
    "objectID": "hw/hw-01-creation.html#exercise-8",
    "href": "hw/hw-01-creation.html#exercise-8",
    "title": "HW 01: Bayesian linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow, assume the researchers are interested in understanding the impact of marital status on the relationship between access to recreational facililities and exercise. Setup a multivariable regression to estimate this association. There is no need to include other confounders. Does the association of interest (i.e., the relationship between access to recreational facilities and exercise) change across marital status? For a married pregnant women, what is the estimated slope of recreational facilities on exercise? What about for single pregnant women?\n\nAnswer: The regression is as follows:\n\n\nreg2 &lt;- lm(exercise ~ recreation * married, data = exercise)\nsummary(reg2)\n\n\nCall:\nlm(formula = exercise ~ recreation * married, data = exercise)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-154.734  -15.102    4.876   21.932  129.803 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         406.383     13.482  30.144   &lt;2e-16 ***\nrecreation            1.504      1.021   1.474   0.1443    \nmarried              11.693     16.742   0.698   0.4869    \nrecreation:married   -2.857      1.236  -2.311   0.0233 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 46.83 on 83 degrees of freedom\nMultiple R-squared:  0.09682,   Adjusted R-squared:  0.06417 \nF-statistic: 2.966 on 3 and 83 DF,  p-value: 0.0367\n\n\nFrom this we see that the interaction between marital status and recreational facilities is significant, with a p-value of 0.0233. The hypotheses that this corresponds to is \\(H_0:\\beta_3 = 0, H_1:\\beta_3 \\neq 0\\) at the \\(\\alpha = 0.05\\) level. Therefore, the association of interest does change across marital status. In particular, for a married pregnant woman the association (or slope) is \\(\\beta_1 + \\beta_3 = 1.504 - 2.857 = -1.353\\), and for single pregnant women, \\(\\beta_1 = 1.504\\).\n\nYou’re done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message “Done with Homework 1!”, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "notes/probability.html",
    "href": "notes/probability.html",
    "title": "Review of Probability",
    "section": "",
    "text": "This is foundational material that you should have already learned in a previous course. I’m reviewing important concepts that are needed for Bayesian inference.\nThe goal of Bayesian statistics is to compute the posterior distribution (i.e., the uncertainty distribution of the parameters, \\(\\boldsymbol{\\theta}\\), after observing the data, \\(\\mathbf{Y}\\))\nThis is the conditional distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{Y}\\)\nTherefore, we need to review the probability concepts that lead to the conditional distribution of one variable conditioned on another"
  },
  {
    "objectID": "notes/probability.html#review-of-probability",
    "href": "notes/probability.html#review-of-probability",
    "title": "Review of Probability",
    "section": "Review of probability",
    "text": "Review of probability\n\nThe goal of Bayesian statistics is to compute the posterior distribution (i.e., the uncertainty distribution of the parameters, \\(\\boldsymbol{\\theta}\\), after observing the data, \\(\\mathbf{Y}\\))\nThis is the conditional distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{Y}\\)\nTherefore, we need to review the probability concepts that lead to the conditional distribution of one variable conditioned on another\n\nProbability mass (pmf) and density (pdf) functions\nJoint distributions\nMarginal and conditional distributions"
  },
  {
    "objectID": "notes/probability.html#random-variables",
    "href": "notes/probability.html#random-variables",
    "title": "Review of Probability",
    "section": "Random variables",
    "text": "Random variables\n\\(X\\) (capital) is a random variable. We want to compute the probability that \\(X\\) takes on a specific value \\(x\\) (lowercase)\n\nThis is denoted \\(P(X = x)\\)\n\nWe also might want to compute the probability of \\(X\\) being in a set \\(\\mathcal A\\).\n\nThis is denoted \\(P(X \\in \\mathcal A)\\)\n\nThe set of possible values that \\(X\\) can take on is called its support, \\(\\mathcal S\\)\n\n\n\n\n\n\nRandom variables - example\n\n\n\n\nExample 1: \\(X\\) is the roll of a die\n\nThe support is \\(\\mathcal S = \\{1, 2, 3, 4, 5, 6\\}\\)\n\\(P(X = 1) = 1/6\\)\n\nExample 2: \\(X\\) is a newborn baby’s weight\n\nThe support is \\(\\mathcal S = (0, \\infty)\\)\n\\(P(X \\in [0, \\infty]) = 1\\)\n\n\n\n\n\nWhat is probability?\nObjective (associated with frequentist)\n\n\\(P(X = x)\\) as a purely mathematical statement\nIf we repeatedly sampled \\(X\\), the the proportion of draws equal to \\(x\\) converges to \\(P(X = x)\\)\n\nSubjective (associated with Bayesian)\n\n\\(P(X = x)\\) represents an individual’s degree of belief\nOften quantified as the amount an individual would be willing to wager that \\(X\\) will be \\(x\\)\n\nA Bayesian analysis makes use of both of these concepts\n\n\nWhat is uncertainty?\nAleatoric uncertainty (likelihood)\n\nUncontrollable randomness in the experiment\nFor example, the results of a fair coin flip can never be predicted with certainty\n\nEpistemic uncertainty (prior/posterior)\n\nUncertainty about a quantity that could theoretically be known\nFor example, if we flipped a coin infinitely-many times we could know the true probability of a head\n\nA Bayesian analysis makes use of both of these concepts\n\n\nProbability versus statistics\nProbability is the forward problem\n\nWe assume we know how the data are being generated and compute the probability of events\nFor example, what is the probability of flipping 5 straight heads if the coin is fair?\n\nStatistics is the inverse problem\n\nWe use data to learn about the data-generating mechanism\nFor example, if we flipped five straight head, can we conclude the coin is biased?\n\nAny statistical analysis obviously relies on probability"
  },
  {
    "objectID": "notes/probability.html#random-variables---example",
    "href": "notes/probability.html#random-variables---example",
    "title": "Review of Probability",
    "section": "Random variables - example",
    "text": "Random variables - example\n\nExample 1: \\(X\\) is the roll of a die\n\nThe support is \\(\\mathcal S = \\{1, 2, 3, 4, 5, 6\\}\\)\n\\(P(X = 1) = 1/6\\)\n\nExample 2: \\(X\\) is a newborn baby’s weight\n\nThe support is \\(\\mathcal S = (0, \\infty)\\)\n\\(P(X \\in [0, \\infty]) = 1\\)"
  },
  {
    "objectID": "notes/probability.html#what-is-probability",
    "href": "notes/probability.html#what-is-probability",
    "title": "Review of Probability",
    "section": "What is probability?",
    "text": "What is probability?\nObjective (associated with frequentist)\n\n\\(P(X = x)\\) as a purely mathematical statement\nIf we repeatedly sampled \\(X\\), the the proportion of draws equal to \\(x\\) converges to \\(P(X = x)\\)\n\nSubjective (associated with Bayesian)\n\n\\(P(X = x)\\) represents an individual’s degree of belief\nOften quantified as the amount an individual would be willing to wager that \\(X\\) will be \\(x\\)\n\nA Bayesian analysis makes use of both of these concepts"
  },
  {
    "objectID": "notes/probability.html#what-is-uncertainty",
    "href": "notes/probability.html#what-is-uncertainty",
    "title": "Review of Probability",
    "section": "What is uncertainty?",
    "text": "What is uncertainty?\nAleatoric uncertainty (likelihood)\n\nUncontrollable randomness in the experiment\nFor example, the results of a fair coin flip can never be predicted with certainty\n\nEpistemic uncertainty (prior/posterior)\n\nUncertainty about a quantity that could theoretically be known\nFor example, if we flipped a coin infinitely-many times we could know the true probability of a head\n\nA Bayesian analysis makes use of both of these concepts"
  },
  {
    "objectID": "notes/probability.html#probability-versus-statistics",
    "href": "notes/probability.html#probability-versus-statistics",
    "title": "Review of Probability",
    "section": "Probability versus statistics",
    "text": "Probability versus statistics\nProbability is the forward problem\n\nWe assume we know how the data are being generated and compute the probability of events\nFor example, what is the probability of flipping 5 straight heads if the coin is fair?\n\nStatistics is the inverse problem\n\nWe use data to learn about the data-generating mechanism\nFor example, if we flipped five straight head, can we conclude the coin is biased?\n\nAny statistical analysis obviously relies on probability"
  },
  {
    "objectID": "notes/probability.html#univariate-distributions",
    "href": "notes/probability.html#univariate-distributions",
    "title": "Review of Probability",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables\nThe random variable \\(X\\) is discrete if its support \\(\\mathcal S\\) is countable\nExamples:\n\n\\(X \\in \\{0, 1, 2, 3\\}\\) is the number of successes in 3 trials\n\n\\(X \\in \\{0, 1, 2, \\ldots\\}\\) is the number of patients with COVID in Durham County"
  },
  {
    "objectID": "notes/probability.html#univariate-distributions-1",
    "href": "notes/probability.html#univariate-distributions-1",
    "title": "Review of Probability",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables\nThe random variable \\(X\\) is continuous if its support \\(\\mathcal S\\) is uncountable\nExamples with \\(\\mathcal S = (0, \\infty)\\):\n\n\\(X &gt; 0\\) is systolic blood pressure\n\n\\(X &gt; 0\\) is a patient’s BMI"
  },
  {
    "objectID": "notes/probability.html#discrete-univariate-distributions",
    "href": "notes/probability.html#discrete-univariate-distributions",
    "title": "Review of Probability",
    "section": "Discrete univariate distributions",
    "text": "Discrete univariate distributions\n\nIf \\(X\\) is discrete we describe its distribution with its probability mass function (pmf)\nThe pmf is \\(f(x) = P(X = x)\\)\nThe domain of \\(X\\) is the set of \\(x\\) with \\(f(x) &gt; 0\\)\nWe must have \\(f(x) \\geq 0\\) and \\(\\sum_x f(x) = 1\\)\nThe mean is \\(\\mathbb E[X] = \\sum_x x f(x)\\)\nThe variance is \\(\\mathbb V(X) = \\sum_x(x − \\mathbb E[X])^2f(x)\\)\nThe last three sums are over \\(X\\)’s domain"
  },
  {
    "objectID": "notes/probability.html#parametric-families-of-distributions",
    "href": "notes/probability.html#parametric-families-of-distributions",
    "title": "Review of Probability",
    "section": "Parametric families of distributions",
    "text": "Parametric families of distributions\n\nA statistical analysis typically proceeds by selecting a pmf that seems to match the distribution of a sample\nWe rarely know the pmf exactly, but we assume it is from a parametric family of distributions\nFor example, Binomial(10, 0.5) and Binomial(4, 0.1) are different but both from the binomial family\nA family of distributions have the same equation for the pmf but differ by some unknown parameters \\(\\boldsymbol{\\theta}\\)\nWe must estimate these parameters"
  },
  {
    "objectID": "notes/probability.html#continuous-univariate-distributions",
    "href": "notes/probability.html#continuous-univariate-distributions",
    "title": "Review of Probability",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nIf \\(X\\) is continuous we describe its distribution with the probability density function (pdf) \\(f(x) \\geq 0\\)\nSince there are uncountably many possible values, \\(P(X = x) = 0\\) for all \\(x\\),\nProbabilities are computed as areas under the pdf curve \\[P(a &lt; X &lt; b) = \\int_a^b f(x)dx\\]\nTherefore, to be valid \\(f(x)\\) must satisfy \\(f(x) \\geq 0\\) and \\[P(−\\infty &lt; X &lt; \\infty) = \\int_{-\\infty}^{\\infty} f(x)dx = 1\\]"
  },
  {
    "objectID": "notes/probability.html#continuous-univariate-distributions-1",
    "href": "notes/probability.html#continuous-univariate-distributions-1",
    "title": "Review of Probability",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nThe domain is the set of \\(x\\) values with \\(f(x) &gt; 0\\)\nThe mean and the variance are defined similarly to the discrete case but with the sums replaced by integrals\nThe mean is \\(\\mathbb E[X] = \\int x f(x)dx\\)\nThe variance is \\(\\mathbb V(X) = \\int (x − \\mathbb E[X])^2 f(x)dx\\)"
  },
  {
    "objectID": "notes/probability.html#joint-distributions",
    "href": "notes/probability.html#joint-distributions",
    "title": "Review of Probability",
    "section": "Joint distributions",
    "text": "Joint distributions\n\n\\(X = (X_1, \\ldots, X_p)\\) is a random vector (vectors and matrices should be in bold).\nFor notational convenience, let’s consider only \\(p = 2\\) random variables \\(X\\) and \\(Y\\).\n\\((X, Y)\\) is discrete if it can take on a countable number of values, such as\n\\(X\\) = number of hearts and \\(Y\\) = number of face cards.\n\\((X, Y)\\) is continuous if it can take on an uncountable number of values, such as\n\\(X\\) = birthweight and \\(Y\\) = gestational age."
  },
  {
    "objectID": "notes/probability.html#discrete-random-variables",
    "href": "notes/probability.html#discrete-random-variables",
    "title": "Review of Probability",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe joint pmf is \\(f(x, y) = P(X = x, Y = y)\\)\n\n\\(\\sum_x \\sum_y f(x, y) = 1\\)\n\nThe marginal pmf for \\(X\\) is \\(f_X(x) = P(X = x) = \\sum_y f(x, y)\\)\nThe marginal pmf for \\(Y\\) is \\(f_Y(y) = P(Y = y) = \\sum_x\nf(x, y)\\)\nThe marginal distribution is the same as univariate distribution as if we ignored the other variable"
  },
  {
    "objectID": "notes/probability.html#discrete-random-variables-1",
    "href": "notes/probability.html#discrete-random-variables-1",
    "title": "Review of Probability",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe conditional pmf of \\(Y\\) given \\(X\\) is \\(f(y|x) = P(Y = y|X = x) = \\frac{P(X = x, Y = y)}{P(X = x)} = \\frac{f(x, y)}{f_X (x)}.\\)\n\\(X\\) and \\(Y\\) are independent if \\(f(x, y) = f_X(x)f_Y(y)\\) for all \\(x\\) and \\(y\\)\n\nVariables are dependent if they are not independent\n\nEquivalently, \\(X\\) and \\(Y\\) are independent if \\(f(x|y) = f_X(x)\\) for all \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "notes/probability.html#discrete-random-variables-2",
    "href": "notes/probability.html#discrete-random-variables-2",
    "title": "Review of Probability",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nNotation: \\(X_1, \\dots, X_n \\overset{\\mathrm{iid}}{\\sim} f(x)\\) means that \\(X_1, \\ldots, X_n\\) are independent and identically distributed\nThis implies the joint pmf is \\[P(X_1 = x_1, \\ldots, X_n = x_n) = \\prod_{i=1}^n f(x_i)\\]\nThe same notation and definitions of independence apply to continuous random variables\nIn this class, assume independence unless otherwise noted"
  },
  {
    "objectID": "notes/probability.html#continuous-random-variables",
    "href": "notes/probability.html#continuous-random-variables",
    "title": "Review of Probability",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nManipulating joint pdfs is similar to joint pmfs but sums are replaced by integrals\nThe joint pdf is denoted \\(f(x, y)\\)\nProbabilities are computed as volume under the pdf: \\[P((X, Y) ∈ A) = \\int_A f(x, y)dxdy\\] where \\(A \\subset \\mathbb{R}^2\\)"
  },
  {
    "objectID": "notes/probability.html#continuous-random-variables-1",
    "href": "notes/probability.html#continuous-random-variables-1",
    "title": "Review of Probability",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nThe marginal pdf of \\(X\\) is \\(f_X(x) = \\int f(x, y)dy\\)\n\\(f_X\\) is the univariate pdf for \\(X\\) as if we never considered \\(Y\\)\nThe conditional pdf of \\(Y\\) given \\(X\\) is \\[f(y|x) = \\frac{f(x, y)}{f_X (x)}\\]\nProper: \\(\\int f(y|x)dy = \\int \\frac{f(x,y)}{f_X(x)}dy = \\int \\frac{f(x,y)dy}{f_X(x)} = 1\\)"
  },
  {
    "objectID": "notes/probability.html#defining-joint-distributions-conditionally",
    "href": "notes/probability.html#defining-joint-distributions-conditionally",
    "title": "Review of Probability",
    "section": "Defining joint distributions conditionally",
    "text": "Defining joint distributions conditionally\n\nSpecifying joint distributions is hard\nEvery joint distribution can be written \\(f(x, y) = f(y|x)f(x)\\)\nTherefore, any joint distribution can be defined by\n\n\\(X\\)’s marginal distribution\nThe conditional distribution of \\(Y|X\\)\n\nThe joint problem reduces to two univariate problems\nThis idea forms the basis of hierarchical modeling"
  },
  {
    "objectID": "slides/01-welcome.html#review-of-probability",
    "href": "slides/01-welcome.html#review-of-probability",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Review of probability",
    "text": "Review of probability\n\nThe goal of Bayesian statistics is to compute the posterior distribution (i.e., the uncertainty distribution of the parameters, \\(\\boldsymbol{\\theta}\\), after observing the data, \\(\\mathbf{Y}\\)).\nThis is the conditional distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{Y}\\).\nTherefore, we need to review the probability concepts that lead to the conditional distribution of one variable conditioned on another.\n\nProbability mass (pmf) and density (pdf) functions\nJoint distributions\nMarginal and conditional distributions"
  },
  {
    "objectID": "slides/01-welcome.html#random-variables",
    "href": "slides/01-welcome.html#random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Random variables",
    "text": "Random variables\n\n\\(X\\) (capital) is a random variable.\nWe want to compute the probability that \\(X\\) takes on a specific value \\(x\\) (lowercase).\n\nThis is denoted \\(P(X = x)\\).\n\nWe also might want to compute the probability of \\(X\\) being in a set \\(\\mathcal A\\).\n\nThis is denoted \\(P(X \\in \\mathcal A)\\).\n\nThe set of possible values that \\(X\\) can take on is called its support, \\(\\mathcal S\\)."
  },
  {
    "objectID": "slides/01-welcome.html#random-variables---example",
    "href": "slides/01-welcome.html#random-variables---example",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Random variables - example",
    "text": "Random variables - example\n\nExample 1: \\(X\\) is the roll of a die.\n\nThe support is \\(\\mathcal S = \\{1, 2, 3, 4, 5, 6\\}\\).\n\\(P(X = 1) = 1/6\\).\n\nExample 2: \\(X\\) is a newborn baby’s weight.\n\nThe support is \\(\\mathcal S = (0, \\infty)\\).\n\\(P(X \\in [0, \\infty]) = 1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-probability",
    "href": "slides/01-welcome.html#what-is-probability",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is probability?",
    "text": "What is probability?\nObjective (associated with frequentist)\n\n\\(P(X = x)\\) as a purely mathematical statement.\nIf we repeatedly sampled \\(X\\), then the proportion of draws equal to \\(x\\) converges to \\(P(X = x)\\).\n\nSubjective (associated with Bayesian)\n\n\\(P(X = x)\\) represents an individual’s degree of belief.\nOften quantified as the amount an individual would be willing to wager that \\(X\\) will be \\(x\\).\n\nA Bayesian analysis makes use of both of these concepts."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-uncertainty",
    "href": "slides/01-welcome.html#what-is-uncertainty",
    "title": "Welcome to BIOSTAT 725!",
    "section": "What is uncertainty?",
    "text": "What is uncertainty?\nAleatoric uncertainty (likelihood)\n\nUncontrollable randomness in the experiment.\nFor example, the results of a fair coin flip can never be predicted with certainty.\n\nEpistemic uncertainty (prior/posterior)\n\nUncertainty about a quantity that could theoretically be known.\nFor example, if we flipped a coin infinitely-many times we could know the true probability of a head.\n\nA Bayesian analysis makes use of both of these concepts."
  },
  {
    "objectID": "slides/01-welcome.html#probability-versus-statistics",
    "href": "slides/01-welcome.html#probability-versus-statistics",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Probability versus statistics",
    "text": "Probability versus statistics\nProbability is the forward problem\n\nWe assume we know how the data are being generated and compute the probability of events\nFor example, what is the probability of flipping 5 straight heads if the coin is fair?\n\nStatistics is the inverse problem\n\nWe use data to learn about the data-generating mechanism\nFor example, if we flipped five straight head, can we conclude the coin is biased?\n\nAny statistical analysis obviously relies on probability"
  },
  {
    "objectID": "slides/01-welcome.html#univariate-distributions",
    "href": "slides/01-welcome.html#univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables.\nThe random variable \\(X\\) is discrete if its support \\(\\mathcal S\\) is countable.\nExamples:\n\n\\(X \\in \\{0, 1, 2, 3\\}\\) is the number of successes in 3 trials.\n\\(X \\in \\{0, 1, 2, \\ldots\\}\\) is the number of patients with COVID in Durham County."
  },
  {
    "objectID": "slides/01-welcome.html#univariate-distributions-1",
    "href": "slides/01-welcome.html#univariate-distributions-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Univariate distributions",
    "text": "Univariate distributions\n\nWe often distinguish between discrete and continuous random variables.\nThe random variable \\(X\\) is continuous if its support \\(\\mathcal S\\) is uncountable.\nExamples with \\(\\mathcal S = (0, \\infty)\\):\n\n\\(X &gt; 0\\) is systolic blood pressure.\n\\(X &gt; 0\\) is a patient’s BMI."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-univariate-distributions",
    "href": "slides/01-welcome.html#discrete-univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete univariate distributions",
    "text": "Discrete univariate distributions\n\nIf \\(X\\) is discrete we describe its distribution with its probability mass function (pmf).\nThe pmf is \\(f(x) = P(X = x)\\).\nThe domain of \\(X\\) is the set of \\(x\\) with \\(f(x) &gt; 0\\).\nWe must have \\(f(x) \\geq 0\\) and \\(\\sum_x f(x) = 1\\).\nThe mean is \\(\\mathbb E[X] = \\sum_x x f(x)\\).\nThe variance is \\(\\mathbb V(X) = \\sum_x(x − \\mathbb E[X])^2f(x)\\).\nThe last three sums are over \\(X\\)’s domain."
  },
  {
    "objectID": "slides/01-welcome.html#parametric-families-of-distributions",
    "href": "slides/01-welcome.html#parametric-families-of-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Parametric families of distributions",
    "text": "Parametric families of distributions\n\nA statistical analysis typically proceeds by selecting a pmf that seems to match the distribution of a sample.\nWe rarely know the pmf exactly, but we assume it is from a parametric family of distributions.\nFor example, Binomial(10, 0.5) and Binomial(4, 0.1) are different but both from the binomial family.\nA family of distributions have the same equation for the pmf but differ by some unknown parameters \\(\\boldsymbol{\\theta}\\).\nWe must estimate these parameters."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-univariate-distributions",
    "href": "slides/01-welcome.html#continuous-univariate-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nIf \\(X\\) is continuous we describe its distribution with the probability density function (pdf) \\(f(x) \\geq 0\\).\nSince there are uncountably many possible values, \\(P(X = x) = 0\\) for all \\(x\\).\nProbabilities are computed as areas under the pdf curve \\[P(a &lt; X &lt; b) = \\int_a^b f(x)dx.\\]\nTherefore, to be valid \\(f(x)\\) must satisfy \\(f(x) \\geq 0\\) and \\[P(−\\infty &lt; X &lt; \\infty) = \\int_{-\\infty}^{\\infty} f(x)dx = 1.\\]"
  },
  {
    "objectID": "slides/01-welcome.html#continuous-univariate-distributions-1",
    "href": "slides/01-welcome.html#continuous-univariate-distributions-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous univariate distributions",
    "text": "Continuous univariate distributions\n\nThe domain is the set of \\(x\\) values with \\(f(x) &gt; 0\\).\nThe mean and the variance are defined similarly to the discrete case but with the sums replaced by integrals.\nThe mean is \\(\\mathbb E[X] = \\int x f(x)dx\\).\nThe variance is \\(\\mathbb V(X) = \\int (x − \\mathbb E[X])^2 f(x)dx\\)."
  },
  {
    "objectID": "slides/01-welcome.html#joint-distributions",
    "href": "slides/01-welcome.html#joint-distributions",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Joint distributions",
    "text": "Joint distributions\n\n\\(\\mathbf{X} = (X_1, \\ldots, X_p)\\) is a random vector (vectors and matrices should be in bold).\nFor notational convenience, let’s consider only \\(p = 2\\) random variables \\(X\\) and \\(Y\\).\n\\((X, Y)\\) is discrete if it can take on a countable number of values, such as:\n\n\\(X\\) = number of hearts and \\(Y\\) = number of face cards.\n\n\\((X, Y)\\) is continuous if it can take on an uncountable number of values, such as:\n\n\\(X\\) = birthweight and \\(Y\\) = gestational age."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables",
    "href": "slides/01-welcome.html#discrete-random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe joint pmf is \\(f(x, y) = P(X = x, Y = y)\\).\n\n\\(\\sum_x \\sum_y f(x, y) = 1\\)\n\nThe marginal pmf for \\(X\\) is \\(f_X(x) = P(X = x) = \\sum_y f(x, y)\\).\nThe marginal pmf for \\(Y\\) is \\(f_Y(y) = P(Y = y) = \\sum_x f(x, y)\\).\nThe marginal distribution is the same as univariate distribution as if we ignored the other variable."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables-1",
    "href": "slides/01-welcome.html#discrete-random-variables-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nThe conditional pmf of \\(Y\\) given \\(X\\) is \\(f(y|x) = P(Y = y|X = x) = \\frac{P(X = x, Y = y)}{P(X = x)} = \\frac{f(x, y)}{f_X (x)}.\\)\n\\(X\\) and \\(Y\\) are independent if \\(f(x, y) = f_X(x)f_Y(y)\\) for all \\(x\\) and \\(y\\).\n\nVariables are dependent if they are not independent.\n\nEquivalently, \\(X\\) and \\(Y\\) are independent if \\(f(x|y) = f_X(x)\\) for all \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "slides/01-welcome.html#discrete-random-variables-2",
    "href": "slides/01-welcome.html#discrete-random-variables-2",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nNotation: \\(X_1, \\dots, X_n \\overset{\\mathrm{iid}}{\\sim} f(x)\\) means that \\(X_1, \\ldots, X_n\\) are independent and identically distributed.\nThis implies the joint pmf is \\[P(X_1 = x_1, \\ldots, X_n = x_n) = \\prod_{i=1}^n f(x_i).\\]\nThe same notation and definitions of independence apply to continuous random variables.\nIn this class, assume independence unless otherwise noted."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-random-variables",
    "href": "slides/01-welcome.html#continuous-random-variables",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nManipulating joint pdfs is similar to joint pmfs but sums are replaced by integrals.\nThe joint pdf is denoted \\(f(x, y)\\).\nProbabilities are computed as volume under the pdf: \\[P((X, Y) ∈ A) = \\int_A f(x, y)dxdy\\] where \\(A \\subset \\mathbb{R}^2\\)."
  },
  {
    "objectID": "slides/01-welcome.html#continuous-random-variables-1",
    "href": "slides/01-welcome.html#continuous-random-variables-1",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nThe marginal pdf of \\(X\\) is \\(f_X(x) = \\int f(x, y)dy\\).\n\\(f_X\\) is the univariate pdf for \\(X\\) as if we never considered \\(Y\\).\nThe conditional pdf of \\(Y\\) given \\(X\\) is \\[f(y|x) = \\frac{f(x, y)}{f_X (x)}.\\]\nProper: \\(\\int f(y|x)dy = \\int \\frac{f(x,y)}{f_X(x)}dy = \\int \\frac{f(x,y)dy}{f_X(x)} = 1\\)."
  },
  {
    "objectID": "slides/01-welcome.html#defining-joint-distributions-conditionally",
    "href": "slides/01-welcome.html#defining-joint-distributions-conditionally",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Defining joint distributions conditionally",
    "text": "Defining joint distributions conditionally\n\nSpecifying joint distributions is hard.\nEvery joint distribution can be written \\(f(x, y) = f(y|x)f(x)\\).\nTherefore, any joint distribution can be defined by,\n\n\\(X\\)’s marginal distribution\nThe conditional distribution of \\(Y|X\\)\n\nThe joint problem reduces to two univariate problems.\nThis idea forms the basis of hierarchical modeling."
  },
  {
    "objectID": "slides/01-welcome.html#bayes-rule",
    "href": "slides/01-welcome.html#bayes-rule",
    "title": "Welcome to BIOSTAT 725!",
    "section": "Bayes rule",
    "text": "Bayes rule\n\n\n\n\n\nThomas Bayes, 1701-1761\n\n\nPierre-Simon Laplace, 1749-1827\n\n\\[f(\\boldsymbol{\\theta}|\\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{\\int f(\\mathbf{Y}|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#defining-the-model",
    "href": "slides/02-monte-carlo.html#defining-the-model",
    "title": "Monte Carlo Sampling",
    "section": "Defining the model",
    "text": "Defining the model\nSuppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\).\nFor the purpose of today’s lecture, we assume \\(\\sigma\\) is known."
  },
  {
    "objectID": "slides/02-monte-carlo.html#defining-the-likelihood",
    "href": "slides/02-monte-carlo.html#defining-the-likelihood",
    "title": "Monte Carlo Sampling",
    "section": "Defining the likelihood",
    "text": "Defining the likelihood\nThe individual likelihood contribution for subject \\(i\\) is given by, \\[Y_i|\\boldsymbol{\\beta} \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta}) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta}),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#defining-the-likelihood-matrix-version",
    "href": "slides/02-monte-carlo.html#defining-the-likelihood-matrix-version",
    "title": "Monte Carlo Sampling",
    "section": "Defining the likelihood (matrix version)",
    "text": "Defining the likelihood (matrix version)\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta} ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#linear-regression-estimation",
    "href": "slides/02-monte-carlo.html#linear-regression-estimation",
    "title": "Monte Carlo Sampling",
    "section": "Linear regression estimation",
    "text": "Linear regression estimation\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#bayesian-estimation",
    "href": "slides/02-monte-carlo.html#bayesian-estimation",
    "title": "Monte Carlo Sampling",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#prior-definition",
    "href": "slides/02-monte-carlo.html#prior-definition",
    "title": "Monte Carlo Sampling",
    "section": "Prior definition",
    "text": "Prior definition\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-posterior",
    "href": "slides/02-monte-carlo.html#computing-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Computing the posterior",
    "text": "Computing the posterior\n\nIn general, computing the marginal likelihood, \\(f(Z)\\), is extremely difficulty.\nAn easier approach is to use the kernel trick.\n\n\\[\\begin{aligned}\nf(\\pi | Z) &\\propto f(Z | \\pi) f(\\pi)\\\\\n&\\propto \\pi^z (1-\\pi)^{n - z} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\\\\n&= \\pi^{\\left(\\alpha + z\\right) - 1} (1-\\pi)^{\\left(\\beta + n - z\\right) - 1}\\\\\n&= Beta\\left(\\alpha + z, \\beta + n - z\\right).\n\\end{aligned}\\]\n\nThis only works when a conjugate prior is used."
  },
  {
    "objectID": "slides/02-monte-carlo.html#techniques-to-find-this-posterior",
    "href": "slides/02-monte-carlo.html#techniques-to-find-this-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Techniques to find this posterior",
    "text": "Techniques to find this posterior\n\nBrute force: complete the square\nEasy: kernel recognition\n\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the multivariate normal pdf, \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#using-the-kernel-to-find-the-posterior",
    "href": "slides/02-monte-carlo.html#using-the-kernel-to-find-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Using the kernel to find the posterior",
    "text": "Using the kernel to find the posterior\n\\[\\begin{aligned}\nf(\\pi | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\pi) f(\\pi)\\\\\n&\\propto \\pi^{\\sum_{i=1}^n Y_i} (1-\\pi)^{n - \\sum_{i=1}^n Y_i} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\\\\n&= \\pi^{\\left(\\alpha + \\sum_{i=1}^n Y_i\\right) - 1} (1-\\pi)^{\\left(\\beta + n - \\sum_{i=1}^n Y_i\\right) - 1}\\\\\n&\\sim Beta\\left(\\alpha + \\sum_{i=1}^n Y_i, \\beta + n - \\sum_{i=1}^n Y_i\\right).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#back-to-the-posterior",
    "href": "slides/02-monte-carlo.html#back-to-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Back to the posterior",
    "text": "Back to the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-can-we-use-the-posterior",
    "href": "slides/02-monte-carlo.html#how-can-we-use-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nLet’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/02-monte-carlo.html#visualize-simulated-data",
    "href": "slides/02-monte-carlo.html#visualize-simulated-data",
    "title": "Monte Carlo Sampling",
    "section": "Visualize simulated data",
    "text": "Visualize simulated data"
  },
  {
    "objectID": "slides/02-monte-carlo.html#inspecting-the-prior",
    "href": "slides/02-monte-carlo.html#inspecting-the-prior",
    "title": "Monte Carlo Sampling",
    "section": "Inspecting the prior",
    "text": "Inspecting the prior\n\n###Define hyperparameters\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10"
  },
  {
    "objectID": "slides/02-monte-carlo.html#comparison-with-olsmle",
    "href": "slides/02-monte-carlo.html#comparison-with-olsmle",
    "title": "Monte Carlo Sampling",
    "section": "Comparison with OLS/MLE",
    "text": "Comparison with OLS/MLE\n\n###Compute posterior moments\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\nLet’s compare the posterior mean with the OLS/MLE estimate for the regression parameter \\(\\boldsymbol{\\beta}\\).\n\n\n\n\n\nparameter\ntrue\nbayes\nols/mle\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984\n\n\n\n\n\n\n\n\nWhat other summaries may we be interested in?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarizing-the-posterior",
    "href": "slides/02-monte-carlo.html#summarizing-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\n\nPosterior means, medians, modes, and variances\nJoint, conditional, and marginal probabilities, for example: \\(P(\\pi &lt; c | \\mathbf{Y})\\)\n\\(\\alpha\\)-quantiles: \\(\\{q_{\\alpha} : P(\\pi &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n\\(\\ldots\\)\n\nSummarizing a posterior (with known parametric form) is straightforward for basic quantities of interest."
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarizing-the-posterior-1",
    "href": "slides/02-monte-carlo.html#summarizing-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution.\n\n\\(P(\\pi \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\pi\\), \\(g\\left(\\pi\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\pi_1 - \\pi_2|\\), \\(\\pi_1/\\pi_2\\), \\(\\max\\left\\{\\pi_1,\\ldots,\\pi_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible.\n\nWhat are our options?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-mc-approximation",
    "href": "slides/02-monte-carlo.html#monte-carlo-mc-approximation",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo (MC) approximation",
    "text": "Monte Carlo (MC) approximation\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#mc-approximation",
    "href": "slides/02-monte-carlo.html#mc-approximation",
    "title": "Monte Carlo Sampling",
    "section": "MC approximation",
    "text": "MC approximation\nLet \\(g\\left(\\pi\\right)\\) be (just about) any function of \\(\\pi\\). The law of large numbers says that if,\n\\[\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\pi|\\mathbf{Y}\\right),\\] then, \\[\\frac{1}{S}\\sum_{s=1}^S g\\left(\\pi^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\pi\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\pi\\right)f\\left(\\pi|\\mathbf{Y}\\right)d\\pi,\\] as \\(S\\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#mc-approximation-1",
    "href": "slides/02-monte-carlo.html#mc-approximation-1",
    "title": "Monte Carlo Sampling",
    "section": "MC approximation",
    "text": "MC approximation\n\nImplications (as \\(S\\rightarrow \\infty\\)):\n\n\\(\\overline{\\pi}=\\frac{1}{S}\\sum_{s=1}^S \\pi^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\pi|\\mathbf{Y}\\right]\\)\n\\(\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\pi^{\\left(s\\right)}-\\overline{\\pi}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\pi|\\mathbf{Y}\\right)\\)\n\\(\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\pi^{\\left(s\\right)}\\leq \\mathbf{c}\\right) \\rightarrow P\\left(\\pi\\leq \\mathbf{c} | \\mathbf{Y}\\right)\\)\n\\(\\alpha\\)-quantile of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}\\)\n\nJust about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample"
  },
  {
    "objectID": "slides/02-monte-carlo.html#posterior-inference-for-arbitrary-functions",
    "href": "slides/02-monte-carlo.html#posterior-inference-for-arbitrary-functions",
    "title": "Monte Carlo Sampling",
    "section": "Posterior inference for arbitrary functions",
    "text": "Posterior inference for arbitrary functions\nInterest in the posterior distribution of a function of \\(\\pi\\), \\(g\\left(\\pi\\right)\\)\n\nMC sampling plan:\n\nDraw \\(\\pi^{\\left(1\\right)}\\) from \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\pi^{\\left(1\\right)}\\right)\\)\n\\(\\ldots\\)\nDraw \\(\\pi^{\\left(S\\right)}\\) from \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), calculate \\(g\\left(\\pi^{\\left(S\\right)}\\right)\\)\n\n\\(\\Rightarrow g\\left(\\pi^{\\left(1\\right)}\\right),\\ldots,g\\left(\\pi^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\pi\\right)|\\mathbf{Y}\\right)\\)\nTherefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-many-samples-to-take",
    "href": "slides/02-monte-carlo.html#how-many-samples-to-take",
    "title": "Monte Carlo Sampling",
    "section": "How many samples to take?",
    "text": "How many samples to take?\nWe can use a central limit theorem: \\(\\sqrt{S}\\left(\\overline{\\pi}-\\mathbb{E}[\\pi | \\mathbf{Y}]\\right)/\\sigma \\stackrel{d}{\\rightarrow} \\text{N}\\left(0,1\\right)\\),\n\n\\(\\overline{\\pi}=\\frac{1}{S}\\sum_{s=1}^S \\pi^{\\left(s\\right)}\\)\n\\(\\sigma^2 = \\mathbb{V}\\left(\\overline{\\pi}\\right) = \\frac{1}{S^2}\\sum_{s=1}^S\\mathbb{V}\\left(\\pi^{(s)}\\right) = \\frac{1}{S}\\mathbb{V}\\left(\\pi | \\mathbf{Y}\\right)\\).\n\n\\(\\implies \\overline{\\pi}\\approx N\\left(\\mathbb{E}[\\pi | \\mathbf{Y}],\\sigma^2/S\\right)\\)\nMC standard error: \\(\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\), \\(\\widehat{\\sigma}^2=\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\pi^{\\left(s\\right)}-\\overline{\\pi}\\right)^2\\)\nApproximate 95% MC confidence interval for the posterior mean: \\(\\overline{\\pi} \\pm 2\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}\\)\n\nChoose \\(S\\) large enough to report the posterior mean with your desired precision\nReporting MC standard errors for the posterior mean is a good way to indicate that \\(S\\) is large enough"
  },
  {
    "objectID": "slides/02-monte-carlo.html#returning-to-linear-regression",
    "href": "slides/02-monte-carlo.html#returning-to-linear-regression",
    "title": "Monte Carlo Sampling",
    "section": "Returning to linear regression",
    "text": "Returning to linear regression\nLet’s obtain \\(S = 1,000\\) samples from our posterior.\n\npi_samples &lt;- rbeta(1000, 1 + 120, 1 + 380)\n\nWe can compute the posterior mean and variance.\n\nprint(mean(pi_samples))\n\n[1] 0.2409367\n\nprint(var(pi_samples))\n\n[1] 0.0003463607"
  },
  {
    "objectID": "slides/02-monte-carlo.html#assessing-accuracy",
    "href": "slides/02-monte-carlo.html#assessing-accuracy",
    "title": "Monte Carlo Sampling",
    "section": "Assessing accuracy",
    "text": "Assessing accuracy"
  },
  {
    "objectID": "slides/02-monte-carlo.html#additional-posterior-summaries",
    "href": "slides/02-monte-carlo.html#additional-posterior-summaries",
    "title": "Monte Carlo Sampling",
    "section": "Additional posterior summaries",
    "text": "Additional posterior summaries\n\n# median\nmedian(pi_samples)\n\n[1] 0.240722\n\n# 95% credible intervals\nquantile(pi_samples, probs = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.2048398 0.2788959 \n\n# evaluating probability\nmean(pi_samples &lt; 0.25)\n\n[1] 0.682\n\n# summarizing arbitrary functions of the parameters\npi_new &lt;- pi_samples^3 - pi_samples\nc(mean(pi_new), quantile(pi_new, probs = c(0.025, 0.975)))\n\n                 2.5%      97.5% \n-0.2268475 -0.2572026 -0.1962448"
  },
  {
    "objectID": "slides/02-monte-carlo.html#matrix-likelihood-specification",
    "href": "slides/02-monte-carlo.html#matrix-likelihood-specification",
    "title": "Monte Carlo Sampling",
    "section": "Matrix likelihood specification",
    "text": "Matrix likelihood specification\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta} ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#review-of-last-lecture",
    "href": "slides/03-mcmc.html#review-of-last-lecture",
    "title": "Markov chain Monte Carlo",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we performed posterior inference for a Beta-Binomial model using Monte Carlo estimation.\nToday we will discuss Bayesian estimation of linear regression:\n\n\\[\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n).\\]\n\nThere is no closed form posterior, so we cannot directly use Monte Carlo sampling.\nWe need Markov chain Monte Carlo (MCMC)!\nOur goal is to gain intuition behind MCMC, the workhorse behind Stan."
  },
  {
    "objectID": "slides/03-mcmc.html#posterior-for-linear-regression",
    "href": "slides/03-mcmc.html#posterior-for-linear-regression",
    "title": "Markov chain Monte Carlo",
    "section": "Posterior for linear regression",
    "text": "Posterior for linear regression\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)d\\boldsymbol{\\beta}d\\sigma^2}.\n\\end{aligned}\\]\nNo closed form exists for the posterior. \\[f(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta}, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#motivation-for-gibbs-sampling",
    "href": "slides/03-mcmc.html#motivation-for-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Motivation for Gibbs sampling",
    "text": "Motivation for Gibbs sampling\n\nSuppose we were given \\(\\sigma^{2(1)}\\), a single sample from the marginal posterior distribution \\(f\\left(\\sigma^2|\\mathbf{Y}\\right)\\) (from where, who knows?)\nUse the sample to generate \\(\\boldsymbol{\\beta}^{(1)}\\) from \\(f\\left(\\boldsymbol{\\beta}|\\mathbf{Y},\\sigma^{2(1)}\\right)\\)\n\\(\\left(\\boldsymbol{\\beta}^{(1)},\\sigma^{2(1)}\\right)\\) is a sample from \\(f\\left(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}\\right)\\)\n\\(\\boldsymbol{\\beta}^{(1)}\\) is a sample from \\(f\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)\\)\n\n\n\n\n\n\n\n\nRecall\n\n\n\\(f\\left(\\boldsymbol{\\beta}, \\sigma^{2}|\\mathbf{Y}\\right) = f\\left(\\boldsymbol{\\beta} | \\sigma^{2},\\mathbf{Y}\\right)f\\left(\\sigma^{2}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\nSuppose we can sample from the following two distribution,\n\n\\(f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\boldsymbol{\\beta})\\)\n\\(f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}) \\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2) f(\\sigma^2)\\)\n\nThese are called full conditional distributions.\nSet initial values for \\(\\boldsymbol{\\theta}^{(0)} = (\\boldsymbol{\\beta}^{(0)}, \\sigma^{2(0)})\\). Then, given a current state of parameters \\(\\boldsymbol{\\theta}^{(s)}\\), we can generate a new state as follows:\n\nSample \\(\\boldsymbol{\\beta}^{(s + 1)} \\sim f(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^{2(s)})\\)\nSample \\(\\sigma^{2(s + 1)} \\sim f(\\sigma^2 | \\mathbf{Y}, \\boldsymbol{\\beta}^{(s + 1)})\\)\nLet \\(\\boldsymbol{\\theta}^{(s+1)} = (\\boldsymbol{\\beta}^{(s + 1)}, \\sigma^{2(s + 1)})\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#why-does-this-work",
    "href": "slides/03-mcmc.html#why-does-this-work",
    "title": "Markov chain Monte Carlo",
    "section": "Why does this work?",
    "text": "Why does this work?\n\n\\(\\boldsymbol{\\theta}^{(0)}\\) isn’t a sample from the posterior, it is an arbitrarily chosen initial value\n\\(\\boldsymbol{\\theta}^{(1)}\\) likely isn’t from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\)\n\\(\\boldsymbol{\\theta}^{(2)}\\) likely isn’t from the posterior either. Its distribution depends on \\(\\boldsymbol{\\theta}^{(0)}\\) and \\(\\boldsymbol{\\theta}^{(1)}\\)\nTheorem: For any initial values, the chain will eventually converge to the posterior\nTheorem: If \\(\\boldsymbol{\\theta}^{(s)}\\) is a sample from the posterior, then \\(\\boldsymbol{\\theta}^{(s+1)}\\) is too"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler",
    "href": "slides/03-mcmc.html#gibbs-sampler",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nUnder mild regulatory conditions that are generally satisfied for most statistical models, one can show that the iteration \\(\\boldsymbol{\\theta}^{(s)}\\) converges in distribution to a draw from the true joint posterior distribution\nSo for \\(s\\) sufficiently large (say, bigger than \\(s_0\\)), \\(\\left\\{\\boldsymbol{\\theta}^{(s)}, s=s_0+1,\\ldots,S\\right\\}\\) is a correlated sample from the true joint posterior (and \\(\\boldsymbol{\\beta}^{(s)}\\) and \\(\\sigma^{2(s)}\\) are samples from the marginals)\nSimilar to Monte Carlo approximation, we can use these samples to estimate posterior quantities of interest"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-1",
    "href": "slides/03-mcmc.html#gibbs-sampler-1",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\n\\(\\boldsymbol{\\theta}^{(t)}\\) depends on \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-1)}\\) only through \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\boldsymbol{\\theta}^{(t)}\\) is conditionally independent of \\(\\boldsymbol{\\theta}^{(0)},\\ldots,\\boldsymbol{\\theta}^{(t-2)}\\) given \\(\\boldsymbol{\\theta}^{(t-1)}\\)\n\\(\\implies\\) Markov property, so the sequence is called a Markov chain\nWe use the samples similar to MC approximation; therefore, Gibbs sampling is a form of Markov chain Monte Carlo (MCMC)\nWe will cover diagnostics for MCMC in another lecture!"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-1",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-1",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nWe need to compute the full conditionals. Before doing this, we require prior distributions.\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-2",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-2",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nComputing the full conditionals.\n\nWe already have the full conditional for \\(\\boldsymbol{\\beta}\\):\n\n\\(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2 \\sim N \\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}], \\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y})\\right)\\)\n\n\\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#how-can-we-use-the-posterior",
    "href": "slides/03-mcmc.html#how-can-we-use-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nLet’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/03-mcmc.html#perform-gibbs-sampling",
    "href": "slides/03-mcmc.html#perform-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Perform Gibbs sampling",
    "text": "Perform Gibbs sampling\n\nsigma2 &lt;- exp(rnorm(1)) # initial value\nsamples &lt;- NULL\nfor (s in 1:5000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n  \n  ###Sample from full conditional for sigma2\n  quadratic &lt;- as.numeric(t(Y - X %*% beta) %*% (Y - X %*% beta))\n  sigma2 &lt;- 1 / rgamma(1, shape = a + n / 2, rate = b + quadratic / 2)\n  \n  ###Save samples after a burn-in\n  samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-mcmc.html#inspect-results",
    "href": "slides/03-mcmc.html#inspect-results",
    "title": "Markov chain Monte Carlo",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-mcmc.html#parameter-estimation-vs.-posterior-approximation",
    "href": "slides/03-mcmc.html#parameter-estimation-vs.-posterior-approximation",
    "title": "Markov chain Monte Carlo",
    "section": "Parameter estimation vs. posterior approximation",
    "text": "Parameter estimation vs. posterior approximation\n\nModel specification: Choice of likelihood and introduction of model parameters\nPrior specification\nCalculation of the posterior\nSummarizing the posterior using MC or MCMC methods:\n\nThese are not models!\nThey do not generate more information than is in \\(\\mathbf{Y}\\) or \\(f\\left(\\boldsymbol{\\theta}\\right)\\)\nThey are simply ways of looking at \\(f\\left(\\boldsymbol{\\theta}|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#additional-topic-metropolis-sampling",
    "href": "slides/03-mcmc.html#additional-topic-metropolis-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Additional topic: Metropolis sampling",
    "text": "Additional topic: Metropolis sampling\n\nBefore we start using Stan for probabilistic programming, we need to understand the MCMC algorithm that is the engine for Stan’s inference, Hamiltonian Monte Carlo.\nTo get us one step closer we will quickly review the concept of Metropolis sampling, another MCMC variant"
  },
  {
    "objectID": "slides/03-mcmc.html#intuition-behind-metropolis-samping",
    "href": "slides/03-mcmc.html#intuition-behind-metropolis-samping",
    "title": "Markov chain Monte Carlo",
    "section": "Intuition behind Metropolis samping",
    "text": "Intuition behind Metropolis samping\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let’s consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)’s in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)’s.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-acceptance-ratio",
    "href": "slides/03-mcmc.html#metropolis-acceptance-ratio",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis acceptance ratio",
    "text": "Metropolis acceptance ratio\n\nFortunately, the comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) can be made even if we cannot compute \\(f(\\boldsymbol{\\theta} | \\mathbf{Y})\\).\n\n\\[\\begin{aligned}\nr &= \\frac{f(\\boldsymbol{\\theta}^* | \\mathbf{Y})}{f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y})}\\frac{f(\\mathbf{Y})}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\n\\end{aligned}\\]\nHaving computed \\(r\\), how should we proceed?"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-intuition",
    "href": "slides/03-mcmc.html#metropolis-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis intuition",
    "text": "Metropolis intuition\nMetropolis ratio: \\(r = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta}^*)f(\\boldsymbol{\\theta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\theta}^{(s)})f(\\boldsymbol{\\theta}^{(s)})}\\)\nIf \\(r &gt; 1:\\)\n\nIntuition: Since \\(\\boldsymbol{\\theta}^{(s)}\\) is already in our set, we should include \\(\\boldsymbol{\\theta}^*\\) as it has a higher probability than \\(\\boldsymbol{\\theta}^{(s)}\\)\nProcedure: Accept \\(\\boldsymbol{\\theta}^*\\) into our set (i.e., set \\(\\boldsymbol{\\theta}^{(s + 1)} = \\boldsymbol{\\theta}^*\\))\n\nIf \\(r &lt; 1:\\)\n\nIntuition: The relative frequency of \\(\\boldsymbol{\\theta}\\)-values in our set equal to \\(\\boldsymbol{\\theta}^*\\) compared to those equal to \\(\\boldsymbol{\\theta}^{(s)}\\) should be \\(r\\). This means that for every instance of \\(\\boldsymbol{\\theta}^{(s)}\\), we should have only a “fraction” of an instance of a \\(\\boldsymbol{\\theta}^*\\) value.\nProcedure: Set \\(\\boldsymbol{\\theta}^{(s + 1)}\\) equal to either \\(\\boldsymbol{\\theta}^*\\) or \\(\\boldsymbol{\\theta}^{(s)}\\), with probability \\(r\\) and \\(1 − r\\) respectively."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-update",
    "href": "slides/03-mcmc.html#metropolis-update",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis update",
    "text": "Metropolis update\nGiven \\(\\boldsymbol{\\theta}^{(s)}\\), the Metropolis algorithm generates a value \\(\\boldsymbol{\\theta}^{(s + 1)}\\) as follows:\n\nSample \\(\\boldsymbol{\\theta}^*\\) from a proposal distribution, \\(\\boldsymbol{\\theta}^* ∼ J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\)\nCompute the acceptance ratio \\(r\\)\nLet \\[\\boldsymbol{\\theta}^{(s + 1)} =\n\\left\\{\n  \\begin{array}{ll}\n\\boldsymbol{\\theta}^* & \\text{with probability }\\min(r, 1) \\\\\n\\boldsymbol{\\theta}^{(s)} & \\text{with probability }1 -\\min(r, 1)\n  \\end{array}\n\\right.\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-proposal-distribution",
    "href": "slides/03-mcmc.html#metropolis-proposal-distribution",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis proposal distribution",
    "text": "Metropolis proposal distribution\n\nThe proposal distribution is symmetric (i.e., \\(J(\\boldsymbol{\\theta}_a | \\boldsymbol{\\theta}_b) = J(\\boldsymbol{\\theta}_b | \\boldsymbol{\\theta}_a)\\)\nUsually \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) is very simple, with samples from \\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)})\\) being near \\(\\boldsymbol{\\theta}\\) with high probability.\nThe most common proposal is a normal distribution\n\n\\(J(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(s)}) = N(\\boldsymbol{\\theta}^{(s)}, \\boldsymbol{\\Delta})\\)\n\nThe value of the parameter \\(\\boldsymbol{\\Delta}\\) is generally chosen to make the approximation algorithm run efficiently"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-and-gibbs-combined",
    "href": "slides/03-mcmc.html#metropolis-and-gibbs-combined",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis and Gibbs combined",
    "text": "Metropolis and Gibbs combined\n\nThe Gibbs and Metropolis samplers are actually both algorithms within a larger class of Metropolis-Hastings algorithms\nWhen performing MCMC, one can actually choose to update a parameter using either a Gibbs or Metropolis update\nLet’s see this in action using our linear regression example"
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs",
    "href": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nIn this example, we will use a Metropolis update for \\(\\sigma^2\\), however we will actually focus on \\(\\log\\sigma^2\\).\n\nMetropolis requires a symmetric proposal, so it is often easier to transform parameters to be on the real line and use a normal proposal.\n\nWe will use the following proposal, \\(\\log\\sigma^{2*} \\sim N\\left(\\log\\sigma^{2(s)}, \\delta\\right)\\), where \\(\\delta = 1\\).\nWe will place the prior: \\(\\log\\sigma^2 \\sim N(0,1)\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs-1",
    "href": "slides/03-mcmc.html#linear-regression-using-metropolisgibbs-1",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression using Metropolis/Gibbs",
    "text": "Linear regression using Metropolis/Gibbs\n\nsigma2 &lt;- exp(rnorm(1))\nsamples &lt;- NULL\ndelta &lt;- 1\nfor (s in 1:10000) {\n  ###Sample from full conditional for beta\n  var_beta &lt;- chol2inv(chol(t(X) %*% X / sigma2 + diag(p + 1) / sigma_beta^2))\n  mean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma2)\n  beta &lt;- as.numeric(rmvnorm(1, mean_beta, var_beta))\n\n  ###Metropolis update for sigma2\n  # Sample a proposal value\n  log_sigma2_proposal &lt;- rnorm(1, log(sigma2), delta)\n  # Compute the ratio r on the log scale for numeric stability\n  # Also, I've decided to update log(sigma2) instead of sigma2, so I can use a normal proposal distribution\n  # I've placed a normal prior on log(sigma2)\n  likelihood_proposal &lt;- sum(dnorm(Y, X %*% beta, sqrt(exp(log_sigma2_proposal)), log = TRUE))\n  likelihood_current &lt;- sum(dnorm(Y, X %*% beta, sqrt(sigma2), log = TRUE))\n  prior_proposal &lt;- dnorm(log_sigma2_proposal, 0, 1, log = TRUE)\n  prior_current &lt;- dnorm(log(sigma2), 0, 1, log = TRUE)\n  log_r &lt;- (likelihood_proposal + prior_proposal) - (likelihood_current + prior_current)\n  # Update beta using Metropolis ratio\n  if (log(runif(1)) &lt; log_r) sigma2 &lt;- exp(log_sigma2_proposal)\n\n  ###Save samples after a burn-in\n  if (s &gt; 5000) samples &lt;- rbind(samples, c(beta, sigma2))\n}"
  },
  {
    "objectID": "slides/03-mcmc.html#inspect-results-1",
    "href": "slides/03-mcmc.html#inspect-results-1",
    "title": "Markov chain Monte Carlo",
    "section": "Inspect results",
    "text": "Inspect results"
  },
  {
    "objectID": "slides/03-mcmc.html#looking-towards-stan",
    "href": "slides/03-mcmc.html#looking-towards-stan",
    "title": "Markov chain Monte Carlo",
    "section": "Looking towards Stan",
    "text": "Looking towards Stan\n\nStan uses an algorithm called Hamiltonian Monte Carlo, which is a form of MCMC that uses a Metropolis update\nStan does all of the MCMC tuning, allowing us to only focus on the modeling!\nThis means that our job moving forward will be to focus on specifying the\n\nlikelihood: \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})\\)\nprior: \\(f(\\boldsymbol{\\theta})\\)\n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-mcmc.html#looking-towards-stan-1",
    "href": "slides/03-mcmc.html#looking-towards-stan-1",
    "title": "Markov chain Monte Carlo",
    "section": "Looking towards Stan",
    "text": "Looking towards Stan\n\nStan uses an algorithm called Hamiltonian Monte Carlo, which is a form of MCMC that uses a Metropolis update\nStan does all of the MCMC tuning, allowing us to only focus on the modeling!\nThis means that our job moving forward will be to focus on specifying the\n\nlikelihood: \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})\\)\nprior: \\(f(\\boldsymbol{\\theta})\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarizing-the-posterior-2",
    "href": "slides/02-monte-carlo.html#summarizing-the-posterior-2",
    "title": "Monte Carlo Sampling",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?"
  },
  {
    "objectID": "slides/02-monte-carlo.html#summarization-can-be-complex",
    "href": "slides/02-monte-carlo.html#summarization-can-be-complex",
    "title": "Monte Carlo Sampling",
    "section": "Summarization can be complex",
    "text": "Summarization can be complex\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\pi &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pbeta}\\))\n\n\nThese methods work well for standard posterior quantities and distributions."
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-mc-sampling",
    "href": "slides/02-monte-carlo.html#monte-carlo-mc-sampling",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo (MC) sampling",
    "text": "Monte Carlo (MC) sampling\n\nIntegration method based on random sampling\nThe general principles and procedures remain relatively constant across a broad class of problems\nSuppose we have \\(S\\) iid samples from our posterior distribution: \\(\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\pi|\\mathbf{Y}\\right)\\)\nThen the empirical distribution of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\) would approximate \\(f\\left(\\pi|\\mathbf{Y}\\right)\\), with the approximation improving as \\(S\\) increases\nThe empirical distribution of \\(\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\) is known as a MC approximation to \\(f\\left(\\pi|\\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#poisson-distribution",
    "href": "slides/02-monte-carlo.html#poisson-distribution",
    "title": "Monte Carlo Sampling",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe Poisson distribution models the number of events occurring within a fixed interval of time or space.\nR Code for Monte Carlo Approximation\n```r set.seed(123) lambda &lt;- 3 n &lt;- 10000 samples &lt;- rpois(n, lambda)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#coefficient-of-variation",
    "href": "slides/02-monte-carlo.html#coefficient-of-variation",
    "title": "Monte Carlo Sampling",
    "section": "Coefficient of Variation",
    "text": "Coefficient of Variation\nSuppose that \\(Y_i \\stackrel{iid}{\\sim} Poisson(\\lambda)\\) for \\(i = 1,\\ldots,n\\). Assume the following conjugate prior, \\(f(\\lambda) \\sim Gamma(a, b)\\), so that \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). We would like to perform inference for the \\(\\lambda\\). We take \\(S=10,000\\) samples.\n\nset.seed(54) # set seed for replicability\nS &lt;- 10000 # number of Monte Carlo samples\nlambda &lt;- 4 # true value of lambda\nn &lt;- 100 # sample size\nY &lt;- rpois(n, lambda)\na &lt;- 3 # hyperprior\nb &lt;- 1 # hyperprior\nsamples &lt;- rgamma(S, a + sum(Y), b + n) # sample from posterior\n\n# Posterior mean\ndata.frame(mean = mean(samples), median = median(samples), lower = quantile(samples, probs = c(0.025)), upper = quantile(samples, probs = c(0.975)))\n\n         mean   median    lower   upper\n2.5% 4.041415 4.038972 3.657051 4.44976\n\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/02-monte-carlo.html#poisson-random-variable",
    "href": "slides/02-monte-carlo.html#poisson-random-variable",
    "title": "Monte Carlo Sampling",
    "section": "Poisson random variable",
    "text": "Poisson random variable\nSuppose that \\(Y_i \\stackrel{iid}{\\sim} Poisson(\\lambda)\\) for \\(i = 1,\\ldots,n\\). Assume the following conjugate prior, \\(f(\\lambda) \\sim Gamma(a, b)\\), so that \\(f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)\\). We would like to perform inference for the \\(\\lambda\\). We take \\(S=10,000\\) samples.\n\nset.seed(54) # set seed for replicability\nS &lt;- 10000 # number of Monte Carlo samples\nlambda &lt;- 4 # true value of lambda\nn &lt;- 100 # sample size\nY &lt;- rpois(n, lambda)\na &lt;- 3 # hyperprior\nb &lt;- 1 # hyperprior\nsamples &lt;- rgamma(S, a + sum(Y), b + n) # sample from posterior\n\n\n\n\n\n\nmin\nq1\nmedian\nq3\nmax\nmean\nsd\n\n\n\n\n3.269\n3.904\n4.039\n4.173\n4.773\n4.041\n0.201"
  },
  {
    "objectID": "slides/02-monte-carlo.html#binomial-random-variable",
    "href": "slides/02-monte-carlo.html#binomial-random-variable",
    "title": "Monte Carlo Sampling",
    "section": "Binomial random variable",
    "text": "Binomial random variable\nSuppose that \\(Y_i \\stackrel{iid}{\\sim} Bernoulli(\\pi)\\) for \\(i = 1,\\ldots,n\\). Assume the following conjugate prior, \\(f(\\pi) \\sim Beta(a, b)\\), so that \\(f(\\pi|\\mathbf{Y}) \\sim Beta(a + \\sum_{i=1}^nY_i,b+n - \\sum_{i=1}^n)\\). We would like to perform inference for the \\(\\pi\\). We take \\(S=10,000\\) samples.\n\nset.seed(54) # set seed for replicability\nS &lt;- 10000 # number of Monte Carlo samples\npi &lt;- 0.35 # true value of lambda\nn &lt;- 1000 # sample size\nY &lt;- rbinom(n, size = 1, prob = pi)\na &lt;- 1 # hyperprior\nb &lt;- 1 # hyperprior\nsamples &lt;- rbeta(S, a + sum(Y), b + n - sum(Y)) # sample from posterior\n\n\n\n\n\n\nmin\nq1\nmedian\nq3\nmax\nmean\nsd\n\n\n\n\n0.291\n0.334\n0.344\n0.354\n0.411\n0.344\n0.015"
  },
  {
    "objectID": "slides/02-monte-carlo.html#sampling-for-any-distribution",
    "href": "slides/02-monte-carlo.html#sampling-for-any-distribution",
    "title": "Monte Carlo Sampling",
    "section": "Sampling for any distribution",
    "text": "Sampling for any distribution\nMonte Carlo sampling does not have to be used solely for posterior inference. Suppose we are interested in computed summaries for \\(X_i \\stackrel{iid}{\\sim} N(3, 4)\\) for \\(i = 1,\\ldots,n\\). We take \\(S = 10,000\\).\n\nS &lt;- 10000\nsamples &lt;- rnorm(S, 3, 2)\nlibrary(moments)\nkurtosis(samples)\n\n[1] 3.041546\n\nskewness(samples)\n\n[1] -0.0109433\n\nmean(samples &gt; 4) # P(X_i &gt; 4)\n\n[1] 0.3087"
  },
  {
    "objectID": "slides/02-monte-carlo.html#combination-of-random-variables",
    "href": "slides/02-monte-carlo.html#combination-of-random-variables",
    "title": "Monte Carlo Sampling",
    "section": "Combination of random variables",
    "text": "Combination of random variables\nSuppose \\(X_i \\stackrel{iid}{\\sim} N(3, 4)\\) and \\(Y_i \\stackrel{iid}{\\sim} \\chi^2(df=3)\\) for \\(i = 1,\\ldots,n\\). \\(X_i\\) and \\(Y_i\\) are independent. We are interested in summaries of \\(Z_i = X_i / Y_i\\). We take \\(S = 10,000\\).\n\nS &lt;- 10000\nx &lt;- rnorm(S, 3, 2)\ny &lt;- rchisq(S, 3)\nz &lt;- x / y\nmean(z)\n\n[1] 2.961057\n\nmedian(z)\n\n[1] 1.146081\n\nmean(z &gt; 1)\n\n[1] 0.5456"
  },
  {
    "objectID": "slides/02-monte-carlo.html#prepare-for-next-class",
    "href": "slides/02-monte-carlo.html#prepare-for-next-class",
    "title": "Monte Carlo Sampling",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nComplete HW 00 which is due before Thursday’s class\nComplete reading to prepare for Thursday’s lecture\nThursday’s lecture: Markov chain Monte Carlo\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-mcmc.html#hmc",
    "href": "slides/03-mcmc.html#hmc",
    "title": "Markov chain Monte Carlo",
    "section": "HMC",
    "text": "HMC\nThe Hamiltonian Monte Carlo (HMC) is a new MCMC approach that has been shown to work better than the usual MH algorithm. It is based on the idea of Hamiltonian dynamics.\nThe high-level idea of HMC is to generate a proposal from a better proposal distribution’ and modify the acceptance part so the it has a higher acceptance rate. In the usual MH algorithm, we are directly sample from a proposal density q(y|x). The HMC modifies this process using two components: a random momentum (velocity) vector ω and the Hamiltonian dynamics. The momentum is required fro every coordinate of the position x. Thus, if x ∈ R d, then we also need a vector of d elements for the momentum. As the name suggest, the momentum vector determines how we move x during the dynamics. The randomness is due to the random momentum vector (and the later acceptance part).\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/03-mcmc.html#summary-of-gibbs-sampling",
    "href": "slides/03-mcmc.html#summary-of-gibbs-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Summary of Gibbs sampling",
    "text": "Summary of Gibbs sampling\n\nGibbs sampling is great when we are able to sample from the full conditional distributions.\nIt has been the main inference machine for Bayesian inference since the early 1990s.\nComputing full conditionals and coding up a Gibbs sampler can be mathematically and computationally rigorous.\nNew classes of MCMC are becoming more common to make Bayesian inference less rigorous."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-samping",
    "href": "slides/03-mcmc.html#metropolis-samping",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis samping",
    "text": "Metropolis samping\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let’s consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)’s in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)’s.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-sampling",
    "href": "slides/03-mcmc.html#metropolis-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis sampling",
    "text": "Metropolis sampling\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let’s consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)’s in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)’s.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#intuition-behind-metropolis-sampling",
    "href": "slides/03-mcmc.html#intuition-behind-metropolis-sampling",
    "title": "Markov chain Monte Carlo",
    "section": "Intuition behind Metropolis sampling",
    "text": "Intuition behind Metropolis sampling\nSuppose we have a working collection \\(\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(s)}\\}\\) to which we would like to add a new value \\(\\boldsymbol{\\theta}^{(s+1)}\\). Let’s consider adding a value \\(\\boldsymbol{\\theta}^*\\) which is nearby \\(\\boldsymbol{\\theta}^{(s)}\\). Should we include \\(\\boldsymbol{\\theta}^*\\) in the set or not?\n\nIf \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &gt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then we want more \\(\\boldsymbol{\\theta}^*\\)’s in the set than \\(\\boldsymbol{\\theta}^{(s)}\\)’s.\n\nSince \\(\\boldsymbol{\\theta}^{(s)}\\) is already in the set, then it seems we should include \\(\\boldsymbol{\\theta}^*\\) as well.\n\nOn the other hand, if \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y}) &lt; f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\) then it seems we should not necessarily include \\(\\boldsymbol{\\theta}^*\\).\nSo, perhaps our decision to include \\(\\boldsymbol{\\theta}^*\\) or not should be based on a comparison of \\(f(\\boldsymbol{\\theta}^* | \\mathbf{Y})\\) to \\(f(\\boldsymbol{\\theta}^{(s)} | \\mathbf{Y})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#summary-of-metropolis-algorithm",
    "href": "slides/03-mcmc.html#summary-of-metropolis-algorithm",
    "title": "Markov chain Monte Carlo",
    "section": "Summary of Metropolis algorithm",
    "text": "Summary of Metropolis algorithm\n\nMore flexible than Gibbs sampling, because we are no longer required to compute the full conditional distribution analytically.\nPosterior samples can be obtained, however the algorithm must be properly tuned (i.e., choosing \\(\\delta\\)) and the samples may take longer to converge.\nFurthermore, choosing a proper proposal distribution can be difficult in practice.\nIn more recent years, Hamiltonian Monte Carlo has emerged as an new MCMC approach that alleviates the aforementioned issues."
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-hastings-algorithm",
    "href": "slides/03-mcmc.html#metropolis-hastings-algorithm",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis-Hastings algorithm",
    "text": "Metropolis-Hastings algorithm\nThe proposal distribution is no longer assumed to be symmetric, so the acceptance ratio is, \\[\\begin{aligned}\nr &= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}^*, \\sigma^{2(s)}) f(\\boldsymbol{\\beta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\beta}^{(s)},\\sigma^{2(s)})f(\\boldsymbol{\\beta}^{(s)})} \\frac{J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*)}{J(\\boldsymbol{\\beta}^* | \\boldsymbol{\\beta}^{(s)})}.\n\\end{aligned}\\]\nBoth Metropolis and Gibbs can easily be seen as subcases of Metropolis Hastings.\n\nMetropolis assumes a symmetric proposal, so the proposal terms cancel.\nGibbs assumes that \\(J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*) = f(\\boldsymbol{\\beta}^* | \\mathbf{Y}, \\sigma^{2(s)})\\), which implies that \\(r=1\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#metropolis-hastings-mh-algorithm",
    "href": "slides/03-mcmc.html#metropolis-hastings-mh-algorithm",
    "title": "Markov chain Monte Carlo",
    "section": "Metropolis-Hastings (MH) algorithm",
    "text": "Metropolis-Hastings (MH) algorithm\nThe proposal distribution is no longer assumed to be symmetric, so the acceptance ratio is, \\[\\begin{aligned}\nr &= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}^*, \\sigma^{2(s)}) f(\\boldsymbol{\\beta}^*)}{f(\\mathbf{Y} | \\boldsymbol{\\beta}^{(s)},\\sigma^{2(s)})f(\\boldsymbol{\\beta}^{(s)})} \\frac{J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*)}{J(\\boldsymbol{\\beta}^* | \\boldsymbol{\\beta}^{(s)})}.\n\\end{aligned}\\]\nBoth Metropolis and Gibbs can easily be seen as subcases of Metropolis Hastings.\n\nMetropolis assumes a symmetric proposal, so the proposal terms cancel.\nGibbs assumes that \\(J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*) = f(\\boldsymbol{\\beta}^* | \\mathbf{Y}, \\sigma^{2(s)})\\), which implies that \\(r=1\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#hamiltonian-monte-carlo-hmc-intuition",
    "href": "slides/03-mcmc.html#hamiltonian-monte-carlo-hmc-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "Hamiltonian Monte Carlo (HMC) intuition",
    "text": "Hamiltonian Monte Carlo (HMC) intuition\n\nHMC is a new MCMC approach that has been shown to work better than the usual MH algorithm.\nIt is based on the idea of Hamiltonian dynamics (a physical concept)\n\n\n\n\n\n\n\nRollercoaster Metaphor\n\n\nImagine you’re on a roller coaster at an amusement park. As the roller coaster moves along the track, it goes up and down hills. When the roller coaster is at the top of a hill, it has a lot of potential energy (like stored energy). When it goes down the hill, that potential energy turns into kinetic energy (energy of motion), making the roller coaster go faster. Hamiltonian dynamics is like a set of rules that tells us how the roller coaster’s energy changes as it moves along the track."
  },
  {
    "objectID": "slides/03-mcmc.html#hmc-intuition",
    "href": "slides/03-mcmc.html#hmc-intuition",
    "title": "Markov chain Monte Carlo",
    "section": "HMC intuition",
    "text": "HMC intuition\n\nHamiltonian dynamics is used to generate a proposal from a better proposal distribution, \\(J(\\boldsymbol{\\beta}^{(s)} | \\boldsymbol{\\beta}^*)\\), and modifies the acceptance part so the it has a higher acceptance rate.\nJust like the roller coaster follows the track smoothly, Hamiltonian Monte Carlo (HMC) helps us explore different possibilities smoothly and efficiently. This way, we can make more efficient samples from the posterior, just like how the roller coaster moves quickly and smoothly along its track.\nHMC requires evaluations of \\(\\log f(\\boldsymbol{\\theta} | \\mathbf{Y})\\) and \\(\\nabla_{\\boldsymbol{\\theta}} \\log f(\\boldsymbol{\\theta} | \\mathbf{Y})\\),\n\n\\[\\log f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto \\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) + \\log f(\\boldsymbol{\\theta})\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#prepare-for-next-class",
    "href": "slides/03-mcmc.html#prepare-for-next-class",
    "title": "Markov chain Monte Carlo",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nBegin HW 01 which is due January 30\nBe sure to turn in your AE by Sunday evening\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Probabilistic Programming (Intro to Stan!)\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/04-stan.html#review-of-last-lecture",
    "href": "slides/04-stan.html#review-of-last-lecture",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Thursday, we performed posterior inference for Bayesian linear regression using Gibbs and Metropolis sampling\n\nWe obtained correlated samples from the posterior using MCMC\nGibbs required a lot math!\nMetropolis required tuning!\n\nToday we will introduce Stan, a probabilistic programming language that uses Hamiltonian Monte Carlo to perform general Bayesian inference"
  },
  {
    "objectID": "slides/04-stan.html#learning-objectives",
    "href": "slides/04-stan.html#learning-objectives",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this lecture you should:\n\nKnow how to start coding up a model in Stan\nAppreciate how easy Stan makes things for us compared to coding up the algorithm ourselves\nKnow what to do when coding goes wrong\nKnow what to do when sampling goes wrong"
  },
  {
    "objectID": "slides/04-stan.html#what-is-stan-and-how-do-we-use-it",
    "href": "slides/04-stan.html#what-is-stan-and-how-do-we-use-it",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "What is Stan and how do we use it?",
    "text": "What is Stan and how do we use it?\n\nStan is an intuitive yet sophisticated programming language that does the hard work for us.\nProgramming language like R, Python, Matlab, C++…\nWorks like most other languages: can use loops, conditional statements, and functions.\nCode up a model in Stan and then it implements HMC (actually something called NUTS) for us."
  },
  {
    "objectID": "slides/04-stan.html#why-should-we-use-stan",
    "href": "slides/04-stan.html#why-should-we-use-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Why should we use Stan?",
    "text": "Why should we use Stan?\n\nStan is the brainchild of Andrew Gelman at Colombia.\nStan’s uses an extension of HMC called NUTS that automatically tunes. It is fast.\nStan is simple to learn.\nStan has excellent documentation (a manual full of extensive examples).\nMost important: Stan has a very active and helpful user forum and development team; for example, typical question answered in less than a couple of hours."
  },
  {
    "objectID": "slides/04-stan.html#how-do-we-use-it",
    "href": "slides/04-stan.html#how-do-we-use-it",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "How do we use it?",
    "text": "How do we use it?\nCode up model in Stan code in a text editor and save as .stan file.\n\nCall Stan to run the model from:\n\nR, python, the command line, Matlab, Stata, Julia\n\nUse one of the above to analyse the data (of course you can export to another one)."
  },
  {
    "objectID": "slides/04-stan.html#bayesian-linear-regression",
    "href": "slides/04-stan.html#bayesian-linear-regression",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Bayesian linear regression",
    "text": "Bayesian linear regression\nSuppose we assume the linear regression model:\n\nLikelihood: \\(\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\)\nPriors: \\(\\boldsymbol{\\beta} \\sim N(\\mathbf{0}, \\sigma_{\\beta}^2\\mathbf{I}_p)\\) and \\(f(\\sigma^2) \\sim IG(a,b)\\)\n\nHow de we code this up in Stan?"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program",
    "href": "slides/04-stan.html#an-example-stan-program",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program",
    "text": "An example Stan program\n\ndata {\n  real Y[10]; // height for 10 people\n}\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  Y ~ normal(mu, sigma); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma ~ inv_gamma(1, 1); // prior for sigma\n}"
  },
  {
    "objectID": "slides/04-stan.html#a-straightforward-example",
    "href": "slides/04-stan.html#a-straightforward-example",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "A straightforward example",
    "text": "A straightforward example\nSuppose:\n\nWe record the height, \\(Y_i\\), of 10 people.\nWe want a model to explain the variation, and choose a normal likelihood: \\[Y_i \\sim N(\\mu, \\sigma^2)\\]\nWe choose the following (independent) priors on each parameter:\n\n\\(\\mu \\sim N(0, 1)\\)\n\\(\\sigma^2 \\sim IG(1, 1)\\)\n\nQuestion: how do we code this up in Stan?"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-data-block",
    "href": "slides/04-stan.html#an-example-stan-program-data-block",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: data block",
    "text": "An example Stan program: data block\n\ndata {\n  real Y[10]; // height for 10 people\n}\n\n\nDeclare all data that you will pass to Stan to estimate your model.\nTerminate all statements with a semi-colon ;.\nUse ## or // for comments."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-data-block-1",
    "href": "slides/04-stan.html#an-example-stan-program-data-block-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: data block",
    "text": "An example Stan program: data block\n\ndata {\n  real Y[10]; // height for 10 people\n}\n\n\nWe need to tell Stan the type of data variable. For example:\n\nreal for continuous data.\nint for discrete data.\nArrays: above we specified Y as an array of continuous data of length 10."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-data-block-2",
    "href": "slides/04-stan.html#an-example-stan-program-data-block-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: data block",
    "text": "An example Stan program: data block\n\ndata {\n  real Y[10]; // height for 10 people\n}\n\nCan place limits on data, for example:\n\nreal&lt;lower = 0, upper = 1&gt; X;\nreal&lt;lower = 0&gt; Z;\n\nVectors and matrices; only contain reals and can be used for matrix operations.\n\nreal Y[10]; // array representation\nvector[10] Y; // vector representation"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-parameter-block",
    "href": "slides/04-stan.html#an-example-stan-program-parameter-block",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: parameter block",
    "text": "An example Stan program: parameter block\n\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma;\n}\n\n\nDeclare all parameters that you use in your model.\nPlace limits on variables, for example:\n\nreal&lt;lower = 0&gt; sigma\n\n\nA multitude of parameter types including some of the aforementioned:\n\nreal for continuous parameters.\nArrays of types, for example real beta[10]"
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-parameter-block-1",
    "href": "slides/04-stan.html#an-example-stan-program-parameter-block-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: parameter block",
    "text": "An example Stan program: parameter block\n\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma;\n}\n\n\nvector or matrix, specified by:\n\nvector[5] beta\nmatrix[5, 3] gamma\n\nsimplex for a parameter vector that must sum to 1.\nMore exotic types like corr_matrix, or ordered."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-parameter-block-2",
    "href": "slides/04-stan.html#an-example-stan-program-parameter-block-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: parameter block",
    "text": "An example Stan program: parameter block\n\nparameters {\n  real mu;\n  real&lt;lower = 0&gt; sigma;\n}\n\nImportant: Stan is not developed yet to work with discrete parameters. Options for discrete parameters in Stan:\n\nMarginalize out the parameter. For example, suppose we have \\(f(\\boldsymbol{\\beta}, \\theta)\\), where \\(\\boldsymbol{\\beta}\\) is continuous and \\(\\theta\\) is discrete:\n\n\n\\(f(\\boldsymbol{\\beta}) = \\sum_{i = 1}^K f(\\boldsymbol{\\beta}, \\theta_i)\\)\n\n\nSome models can be reformulated without discrete parameters."
  },
  {
    "objectID": "slides/04-stan.html#r-packages-that-interface-with-stan",
    "href": "slides/04-stan.html#r-packages-that-interface-with-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "R packages that interface with Stan",
    "text": "R packages that interface with Stan\n\nrstan, brms, cmdstanr, rstanarm\nrstan and cmdstanr you write the Stan code, which gives you the most options.\n\nrstan has a more intuitive user interface.\ncmdstanr is more memory efficient and a lightweight interface to Stan.\n\nrstanarm and brms you don’t need to write the Stan code yourself, which makes it easier to use Stan, but is limiting.\n\nrstanarm’s biggest advantage is that the models are pre-compiled, but this is also it’s biggest limitation.\nbrms writes Stan code on the fly, so has many more models, some that are pretty advanced."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-model-block",
    "href": "slides/04-stan.html#an-example-stan-program-model-block",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: model block",
    "text": "An example Stan program: model block\n\nmodel {\n  Y ~ normal(mu, sigma); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma ~ inv_gamma(1, 1); // prior for sigma\n}\n\n\nUsed to define:\n\nLikelihood.\nPriors on parameters.\n\n\nIf don’t specify priors on parameters Stan assumes you are using flat priors (which can be improper)."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-model-block-1",
    "href": "slides/04-stan.html#an-example-stan-program-model-block-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: model block",
    "text": "An example Stan program: model block\n\nmodel {\n  Y ~ normal(mu, sigma); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma ~ inv_gamma(1, 1); // prior for sigma\n}\n\nHuge range of probability distributions covered, across a range of parameterizations. For example:\n\nDiscrete: Bernoulli, binomial, normal, Poisson, beta-binomial, negative-binomial, categorical, multinomial.\nContinuous unbounded: normal, skew-normal, student-t, Cauchy, logistic."
  },
  {
    "objectID": "slides/04-stan.html#an-example-stan-program-model-block-2",
    "href": "slides/04-stan.html#an-example-stan-program-model-block-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "An example Stan program: model block",
    "text": "An example Stan program: model block\n\nmodel {\n  Y ~ normal(mu, sigma); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma ~ inv_gamma(1, 1); // prior for sigma\n}\n\n\nContinuous bounded: uniform, beta, log-normal, exponential, gamma, chi-squared, inverse-chi-squared, Weibull, Wiener diffusion, Pareto.\nMultivariate continuous: normal, student-t, Gaussian process.\nExotics: Dirichlet, LKJ correlation distribution, Wishart and its inverse, Von-Mises."
  },
  {
    "objectID": "slides/04-stan.html#running-stan",
    "href": "slides/04-stan.html#running-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Running Stan",
    "text": "Running Stan\nWrite model in a text editing program and save as a .stan file.\n\nTo create a .stan file from RStudio, File -&gt; New File -&gt; Stan File.\n\n\n###Load packages\nlibrary(rstan)\n\n###Generate fake data\nY &lt;- rnorm(10, mean = 0, sd = 1)\n\n###Compile and run model, and save in fit\nfit &lt;- stan(file = 'straightforward.stan', data = list(Y= Y), \n            iter = 1000, chains = 4, seed = 1)"
  },
  {
    "objectID": "slides/04-stan.html#running-stan-on-example-model",
    "href": "slides/04-stan.html#running-stan-on-example-model",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Running Stan on example model",
    "text": "Running Stan on example model\n\n###Compile and run model, and save in fit\nfit &lt;- stan(file = 'straightforward.stan', data = list(Y = Y), \n            iter = 1000, chains = 4, seed = 1)\n\nThe above R code runs NUTS for our model with the following options:\n\n\\(S=1,000\\) MCMC samples of which 500 are discarded as warm-up.\nAcross 4 chains.\nUsing a random number seed of 1 (good to ensure you can reproduce results)."
  },
  {
    "objectID": "slides/04-stan.html#example-model-results",
    "href": "slides/04-stan.html#example-model-results",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Example model: results",
    "text": "Example model: results\n\n###Print summary statistics\nprint(fit, probs = c(0.25, 0.5, 0.75))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n       mean se_mean   sd   25%   50%   75% n_eff Rhat\nmu     0.37    0.01 0.31  0.17  0.38  0.55   875 1.00\nsigma  0.94    0.01 0.26  0.76  0.89  1.06   785 1.00\nlp__  -5.08    0.05 1.20 -5.53 -4.70 -4.23   565 1.01\n\nSamples were drawn using NUTS(diag_e) at Thu Nov 21 06:59:52 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/04-stan.html#example-model-results-1",
    "href": "slides/04-stan.html#example-model-results-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Example model: results",
    "text": "Example model: results\n\n###Extract posterior samples\npars &lt;- extract(fit)\nclass(pars)\n\n[1] \"list\"\n\nnames(pars)\n\n[1] \"mu\"    \"sigma\" \"lp__\" \n\n###Extract samples for particular parameters\npars &lt;- extract(fit, pars = \"mu\")\nclass(pars$mu)\n\n[1] \"array\"\n\ndim(pars$mu)\n\n[1] 2000"
  },
  {
    "objectID": "slides/04-stan.html#visualize-posterior",
    "href": "slides/04-stan.html#visualize-posterior",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Visualize posterior",
    "text": "Visualize posterior\n###Extract samples for particular parameters\nlibrary(ggplot2)\ndata.frame(mu = pars$mu) |&gt;\n  ggplot(aes(x = mu)) +\n  geom_histogram() +\n  labs(x = expression(mu), y = \"Count\", \n       subtitle = bquote(\"Posterior distribution for \" ~ mu))"
  },
  {
    "objectID": "slides/04-stan.html#quick-note-what-does-sim-actually-mean",
    "href": "slides/04-stan.html#quick-note-what-does-sim-actually-mean",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Quick note: what does \\(\\sim\\) actually mean?",
    "text": "Quick note: what does \\(\\sim\\) actually mean?\n\nmodel {\n  Y ~ normal(mu, sigma); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma ~ inv_gamma(1, 1); // prior for sigma\n}\n\n\n\\(\\sim\\) doesn’t mean sampling, although often times it can be thought of as sampling\nMCMC/HMC makes use of the log-posterior\n\n\\[\\log \\prod_{i=1}^nf(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto \\log f(\\boldsymbol{\\theta}) + \\sum_{i=1}^n \\log f(\\mathbf{Y} | \\boldsymbol{\\theta})\\]\n\nAs such \\(\\sim\\) really means increment log probability"
  },
  {
    "objectID": "slides/04-stan.html#quick-note-what-does-sim-mean",
    "href": "slides/04-stan.html#quick-note-what-does-sim-mean",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Quick note: what does \\(\\sim\\) mean?",
    "text": "Quick note: what does \\(\\sim\\) mean?\n\nmodel {\n  Y ~ normal(mu, sigma); // likelihood\n  mu ~ normal(0, 1); // prior for mu\n  sigma ~ inv_gamma(1, 1); // prior for sigma\n}\n\n\n\\(\\sim\\) doesn’t mean sampling, although often times it can be thought of as sampling\nMCMC/HMC makes use of the log-posterior\n\n\\[\\log f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto \\log f(\\boldsymbol{\\theta}) + \\sum_{i=1}^n \\log f(\\mathbf{Y} | \\boldsymbol{\\theta})\\]\n\nAs such \\(\\sim\\) really means increment log probability\nAll we have to do in Stan is specify the log-posterior!"
  },
  {
    "objectID": "slides/04-stan.html#alternate-way-of-specifying-stan-models",
    "href": "slides/04-stan.html#alternate-way-of-specifying-stan-models",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Alternate way of specifying Stan models",
    "text": "Alternate way of specifying Stan models\n\nmodel {\n  target += normal_lpdf(Y | mu, sigma); // likelihood\n  target += normal_lpdf(mu | 0, 1); // prior for mu\n  target += inv_gamma_lpdf(sigma | 1, 1); // prior for sigma\n}\n\n\ntarget is a not a variable, but a special object that represents incremental log probability.\ntarget is initialized to zero.\nnormal_lpdf is the log of the normal density of y given location mu and scale sigma:\nStan documentation for normal distribution\n\n\ntarget += std_normal_lpdf(mu) // prior for mu using standard normal"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan",
    "href": "slides/04-stan.html#linear-regression-using-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan",
    "text": "Linear regression using Stan\n\n// saved in linear_regression.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates\n  vector[n] Y; // outcome vector\n  matrix[n, p + 1] X; // covariate vector\n  real beta0; // location hyperparameter for beta\n  real&lt;lower = 0&gt; sigma_beta; // scale hyperparameter for beta\n  real&lt;lower = 0&gt; a; // shape hyperparameter for sigma\n  real&lt;lower = 0&gt; b; // scale hyperparameter for sigma\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, sigma); // likelihood\n  target += normal_lpdf(beta | beta0, sigma_beta); // prior for beta\n  target += inv_gamma_lpdf(sigma | a, b); // prior for sigma\n}"
  },
  {
    "objectID": "slides/04-stan.html#fit-model",
    "href": "slides/04-stan.html#fit-model",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Fit model",
    "text": "Fit model\n\n###Load packages\nlibrary(rstan)\n\n###Generate fake data\nY &lt;- rnorm(10, mean = 0, sd = 1)\n\n###Compile and run model, and save in fit\nfit &lt;- stan(file = 'straightforward.stan', data = list(Y= Y), \n            iter = 1000, chains = 4)"
  },
  {
    "objectID": "slides/04-stan.html#running-stan-on-example-model-1",
    "href": "slides/04-stan.html#running-stan-on-example-model-1",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Running Stan on example model",
    "text": "Running Stan on example model\n\n###Compile and run model, and save in fit\nfit &lt;- stan(file = 'straightforward.stan', data = list(Y = Y), \n            iter = 1000, chains = 4)"
  },
  {
    "objectID": "slides/04-stan.html#lets-simulate-some-data-again",
    "href": "slides/04-stan.html#lets-simulate-some-data-again",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Let’s simulate some data again",
    "text": "Let’s simulate some data again\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/04-stan.html#fit-linear-regression-using-stan",
    "href": "slides/04-stan.html#fit-linear-regression-using-stan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Fit linear regression using Stan",
    "text": "Fit linear regression using Stan\n\n###Load packages\nlibrary(rstan)\n\n###Create stan data object\nstan_data &lt;- list(n = n,\n                  p = p,\n                  Y = Y,\n                  X = X,\n                  beta0 = 0,\n                  sigma_beta = 10,\n                  a = 3, \n                  b = 1)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_fit.rds\")"
  },
  {
    "objectID": "slides/04-stan.html#example-model-results-2",
    "href": "slides/04-stan.html#example-model-results-2",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Example model: results",
    "text": "Example model: results\n\n###Print summary statistics\nprint(fit, probs = c(0.25, 0.5, 0.75))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n           mean se_mean   sd     25%     50%     75% n_eff Rhat\nbeta[1]   -1.47    0.00 0.15   -1.57   -1.47   -1.37  1840    1\nbeta[2]    3.30    0.00 0.15    3.20    3.30    3.40  1897    1\nsigma      1.55    0.00 0.11    1.47    1.55    1.62  1748    1\nlp__    -195.93    0.04 1.30 -196.50 -195.60 -194.95   969    1\n\nSamples were drawn using NUTS(diag_e) at Thu Nov 21 08:51:41 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/04-stan.html#stan-plots-point-estimate-and-intervals",
    "href": "slides/04-stan.html#stan-plots-point-estimate-and-intervals",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan plots: point estimate and intervals",
    "text": "Stan plots: point estimate and intervals\n\nstan_plot(fit, pars = c(\"beta\", \"sigma\"), include_warmup = FALSE,\n          point_est = \"median\", ci_level = 0.8, outer_level = 0.95)"
  },
  {
    "objectID": "slides/04-stan.html#stan-plots-histogram",
    "href": "slides/04-stan.html#stan-plots-histogram",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan plots: histogram",
    "text": "Stan plots: histogram\n\nstan_hist(fit)"
  },
  {
    "objectID": "slides/04-stan.html#stan-plots-density",
    "href": "slides/04-stan.html#stan-plots-density",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan plots: density",
    "text": "Stan plots: density\n\nstan_dens(fit)"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan-data-and-parameter-chunks",
    "href": "slides/04-stan.html#linear-regression-using-stan-data-and-parameter-chunks",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan: data and parameter chunks",
    "text": "Linear regression using Stan: data and parameter chunks\n\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates\n  vector[n] Y; // outcome vector\n  matrix[, p + 1] X; // covariate vector\n  real beta0; // location hyperparameter for beta\n  real&lt;lower = 0&gt; sigma_beta; // scale hyperparameter for beta\n  real&lt;lower = 0&gt; a; // shape hyperparameter for sigma\n  real&lt;lower = 0&gt; b; // scale hyperparameter for sigma\n}\nparameters {\n  vector[p + 1] beta;\n  real&lt;lower = 0&gt; sigma;\n}"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan-model-chunk",
    "href": "slides/04-stan.html#linear-regression-using-stan-model-chunk",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan: model chunk",
    "text": "Linear regression using Stan: model chunk\n\nmodel {\n  for (i in 1:n) {\n    target += normal_lpdf(Y[i] | X[i, ] * beta, sigma); // likelihood\n  }\n  target += normal_lpdf(beta | beta0, sigma_beta); // prior for beta\n  target += inv_gamma_lpdf(sigma | a, b); // prior for sigma\n}"
  },
  {
    "objectID": "slides/04-stan.html#linear-regression-using-stan-vectorization",
    "href": "slides/04-stan.html#linear-regression-using-stan-vectorization",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Linear regression using Stan: vectorization",
    "text": "Linear regression using Stan: vectorization\nIt is always a good idea to vectorize Stan code for faster and more efficient inference\n\nmodel {\n  target += normal_lpdf(Y | X * beta, sigma); // likelihood\n  target += normal_lpdf(beta | beta0, sigma_beta); // prior for beta\n  target += inv_gamma_lpdf(sigma | a, b); // prior for sigma\n}"
  },
  {
    "objectID": "slides/04-stan.html#stan-a-few-of-the-loops-and-conditions",
    "href": "slides/04-stan.html#stan-a-few-of-the-loops-and-conditions",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan: a few of the loops and conditions",
    "text": "Stan: a few of the loops and conditions\nStan has pretty much the full range of language constructs to allow pretty much any model to be coded.\nfor (i in 1:10) {something;}\n\n\nwhile (i &gt; 1) {something;}\n\n\nif (i &gt; 1) {something 1;}\nelse if (i == 0) {something2;}\nelse {something 3;}"
  },
  {
    "objectID": "slides/04-stan.html#stan-speed-concerns",
    "href": "slides/04-stan.html#stan-speed-concerns",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan speed concerns",
    "text": "Stan speed concerns\nWhilst Stan is fast it pays to know the importance of each code block for efficiency.\n\ndata: called once at beginning of execution.\ntransformed data: called once at beginning of execution.\nparameters: every log probability evaluation!\ntransformed parameters: every log probability evaluation!\nmodel: every log probability evaluation!\ngenerated quantities: once per sample.\nfunctions: how many times it is called depends on the function’s nature."
  },
  {
    "objectID": "slides/04-stan.html#stan-in-parallel",
    "href": "slides/04-stan.html#stan-in-parallel",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan in parallel",
    "text": "Stan in parallel\nIn R can run chains in parallel easily using:\n\nlibrary(rstan)\noptions(mc.cores = 8)"
  },
  {
    "objectID": "slides/04-stan.html#stan-summary",
    "href": "slides/04-stan.html#stan-summary",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Stan summary",
    "text": "Stan summary\n\nStan works by default with a HMC-like algorithm called NUTS.\nThe Stan language is similar in nature to other common languages with loops, conditional statements and user-definable functions (didn’t cover here).\nStan makes life easier for us than coding up the MCMC algorithms ourselves."
  },
  {
    "objectID": "slides/06-model-checking.html#review-of-last-lecture",
    "href": "slides/06-model-checking.html#review-of-last-lecture",
    "title": "Model checking",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Thursday, we learned about\n\nDifferent types of priors that can be specified\nPosterior summaries: point estimates, intervals, and probabilities\nPosterior predictive distributions\nWe learned about the generated quantities code chunk\n\nToday, we will dive into (1) methods for assessing MCMC convergence, and (2) model performance techniques."
  },
  {
    "objectID": "slides/06-model-checking.html#learning-objectives",
    "href": "slides/06-model-checking.html#learning-objectives",
    "title": "Model checking",
    "section": "Learning objectives",
    "text": "Learning objectives"
  },
  {
    "objectID": "slides/06-model-checking.html#how-to-debug-a-stan-model",
    "href": "slides/06-model-checking.html#how-to-debug-a-stan-model",
    "title": "Model checking",
    "section": "How to debug a Stan model?",
    "text": "How to debug a Stan model?\nTwo issue flavors, each with their own response.\n\nCoding errors.\nSampling issues."
  },
  {
    "objectID": "slides/06-model-checking.html#coding-errors",
    "href": "slides/06-model-checking.html#coding-errors",
    "title": "Model checking",
    "section": "Coding errors",
    "text": "Coding errors\nStan error messages are generally quite informative, however inevitably there are times when it is less clear why code fails.\n\nOne option is to debug by print.\n\n\nmodel {\n  ...\n  print(theta);\n}\n\nIn R this prints (neatly) to the console output.\nMore details on printing can be found on Stan: print statements"
  },
  {
    "objectID": "slides/06-model-checking.html#coding-errors-1",
    "href": "slides/06-model-checking.html#coding-errors-1",
    "title": "Model checking",
    "section": "Coding errors",
    "text": "Coding errors\nImportant: failing a resolution via the above go to http://mc-stan.org/ and do:\n\nLook through manual for a solution.\nLook through user forum for previous answers to similar problems.\nAsk a question; be clear, and thorough - post as simple a model that replicates the issue.\n\nOutside of this, you have an endless source of resources using Google/stackoverflow/stackexchange/ChatGPT. You can also post to Ed Discussion, go to office hours, and ask in lecture!"
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues",
    "href": "slides/06-model-checking.html#sampling-issues",
    "title": "Model checking",
    "section": "Sampling issues",
    "text": "Sampling issues\nDifferent sort of issue to a coding error, and falls into two (often related) issues:\n\nSlow convergence: still have \\(\\widehat{R} &gt; 1.1\\) after many thousands of iterations.\nDivergent iterations: get a warning in output from Stan with the number of iterations where the NUTS sampler has terminated prematurely.\n\n\nWarning messages:\n1: There were 62 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n2: There were 8 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttps://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded"
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-1",
    "href": "slides/06-model-checking.html#sampling-issues-1",
    "title": "Model checking",
    "section": "Sampling issues",
    "text": "Sampling issues\nGelman’s Folk Theorem:\n\n\nWhen you have computational problems, often there’s a problem with your model.\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-slow-convergence",
    "href": "slides/06-model-checking.html#sampling-issues-slow-convergence",
    "title": "Model checking",
    "section": "Sampling issues: slow convergence",
    "text": "Sampling issues: slow convergence\nPoor chain mixing is usually due to lack of parameter identification.\n\nA parameter is identified if it has some unique effect on the data generating process that can be separated from the effect of the other parameters.\n\nSolution: use simulated data where you know the true parameter values.\n\nInformative as to whether the model is sufficient to estimate a parameter’s value."
  },
  {
    "objectID": "slides/04-stan.html#shinystan",
    "href": "slides/04-stan.html#shinystan",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "shinystan",
    "text": "shinystan\n\nlibrary(shinystan)\nmy_sso &lt;- launch_shinystan(fit)\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/05-priors-ppds.html#review-of-last-lecture",
    "href": "slides/05-priors-ppds.html#review-of-last-lecture",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Tuesday, we learned about Stan\n\nA probabilistic programming language for Bayesian inference\nWe learned about the data, parameter, and model code chunks\nWe used Stan to fit a Bayesian linear regression\n\nToday, we will dive into priors, posterior summaries, and posterior predictive distributions (PPDs)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#learning-objectives",
    "href": "slides/05-priors-ppds.html#learning-objectives",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Learning objectives",
    "text": "Learning objectives"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-linear-regression",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-linear-regression",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: linear regression",
    "text": "Posterior predictive distribution: linear regression\n\ngenerated quantities {\n  vector[n] ppd; // store ppd samples\n  for (i in 1:n) {\n    ppd[i] = normal_rng(X[i, ] * beta, sigma);\n  }\n}\n\nThe function normal_rng generates a single independent sample from a normal distribution with parameters:\n\nmean = X[i, ] ∗ beta, where beta is a sample from the estimated posterior.\nsd = sigma, where sigma is a sample from the estimated posterior.\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-linear-regression-1",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-linear-regression-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: linear regression",
    "text": "Posterior predictive distribution: linear regression\n\ngenerated quantities {\n  vector[n] ppd; // store ppd samples\n  for (i in 1:n) {\n    ppd[i] = normal_rng(X[i, ] * beta, sigma);\n  }\n}\n\nThe function normal_rng generates a single independent sample from a normal distribution with parameters:\n\nmean = X[i, ] ∗ beta, where beta is a sample from the estimated posterior.\nsd = sigma, where sigma is a sample from the estimated posterior.\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-slow-convergence-1",
    "href": "slides/06-model-checking.html#sampling-issues-slow-convergence-1",
    "title": "Model checking",
    "section": "Sampling issues: slow convergence",
    "text": "Sampling issues: slow convergence\n\nIndicates that the stepwise integrator used to approximate Hamiltonian dynamics has likely diverged from exact trajectory.\nTherefore these samples cannot be viewed as being from the posterior.\nCauses a bias away from problem area of parameter space.\nAlmost always because the step size is too large relative to the curvature of the posterior.\nHowever can be due to placing limits on parameters that preclude an area of high probability mass.\n\nDiagnosing problem: use Shiny Stan (or otherwise) to make pairwise plots of variables =⇒ look for parameters with high bivariate correlation; indicates high curvature."
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-slow-convergence-2",
    "href": "slides/06-model-checking.html#sampling-issues-slow-convergence-2",
    "title": "Model checking",
    "section": "Sampling issues: slow convergence",
    "text": "Sampling issues: slow convergence\nSolution: If significant number of divergent iterations do the following:\n\nLower step size and increase acceptance rate in the call to Stan from R or otherwise.\nIf above doesn’t help change priors then likelihood.\n\nAgain failing all the above look at the Stan user forums, then ask a question."
  },
  {
    "objectID": "slides/06-model-checking.html#what-to-do-when-things-go-wrong-summary",
    "href": "slides/06-model-checking.html#what-to-do-when-things-go-wrong-summary",
    "title": "Model checking",
    "section": "What to do when things go wrong: summary",
    "text": "What to do when things go wrong: summary\n\nProblems with sampling are almost invariably problems with the underlying model not the sampling algorithm.\nUse fake data with all models to test for parameter identification (and that you’ve coded up correctly).\nTo debug a model that fails read error messages carefully, then try print statements.\nStan has an active developer and user forum, great documentation, and an extensive answer bank."
  },
  {
    "objectID": "slides/06-model-checking.html#shinystan",
    "href": "slides/06-model-checking.html#shinystan",
    "title": "Model checking",
    "section": "shinystan",
    "text": "shinystan\n\nlibrary(shinystan)\nmy_sso &lt;- launch_shinystan(fit)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-elicitation",
    "href": "slides/05-priors-ppds.html#prior-elicitation",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior elicitation",
    "text": "Prior elicitation\n\nSelecting the prior is one of the most important steps in a Bayesian analysis.\nThere is no “right” way to select a prior.\nThe choices often depend on the objective of the study and the nature of the data.\n\nConjugate versus non-conjugate\nInformative versus uninformative\nProper versus improper\nSubjective versus objective"
  },
  {
    "objectID": "slides/05-priors-ppds.html#conjugate-priors",
    "href": "slides/05-priors-ppds.html#conjugate-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Conjugate priors",
    "text": "Conjugate priors\n\nA prior is conjugate if the posterior is a member of the same parametric family.\nWe have seen that if the response is normal and we use a normal prior on the regression parameter, the posterior is also a normal (if we use an inverse gamma distribution for the variance, the posterior is also inverse gamma).\n\nThis requires a pairing of the likelihood and prior.\nThere is a long list of conjugate priors.\n\nThe advantage of a conjugate prior is that the posterior is available in closed form.\nNo longer critical with Stan!"
  },
  {
    "objectID": "slides/05-priors-ppds.html#informative-versus-uninformative-priors",
    "href": "slides/05-priors-ppds.html#informative-versus-uninformative-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Informative versus uninformative priors",
    "text": "Informative versus uninformative priors\n\nIn some cases informative priors are available.\nPotential sources include: literature reviews; pilot studies; expert opinions; etc.\nPrior elicitation is the process of converting expert information to prior distribution.\nFor example, the expert might not comprehend an inverse gamma pdf, but if they give you an estimate and a spread you can back out \\(a\\) and \\(b\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-1",
    "href": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Informative versus uninformative priors",
    "text": "Informative versus uninformative priors\n\nStrong priors for the parameters of interest can be hard to defend.\nStrong priors for nuisance parameters are more common.\nFor example, say you are doing a Bayesian t-test to study the mean \\(\\mu\\), you might use an informative prior for the nuisance parameter \\(\\sigma^2\\).\nAny time informative priors are used you should conduct a sensitivity analysis.\n\nThat is, compare the posterior for several priors."
  },
  {
    "objectID": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-2",
    "href": "slides/05-priors-ppds.html#informative-versus-uninformative-priors-2",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Informative versus uninformative priors",
    "text": "Informative versus uninformative priors\n\nIn most cases prior information is not available and so uninformative priors are used.\nOther names: vague, weak, flat, diffuse, etc.\n\nThese all refer to priors with large variance.\n\nExamples: \\(\\theta \\sim Uniform(0, 1)\\) or \\(\\mu ∼ N(0, 1000^2)\\)\nUninformative priors can be conjugate or not conjugate.\nThe idea is that the likelihood overwhelms the prior.\nYou should verify this with a sensitivity analysis."
  },
  {
    "objectID": "slides/05-priors-ppds.html#improper-priors",
    "href": "slides/05-priors-ppds.html#improper-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Improper priors",
    "text": "Improper priors\n\nExtreme case: \\(\\mu \\sim N(0, \\tau^2)\\) and we set \\(\\tau = \\infty\\).\nA “prior” that doesn’t integrate to one is called improper.\nExample: \\(f(\\mu) = 1\\) for all \\(\\mu \\in \\mathbb{R}\\).\nIt’s OK to use an improper prior so long as you verify that the posterior integrates to one.\nFor example, in linear regression an improper prior can be used for the slopes as long as the number of observations exceeds the number of covariates and there are no redundant predictors."
  },
  {
    "objectID": "slides/05-priors-ppds.html#subjective-versus-objective-priors",
    "href": "slides/05-priors-ppds.html#subjective-versus-objective-priors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Subjective versus objective priors",
    "text": "Subjective versus objective priors\n\nA subjective Bayesian picks a prior that corresponds to their current state of knowledge before collecting data.\nOf course, if the reader does not share this prior then they might not accept the analysis, and so sensitivity analysis is common.\nAn objective analysis is one that requires no subjective decisions by the analyst.\nSubjective decisions include picking the likelihood, treatment of outliers, transformations, … and prior specification.\nA completely objective analysis may be feasible in tightly controlled experiments, but is impossible in many analyses."
  },
  {
    "objectID": "slides/05-priors-ppds.html#objective-bayes",
    "href": "slides/05-priors-ppds.html#objective-bayes",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Objective Bayes",
    "text": "Objective Bayes\n\nAn objective Bayesian attempts to replace the subjective choice of prior with an algorithm that determines the prior.\nThere are many approaches: Jeffreys, reference, probability matching, maximum entropy, empirical Bayes, penalized complexity, etc.\nJeffreys priors are the most common: \\(f(\\boldsymbol{\\theta}) \\propto \\sqrt{\\det[I(\\boldsymbol{\\theta})]}\\), where \\(I(\\boldsymbol{\\theta})_{ij} = \\mathbb{E}\\left[\\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})}{\\partial \\theta_i}\\frac{f(\\mathbf{Y}|\\boldsymbol{\\theta})}{\\partial \\theta_j}\\right]\\) is the Fisher Information matrix.\nMany of these priors are improper and so you have to check that the posterior is proper."
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-predictive-checks",
    "href": "slides/05-priors-ppds.html#prior-predictive-checks",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\n\nUse the prior distribution to obtain samples of the data, \\(\\mathbf{Y}\\).\n\n\\[\\begin{aligned}\nf(\\mathbf{Y}) &= \\int f(\\mathbf{Y}, \\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\\\\\n&= \\int f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\n\\end{aligned}\\]\n\nThis is easy to sample from using the following steps, 1. \\(\\boldsymbol{\\theta}^{sim} \\sim f(\\boldsymbol{\\theta})\\) and 2. \\(\\mathbf{Y}^{sim} \\sim f(\\mathbf{Y} | \\boldsymbol{\\theta}^{sim})\\).\nSimilar to Gibbs sampling, this gives a sample from the joint \\((\\mathbf{Y}^{sim}, \\boldsymbol{\\theta}^{sim})\\) and also the marginal \\(\\mathbf{Y}^{sim}\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-point-estimates",
    "href": "slides/05-priors-ppds.html#posterior-summaries-point-estimates",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: point estimates",
    "text": "Posterior summaries: point estimates\n\nThe output of Bayesian inference is a probability distribution. It is often convenient to summarize the posterior in various ways.\nUsually summaries are computed for individual parameters using the marginal distributions,\n\n\\[f(\\theta_i|\\mathbf{Y}) = \\int f(\\boldsymbol{\\theta} | \\mathbf{Y}) d\\boldsymbol{\\theta}_{-i}.\\]\n\nThe posterior mean is defined by,\n\n\\[\\int_{-\\infty}^{\\infty}\\theta_i f(\\theta_i|\\mathbf{Y}) d\\theta_i.\\]"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-point-estimates-1",
    "href": "slides/05-priors-ppds.html#posterior-summaries-point-estimates-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: point estimates",
    "text": "Posterior summaries: point estimates\n\nThe posterior median m is defined by,\n\n\\[\\int_{-\\infty}^{m} f(\\theta_i|\\mathbf{Y}) d\\theta_i = 0.5 = \\int_m^{\\infty} f(\\theta_i|\\mathbf{Y}) d\\theta_i.\\]\n\nThe posterior mode is given by,\n\n\\[M=\\text{argmax} f(\\theta_i|\\mathbf{Y}) \\]\n\nThe mean and mode are also well defined for the joint posterior distribution."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-intervals",
    "href": "slides/05-priors-ppds.html#posterior-summaries-intervals",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: intervals",
    "text": "Posterior summaries: intervals\n\nThe Bayesian analogue of a frequentist confidence interval is a credible interval.\nAn interval \\((a,b)\\) is a \\(100(1-\\alpha)\\%\\) posterior credible interval for \\(\\theta_i\\) if\n\n\\[\\int_a^b f(\\theta_i|\\mathbf{Y}) d\\theta_i = (1-\\alpha),\\quad 0\\leq \\alpha \\leq 1.\\]\n\nA credible region can be defined similarly for a joint distribution."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-intervals-1",
    "href": "slides/05-priors-ppds.html#posterior-summaries-intervals-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: intervals",
    "text": "Posterior summaries: intervals\n\nCredible intervals are not unique. The two most common are symmetric and highest posterior density (HPD).\n\nSymmetric: An interval \\((a,b)\\) is a symmetric \\(100(1-\\alpha)\\%\\) credible interval if,\n\n\\[\\int_{-\\infty}^a f(\\theta_i|\\mathbf{Y}) d\\theta_i = \\frac{\\alpha}{2} = \\int_b^{\\infty} f(\\theta_i|\\mathbf{Y}) d\\theta_i.\\]\n\nHighest posterior density (HPD): An interval \\((a,b)\\) is a \\(100(1-\\alpha)\\%\\) HPD interval if,\n\n\\([a,b]\\) is a \\(100(1-\\alpha)\\%\\) credible interval for \\(\\theta_i\\)\nFor all \\(\\theta_i \\in [a,b]\\) and \\(\\theta_i^* \\notin [a,b]\\), \\(f(\\theta_i|\\mathbf{Y}) \\geq f(\\theta_i^*|\\mathbf{Y})\\)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval",
    "href": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: HPD credible interval",
    "text": "Posterior summaries: HPD credible interval"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval-1",
    "href": "slides/05-priors-ppds.html#posterior-summaries-hpd-credible-interval-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: HPD credible interval",
    "text": "Posterior summaries: HPD credible interval"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-probability",
    "href": "slides/05-priors-ppds.html#posterior-summaries-probability",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: probability",
    "text": "Posterior summaries: probability\n\nWe may be interested in a hypothesis test: \\(H_0: \\theta \\leq c\\) versus \\(H_1: \\theta \\geq c\\)\nWe can report the posterior probability of the null hypothesis\n\n\\(P(\\theta \\leq c | \\mathbf{Y}) = \\mathbb{E}[1(\\theta \\leq c) | \\mathbf{Y}]\\)\n\nInterpretation of the posteriour probability:\n\nProbability that the null is true \\(P(\\theta \\leq c | \\mathbf{Y})\\)\n\nInterpretation of the p-value:\n\nProbability of observing a test-statistic as or more extreme given that the null is true\nEvidence for or against the null (reject or fail to reject)"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-summaries-estimation",
    "href": "slides/05-priors-ppds.html#posterior-summaries-estimation",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior summaries: estimation",
    "text": "Posterior summaries: estimation\n\nWe have already seen that we can use MC or MCMC to estimate these posterior summaries!\nTo compute the HPD interval we can use the hdi function from the ggdist R package\n\n\nlibrary(ggdist)\ny &lt;- rgamma(10000, 3, 1)\nquantile(y, probs = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.6211652 7.2186263 \n\nggdist::hdi(y)\n\n         [,1]     [,2]\n[1,] 0.335666 6.421781"
  },
  {
    "objectID": "slides/05-priors-ppds.html#linear-regression-recall",
    "href": "slides/05-priors-ppds.html#linear-regression-recall",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Linear regression recall",
    "text": "Linear regression recall\n\nAssume we observe \\((Y_i,\\mathbf{x}_i)\\) for \\(i = 1,\\ldots,n\\), where \\[Y_i|\\boldsymbol{\\beta},\\sigma^2 \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2).\\]\nThe full data likelihood is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta},\\sigma^2),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\).\nWe have parameters \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\beta}^\\top,\\sigma^2)\\) and \\(f(\\boldsymbol{\\theta}) = f(\\boldsymbol{\\beta})f(\\sigma^2)\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#ppd-defintion",
    "href": "slides/05-priors-ppds.html#ppd-defintion",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "PPD defintion",
    "text": "PPD defintion\n\nAssume we observe a new \\(\\mathbf{x}^*\\) and we would like to make some type of prediction about \\(Y^*\\) given the data we have already observed, \\(\\mathbf{Y}\\).\nThe posterior predictive distribution is defined as \\(f(Y^*|\\mathbf{Y})\\) and can be written as, \\[\\begin{aligned}\nf(Y^* | \\mathbf{Y}) &= \\int f(Y^* , \\boldsymbol{\\theta} | \\mathbf{Y}) d\\boldsymbol{\\theta},\\quad\\text{(marginal)}\\\\\n&= \\int f(Y^* | \\boldsymbol{\\theta},\\mathbf{Y}) f(\\boldsymbol{\\theta} | \\mathbf{Y})d\\boldsymbol{\\theta},\\quad\\text{(conditional)}\\\\\n&= \\int \\underbrace{f(Y^* | \\boldsymbol{\\theta})}_{likelihood} \\underbrace{f(\\boldsymbol{\\theta} | \\mathbf{Y})}_{posterior}d\\boldsymbol{\\theta}.\\quad\\text{(independence)}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-priors-ppds.html#next",
    "href": "slides/05-priors-ppds.html#next",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Next",
    "text": "Next\n\nWe want to compute the posterior predictive distribution.\n\nPosterior predictive checks\nPrediction\n\nUse generated quantities block.\n\n\ngenerated quantities {\n  vector[n] ppd; // store ppd samples\n  for (i in 1:n) {\n    ppd[i] = normal_rng(X[i] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-definition",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-definition",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: definition",
    "text": "Posterior predictive distribution: definition\n\nAssume we observe a new \\(\\mathbf{x}'\\) and we would like to make some type of prediction about \\(Y'\\) given the data we have already observed, \\(\\mathbf{Y}\\).\nThe posterior predictive distribution is defined as \\(f(Y'|\\mathbf{Y})\\) and can be written as, \\[\\begin{aligned}\nf(Y' | \\mathbf{Y}) &= \\int f(Y' , \\boldsymbol{\\theta} | \\mathbf{Y}) d\\boldsymbol{\\theta},\\quad\\text{(marginal)}\\\\\n&= \\int f(Y' | \\boldsymbol{\\theta},\\mathbf{Y}) f(\\boldsymbol{\\theta} | \\mathbf{Y})d\\boldsymbol{\\theta},\\quad\\text{(conditional)}\\\\\n&= \\int \\underbrace{f(Y' | \\boldsymbol{\\theta})}_{likelihood} \\underbrace{f(\\boldsymbol{\\theta} | \\mathbf{Y})}_{posterior}d\\boldsymbol{\\theta}.\\quad\\text{(independence)}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-estimation",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-estimation",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: estimation",
    "text": "Posterior predictive distribution: estimation\n\nThe PPD can be written as an expectation,\n\n\\[f(Y' | \\mathbf{Y})  = \\int f(Y' | \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\mathbf{Y})d\\boldsymbol{\\theta} = \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[f(Y' | \\boldsymbol{\\theta})\\right].\\]\n\nThus, we can estimate the PPD using a Monte Carlo estimate,\n\n\\[\\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[f(Y' | \\boldsymbol{\\theta})\\right] \\approx \\frac{1}{S} \\sum_{s = 1}^S f\\left(Y' | \\boldsymbol{\\theta}^{(s)}\\right),\\]\nwhere \\(\\left\\{\\boldsymbol{\\theta}^{(1)},\\ldots,\\boldsymbol{\\theta}^{(S)}\\right\\}\\) are samples from the posterior."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: Stan",
    "text": "Posterior predictive distribution: Stan\n\nWe want to compute the posterior predictive distribution.\nUse generated quantities block.\n\n\ngenerated quantities {\n  vector[n] in_sample;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(X[i] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-1",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: Stan",
    "text": "Posterior predictive distribution: Stan\n\ngenerated quantities {\n  vector[n] in_sample;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(X[i] * beta, sigma);\n  }\n}\n\nThe function normal_rng generates a single independent sample from a normal distribution with parameters:\n\nmean = X[i, ] * beta, where beta is a sample from the estimated posterior.\nsd = sigma, where sigma is a sample from the estimated posterior.\n\nThis computes the posterior predictive distribution for the original data."
  },
  {
    "objectID": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-2",
    "href": "slides/05-priors-ppds.html#posterior-predictive-distribution-stan-2",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Posterior predictive distribution: Stan",
    "text": "Posterior predictive distribution: Stan\nThe following additions are added to the linear regression Stan code.\n\n// saved in linear_regression_ppd.stan\ndata {\n  ...\n  int&lt;lower = 1&gt; n_pred; // number of new observations\n  matrix[n_pred, p + 1] X_pred; // covariate matrix for new observations\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n_pred] out_sample;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(X[i, ] * beta, sigma);\n  }\n  for (i in 1:n_pred) {\n    out_sample[i] = normal_rng(X_pred[i, ] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/05-priors-ppds.html#lets-simulate-some-data-again",
    "href": "slides/05-priors-ppds.html#lets-simulate-some-data-again",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Let’s simulate some data again",
    "text": "Let’s simulate some data again\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\nn_pred &lt;- 10\n  \n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\nX_pred &lt;- cbind(1, matrix(rnorm(n_pred * p), ncol = p))\nY_pred &lt;- as.numeric(X_pred %*% beta + rnorm(n_pred, 0, sigma))"
  },
  {
    "objectID": "slides/05-priors-ppds.html#fit-linear-regression-using-stan",
    "href": "slides/05-priors-ppds.html#fit-linear-regression-using-stan",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Fit linear regression using Stan",
    "text": "Fit linear regression using Stan\n\n###Load packages\nlibrary(rstan)\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_fit.rds\")"
  },
  {
    "objectID": "slides/05-priors-ppds.html#examining-prediction-performance",
    "href": "slides/05-priors-ppds.html#examining-prediction-performance",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Examining prediction performance",
    "text": "Examining prediction performance\n\n###Inspect the ppd\nppds &lt;- extract(fit, pars = c(\"in_sample\", \"out_sample\"))\nlapply(ppds, dim)\n\n$in_sample\n[1] 2000  100\n\n$out_sample\n[1] 2000   10"
  },
  {
    "objectID": "slides/04-stan.html#prepare-for-next-class",
    "href": "slides/04-stan.html#prepare-for-next-class",
    "title": "Probabilistic Programming (Intro to Stan!)",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due January 30\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Priors, Posteriors, and PPDs!\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/05-priors-ppds.html#prepare-for-next-class",
    "href": "slides/05-priors-ppds.html#prepare-for-next-class",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due next Tuesday before class\nBe sure to turn in your AE by Sunday evening\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Model checking\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/06-model-checking.html#traceplots-1",
    "href": "slides/06-model-checking.html#traceplots-1",
    "title": "Model checking",
    "section": "Traceplots",
    "text": "Traceplots\n\nlibrary(rstan)\nfit &lt;- readRDS(file = \"robjects/linear_regression_ppd_fit.rds\")\nrstan::traceplot(fit, pars = c(\"beta\", \"sigma\"), \n                 inc_warmup = TRUE)"
  },
  {
    "objectID": "slides/06-model-checking.html#autocorrelation",
    "href": "slides/06-model-checking.html#autocorrelation",
    "title": "Model checking",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nIdeally the samples would be independent across iteration.\nWhen using MCMC we are obtaining dependent samples from the posterior.\nThe autocorrelation function \\(\\rho(h)\\) is the correlation between samples \\(h\\) iterations apart.\nLower values are better, but if the chains are long enough even large values can be OK.\nHighly correlated samples have less information than independent samples."
  },
  {
    "objectID": "slides/06-model-checking.html#effective-sample-size",
    "href": "slides/06-model-checking.html#effective-sample-size",
    "title": "Model checking",
    "section": "Effective sample size",
    "text": "Effective sample size\nThe effective samples size is,\n\\[n_{eff}=ESS(\\theta_i) = \\frac{mS}{1 + 2 \\sum_{h = 1}^{\\infty} \\rho (h)},\\] where \\(m\\) is the number of chains, \\(S\\) is the number of MCMC samples, and \\(\\rho(h)\\) is the \\(h\\)th order autocorrelation for \\(\\theta_i\\).\n\nThe correlated MCMC sample of length \\(mS\\) has the same information as \\(n_{eff}\\) independent samples.\nRule of thumb: \\(n_{eff}\\) should be at least a thousand for all parameters."
  },
  {
    "objectID": "slides/06-model-checking.html#effective-sample-size-1",
    "href": "slides/06-model-checking.html#effective-sample-size-1",
    "title": "Model checking",
    "section": "Effective sample size",
    "text": "Effective sample size\n\nAs the dependence \\(\\rho\\) increases, the incremental information conveyed by each sample decreases.\n\nwhere \\(m\\) is the number of chains, and \\(S\\) is the number of sampler per chain, and \\(\\rho_{\\tau}\\) is the \\(\\tau\\)th order autocorrelation for \\(\\theta_i\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#prepare-for-next-class",
    "href": "slides/06-model-checking.html#prepare-for-next-class",
    "title": "Model checking",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due before next class\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Bayesian Workflow\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-in-a-few-iterations",
    "href": "slides/06-model-checking.html#convergence-in-a-few-iterations",
    "title": "Model checking",
    "section": "Convergence in a few iterations",
    "text": "Convergence in a few iterations"
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-in-a-few-hundred-iterations",
    "href": "slides/06-model-checking.html#convergence-in-a-few-hundred-iterations",
    "title": "Model checking",
    "section": "Convergence in a few hundred iterations",
    "text": "Convergence in a few hundred iterations"
  },
  {
    "objectID": "slides/06-model-checking.html#this-one-never-converged",
    "href": "slides/06-model-checking.html#this-one-never-converged",
    "title": "Model checking",
    "section": "This one never converged",
    "text": "This one never converged"
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-is-questionable",
    "href": "slides/06-model-checking.html#convergence-is-questionable",
    "title": "Model checking",
    "section": "Convergence is questionable",
    "text": "Convergence is questionable"
  },
  {
    "objectID": "slides/06-model-checking.html#traceplots-for-linear-regression",
    "href": "slides/06-model-checking.html#traceplots-for-linear-regression",
    "title": "Model checking",
    "section": "Traceplots for linear regression",
    "text": "Traceplots for linear regression\n\nlibrary(rstan)\nfit &lt;- readRDS(file = \"linear_regression_ppd_fit.rds\")\nrstan::traceplot(fit, pars = c(\"beta\", \"sigma\"), \n                 inc_warmup = TRUE)"
  },
  {
    "objectID": "slides/06-model-checking.html#autocorrelation-for-linear-regression",
    "href": "slides/06-model-checking.html#autocorrelation-for-linear-regression",
    "title": "Model checking",
    "section": "Autocorrelation for linear regression",
    "text": "Autocorrelation for linear regression\n\nstan_ac(fit, pars = c(\"beta\", \"sigma\"), \n        separate_chains = TRUE, lags = 25)"
  },
  {
    "objectID": "slides/06-model-checking.html#autocorrelation-for-linear-regression-1",
    "href": "slides/06-model-checking.html#autocorrelation-for-linear-regression-1",
    "title": "Model checking",
    "section": "Autocorrelation for linear regression",
    "text": "Autocorrelation for linear regression\n\nlibrary(bayesplot)\nmcmc_acf(fit, regex_pars = c(\"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/06-model-checking.html#effective-sample-size-for-linear-regression",
    "href": "slides/06-model-checking.html#effective-sample-size-for-linear-regression",
    "title": "Model checking",
    "section": "Effective sample size for linear regression",
    "text": "Effective sample size for linear regression\n\nprint(fit, pars = c(\"beta\", \"sigma\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -1.48       0 0.16 -1.79 -1.58 -1.48 -1.37 -1.16  2002    1\nbeta[2]  3.30       0 0.15  3.01  3.19  3.29  3.40  3.59  1980    1\nsigma    1.55       0 0.11  1.36  1.47  1.54  1.62  1.78  1909    1\n\nSamples were drawn using NUTS(diag_e) at Fri Nov 22 10:44:51 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/06-model-checking.html#assessing-mixing-using-between--and-within-sequence-variances",
    "href": "slides/06-model-checking.html#assessing-mixing-using-between--and-within-sequence-variances",
    "title": "Model checking",
    "section": "Assessing mixing using between- and within-sequence variances",
    "text": "Assessing mixing using between- and within-sequence variances\nFor a scalar parameter, \\(\\theta\\), define the MCMC samples as \\(\\theta_{ij}\\) for chain \\(j=1,\\ldots,m\\) and simulations \\(i = 1,\\ldots,n\\). We can compute the between- and within-sequence variances:\n\\[\\begin{aligned}\nB &= \\frac{n}{m-1}\\sum_{j=1}^m \\left(\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot}\\right)^2,\\quad \\bar{\\theta}_{\\cdot j} = \\frac{1}{n}\\sum_{i=1}^n \\theta_{ij},\\quad\\bar{\\theta}_{\\cdot \\cdot} = \\frac{1}{m} \\sum_{j=1}^m \\bar{\\theta}_{\\cdot j}\\\\\nW &= \\frac{1}{m}\\sum_{j=1}^m s_j^2,\\quad s_j^2=\\frac{1}{n-1}\\sum_{i=1}^n \\left(\\theta_{ij} - \\bar{\\theta}_{\\cdot j}\\right)^2\n\\end{aligned}\\]\nThe between-sequence variance, \\(B\\) contains a factor of \\(n\\) because it is based on the variance of the within-sequence means, \\(\\bar{\\theta}_{\\cdot j}\\), each of which is an average of \\(n\\) values \\(\\theta_{ij}\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#widehatr",
    "href": "slides/06-model-checking.html#widehatr",
    "title": "Model checking",
    "section": "\\(\\widehat{R}\\)",
    "text": "\\(\\widehat{R}\\)\nEstimate a total variance \\(\\mathbb{V}(\\theta | \\mathbf{Y})\\) as a weighted mean of \\(W\\) and \\(B\\) \\[\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y}) = \\frac{n-1}{n}W + \\frac{1}{n}B\\] - This overestiamtes marginal posterior variance if starting points are overdispersed.\n\nGiven finite \\(n\\), \\(W\\) underestimates marginal posterior variance.\n\nSingle chains have not yet visited all points in the distribution.\nWhen \\(n \\rightarrow \\infty\\), \\(\\mathbb{E}[W] \\rightarrow \\mathbb{V}(\\theta | \\mathbf{Y})\\)\n\nAs \\(\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})\\) overestimates and \\(W\\) underestimates, we can compute\n\n\\[\\widehat{R} = \\sqrt{\\frac{\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})}{W}}.\\]"
  },
  {
    "objectID": "slides/06-model-checking.html#convergence-metric-widehatr",
    "href": "slides/06-model-checking.html#convergence-metric-widehatr",
    "title": "Model checking",
    "section": "Convergence metric: \\(\\widehat{R}\\)",
    "text": "Convergence metric: \\(\\widehat{R}\\)\nEstimate a total variance \\(\\mathbb{V}(\\theta | \\mathbf{Y})\\) as a weighted mean of \\(W\\), \\(B\\) \\[\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y}) = \\frac{n-1}{n}W + \\frac{1}{n}B\\]\n\nThis overestiamtes marginal posterior variance if starting points are overdispersed.\nGiven finite \\(n\\), \\(W\\) underestimates marginal posterior variance.\n\nSingle chains have not yet visited all points in the distribution.\nWhen \\(n \\rightarrow \\infty\\), \\(\\mathbb{E}[W] \\rightarrow \\mathbb{V}(\\theta | \\mathbf{Y})\\)\n\nAs \\(\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})\\) overestimates and \\(W\\) underestimates, we can compute\n\n\\[\\widehat{R} = \\sqrt{\\frac{\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})}{W}}, \\quad \\widehat{R} \\rightarrow 1 \\text{ as }n \\rightarrow \\infty.\\]"
  },
  {
    "objectID": "slides/06-model-checking.html#widehatr-for-linear-regression",
    "href": "slides/06-model-checking.html#widehatr-for-linear-regression",
    "title": "Model checking",
    "section": "\\(\\widehat{R}\\) for linear regression",
    "text": "\\(\\widehat{R}\\) for linear regression\n\nprint(fit, pars = c(\"beta\", \"sigma\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -1.48       0 0.16 -1.79 -1.58 -1.48 -1.37 -1.16  2002    1\nbeta[2]  3.30       0 0.15  3.01  3.19  3.29  3.40  3.59  1980    1\nsigma    1.55       0 0.11  1.36  1.47  1.54  1.62  1.78  1909    1\n\nSamples were drawn using NUTS(diag_e) at Fri Nov 22 10:44:51 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nA good rule of thumb is to want \\(\\widehat{R} \\leq 1.1\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-divergent-iterations",
    "href": "slides/06-model-checking.html#sampling-issues-divergent-iterations",
    "title": "Model checking",
    "section": "Sampling issues: divergent iterations",
    "text": "Sampling issues: divergent iterations\nYou may get a warning in output from Stan with the number of iterations where the NUTS sampler has terminated prematurely.\n\nWarning messages:\n1: There were 62 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n2: There were 8 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttps://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded \n\nSolution: If this warning appears, you can try:\n\nIncrease adapt_delta (double, between 0 and 1, defaults to 0.8).\nIncrease max_treedepth (integer, positive, defaults to 10).\nPlay with stepsize (double, positive, defaults to 1)."
  },
  {
    "objectID": "slides/06-model-checking.html#sampling-issues-divergent-iterations-1",
    "href": "slides/06-model-checking.html#sampling-issues-divergent-iterations-1",
    "title": "Model checking",
    "section": "Sampling issues: divergent iterations",
    "text": "Sampling issues: divergent iterations\n\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000,\n                control = list(adapt_delta = 0.8, \n                               max_treedepth = 10,\n                               stepsize = 1))\n\n\nSee help(stan) for a full list of options that can be set in control.\nComplete set of recommendations: runtime warnings and convergence problems\n\nIf the above doesn’t help change priors then likelihood!"
  },
  {
    "objectID": "slides/06-model-checking.html#what-to-do-if-your-mcmc-does-not-converge",
    "href": "slides/06-model-checking.html#what-to-do-if-your-mcmc-does-not-converge",
    "title": "Model checking",
    "section": "What to do if your MCMC does not converge?",
    "text": "What to do if your MCMC does not converge?\nGelman’s Folk Theorem:\n\n\nWhen you have computational problems, often there’s a problem with your model.\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/06-model-checking.html#debugging-with-shinystan",
    "href": "slides/06-model-checking.html#debugging-with-shinystan",
    "title": "Model checking",
    "section": "Debugging with shinystan",
    "text": "Debugging with shinystan\n\nlibrary(shinystan)\nmy_sso &lt;- launch_shinystan(fit)"
  },
  {
    "objectID": "slides/06-model-checking.html#standard-errors-of-posterior-mean-estimates",
    "href": "slides/06-model-checking.html#standard-errors-of-posterior-mean-estimates",
    "title": "Model checking",
    "section": "Standard errors of posterior mean estimates",
    "text": "Standard errors of posterior mean estimates\n\nThe sample mean from our MCMC draws is a point estimate for the posterior mean.\nThe standard error (SE) of this estimate can be used as a diagnostic.\nAssuming independence, the Monte Carlo standard error is \\(\\text{MCSE} = \\frac{s}{\\sqrt{S}},\\) where \\(s\\) is the sample standard deviation and \\(S\\) is the number of samples.\nA more realistic standard error is \\(\\text{MCSE} = \\frac{s}{\\sqrt{n_{eff}}}.\\)\n\\(\\text{MCSE} \\rightarrow 0\\) as \\(S \\rightarrow \\infty\\), whereas the standard deviation of the posterior draws approaches the standard deviation of the posterior distribution."
  },
  {
    "objectID": "slides/06-model-checking.html#posterior-predictive-checks",
    "href": "slides/06-model-checking.html#posterior-predictive-checks",
    "title": "Model checking",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\n\nLast lecture we learned about posterior predictive distributions.\nThese can be used to check the model fit in our observed data.\nThe goal is to check how well our model can generate data that matches the observed data.\nIf our model is “good”, it should be able to generate new observations that resemble the observed data.\nTo perform the posterior predictive check, we must include the generated quantities code chunk."
  },
  {
    "objectID": "slides/06-model-checking.html#compute-posterior-predictive-checks",
    "href": "slides/06-model-checking.html#compute-posterior-predictive-checks",
    "title": "Model checking",
    "section": "Compute posterior predictive checks",
    "text": "Compute posterior predictive checks\nlibrary(rstan)\nlibrary(bayesplot)\nfit &lt;- readRDS(file = \"robjects/linear_regression_ppd_fit.rds\")\nY_in_sample &lt;- extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(Y, Y_in_sample[1:100, ])"
  },
  {
    "objectID": "slides/06-model-checking.html#comparing-the-ppd-to-the-observed-data-distribution",
    "href": "slides/06-model-checking.html#comparing-the-ppd-to-the-observed-data-distribution",
    "title": "Model checking",
    "section": "Comparing the PPD to the observed data distribution",
    "text": "Comparing the PPD to the observed data distribution\nlibrary(rstan)\nlibrary(bayesplot)\nY_in_sample &lt;- extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(Y, Y_in_sample)"
  },
  {
    "objectID": "slides/06-model-checking.html#ppd-test-statistics",
    "href": "slides/06-model-checking.html#ppd-test-statistics",
    "title": "Model checking",
    "section": "PPD test statistics",
    "text": "PPD test statistics\n\nThe procedure for carrying out a posterior predictive check requires a test quantity, \\(T(\\mathbf{Y})\\), for our observed data.\nSuppose that we have samples from the posterior predictive distribution, \\(\\mathbf{Y}^{(s)} = \\left\\{Y_1^{(s)},\\ldots,Y_n^{(s)}\\right\\}\\) for \\(s = 1,\\ldots,S\\).\nWe can compute: \\(T\\left(\\mathbf{Y}^{(s)}\\right) = \\left\\{T\\left(Y_1^{(s)}\\right),\\ldots,\\left(Y_n^{(s)}\\right)\\right\\}\\)\nOur posterior predictive check will then compare the distribution of \\(T\\left(\\mathbf{Y}^{(s)}\\right)\\) to the value from our observed data, \\(T(\\mathbf{Y})\\).\n\\(T(\\cdot)\\) can be any statistics, including mean, median, etc.\nWhen the predictive distribution is not consistent with the observed statistics it indicates poor model fit."
  },
  {
    "objectID": "slides/06-model-checking.html#ploting",
    "href": "slides/06-model-checking.html#ploting",
    "title": "Model checking",
    "section": "Ploting",
    "text": "Ploting\nppc_stat(Y, Y_in_sample, stat = \"mean\")\nppc_stat(Y, Y_in_sample, stat = \"sd\")\nq25 &lt;- function(y) quantile(y, 0.25)\nq75 &lt;- function(y) quantile(y, 0.75)\nppc_stat(Y, Y_in_sample, stat = \"q25\")\nppc_stat(Y, Y_in_sample, stat = \"q75\")"
  },
  {
    "objectID": "slides/06-model-checking.html#visually-assessing-posterior-predictive-check",
    "href": "slides/06-model-checking.html#visually-assessing-posterior-predictive-check",
    "title": "Model checking",
    "section": "Visually assessing posterior predictive check",
    "text": "Visually assessing posterior predictive check\nbayesplot::ppc_stat(Y, Y_in_sample, stat = \"mean\")\nppc_stat(Y, Y_in_sample, stat = \"sd\")\nq25 &lt;- function(y) quantile(y, 0.25)\nq75 &lt;- function(y) quantile(y, 0.75)\nppc_stat(Y, Y_in_sample, stat = \"q25\")\nppc_stat(Y, Y_in_sample, stat = \"q75\")"
  },
  {
    "objectID": "slides/06-model-checking.html#visualizing-posterior-predictive-check",
    "href": "slides/06-model-checking.html#visualizing-posterior-predictive-check",
    "title": "Model checking",
    "section": "Visualizing posterior predictive check",
    "text": "Visualizing posterior predictive check\nppc_stat(Y, Y_in_sample, stat = \"mean\") # from bayesplot\nppc_stat(Y, Y_in_sample, stat = \"sd\")\nq25 &lt;- function(y) quantile(y, 0.25)\nq75 &lt;- function(y) quantile(y, 0.75)\nppc_stat(Y, Y_in_sample, stat = \"q25\")\nppc_stat(Y, Y_in_sample, stat = \"q75\")"
  },
  {
    "objectID": "slides/06-model-checking.html#posterior-predictive-p-values",
    "href": "slides/06-model-checking.html#posterior-predictive-p-values",
    "title": "Model checking",
    "section": "Posterior predictive p-values",
    "text": "Posterior predictive p-values\nplot &lt;- ppc_stat(Y, Y_in_sample, stat = \"median\") # from bayesplot\npvalue &lt;- mean(apply(Y_in_sample, 1, median) &gt; median(Y))\nplot + yaxis_text() + # just so I can see y-axis values for specifying them in annotate() below, but can remove this if you don't want the useless y-axis values displayed \n  annotate(\"text\", x = -0.75, y = 150, label = paste(\"p =\", pvalue))\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian p-value: \\(p_B = P\\left(T\\left(\\mathbf{Y}^{(s)}\\right) \\geq T\\left(\\mathbf{Y}\\right) | \\mathbf{Y}\\right)\\)"
  },
  {
    "objectID": "slides/06-model-checking.html#watanabe-akaike-information-criteria-waic",
    "href": "slides/06-model-checking.html#watanabe-akaike-information-criteria-waic",
    "title": "Model checking",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\\[\\text{WAIC}(\\mathbf{Y},\\boldsymbol{\\theta}) = -2\\left(\\text{lppd} - p_{\\text{WAIC}}\\right),\\] where \\(p_{\\text{WAIC}} = \\sum_{i = 1}^n \\mathbb{V}_{\\boldsymbol{\\theta}}\\left(\\log f\\left(Y_i | \\boldsymbol{\\theta}\\right)\\right)\\) and log pointwise predictive density (lppd) is, \\[\\text{lppd}(\\mathbf{Y},\\boldsymbol{\\theta}) = \\sum_{i = 1}^n \\log \\frac{1}{S} \\sum_{s = 1}^S f\\left({Y}_i | \\boldsymbol{\\theta}^{(s)}\\right).\\]"
  },
  {
    "objectID": "slides/06-model-checking.html#computing-waic-using-stan",
    "href": "slides/06-model-checking.html#computing-waic-using-stan",
    "title": "Model checking",
    "section": "Computing WAIC using Stan",
    "text": "Computing WAIC using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/06-model-checking.html#lets-refit-the-stan-model",
    "href": "slides/06-model-checking.html#lets-refit-the-stan-model",
    "title": "Model checking",
    "section": "Let’s refit the Stan model",
    "text": "Let’s refit the Stan model\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_log_lik_fit.rds\")"
  },
  {
    "objectID": "slides/06-model-checking.html#we-can-now-compute-the-waic",
    "href": "slides/06-model-checking.html#we-can-now-compute-the-waic",
    "title": "Model checking",
    "section": "We can now compute the WAIC",
    "text": "We can now compute the WAIC\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", \n                           merge_chains = TRUE)\nwaic_fit &lt;- loo::waic(log_lik)\nprint(waic_fit)\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -188.4  7.4\np_waic         3.2  0.6\nwaic         376.9 14.7"
  },
  {
    "objectID": "slides/06-model-checking.html#log-pointwise-predictive-density",
    "href": "slides/06-model-checking.html#log-pointwise-predictive-density",
    "title": "Model checking",
    "section": "Log pointwise predictive density",
    "text": "Log pointwise predictive density\nThe log pointwise predictive density (lppd) is defined as:\n\\[\\text{lppd}(\\mathbf{Y},\\boldsymbol{\\theta}) = \\sum_{i = 1}^n \\log \\frac{1}{S} \\sum_{s = 1}^S f\\left({Y}_i | \\boldsymbol{\\theta}^{(s)}\\right).\\] ## Watanabe-Akaike information criteria (WAIC)\n\\[\\text{WAIC}(\\mathbf{Y},\\boldsymbol{\\theta}) = -2\\left(\\text{lppd} - p_{\\text{WAIC}}\\right),\\] where \\(p_{\\text{WAIC}} = \\sum_{i = 1}^n \\mathbb{V}_{\\boldsymbol{\\theta}}\\left(\\log f\\left(Y_i | \\boldsymbol{\\theta}\\right)\\right)\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#cross-validation",
    "href": "slides/06-model-checking.html#cross-validation",
    "title": "Model checking",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nCross-validation is a strategy for estimating a model’s predictive accuracy on another sample.\nThe sample data is divided into a number of chunks, called “folds” and the model is asked to predict each fold, after training on all the others.\nThe number of folds is given by \\(k\\)."
  },
  {
    "objectID": "slides/06-model-checking.html#leave-one-out-cross-validation",
    "href": "slides/06-model-checking.html#leave-one-out-cross-validation",
    "title": "Model checking",
    "section": "Leave-one-out cross-validation",
    "text": "Leave-one-out cross-validation\n\nLeave-one-out cross-validation (LOO-CV) represents cross-validation at the extreme, when \\(k = n\\).\nIt is computationally costly to compute LOO-CV, since it requires fitting the model \\(n\\) times.\nLuckily, there exists an approximation to LOO-CV using Pareto smoothed importance-sampling by Vehtari, Gelman, and Gabry (2017).\nThis can be computed easily using R."
  },
  {
    "objectID": "slides/06-model-checking.html#information-criteria",
    "href": "slides/06-model-checking.html#information-criteria",
    "title": "Model checking",
    "section": "Information criteria",
    "text": "Information criteria\n\nIt is sometimes useful to consider predictive accuracy given a point estimate \\(\\hat{\\theta}(\\mathbf{Y})\\), thus,\n\n\\[\\text{expected log predictive density, given }\\hat{\\theta}(\\mathbf{Y}): \\mathbb{E}[]\\]\n\nThe frequentist Akaike information criterion (AIC) is the oldest and most restrictive,\n\n\\[AIC = -2 \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{MLE}) + 2d,\\] where \\(d\\) is the number of parameters in the model.\n\nAmong Bayesians, the deviance information criterion (DIC) has been widely used for some time, now. However, the DIC is limited in that it presumes the posterior is multivariate Gaussian, which is not always the case.\nThe widely applicable information criterion (WAIC) does not impose assumptions on the shape of the posterior distribution.\n\nNote: WAIC provides an estimate of out-of-sample deviance and converges with LOO-CV as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/05-priors-ppds.html#hyperpriors",
    "href": "slides/05-priors-ppds.html#hyperpriors",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Hyperpriors",
    "text": "Hyperpriors\nPrior is the name for the distribution of model parameters that show up in the likelihood.\n\nFor example: In linear regression, \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are parameters in the likelihood, thus \\(f(\\boldsymbol{\\beta})\\) and \\(f(\\sigma^2)\\) are called priors.\n\nHyperprior is the name for the distribution of model parameters not in the likelihood.\nHyperparameter is the name for the parameters in a hyperprior."
  },
  {
    "objectID": "slides/05-priors-ppds.html#hyperpriors-1",
    "href": "slides/05-priors-ppds.html#hyperpriors-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Hyperpriors",
    "text": "Hyperpriors\n\nFor example: Suppose in linear regression, we place the following prior for \\(\\boldsymbol{\\beta}\\), \\(f(\\boldsymbol{\\beta} | \\boldsymbol{\\beta}_0, \\sigma_{\\beta}^2) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I})\\) and\n\n\\(f(\\boldsymbol{\\beta}_0) = N(\\mathbf{0}, \\mathbf{I})\\)\n\\(f(\\sigma_{\\beta}^2) = IG(a_{\\beta}, b_{\\beta})\\)\n\n\\(f(\\boldsymbol{\\beta}_0)\\) and \\(f(\\sigma_{\\beta}^2)\\) are hyperpriors.\n\\(a_{\\beta}, b_{\\beta}\\) are hyperparameters.\n\n\nQuestion: Why would a researcher be interested in a hyperprior?"
  },
  {
    "objectID": "slides/07-workflow.html#review-of-last-lecture",
    "href": "slides/07-workflow.html#review-of-last-lecture",
    "title": "Bayesian Workflow",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Tuesday, we learned about various ways to check MCMC convergence and model fit.\n\nTraceplots, effective sample size (\\(n_{eff}\\)), MC standard error, \\(\\hat{R}\\), sampling issues\nPosterior predictive checks\nModel checks using shinystan\n\nToday, we will put these concepts within the larger framework of the Bayesian workflow."
  },
  {
    "objectID": "slides/07-workflow.html#cross-validation",
    "href": "slides/07-workflow.html#cross-validation",
    "title": "Bayesian Workflow",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nCross-validation is a strategy for estimating a model’s predictive accuracy on another sample.\nCross-validation methods capture out-of-sample prediction error by fitting the model to training data and evaluating this predictive accuracy on a holdout set.\nThey can be computationally expensive but avoid the problem of overfitting.\nThe sample data is divided into a number of chunks, called “folds” and the model is asked to predict each fold, after training on all the others. The number of folds is given by \\(k\\).\nLeave-one-out cross-validation (LOO-CV) represents cross-validation at the extreme, when \\(k = n\\)."
  },
  {
    "objectID": "slides/07-workflow.html#leave-one-out-cross-validation",
    "href": "slides/07-workflow.html#leave-one-out-cross-validation",
    "title": "Bayesian Workflow",
    "section": "Leave-one-out cross-validation",
    "text": "Leave-one-out cross-validation\n\nIt is computationally costly to compute LOO-CV, since it requires fitting the model \\(n\\) times.\nLuckily, there exists an approximation to LOO-CV using Pareto smoothed importance-sampling by Vehtari, Gelman, and Gabry (2017).\nThis can be computed easily using R."
  },
  {
    "objectID": "slides/07-workflow.html#information-criteria",
    "href": "slides/07-workflow.html#information-criteria",
    "title": "Bayesian Workflow",
    "section": "Information criteria",
    "text": "Information criteria\n\nSeveral information criteria have been proposed that do not require fitting the model several times.\nMany are functions of the deviance, i.e., twice the negative log likelihood, \\(D(\\mathbf{Y}|\\boldsymbol{\\theta}) = −2 \\log f(\\mathbf{Y}|\\boldsymbol{\\theta}).\\)\nIdeally, models will have small deviance.\nHowever, if a model is too complex it will have small deviance but be unstable (over-fitting)\nThe Akaike information criteria has a complexity penalty \\(AIC = D(\\mathbf{Y}|\\hat{\\boldsymbol{\\theta}})+ 2p\\), where \\(\\hat{\\boldsymbol{\\theta}}\\) is the MLE.\nModel with smaller AIC are preferred."
  },
  {
    "objectID": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic",
    "href": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic",
    "title": "Bayesian Workflow",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC is an alternative to DIC\nIt is motivated as an approximation to leave-one-out CV\nIn the end WAIC has model-fit and model-complexity components\nIt is used the same as DIC with smaller WAIC preferred\nIn practice the two often give similar results, but WAIC is arguably more theoretically justified"
  },
  {
    "objectID": "slides/07-workflow.html#computing-waic-using-stan",
    "href": "slides/07-workflow.html#computing-waic-using-stan",
    "title": "Bayesian Workflow",
    "section": "Computing WAIC using Stan",
    "text": "Computing WAIC using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/07-workflow.html#lets-refit-the-stan-model",
    "href": "slides/07-workflow.html#lets-refit-the-stan-model",
    "title": "Bayesian Workflow",
    "section": "Let’s refit the Stan model",
    "text": "Let’s refit the Stan model\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_log_lik_fit.rds\")"
  },
  {
    "objectID": "slides/07-workflow.html#we-can-now-compute-the-waic",
    "href": "slides/07-workflow.html#we-can-now-compute-the-waic",
    "title": "Bayesian Workflow",
    "section": "We can now compute the WAIC",
    "text": "We can now compute the WAIC\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", \n                           merge_chains = TRUE)\nwaic_fit &lt;- loo::waic(log_lik)\nprint(waic_fit)\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -188.4  7.4\np_waic         3.2  0.6\nwaic         376.9 14.7"
  },
  {
    "objectID": "slides/07-workflow.html#debugging-with-shinystan",
    "href": "slides/07-workflow.html#debugging-with-shinystan",
    "title": "Bayesian Workflow",
    "section": "Debugging with shinystan",
    "text": "Debugging with shinystan\n\nlibrary(shinystan)\nmy_sso &lt;- launch_shinystan(fit)"
  },
  {
    "objectID": "slides/07-workflow.html#prepare-for-next-class",
    "href": "slides/07-workflow.html#prepare-for-next-class",
    "title": "Bayesian Workflow",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 01 which is due before next class\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Bayesian Workflow\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/06-model-checking.html#posterior-predictive-checks-1",
    "href": "slides/06-model-checking.html#posterior-predictive-checks-1",
    "title": "Model checking",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\n\nLast lecture we learned about posterior predictive distributions.\nThese can be used to check the model fit in our observed data.\nThe goal is to check how well our model can generate data that matches the observed data.\nIf our model is “good”, it should be able to generate new observations that resemble the observed data.\nTo perform the posterior predictive check, we must include the generated quantities code chunk."
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-predictive-checks-1",
    "href": "slides/05-priors-ppds.html#prior-predictive-checks-1",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\n\nSuppose that I am interested in modeling the number of patients who are in the waiting room during an hour in the Duke ED, \\(Y_i\\).\nWe can model this random variable as \\(Y_i \\stackrel{iid}{\\sim}Poisson(\\lambda)\\) for \\(i = 1,\\ldots,n\\). A conjugate prior for \\(\\lambda\\) is \\(\\lambda \\sim Gamma(a,b)\\).\nWe know from experience that on average there are 20 patients waiting during an hour, so we want \\(\\mathbb{E}[Y_i]=\\lambda = 20\\).\nThus, we place a prior on \\(\\lambda\\) that is centered at 20, which requires that \\(a/b=20\\).\nThere are infinitely many priors specifications."
  },
  {
    "objectID": "slides/05-priors-ppds.html#visualizing-prior-predictive-checks",
    "href": "slides/05-priors-ppds.html#visualizing-prior-predictive-checks",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Visualizing prior predictive checks",
    "text": "Visualizing prior predictive checks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the samples of \\(Y_i\\) under different prior specifications to compute a summary statstic, for example for the maximum value."
  },
  {
    "objectID": "slides/05-priors-ppds.html#prior-predictive-checks-2",
    "href": "slides/05-priors-ppds.html#prior-predictive-checks-2",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\n\nPrior predictive checks can be used to determine a realistic specification for \\(a\\) and \\(b\\).\n\n\ndata {\n  int&lt;lower = 1&gt; n;\n  real&lt;lower = 0&gt; a;\n  real&lt;lower = 0&gt; b;\n}\ngenerated quantities {\n  real lambda = gamma_rng(a, b);\n  vector[n] y_sim;\n  for (i in 1:n) y_sim[i] = poisson_rng(lambda);\n}\n\n\nWhen running Stan for prior predictive checks you must specify algorithm = \"Fixed_param\""
  },
  {
    "objectID": "slides/07-workflow.html#bayes-theorem",
    "href": "slides/07-workflow.html#bayes-theorem",
    "title": "Bayesian Workflow",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{f(\\mathbf{Y})}\\]\n\n\nRethinking Bayes theorem:\n\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto f(\\mathbf{Y}, \\boldsymbol{\\theta}) = f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) \\]\n\n\n\nIn Stan:\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) + \\log f(\\boldsymbol{\\theta})\\]"
  },
  {
    "objectID": "slides/07-workflow.html#rethinking-bayes-theorem",
    "href": "slides/07-workflow.html#rethinking-bayes-theorem",
    "title": "Bayesian Workflow",
    "section": "Rethinking Bayes theorem",
    "text": "Rethinking Bayes theorem\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto f(\\mathbf{Y}, \\boldsymbol{\\theta}) = f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) \\]"
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-statistics",
    "href": "slides/07-workflow.html#bayesian-statistics",
    "title": "Bayesian Workflow",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\nAdvantages:\n\nNatural approach to expressing uncertainty\nAbility to incorporate prior information\nIncreased modeling flexibility\nFull posterior distribution of parameters\nNatural propagation of uncertainty\n\nDisadvantages:\n\nSlow speed of model estimation"
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-workflow",
    "href": "slides/07-workflow.html#bayesian-workflow",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\n\n\n\n\nGelman A., Vehtari A., Simpson D., Margossian, C., Carpenter, B. and Yao, Y., Kennedy, L., Gabry, J., Bürkner P. C., & Modrák M. (2020). Bayesian Workflow."
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-workflow-1",
    "href": "slides/07-workflow.html#bayesian-workflow-1",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\n\n\nTaken from Bayesian workflow by Francesca Capel\n\n\n\nToday we will talk about a general strategy for taking a question and data to a robust conclusion."
  },
  {
    "objectID": "slides/07-workflow.html#a-simplified-workflow",
    "href": "slides/07-workflow.html#a-simplified-workflow",
    "title": "Bayesian Workflow",
    "section": "A simplified workflow",
    "text": "A simplified workflow\n\nSetting up a full probability model: a joint probability distribution for all observable and unobservable quantities in a problem. The model should be consistent with knowledge about the underlying scientific problem and the data collection process.\nConditioning on observed data: calculating and interpreting the appropriate posterior distribution — the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data.\nEvaluating the fit of the model and the implications of the resulting posterior distribution: how well does the model fit the data, are the substantive conclusions reasonable, and how sensitive are the results to the modeling assumptions in step 1? In response, one can alter or expand the model and repeat the three steps.\n\n\nFrom BDA3."
  },
  {
    "objectID": "slides/07-workflow.html#motivating-example",
    "href": "slides/07-workflow.html#motivating-example",
    "title": "Bayesian Workflow",
    "section": "Motivating example",
    "text": "Motivating example\nWe will use the bdims dataset from the openintro package. \nResearch question: How much people’s weight tends to increase when height increases, and how certain we can be about the magnitude of the increase. In particular, we might be interested in predicting a person’s weight based on their height.\n\nlibrary(openintro)\nglimpse(bdims)\n\nRows: 507\nColumns: 25\n$ bia_di &lt;dbl&gt; 42.9, 43.7, 40.1, 44.3, 42.5, 43.3, 43.5, 44.4, 43.5, 42.0, 40.…\n$ bii_di &lt;dbl&gt; 26.0, 28.5, 28.2, 29.9, 29.9, 27.0, 30.0, 29.8, 26.5, 28.0, 29.…\n$ bit_di &lt;dbl&gt; 31.5, 33.5, 33.3, 34.0, 34.0, 31.5, 34.0, 33.2, 32.1, 34.0, 33.…\n$ che_de &lt;dbl&gt; 17.7, 16.9, 20.9, 18.4, 21.5, 19.6, 21.9, 21.8, 15.5, 22.5, 20.…\n$ che_di &lt;dbl&gt; 28.0, 30.8, 31.7, 28.2, 29.4, 31.3, 31.7, 28.8, 27.5, 28.0, 30.…\n$ elb_di &lt;dbl&gt; 13.1, 14.0, 13.9, 13.9, 15.2, 14.0, 16.1, 15.1, 14.1, 15.6, 13.…\n$ wri_di &lt;dbl&gt; 10.4, 11.8, 10.9, 11.2, 11.6, 11.5, 12.5, 11.9, 11.2, 12.0, 10.…\n$ kne_di &lt;dbl&gt; 18.8, 20.6, 19.7, 20.9, 20.7, 18.8, 20.8, 21.0, 18.9, 21.1, 19.…\n$ ank_di &lt;dbl&gt; 14.1, 15.1, 14.1, 15.0, 14.9, 13.9, 15.6, 14.6, 13.2, 15.0, 14.…\n$ sho_gi &lt;dbl&gt; 106.2, 110.5, 115.1, 104.5, 107.5, 119.8, 123.5, 120.4, 111.0, …\n$ che_gi &lt;dbl&gt; 89.5, 97.0, 97.5, 97.0, 97.5, 99.9, 106.9, 102.5, 91.0, 93.5, 9…\n$ wai_gi &lt;dbl&gt; 71.5, 79.0, 83.2, 77.8, 80.0, 82.5, 82.0, 76.8, 68.5, 77.5, 81.…\n$ nav_gi &lt;dbl&gt; 74.5, 86.5, 82.9, 78.8, 82.5, 80.1, 84.0, 80.5, 69.0, 81.5, 81.…\n$ hip_gi &lt;dbl&gt; 93.5, 94.8, 95.0, 94.0, 98.5, 95.3, 101.0, 98.0, 89.5, 99.8, 98…\n$ thi_gi &lt;dbl&gt; 51.5, 51.5, 57.3, 53.0, 55.4, 57.5, 60.9, 56.0, 50.0, 59.8, 60.…\n$ bic_gi &lt;dbl&gt; 32.5, 34.4, 33.4, 31.0, 32.0, 33.0, 42.4, 34.1, 33.0, 36.5, 34.…\n$ for_gi &lt;dbl&gt; 26.0, 28.0, 28.8, 26.2, 28.4, 28.0, 32.3, 28.0, 26.0, 29.2, 27.…\n$ kne_gi &lt;dbl&gt; 34.5, 36.5, 37.0, 37.0, 37.7, 36.6, 40.1, 39.2, 35.5, 38.3, 38.…\n$ cal_gi &lt;dbl&gt; 36.5, 37.5, 37.3, 34.8, 38.6, 36.1, 40.3, 36.7, 35.0, 38.6, 40.…\n$ ank_gi &lt;dbl&gt; 23.5, 24.5, 21.9, 23.0, 24.4, 23.5, 23.6, 22.5, 22.0, 22.2, 23.…\n$ wri_gi &lt;dbl&gt; 16.5, 17.0, 16.9, 16.6, 18.0, 16.9, 18.8, 18.0, 16.5, 16.9, 16.…\n$ age    &lt;int&gt; 21, 23, 28, 23, 22, 21, 26, 27, 23, 21, 23, 22, 20, 26, 23, 22,…\n$ wgt    &lt;dbl&gt; 65.6, 71.8, 80.7, 72.6, 78.8, 74.8, 86.4, 78.4, 62.0, 81.6, 76.…\n$ hgt    &lt;dbl&gt; 174.0, 175.3, 193.5, 186.5, 187.2, 181.5, 184.0, 184.5, 175.0, …\n$ sex    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …"
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-workflow-2",
    "href": "slides/07-workflow.html#bayesian-workflow-2",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\nResearch question: What are your dependent and indepednent variables? What associations are you interested in? EDA.\n\n\n\nSpecify likelihood & priors: Use knowledge of the problem to construct a generative model.\n\n\n\n\nCheck the model with simulated data: Generate data from the model and evaluate fit as a sanity check (prior predictive checks).\n\n\n\n\nFit the model to real data: Estimate parameters in the model using MCMC."
  },
  {
    "objectID": "slides/07-workflow.html#motivating-example-predicting-weight",
    "href": "slides/07-workflow.html#motivating-example-predicting-weight",
    "title": "Bayesian Workflow",
    "section": "Motivating example: predicting weight",
    "text": "Motivating example: predicting weight\nResearch question: We would like to understand the relationship between a person’s height and weight. A few particular questions we have are:\n\nHow much does a person’s weight increase when their height increases?\nHow certain we can be about the magnitude of the increase?\nCan we predict a person’s weight based on their height?\n\nData: We will use the bdims dataset from the openintro package. This dataset contains body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender."
  },
  {
    "objectID": "slides/07-workflow.html#asdf",
    "href": "slides/07-workflow.html#asdf",
    "title": "Bayesian Workflow",
    "section": "asdf",
    "text": "asdf\n\nlibrary(openintro)\nglimpse(bdims)\n\nRows: 507\nColumns: 25\n$ bia_di &lt;dbl&gt; 42.9, 43.7, 40.1, 44.3, 42.5, 43.3, 43.5, 44.4, 43.5, 42.0, 40.…\n$ bii_di &lt;dbl&gt; 26.0, 28.5, 28.2, 29.9, 29.9, 27.0, 30.0, 29.8, 26.5, 28.0, 29.…\n$ bit_di &lt;dbl&gt; 31.5, 33.5, 33.3, 34.0, 34.0, 31.5, 34.0, 33.2, 32.1, 34.0, 33.…\n$ che_de &lt;dbl&gt; 17.7, 16.9, 20.9, 18.4, 21.5, 19.6, 21.9, 21.8, 15.5, 22.5, 20.…\n$ che_di &lt;dbl&gt; 28.0, 30.8, 31.7, 28.2, 29.4, 31.3, 31.7, 28.8, 27.5, 28.0, 30.…\n$ elb_di &lt;dbl&gt; 13.1, 14.0, 13.9, 13.9, 15.2, 14.0, 16.1, 15.1, 14.1, 15.6, 13.…\n$ wri_di &lt;dbl&gt; 10.4, 11.8, 10.9, 11.2, 11.6, 11.5, 12.5, 11.9, 11.2, 12.0, 10.…\n$ kne_di &lt;dbl&gt; 18.8, 20.6, 19.7, 20.9, 20.7, 18.8, 20.8, 21.0, 18.9, 21.1, 19.…\n$ ank_di &lt;dbl&gt; 14.1, 15.1, 14.1, 15.0, 14.9, 13.9, 15.6, 14.6, 13.2, 15.0, 14.…\n$ sho_gi &lt;dbl&gt; 106.2, 110.5, 115.1, 104.5, 107.5, 119.8, 123.5, 120.4, 111.0, …\n$ che_gi &lt;dbl&gt; 89.5, 97.0, 97.5, 97.0, 97.5, 99.9, 106.9, 102.5, 91.0, 93.5, 9…\n$ wai_gi &lt;dbl&gt; 71.5, 79.0, 83.2, 77.8, 80.0, 82.5, 82.0, 76.8, 68.5, 77.5, 81.…\n$ nav_gi &lt;dbl&gt; 74.5, 86.5, 82.9, 78.8, 82.5, 80.1, 84.0, 80.5, 69.0, 81.5, 81.…\n$ hip_gi &lt;dbl&gt; 93.5, 94.8, 95.0, 94.0, 98.5, 95.3, 101.0, 98.0, 89.5, 99.8, 98…\n$ thi_gi &lt;dbl&gt; 51.5, 51.5, 57.3, 53.0, 55.4, 57.5, 60.9, 56.0, 50.0, 59.8, 60.…\n$ bic_gi &lt;dbl&gt; 32.5, 34.4, 33.4, 31.0, 32.0, 33.0, 42.4, 34.1, 33.0, 36.5, 34.…\n$ for_gi &lt;dbl&gt; 26.0, 28.0, 28.8, 26.2, 28.4, 28.0, 32.3, 28.0, 26.0, 29.2, 27.…\n$ kne_gi &lt;dbl&gt; 34.5, 36.5, 37.0, 37.0, 37.7, 36.6, 40.1, 39.2, 35.5, 38.3, 38.…\n$ cal_gi &lt;dbl&gt; 36.5, 37.5, 37.3, 34.8, 38.6, 36.1, 40.3, 36.7, 35.0, 38.6, 40.…\n$ ank_gi &lt;dbl&gt; 23.5, 24.5, 21.9, 23.0, 24.4, 23.5, 23.6, 22.5, 22.0, 22.2, 23.…\n$ wri_gi &lt;dbl&gt; 16.5, 17.0, 16.9, 16.6, 18.0, 16.9, 18.8, 18.0, 16.5, 16.9, 16.…\n$ age    &lt;int&gt; 21, 23, 28, 23, 22, 21, 26, 27, 23, 21, 23, 22, 20, 26, 23, 22,…\n$ wgt    &lt;dbl&gt; 65.6, 71.8, 80.7, 72.6, 78.8, 74.8, 86.4, 78.4, 62.0, 81.6, 76.…\n$ hgt    &lt;dbl&gt; 174.0, 175.3, 193.5, 186.5, 187.2, 181.5, 184.0, 184.5, 175.0, …\n$ sex    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …"
  },
  {
    "objectID": "slides/07-workflow.html#the-dataset",
    "href": "slides/07-workflow.html#the-dataset",
    "title": "Bayesian Workflow",
    "section": "The dataset",
    "text": "The dataset\n\nlibrary(openintro)\nglimpse(bdims)\n\nRows: 507\nColumns: 25\n$ bia_di &lt;dbl&gt; 42.9, 43.7, 40.1, 44.3, 42.5, 43.3, 43.5, 44.4, 43.5, 42.0, 40.…\n$ bii_di &lt;dbl&gt; 26.0, 28.5, 28.2, 29.9, 29.9, 27.0, 30.0, 29.8, 26.5, 28.0, 29.…\n$ bit_di &lt;dbl&gt; 31.5, 33.5, 33.3, 34.0, 34.0, 31.5, 34.0, 33.2, 32.1, 34.0, 33.…\n$ che_de &lt;dbl&gt; 17.7, 16.9, 20.9, 18.4, 21.5, 19.6, 21.9, 21.8, 15.5, 22.5, 20.…\n$ che_di &lt;dbl&gt; 28.0, 30.8, 31.7, 28.2, 29.4, 31.3, 31.7, 28.8, 27.5, 28.0, 30.…\n$ elb_di &lt;dbl&gt; 13.1, 14.0, 13.9, 13.9, 15.2, 14.0, 16.1, 15.1, 14.1, 15.6, 13.…\n$ wri_di &lt;dbl&gt; 10.4, 11.8, 10.9, 11.2, 11.6, 11.5, 12.5, 11.9, 11.2, 12.0, 10.…\n$ kne_di &lt;dbl&gt; 18.8, 20.6, 19.7, 20.9, 20.7, 18.8, 20.8, 21.0, 18.9, 21.1, 19.…\n$ ank_di &lt;dbl&gt; 14.1, 15.1, 14.1, 15.0, 14.9, 13.9, 15.6, 14.6, 13.2, 15.0, 14.…\n$ sho_gi &lt;dbl&gt; 106.2, 110.5, 115.1, 104.5, 107.5, 119.8, 123.5, 120.4, 111.0, …\n$ che_gi &lt;dbl&gt; 89.5, 97.0, 97.5, 97.0, 97.5, 99.9, 106.9, 102.5, 91.0, 93.5, 9…\n$ wai_gi &lt;dbl&gt; 71.5, 79.0, 83.2, 77.8, 80.0, 82.5, 82.0, 76.8, 68.5, 77.5, 81.…\n$ nav_gi &lt;dbl&gt; 74.5, 86.5, 82.9, 78.8, 82.5, 80.1, 84.0, 80.5, 69.0, 81.5, 81.…\n$ hip_gi &lt;dbl&gt; 93.5, 94.8, 95.0, 94.0, 98.5, 95.3, 101.0, 98.0, 89.5, 99.8, 98…\n$ thi_gi &lt;dbl&gt; 51.5, 51.5, 57.3, 53.0, 55.4, 57.5, 60.9, 56.0, 50.0, 59.8, 60.…\n$ bic_gi &lt;dbl&gt; 32.5, 34.4, 33.4, 31.0, 32.0, 33.0, 42.4, 34.1, 33.0, 36.5, 34.…\n$ for_gi &lt;dbl&gt; 26.0, 28.0, 28.8, 26.2, 28.4, 28.0, 32.3, 28.0, 26.0, 29.2, 27.…\n$ kne_gi &lt;dbl&gt; 34.5, 36.5, 37.0, 37.0, 37.7, 36.6, 40.1, 39.2, 35.5, 38.3, 38.…\n$ cal_gi &lt;dbl&gt; 36.5, 37.5, 37.3, 34.8, 38.6, 36.1, 40.3, 36.7, 35.0, 38.6, 40.…\n$ ank_gi &lt;dbl&gt; 23.5, 24.5, 21.9, 23.0, 24.4, 23.5, 23.6, 22.5, 22.0, 22.2, 23.…\n$ wri_gi &lt;dbl&gt; 16.5, 17.0, 16.9, 16.6, 18.0, 16.9, 18.8, 18.0, 16.5, 16.9, 16.…\n$ age    &lt;int&gt; 21, 23, 28, 23, 22, 21, 26, 27, 23, 21, 23, 22, 20, 26, 23, 22,…\n$ wgt    &lt;dbl&gt; 65.6, 71.8, 80.7, 72.6, 78.8, 74.8, 86.4, 78.4, 62.0, 81.6, 76.…\n$ hgt    &lt;dbl&gt; 174.0, 175.3, 193.5, 186.5, 187.2, 181.5, 184.0, 184.5, 175.0, …\n$ sex    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …"
  },
  {
    "objectID": "slides/07-workflow.html#visualize-data",
    "href": "slides/07-workflow.html#visualize-data",
    "title": "Bayesian Workflow",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/07-workflow.html#prepare-data",
    "href": "slides/07-workflow.html#prepare-data",
    "title": "Bayesian Workflow",
    "section": "Prepare data",
    "text": "Prepare data\n\nlibrary(openintro)\ndat &lt;- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))\nhead(dat)\n\n    weight   height  sex\n1 144.6231 68.50397 Male\n2 158.2917 69.01579 Male\n3 177.9128 76.18114 Male\n4 160.0554 73.42524 Male\n5 173.7241 73.70083 Male\n6 164.9056 71.45673 Male"
  },
  {
    "objectID": "slides/07-workflow.html#visualize-data-1",
    "href": "slides/07-workflow.html#visualize-data-1",
    "title": "Bayesian Workflow",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/07-workflow.html#visualize-data-2",
    "href": "slides/07-workflow.html#visualize-data-2",
    "title": "Bayesian Workflow",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/07-workflow.html#motivating-example-predicting-weight-from-height",
    "href": "slides/07-workflow.html#motivating-example-predicting-weight-from-height",
    "title": "Bayesian Workflow",
    "section": "Motivating example: predicting weight from height",
    "text": "Motivating example: predicting weight from height\nResearch question: We would like to understand the relationship between a person’s height and weight. A few particular questions we have are:\n\nHow much does a person’s weight increase when their height increases?\nHow certain we can be about the magnitude of the increase?\nCan we predict a person’s weight based on their height?\n\nData: We will use the bdims dataset from the openintro package. This dataset contains body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender."
  },
  {
    "objectID": "slides/07-workflow.html#scope-out-your-problem",
    "href": "slides/07-workflow.html#scope-out-your-problem",
    "title": "Bayesian Workflow",
    "section": "1. Scope out your problem",
    "text": "1. Scope out your problem"
  },
  {
    "objectID": "slides/07-workflow.html#scope-out-your-problem-1",
    "href": "slides/07-workflow.html#scope-out-your-problem-1",
    "title": "Bayesian Workflow",
    "section": "1. Scope out your problem",
    "text": "1. Scope out your problem"
  },
  {
    "objectID": "slides/07-workflow.html#scope-out-your-problem-2",
    "href": "slides/07-workflow.html#scope-out-your-problem-2",
    "title": "Bayesian Workflow",
    "section": "1. Scope out your problem",
    "text": "1. Scope out your problem"
  },
  {
    "objectID": "slides/07-workflow.html#specify-likelihood-priors",
    "href": "slides/07-workflow.html#specify-likelihood-priors",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\ndata {\n  int&lt;lower = 1&gt; n;\n  vector[n] height;\n  vector[n] weight;\n}"
  },
  {
    "objectID": "slides/07-workflow.html#specify-likelihood-priors-1",
    "href": "slides/07-workflow.html#specify-likelihood-priors-1",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\nparameter {\n  real alpha;\n  real beta;\n  real&lt;lower = 0&gt; sigma;\n}"
  },
  {
    "objectID": "slides/07-workflow.html#specify-likelihood-priors-2",
    "href": "slides/07-workflow.html#specify-likelihood-priors-2",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\nmodel {\n  target += normal_lpdf(weight | alpha + beta * height, sigma);\n}"
  },
  {
    "objectID": "slides/07-workflow.html#specify-likelihood-priors-3",
    "href": "slides/07-workflow.html#specify-likelihood-priors-3",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha\\) is the intercept, or average weight for someone who is zero inches (not a particularly useful number on its own)\n\\(\\beta\\) measures the association between weight and height, in pounds/inch\n\\(\\sigma\\) is the measurement error for the population\n\n\nmodel {\n  target += normal_lpdf(weight | alpha + beta * height, sigma);\n  target += normal_lpdf(alpha | 0, 100);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}"
  },
  {
    "objectID": "slides/07-workflow.html#fake-data",
    "href": "slides/07-workflow.html#fake-data",
    "title": "Bayesian Workflow",
    "section": "3. Fake data",
    "text": "3. Fake data\nSanity check:\n\nDraw parameter values from priors.\nGenerate data based on those parameter values.\nFit model to generated data.\nCheck fit is reasonable.\n\n\ngenerated quantities {\n  vector[n] weight;\n  real alpha = normal_rng(150, 5);\n  real beta = normal_rng(0, 10);\n  real sigma = fabs(normal_rng(0, 5));\n  for (i in 1:n) {\n    weight[i] = normal_rng(alpha + beta * height_c[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/07-workflow.html#quick-aside",
    "href": "slides/07-workflow.html#quick-aside",
    "title": "Bayesian Workflow",
    "section": "Quick aside",
    "text": "Quick aside\nWhat does it mean to use the prior sigma ~ normal(0, 5)?\n\nWhen a parameter is truncated, for example real&lt;lower = 0&gt; sigma, priors can still be placed across the real line, \\(\\mathbb{R}\\).\n\n\nparameters {\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(sigma | 0, 5);\n}\n\n\nThis specification induces a prior on the truncated space \\(\\mathbb{R}^+\\).\nThe induced prior for sigma is a half-normal distribution."
  },
  {
    "objectID": "slides/07-workflow.html#quick-aside-1",
    "href": "slides/07-workflow.html#quick-aside-1",
    "title": "Bayesian Workflow",
    "section": "Quick aside",
    "text": "Quick aside\n\nThe half-normal is a useful prior for nonnegative parameters that should not be too large and may be very close to zero.\nSimilar distributions for scale parameters are half-t and half-Cauchy priors, these have heavier tales."
  },
  {
    "objectID": "slides/07-workflow.html#specify-likelihood-priors-4",
    "href": "slides/07-workflow.html#specify-likelihood-priors-4",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[weight_i] = \\alpha + \\beta \\times (height_i - \\bar{x)},\\quad\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n height_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha\\) is the intercept, or average weight for someone who is an average height\n\n\nmodel {\n  target += normal_lpdf(weight | alpha + beta * height_c, sigma);\n  target += normal_lpdf(alpha | 150, 5);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}"
  },
  {
    "objectID": "slides/07-workflow.html#fake-data-1",
    "href": "slides/07-workflow.html#fake-data-1",
    "title": "Bayesian Workflow",
    "section": "3. Fake data",
    "text": "3. Fake data"
  },
  {
    "objectID": "slides/07-workflow.html#fit-model-to-real-data",
    "href": "slides/07-workflow.html#fit-model-to-real-data",
    "title": "Bayesian Workflow",
    "section": "4. Fit model to real data",
    "text": "4. Fit model to real data\n\n// saved in linear_regression_workflow.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  vector[n] weight; // outcome vector\n  vector[n] height_c; // covariate vector\n  int&lt;lower = 1&gt; n_pred; // number of new observations\n  vector[n_pred] height_c_pred; // vector for new observations\n}\nparameters {\n  real alpha;\n  real beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(weight | alpha + height_c * beta, sigma); // likelihood\n  target += normal_lpdf(alpha | 150, 5);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n_pred] out_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(alpha + height_c[i] * beta, sigma);\n    log_lik[i] = normal_lpdf(weight[i] | alpha + height_c[i] * beta, sigma);\n  }\n  for (i in 1:n_pred) {\n    out_sample[i] = normal_rng(alpha + height_c_pred[i] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/07-workflow.html#fit-model-to-real-data-1",
    "href": "slides/07-workflow.html#fit-model-to-real-data-1",
    "title": "Bayesian Workflow",
    "section": "4. Fit model to real data",
    "text": "4. Fit model to real data\n\nstan_data &lt;- list(n = nrow(dat), \n                  height_c = (dat$height - mean(dat$height)), \n                  weight = dat$weight)\nregression_model &lt;- stan_model(file = \"linear_regression_workflow.stan\")\nfit &lt;- sampling(regression_model, data = stan_data)\nprint(fit)\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd   2.5%    50%  97.5% n_eff Rhat\nalpha 152.35    0.01 0.88 150.63 152.35 154.08  3773    1\nbeta    5.69    0.00 0.25   5.21   5.69   6.18  4215    1\nsigma  20.25    0.01 0.60  19.13  20.23  21.50  3571    1\n\nSamples were drawn using NUTS(diag_e) at Wed Nov 27 14:08:19 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/07-workflow.html#diagnostics",
    "href": "slides/07-workflow.html#diagnostics",
    "title": "Bayesian Workflow",
    "section": "5. Diagnostics",
    "text": "5. Diagnostics\n\nrstan::traceplot(fit, pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/07-workflow.html#diagnostics-1",
    "href": "slides/07-workflow.html#diagnostics-1",
    "title": "Bayesian Workflow",
    "section": "5. Diagnostics",
    "text": "5. Diagnostics\n\nlibrary(bayesplot)\nmcmc_acf(fit, regex_pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/07-workflow.html#graph-fit",
    "href": "slides/07-workflow.html#graph-fit",
    "title": "Bayesian Workflow",
    "section": "6. Graph fit",
    "text": "6. Graph fit"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-checking",
    "href": "slides/07-workflow.html#posterior-predictive-checking",
    "title": "Bayesian Workflow",
    "section": "Posterior predictive checking",
    "text": "Posterior predictive checking"
  },
  {
    "objectID": "slides/07-workflow.html#graph-fit-1",
    "href": "slides/07-workflow.html#graph-fit-1",
    "title": "Bayesian Workflow",
    "section": "6. Graph fit",
    "text": "6. Graph fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior mean and 95% credible interval for \\(\\mu = \\alpha + \\beta \\times height_i\\)."
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-fit",
    "href": "slides/07-workflow.html#posterior-predictive-fit",
    "title": "Bayesian Workflow",
    "section": "Posterior predictive fit",
    "text": "Posterior predictive fit"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive",
    "href": "slides/07-workflow.html#posterior-predictive",
    "title": "Bayesian Workflow",
    "section": "Posterior predictive",
    "text": "Posterior predictive\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(dat$weight, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/07-workflow.html#visualizing-posterior-predictive-check",
    "href": "slides/07-workflow.html#visualizing-posterior-predictive-check",
    "title": "Bayesian Workflow",
    "section": "Visualizing posterior predictive check",
    "text": "Visualizing posterior predictive check\nppc_stat(dat$weight, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(dat$weight, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(dat$weight, y_pred, stat = \"q025\")\nppc_stat(dat$weight, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-checks",
    "href": "slides/07-workflow.html#posterior-predictive-checks",
    "title": "Bayesian Workflow",
    "section": "7. Posterior predictive checks",
    "text": "7. Posterior predictive checks\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(dat$weight, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-checks-1",
    "href": "slides/07-workflow.html#posterior-predictive-checks-1",
    "title": "Bayesian Workflow",
    "section": "7. Posterior predictive checks",
    "text": "7. Posterior predictive checks\nppc_stat(dat$weight, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(dat$weight, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(dat$weight, y_pred, stat = \"q025\")\nppc_stat(dat$weight, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-regression-fit",
    "href": "slides/07-workflow.html#posterior-predictive-regression-fit",
    "title": "Bayesian Workflow",
    "section": "Posterior predictive regression fit",
    "text": "Posterior predictive regression fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior predictive distribution mean and 95% credible interval, \\(f(weight_i' | weight_{1:n})\\)."
  },
  {
    "objectID": "slides/07-workflow.html#shinystan",
    "href": "slides/07-workflow.html#shinystan",
    "title": "Bayesian Workflow",
    "section": "shinystan",
    "text": "shinystan\n\nlibrary(shinystan)\ny &lt;- dat$weight # need to define outcome as a global variable to be accessible\nsso &lt;- shinystan::launch_shinystan(fit)"
  },
  {
    "objectID": "slides/07-workflow.html#posterior-predictive-checks-2",
    "href": "slides/07-workflow.html#posterior-predictive-checks-2",
    "title": "Bayesian Workflow",
    "section": "7. Posterior predictive checks",
    "text": "7. Posterior predictive checks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior predictive distribution mean and 95% credible interval, \\(f(weight_i' | weight_{1:n})\\)."
  },
  {
    "objectID": "slides/07-workflow.html#compare-models",
    "href": "slides/07-workflow.html#compare-models",
    "title": "Bayesian Workflow",
    "section": "8. Compare models:",
    "text": "8. Compare models:\n\nIterate on model design, choose a model.\nWe’ve talked about performing sensitivity analyses to choice of prior.\nWe have not introduced formal methods for model comparison."
  },
  {
    "objectID": "slides/07-workflow.html#model-comparison",
    "href": "slides/07-workflow.html#model-comparison",
    "title": "Bayesian Workflow",
    "section": "Model comparison",
    "text": "Model comparison\n\nModel selection criteria are designed to help comparing several models.\nAn ideal criteria will not just based on their fit with training data, but on an estimation of their prediction accuracy with new data.\nCriteria often reward models that offer a good compromise between simplicity and accuracy.\nExamples: Likelihood ratio test, AIC"
  },
  {
    "objectID": "slides/07-workflow.html#likelihood-ratio",
    "href": "slides/07-workflow.html#likelihood-ratio",
    "title": "Bayesian Workflow",
    "section": "Likelihood ratio",
    "text": "Likelihood ratio\n\nThe likelihood ratio method consists in assessing whether increasing the complexity of a model results in a significant improvement of likelihood which justifies this increased complexity.\nIt is a computationally inexpensive method, since it only relies on the value of the total likelihood function, but can only be used to compare nested models.\nNot often used for Bayesian models."
  },
  {
    "objectID": "slides/07-workflow.html#waic",
    "href": "slides/07-workflow.html#waic",
    "title": "Bayesian Workflow",
    "section": "WAIC",
    "text": "WAIC\n(Gelman, Hwang, and Vehtari (2014)). A fully Bayesian criterion is the Widely Applicable Information Criterion (WAIC) (Watanabe and Opper (2010)), asymptotically equal to the Bayesian leave-one-out cross validation criterion."
  },
  {
    "objectID": "slides/07-workflow.html#information-criteria-1",
    "href": "slides/07-workflow.html#information-criteria-1",
    "title": "Bayesian Workflow",
    "section": "Information criteria",
    "text": "Information criteria\n\nIt is sometimes useful to consider predictive accuracy given a point estimate \\(\\hat{\\theta}(\\mathbf{Y})\\), thus,\n\n\\[\\text{expected log predictive density, given }\\hat{\\theta}(\\mathbf{Y}): \\mathbb{E}[]\\]\n\nThe frequentist Akaike information criterion (AIC) is the oldest and most restrictive,\n\n\\[AIC = -2 \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{MLE}) + 2d,\\] where \\(d\\) is the number of parameters in the model.\n\nAmong Bayesians, the deviance information criterion (DIC) has been widely used for some time, now. However, the DIC is limited in that it presumes the posterior is multivariate Gaussian, which is not always the case.\nThe widely applicable information criterion (WAIC) does not impose assumptions on the shape of the posterior distribution.\n\nNote: WAIC provides an estimate of out-of-sample deviance and converges with LOO-CV as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/07-workflow.html#compare-models-1",
    "href": "slides/07-workflow.html#compare-models-1",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\nSuppose we would like to predict weight and we would like to compare our original model with a model that also includes sex. We can compare these models using WAIC."
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-information-criteria-bic",
    "href": "slides/07-workflow.html#bayesian-information-criteria-bic",
    "title": "Bayesian Workflow",
    "section": "Bayesian information criteria (BIC)",
    "text": "Bayesian information criteria (BIC)\n\nThe Bayesian information criteria is similar\n\n\\[BIC = D(\\mathbf{Y}|\\hat{\\boldsymbol{\\theta}})+ \\log(n)p\\]\n\nThis is motivated as an approximation to the log Bayes factor of the model compared to the null model.\nHowever, this is only an asymptotic (large \\(n\\)) approximation.\nWith large \\(n\\) the prior is irrelevant, and so this is not satisfying to a subjective Bayesian."
  },
  {
    "objectID": "slides/07-workflow.html#deviance-information-criteria-dic",
    "href": "slides/07-workflow.html#deviance-information-criteria-dic",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nDIC is a popular Bayesian analog of AIC or BIC.\nUnlike CV, DIC requires only one model fit.\nHowever, proceed with caution.\nDIC really only applies when the posterior is approximately normal, and will give misleading results when the posterior far from normality (e.g., bimodal).\nDIC is also criticized for selecting overly-complex models."
  },
  {
    "objectID": "slides/07-workflow.html#deviance-information-criteria-dic-1",
    "href": "slides/07-workflow.html#deviance-information-criteria-dic-1",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nLet \\(\\bar{D} = \\mathbb{E}[D(\\mathbf{Y}|\\boldsymbol{\\theta})|\\mathbf{Y}]\\) be the posterior mean of the deviance.\nDenote \\(\\hat{\\boldsymbol{\\theta}}\\) as the posterior mean of \\(\\boldsymbol{\\theta}\\).\nThe effective number of parameters is \\(p_D = \\bar{D} − D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}})\\).\nDIC can be written like AIC,\n\n\\[DIC = \\bar{D} + p_D =D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}}) + 2p_D\\]\n\nModels with small \\(\\bar{D}\\) fit the data well.\nModels with small \\(p_D\\) are simple.\nWe select the model with smallest DIC."
  },
  {
    "objectID": "slides/07-workflow.html#deviance-information-criteria-dic-2",
    "href": "slides/07-workflow.html#deviance-information-criteria-dic-2",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nThe effective number of parameters is a useful measure of model complexity\nIntuitively, if there are \\(p\\) parameters and we have uninformative priors then \\(p_D \\approx p\\)\nHowever, \\(p_D \\ll p\\) if there are strong priors.\nAs with AIC or BIC, the actual value is meaningless, only differences are relevant.\nDIC can only be used to compare models with the same likelihood."
  },
  {
    "objectID": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic-1",
    "href": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic-1",
    "title": "Bayesian Workflow",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC is written in terms of the posterior of the likelihood rather than parameters\nLet \\(m_i\\) be the posterior mean of \\(f(Y_i|\\boldsymbol{\\theta})\\) and \\(v_i\\) be the posterior variance of \\(\\log f(Y_i|\\boldsymbol{\\theta})\\)\nThe effective model size is \\(p_W = \\sum_{i=1}^n v_i\\)\nThe criteria is\n\n\\[WAIC = −2 \\sum_{i=1}^n \\log (m_i) + 2p_W\\]"
  },
  {
    "objectID": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic-2",
    "href": "slides/07-workflow.html#watanabe-akaike-information-criteria-waic-2",
    "title": "Bayesian Workflow",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\\[\\text{WAIC}(\\mathbf{Y},\\boldsymbol{\\theta}) = -2\\left(\\text{lppd} - p_{\\text{WAIC}}\\right),\\] where \\(p_{\\text{WAIC}} = \\sum_{i = 1}^n \\mathbb{V}_{\\boldsymbol{\\theta}}\\left(\\log f\\left(Y_i | \\boldsymbol{\\theta}\\right)\\right)\\) and log pointwise predictive density (lppd) is, \\[\\text{lppd}(\\mathbf{Y},\\boldsymbol{\\theta}) = \\sum_{i = 1}^n \\log \\frac{1}{S} \\sum_{s = 1}^S f\\left({Y}_i | \\boldsymbol{\\theta}^{(s)}\\right).\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html",
    "href": "slides/02-monte-carlo.html",
    "title": "Monte Carlo Sampling",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine"
  },
  {
    "objectID": "slides/02-monte-carlo.html#simulating-pi-using-monte-carlo",
    "href": "slides/02-monte-carlo.html#simulating-pi-using-monte-carlo",
    "title": "Monte Carlo Sampling",
    "section": "Simulating \\(\\pi\\) using Monte Carlo",
    "text": "Simulating \\(\\pi\\) using Monte Carlo\nSuppose we are interested in estimating \\(\\pi\\).\n\n\n\nWe can formulate \\(\\pi\\) as a function of the area of a square and circle.\nArea of a circle: \\(A_c = \\pi r^2\\)\nArea of a square: \\(A_s = 4 r^2\\)\nThe ratio of the two areas is: \\(\\frac{A_c}{A_s} = \\frac{\\pi r^2}{4 r^2} \\implies \\pi = \\frac{4 A_c}{A_s}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we have an estimate for the ratio we can solve for \\(\\pi\\). The challenge becomes estimating this ratio."
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-simulations",
    "href": "slides/02-monte-carlo.html#monte-carlo-simulations",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo Simulations",
    "text": "Monte Carlo Simulations\nThis is where we can take advantage of how quickly a computer can generate pseudorandom numbers. There is a whole class of algorithms called Monte Carlo simulations that exploit randomness to estimate real world scenarios that would otherwise be difficult to explicitly calculate. We can use a Monte Carlo simulation to estimate the area ratio of the circle to the square. Imagine you randomly drop grains of sand into the area of the square. By counting the total number of sand grains in the square (all of them since you’re an accurate dropper) to the number of sand grains inside the circle we get this estimate. Multiply the estimated ratio by four and you get an estimate for π. The more sand grains you use the more accurate your estimate of π.\nThe algorithm for this method is straightforward:\n\nGenerate a random point \\((x, y)\\) inside a square of side 2 centered at the origin. This is equivalent to assuming the following:\n\n\n\\(x \\sim Uniform(-1, 1)\\)\n\\(y \\sim Uniform(-1, 1)\\)\n\n\nDetermine whether the point falls inside the unit circle inscribed in the square by checking whether \\(x^2 + y^2 \\leq 1\\).\nRepeat steps 1 and 2 for a large number of points (\\(n\\)).\nCalculate the ratio of the number of points that fell inside the circle to the total number of points generated.\nMultiply the ratio by 4 to estimate the value of pi."
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-big-of-a-difference-does-n-make",
    "href": "slides/02-monte-carlo.html#how-big-of-a-difference-does-n-make",
    "title": "Monte Carlo Sampling",
    "section": "How big of a difference does \\(n\\) make",
    "text": "How big of a difference does \\(n\\) make\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n###Computing pi with differenct \nestimate_pi &lt;- function(n) {\n  count &lt;- 0\n  x &lt;- runif(n, -1, 1)\n  y &lt;- runif(n, -1, 1)\n  4 * mean(x^2 + y^2 &lt;= 1)\n}\nn_seq_log &lt;- seq(1, 7, length.out = 50)\nn_seq &lt;- 10^n_seq_log\npis &lt;- unlist(lapply(n_seq, estimate_pi))\npar(mfcol = c(1, 1))\nplot(n_seq_log, pis, type = \"b\", ylab = expression(paste(\"Estimated value of \", pi)), pch = 16, xaxt = \"n\", xlab = \"Number of points\")\naxis(1, at = 1:7, labels = c(expression(10^1), expression(10^2), expression(10^3), expression(10^4), expression(10^5), expression(10^6), expression(10^7)))\nabline(h = pi, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", legend = \"True value\", lwd = 2, lty = 2, col = \"red\", bty = \"n\")\n\n\n\n\n\n\n\npar(mfcol = c(1, 1))\nplot(n_seq_log, abs(pis - pi), type = \"b\", ylab = expression(paste(\"Estimated value of \", pi)), pch = 16, xaxt = \"n\", xlab = \"Number of points\")\naxis(1, at = 1:7, labels = c(expression(10^1), expression(10^2), expression(10^3), expression(10^4), expression(10^5), expression(10^6), expression(10^7)))\nabline(h = pi, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", legend = \"True value\", lwd = 2, lty = 2, col = \"red\", bty = \"n\")"
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-sampling-1",
    "href": "slides/02-monte-carlo.html#monte-carlo-sampling-1",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo sampling",
    "text": "Monte Carlo sampling\n\nWe can take advantage of how quickly a computer can generate pseudo-random numbers.\nThere is a class of algorithms called Monte Carlo sampling that exploit randomness to estimate real world scenarios that would otherwise be difficult to explicitly calculate.\nThe name comes from the Monte Carlo Casino in Monaco, where the primary developer of the method, mathematician Stanislaw Ulam, was inspired by his uncle’s gambling habits. (This is who Stan was named after!)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#algorithm-for-estimating-pi",
    "href": "slides/02-monte-carlo.html#algorithm-for-estimating-pi",
    "title": "Monte Carlo Sampling",
    "section": "Algorithm for estimating \\(\\pi\\)",
    "text": "Algorithm for estimating \\(\\pi\\)\n\nGenerate a random point \\((x, y)\\) inside a square centered at the origin with length 2.\n\n\nThis is equivalent to assuming:\n\n\\(f_{X,Y}(x,y) = f_X(x)f_Y(y)\\)\n\\(f_X(x) = Uniform(-1, 1)\\), \\(f_Y(y) = Uniform(-1, 1)\\)\n\n\n\n\nDetermine whether the point falls inside the unit circle inscribed in the square by checking whether \\(x^2 + y^2 \\leq 1\\).\n\n\n\n\nRepeat steps 1 and 2 for a large number of points (\\(S\\))."
  },
  {
    "objectID": "slides/02-monte-carlo.html#how-big-of-a-difference-does-s-make",
    "href": "slides/02-monte-carlo.html#how-big-of-a-difference-does-s-make",
    "title": "Monte Carlo Sampling",
    "section": "How big of a difference does \\(S\\) make",
    "text": "How big of a difference does \\(S\\) make"
  },
  {
    "objectID": "slides/02-monte-carlo.html#estimating-pi-with-increasing-s",
    "href": "slides/02-monte-carlo.html#estimating-pi-with-increasing-s",
    "title": "Monte Carlo Sampling",
    "section": "Estimating \\(\\pi\\) with increasing \\(S\\)",
    "text": "Estimating \\(\\pi\\) with increasing \\(S\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#error-in-estimating-pi-using-monte-carlo",
    "href": "slides/02-monte-carlo.html#error-in-estimating-pi-using-monte-carlo",
    "title": "Monte Carlo Sampling",
    "section": "Error in estimating \\(\\pi\\) using Monte Carlo",
    "text": "Error in estimating \\(\\pi\\) using Monte Carlo"
  },
  {
    "objectID": "slides/02-monte-carlo.html#error-in-estimating-pi",
    "href": "slides/02-monte-carlo.html#error-in-estimating-pi",
    "title": "Monte Carlo Sampling",
    "section": "Error in estimating \\(\\pi\\)",
    "text": "Error in estimating \\(\\pi\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#monte-carlo-estimation-of-the-ratio",
    "href": "slides/02-monte-carlo.html#monte-carlo-estimation-of-the-ratio",
    "title": "Monte Carlo Sampling",
    "section": "Monte Carlo estimation of the ratio",
    "text": "Monte Carlo estimation of the ratio\n\nWe can use a Monte Carlo simulation to estimate the area ratio of the circle to the square.\nImagine you randomly drop grains of sand into the area of the square. By counting the total number of sand grains in the square (all of them since you’re an accurate dropper) to the number of sand grains inside the circle we get this estimate.\nMultiply the estimated ratio by 4 and you get an estimate for \\(\\pi\\).\nThe more grains of sand that are used the more accurate your estimate of \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling",
    "href": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling",
    "title": "Monte Carlo Sampling",
    "section": "Intuition behing Monte Carlo sampling",
    "text": "Intuition behing Monte Carlo sampling\n\nAssume \\(X_i \\sim Uniform(-1,1)\\) and \\(Y_i \\sim Uniform(-1,1)\\) for \\(i = 1,\\ldots,S\\).\nWe can write our problem as, \\(\\pi = 4P(X^2 + Y^2 \\leq 1)\\).\nHow could we do this without Monte Carlo?\nDefine, \\(Z = X^2 + Y^2\\). We could then use change-of-variables to compute the density of \\(Z\\) and then compute \\(P(Z \\leq 1)\\).\n\nThis is generally difficult!"
  },
  {
    "objectID": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling-1",
    "href": "slides/02-monte-carlo.html#intuition-behing-monte-carlo-sampling-1",
    "title": "Monte Carlo Sampling",
    "section": "Intuition behing Monte Carlo sampling",
    "text": "Intuition behing Monte Carlo sampling\n\nInstead, we could write our problem as an expectation and use the law of large numbers,\n\n\\[P(X^2 + Y^2 \\leq 1) = \\mathbb{E}_{X,Y}\\left[1\\left(X^2 + Y^2 \\leq 1\\right)\\right].\\]\n\nRecall that an expectation of an indicator is the probability of the event that it indicates, \\(\\mathbb{E}_X[1(A)] = \\int_A f_X(x)dx = P(X \\in A)\\).\nWe then have that, \\[\\frac{1}{S}\\sum_{i = 1}^S 1\\left(X_i^2 + Y_i^2 \\leq 1\\right) \\rightarrow \\mathbb{E}_{X,Y}\\left[1\\left(X^2 + Y^2 \\leq 1\\right)\\right],\\]\n\nwhere \\((X_i,Y_i) \\sim f(X_i,Y_i)\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#a-motivating-example",
    "href": "slides/02-monte-carlo.html#a-motivating-example",
    "title": "Monte Carlo Sampling",
    "section": "A motivating example",
    "text": "A motivating example\n\nSuppose we are interested in estimating the prevalence of diabetes in Durham County. We aim to estimate this prevalence by taking a sample of \\(n\\) individuals in Durham County and we record whether or not they have diabetes, \\(Y_i\\).\nWe assume that \\(Z = \\sum_{i=1}^n Y_i \\sim Binomial(n, \\pi)\\) for \\(i = 1,\\ldots,n\\).\nOur goal is to estimate \\(\\pi\\) and perform statistical inference (e.g., point estimation, interval estimation, etc.)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#posterior-inference",
    "href": "slides/02-monte-carlo.html#posterior-inference",
    "title": "Monte Carlo Sampling",
    "section": "Posterior inference",
    "text": "Posterior inference\n\nIn Bayesian statistics, inference is encoded through the posterior distribution,\n\n\\[\\begin{aligned}\nf(\\pi | Z) &= \\frac{f(Z | \\pi)f(\\pi)}{f(Z)},\\\\\n&= \\frac{f(Z | \\pi)f(\\pi)}{\\int f(Z | \\pi)f(\\pi)d\\pi}.\n\\end{aligned}\\]\n\nAll we have to do is specify the likelihood and prior."
  },
  {
    "objectID": "slides/02-monte-carlo.html#likelihood-specification",
    "href": "slides/02-monte-carlo.html#likelihood-specification",
    "title": "Monte Carlo Sampling",
    "section": "Likelihood specification",
    "text": "Likelihood specification\n\\(Z\\) is a Binomial distribution with pmf,\n\\[f(Z | \\pi) = P(Z = z) = {n \\choose z} \\pi^z(1-\\pi)^{n-z},\\]\nwhere \\(z \\in \\{0, 1, \\ldots, n\\}\\).\n\n\\({n \\choose z} = \\frac{n!}{z!(n-z)!} = \\frac{\\Gamma(n+1)}{\\Gamma(z+1)\\Gamma(n-z+1)}\\)\n\\(\\Gamma(x) = (x-1)!\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#prior-specification",
    "href": "slides/02-monte-carlo.html#prior-specification",
    "title": "Monte Carlo Sampling",
    "section": "Prior specification",
    "text": "Prior specification\nWhat do we know about \\(\\pi\\)?\n\n\\(\\pi\\) is continuous.\n\\(\\pi \\in (0,1)\\).\n\nWe should place a distribution on \\(\\pi\\) that permits these properties.\nOne option is the Beta distribution, \\[f(\\pi) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}.\\]\nIf we assume that \\(\\pi \\sim Beta(\\alpha,\\beta)\\), we can proceed to compute the posterior."
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-posterior-1",
    "href": "slides/02-monte-carlo.html#computing-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Computing the posterior",
    "text": "Computing the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#using-the-kernel-to-find-the-posterior-1",
    "href": "slides/02-monte-carlo.html#using-the-kernel-to-find-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Using the kernel to find the posterior",
    "text": "Using the kernel to find the posterior\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{aligned}\\]\nThis is the kernel of a multivariate normal for \\(\\boldsymbol{\\beta}\\), with \\(\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}\\) and \\(\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\). It’s easy to see then that, \\(f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\\)\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#suppose-we-have",
    "href": "slides/02-monte-carlo.html#suppose-we-have",
    "title": "Monte Carlo Sampling",
    "section": "Suppose we have",
    "text": "Suppose we have\nSuppose we conducted a simple random sample of 500 individuals in Durham County and 120 responsed that they had diabetes and 380 that they did not."
  },
  {
    "objectID": "slides/02-monte-carlo.html#lets-inspect-the-posterior",
    "href": "slides/02-monte-carlo.html#lets-inspect-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Let’s inspect the posterior",
    "text": "Let’s inspect the posterior\n\nSuppose we conducted a simple random sample of 500 individuals in Durham County and 120 responsed that they had diabetes and 380 that they did not.\nThe posterior becomes, \\(Beta\\left(\\alpha + 120, \\beta + 380\\right)\\).\nWe must choose our prior distribution wisely.\nNote that:\n\n\\(\\mathbb{E}[\\pi] = \\alpha/(\\alpha + \\beta)\\)\n\\(\\mathbb{V}(\\pi) = (\\alpha\\beta)/[(\\alpha + \\beta)^2(\\alpha + \\beta + 1)]\\).\n\nTypically, \\(\\alpha = \\beta = 1\\), which corresponds to a uniform prior on \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#lets-inspect-the-posterior-1",
    "href": "slides/02-monte-carlo.html#lets-inspect-the-posterior-1",
    "title": "Monte Carlo Sampling",
    "section": "Let’s inspect the posterior",
    "text": "Let’s inspect the posterior"
  },
  {
    "objectID": "slides/02-monte-carlo.html#suppose-my-prior-changes",
    "href": "slides/02-monte-carlo.html#suppose-my-prior-changes",
    "title": "Monte Carlo Sampling",
    "section": "Suppose my prior changes",
    "text": "Suppose my prior changes"
  },
  {
    "objectID": "slides/02-monte-carlo.html#returning-to-our-posterior",
    "href": "slides/02-monte-carlo.html#returning-to-our-posterior",
    "title": "Monte Carlo Sampling",
    "section": "Returning to our posterior",
    "text": "Returning to our posterior\nLet’s obtain \\(S = 1,000\\) samples from our posterior.\n\npi_samples &lt;- rbeta(1000, 1 + 120, 1 + 380)\n\nWe can compute the posterior mean and variance.\n\nprint(mean(pi_samples))\n\n[1] 0.2403665\n\nprint(var(pi_samples))\n\n[1] 0.0003811683"
  },
  {
    "objectID": "slides/02-monte-carlo.html#algorithm-for-estimating-pi-1",
    "href": "slides/02-monte-carlo.html#algorithm-for-estimating-pi-1",
    "title": "Monte Carlo Sampling",
    "section": "Algorithm for estimating \\(\\pi\\)",
    "text": "Algorithm for estimating \\(\\pi\\)\n\nGenerate a random point \\((x, y)\\) inside a square centered at the origin with length 2.\nDetermine whether the point falls inside the unit circle inscribed in the square by checking whether \\(x^2 + y^2 \\leq 1\\).\nRepeat steps 1 and 2 for a large number of points (\\(S\\)).\nCalculate the ratio of the number of points that fell inside the circle to the total number of points generated.\n\n\n\nMultiply the ratio by 4 to estimate the value of \\(\\pi\\)."
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-marginal-likelihood",
    "href": "slides/02-monte-carlo.html#computing-the-marginal-likelihood",
    "title": "Monte Carlo Sampling",
    "section": "Computing the marginal likelihood",
    "text": "Computing the marginal likelihood\nUnder our prior specification, we can compute the marginal likelihood, \\(f(Z)\\):\n\\[\\begin{aligned}\nf(Z) &= \\int f(Z | \\pi) f(\\pi) d\\pi\\\\\n&= \\int {n \\choose z} \\pi^z(1-\\pi)^{n-z} \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} d\\pi\\\\\n&= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\int \\underbrace{\\pi^{(\\alpha + z) - 1}(1-\\pi)^{(\\beta + n - z) - 1}}_{Beta\\text{ }kernel} d\\pi\n\\end{aligned}\\]\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the Beta pdf is \\(\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\)"
  },
  {
    "objectID": "slides/02-monte-carlo.html#computing-the-marginal-likelihood-1",
    "href": "slides/02-monte-carlo.html#computing-the-marginal-likelihood-1",
    "title": "Monte Carlo Sampling",
    "section": "Computing the marginal likelihood",
    "text": "Computing the marginal likelihood\nUnder our prior specification, we can compute the marginal likelihood, \\(f(Z)\\):\n\\[\\begin{aligned}\nf(Z) &= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\int \\pi^{(\\alpha + z) - 1}(1-\\pi)^{(\\beta + n - z) - 1} d\\pi\\\\\n&= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta + n)}{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#we-can-then-compute-the-posetrior",
    "href": "slides/02-monte-carlo.html#we-can-then-compute-the-posetrior",
    "title": "Monte Carlo Sampling",
    "section": "We can then compute the posetrior",
    "text": "We can then compute the posetrior\n\\[\\begin{aligned}\nf(\\pi | Z) &\\propto f(Z | \\pi) f(\\pi)\\\\\n&= \\frac{{n \\choose z} \\pi^z(1-\\pi)^{n-z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}}{{n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta) }{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-monte-carlo.html#we-can-then-compute-the-posterior",
    "href": "slides/02-monte-carlo.html#we-can-then-compute-the-posterior",
    "title": "Monte Carlo Sampling",
    "section": "We can then compute the posterior",
    "text": "We can then compute the posterior\n\\[\\begin{aligned}\nf(\\pi | Z) &= \\frac{f(Z | \\pi) f(\\pi)}{f(Z)}\\\\\n&= \\frac{{n \\choose z} \\pi^z(1-\\pi)^{n-z} \\times \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}}{{n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta) }{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}}\\\\\n&=\\frac{\\Gamma(\\alpha+z)\\Gamma(\\beta+n-z)}{\\Gamma(\\alpha + \\beta + n)}\\pi^{(\\alpha + z) - 1} (1 - \\pi)^{(\\beta + n - z) - 1}\\\\\n&=Beta\\left(\\alpha + z, \\beta + n - z\\right).\n\\end{aligned}\\]\n\nA prior that is considered conjugate yields a posterior with the same distribution.\nThe Beta distribution is conjugate for the Bernoulli/Binomial distributions."
  },
  {
    "objectID": "slides/03-mcmc.html#defining-the-model",
    "href": "slides/03-mcmc.html#defining-the-model",
    "title": "Markov chain Monte Carlo",
    "section": "Defining the model",
    "text": "Defining the model\nSuppose we have an observation \\(Y_i\\) for subject \\(i\\) (\\(i=1,\\ldots,n\\)), that is modeled as follows,\n\\[\\begin{aligned}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{aligned}\\]\n\n\\(\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})\\) is a \\((p+1)\\)-dimensional row vector of covariates (and intercept).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top\\) is a \\((p+1)\\)-dimensional column vector of population regression parameters.\n\\(\\epsilon_i\\) is a Gaussian measurement error term with variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#defining-the-likelihood",
    "href": "slides/03-mcmc.html#defining-the-likelihood",
    "title": "Markov chain Monte Carlo",
    "section": "Defining the likelihood",
    "text": "Defining the likelihood\nThe individual likelihood contribution for subject \\(i\\) is given by, \\[Y_i|\\boldsymbol{\\beta},\\sigma^2 \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta},\\sigma^2) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),\\] and the full data likelihood (or observed data likelihood) is given by, \\[f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta},\\sigma^2),\\] where \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#matrix-likelihood-specification",
    "href": "slides/03-mcmc.html#matrix-likelihood-specification",
    "title": "Markov chain Monte Carlo",
    "section": "Matrix likelihood specification",
    "text": "Matrix likelihood specification\nWe can also write the likelihood directly, \\[\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\\] where \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) dimensional matrix with row \\(\\mathbf{x}_i\\) and \\(\\mathbf{I}_n\\) is an \\(n\\)-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\\[\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2 ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#linear-regression-estimation",
    "href": "slides/03-mcmc.html#linear-regression-estimation",
    "title": "Markov chain Monte Carlo",
    "section": "Linear regression estimation",
    "text": "Linear regression estimation\n\nOrdinary least squares (OLS)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nMaximum likelihood estimation (MLE)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})\\)\n\n\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#bayesian-estimation",
    "href": "slides/03-mcmc.html#bayesian-estimation",
    "title": "Markov chain Monte Carlo",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{aligned}\\]\nTo compute the posterior distribution, we need the prior \\(f(\\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "slides/03-mcmc.html#prior-definition",
    "href": "slides/03-mcmc.html#prior-definition",
    "title": "Markov chain Monte Carlo",
    "section": "Prior definition",
    "text": "Prior definition\nLet’s assume that the prior for \\(\\boldsymbol{\\beta}\\) is Gassian,\n\\[f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).\\]\n\n\\(\\boldsymbol{\\beta}_0\\) is the prior mean (i.e., our a-priori guess for the likely value of \\(\\boldsymbol{\\beta}\\))\n\\(\\sigma_{\\beta}^2\\) is the prior variance (i.e., encodes our certainty for our a-priori guess)"
  },
  {
    "objectID": "slides/03-mcmc.html#computing-the-posterior",
    "href": "slides/03-mcmc.html#computing-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Computing the posterior",
    "text": "Computing the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#techniques-to-find-this-posterior",
    "href": "slides/03-mcmc.html#techniques-to-find-this-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Techniques to find this posterior",
    "text": "Techniques to find this posterior\n\nBrute force: complete the square\nEasy: kernel recognition\n\n\n\n\n\n\n\nDefinition\n\n\nThe part of the pdf/pmf that depends on the variable is called the kernel.\nExample:\n\nThe kernel of the multivariate normal pdf, \\(\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is\n\n\\[\\begin{aligned}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-mcmc.html#using-the-kernel-to-find-the-posterior",
    "href": "slides/03-mcmc.html#using-the-kernel-to-find-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Using the kernel to find the posterior",
    "text": "Using the kernel to find the posterior\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{aligned}\\]\nThis is the kernel of a multivariate normal for \\(\\boldsymbol{\\beta}\\), with \\(\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}\\) and \\(\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\). It’s easy to see then that, \\(f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\\)\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#back-to-the-posterior",
    "href": "slides/03-mcmc.html#back-to-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Back to the posterior",
    "text": "Back to the posterior\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#visualize-simulated-data",
    "href": "slides/03-mcmc.html#visualize-simulated-data",
    "title": "Markov chain Monte Carlo",
    "section": "Visualize simulated data",
    "text": "Visualize simulated data"
  },
  {
    "objectID": "slides/03-mcmc.html#inspecting-the-prior",
    "href": "slides/03-mcmc.html#inspecting-the-prior",
    "title": "Markov chain Monte Carlo",
    "section": "Inspecting the prior",
    "text": "Inspecting the prior\n\n###Define hyperparameters\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\na &lt;- 3\nb &lt;- 1"
  },
  {
    "objectID": "slides/03-mcmc.html#comparison-with-olsmle",
    "href": "slides/03-mcmc.html#comparison-with-olsmle",
    "title": "Markov chain Monte Carlo",
    "section": "Comparison with OLS/MLE",
    "text": "Comparison with OLS/MLE\n\n###Compute posterior moments\nvar_beta &lt;- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta &lt;- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n\nLet’s compare the posterior mean with the OLS/MLE estimate for the regression parameter \\(\\boldsymbol{\\beta}\\).\n\n\n\n\n\nparameter\ntrue\nbayes\nols/mle\n\n\n\n\nintercept\n-1.5\n-1.475944\n-1.476236\n\n\nslope\n3.0\n3.296296\n3.296984"
  },
  {
    "objectID": "slides/03-mcmc.html#summarizing-the-posterior",
    "href": "slides/03-mcmc.html#summarizing-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\n\nPosterior means, medians, modes, and variances\nJoint, conditional, and marginal probabilities, for example: \\(P(\\beta_j &lt; c | \\mathbf{Y})\\)\n\\(\\alpha\\)-quantiles: \\(\\{q_{\\alpha} : P(\\beta_j &lt; q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)\\)\n\\(\\ldots\\)\n\nSummarizing a posterior (with known parametric form) is straightforward for basic quantities of interest."
  },
  {
    "objectID": "slides/03-mcmc.html#summarization-can-be-complex",
    "href": "slides/03-mcmc.html#summarization-can-be-complex",
    "title": "Markov chain Monte Carlo",
    "section": "Summarization can be complex",
    "text": "Summarization can be complex\n\nPosteriors are often not available in closed form.\nEven when we have a closed form posterior, it can be difficult to compute summaries of interest.\nFor example, consider \\(P(\\beta_j &lt; c |\\mathbf{Y})\\). What are our options to calculate this probability?\n\nDirect integration (by hand)\nNumerical integration/software packages (e.g., \\(\\texttt{pnorm}\\))\n\n\nThese methods work well for standard posterior quantities and distributions."
  },
  {
    "objectID": "slides/03-mcmc.html#summarizing-the-posterior-1",
    "href": "slides/03-mcmc.html#summarizing-the-posterior-1",
    "title": "Markov chain Monte Carlo",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nHowever, sometimes we will want to summarize other aspects of a posterior distribution.\n\n\\(P(\\beta_j \\in A|\\mathbf{Y})\\) for some arbitrary set \\(A\\)\nMeans and standard deviations of some function of \\(\\beta_j\\), \\(g\\left(\\beta_j\\right)\\)\nThe posterior distribution of functions of many parameters:\n\n\\(|\\beta_1 - \\beta_2|\\), \\(\\beta_1/\\beta_2\\), \\(\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}\\), \\(\\dots\\)\n\nObtaining exact values for these posterior quantities can be difficult or even impossible.\n\nWhat are our options?"
  },
  {
    "objectID": "slides/03-mcmc.html#how-can-we-use-the-posterior-1",
    "href": "slides/03-mcmc.html#how-can-we-use-the-posterior-1",
    "title": "Markov chain Monte Carlo",
    "section": "How can we use the posterior?",
    "text": "How can we use the posterior?\nLet’s simulate some data again:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n\n###Define hyperparameteters\nbeta0 &lt;- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta &lt;- 10\na &lt;- 3\nb &lt;- 1"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta",
    "href": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\boldsymbol{\\beta}\\)",
    "text": "Full conditional for \\(\\boldsymbol{\\beta}\\)\n\\[\\begin{aligned}\nf(\\boldsymbol{\\beta},\\sigma^2 | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta},\\sigma^2) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}^\\top\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{aligned}\\]\nThis is the kernel of a multivariate normal for \\(\\boldsymbol{\\beta}\\), with \\(\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}\\) and \\(\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\). It’s easy to see then that, \\(f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).\\)\n\n\n\n\n\n\nRecall the kernel for the multivariate normal: \\(\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-3",
    "href": "slides/03-mcmc.html#gibbs-sampler-for-linear-regression-3",
    "title": "Markov chain Monte Carlo",
    "section": "Gibbs sampler for linear regression",
    "text": "Gibbs sampler for linear regression\n\nComputing the full conditionals.\n\n\\(\\boldsymbol{\\beta} | \\mathbf{Y}, \\sigma^2 \\sim N \\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}], \\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y})\\right)\\)\nFull conditional for \\(\\sigma^2\\), assuming \\(f(\\sigma^2) \\sim IG(a, b)\\):\n\n\\[\\sigma^2 |  \\mathbf{Y} , \\boldsymbol{\\beta} \\sim IG\\left(a + \\frac{n}{2},b+\\frac{\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)}{2}\\right)\\]\n\nWhy inverse-Gamma (\\(IG\\)) distribution for \\(\\sigma^2\\)?"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta-1",
    "href": "slides/03-mcmc.html#full-conditional-for-boldsymbolbeta-1",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\boldsymbol{\\beta}\\)",
    "text": "Full conditional for \\(\\boldsymbol{\\beta}\\)\nThe full conditional can be found in closed-form and is Gaussian with the following moments: \\[\\begin{aligned}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{aligned}\\]\n\n\n\\(\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}\\)\n\n\n\n\n\\(\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y},\\sigma^2] = \\boldsymbol{\\beta}_0\\)"
  },
  {
    "objectID": "slides/03-mcmc.html#full-conditional-for-sigma2",
    "href": "slides/03-mcmc.html#full-conditional-for-sigma2",
    "title": "Markov chain Monte Carlo",
    "section": "Full conditional for \\(\\sigma^2\\)",
    "text": "Full conditional for \\(\\sigma^2\\)\nFull conditional for \\(\\sigma^2\\), assuming \\(f(\\sigma^2) \\sim IG(a, b)\\):\n\\[\\sigma^2 |  \\mathbf{Y} , \\boldsymbol{\\beta} \\sim IG\\left(a + \\frac{n}{2},b+\\frac{\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\right)}{2}\\right)\\]\n\nWhy inverse-Gamma (\\(IG\\)) distribution for \\(\\sigma^2\\)?"
  },
  {
    "objectID": "slides/03-mcmc.html#sampling-from-the-posterior",
    "href": "slides/03-mcmc.html#sampling-from-the-posterior",
    "title": "Markov chain Monte Carlo",
    "section": "Sampling from the posterior",
    "text": "Sampling from the posterior\nLet’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))"
  },
  {
    "objectID": "slides/07-workflow.html#bayesian-workflow-3",
    "href": "slides/07-workflow.html#bayesian-workflow-3",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\nCheck diagnostics: Use MCMC diagnostics to guarentee that the algorithm converged.\n\n\n\nExamine posterior fit: Create posterior summaries that are relevant to the research question.\n\n\n\n\nCheck predictions: Examing posterior predictive checks.\n\n\n\n\nCompare models: Iterate on model design and choose a model."
  },
  {
    "objectID": "slides/07-workflow.html#research-question",
    "href": "slides/07-workflow.html#research-question",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/07-workflow.html#research-question-1",
    "href": "slides/07-workflow.html#research-question-1",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/07-workflow.html#research-question-2",
    "href": "slides/07-workflow.html#research-question-2",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/07-workflow.html#check-the-model-with-simulated-data",
    "href": "slides/07-workflow.html#check-the-model-with-simulated-data",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\nSanity check:\n\nDraw parameter values from priors.\nGenerate data based on those parameter values.\nFit model to generated data.\nCheck fit is reasonable.\n\n\ngenerated quantities {\n  vector[n] weight;\n  real alpha = normal_rng(150, 5);\n  real beta = normal_rng(0, 10);\n  real sigma = fabs(normal_rng(0, 5));\n  for (i in 1:n) {\n    weight[i] = normal_rng(alpha + beta * height_c[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/07-workflow.html#check-the-model-with-simulated-data-1",
    "href": "slides/07-workflow.html#check-the-model-with-simulated-data-1",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:"
  },
  {
    "objectID": "slides/07-workflow.html#fit-the-model-to-real-data",
    "href": "slides/07-workflow.html#fit-the-model-to-real-data",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\n// saved in linear_regression_workflow.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  vector[n] weight; // outcome vector\n  vector[n] height_c; // covariate vector\n  int&lt;lower = 1&gt; n_pred; // number of new observations\n  vector[n_pred] height_c_pred; // vector for new observations\n}\nparameters {\n  real alpha;\n  real beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(weight | alpha + height_c * beta, sigma); // likelihood\n  target += normal_lpdf(alpha | 150, 5);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n_pred] out_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(alpha + height_c[i] * beta, sigma);\n    log_lik[i] = normal_lpdf(weight[i] | alpha + height_c[i] * beta, sigma);\n  }\n  for (i in 1:n_pred) {\n    out_sample[i] = normal_rng(alpha + height_c_pred[i] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/07-workflow.html#fit-the-model-to-real-data-1",
    "href": "slides/07-workflow.html#fit-the-model-to-real-data-1",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\nstan_data &lt;- list(n = nrow(dat), \n                  height_c = (dat$height - mean(dat$height)), \n                  weight = dat$weight)\nregression_model &lt;- stan_model(file = \"linear_regression_workflow.stan\")\nfit &lt;- sampling(regression_model, data = stan_data)\nprint(fit)\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd   2.5%    50%  97.5% n_eff Rhat\nalpha 152.35    0.01 0.88 150.63 152.35 154.08  3773    1\nbeta    5.69    0.00 0.25   5.21   5.69   6.18  4215    1\nsigma  20.25    0.01 0.60  19.13  20.23  21.50  3571    1\n\nSamples were drawn using NUTS(diag_e) at Wed Nov 27 14:08:19 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/07-workflow.html#check-diagnostics",
    "href": "slides/07-workflow.html#check-diagnostics",
    "title": "Bayesian Workflow",
    "section": "5. Check diagnostics:",
    "text": "5. Check diagnostics:\n\nrstan::traceplot(fit, pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/07-workflow.html#check-diagnostics-1",
    "href": "slides/07-workflow.html#check-diagnostics-1",
    "title": "Bayesian Workflow",
    "section": "5. Check diagnostics:",
    "text": "5. Check diagnostics:\n\nlibrary(bayesplot)\nmcmc_acf(fit, regex_pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/07-workflow.html#examine-posterior-fit",
    "href": "slides/07-workflow.html#examine-posterior-fit",
    "title": "Bayesian Workflow",
    "section": "6.Examine posterior fit:",
    "text": "6.Examine posterior fit:"
  },
  {
    "objectID": "slides/07-workflow.html#examine-posterior-fit-1",
    "href": "slides/07-workflow.html#examine-posterior-fit-1",
    "title": "Bayesian Workflow",
    "section": "6.Examine posterior fit:",
    "text": "6.Examine posterior fit:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior mean and 95% credible interval for \\(\\mu = \\alpha + \\beta \\times height_i\\)."
  },
  {
    "objectID": "slides/07-workflow.html#check-predictions",
    "href": "slides/07-workflow.html#check-predictions",
    "title": "Bayesian Workflow",
    "section": "7.Check predictions:",
    "text": "7.Check predictions:\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(dat$weight, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/07-workflow.html#check-predictions-1",
    "href": "slides/07-workflow.html#check-predictions-1",
    "title": "Bayesian Workflow",
    "section": "7. Check predictions:",
    "text": "7. Check predictions:\nppc_stat(dat$weight, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(dat$weight, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(dat$weight, y_pred, stat = \"q025\")\nppc_stat(dat$weight, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/07-workflow.html#check-predictions-2",
    "href": "slides/07-workflow.html#check-predictions-2",
    "title": "Bayesian Workflow",
    "section": "7. Check predictions:",
    "text": "7. Check predictions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior predictive distribution mean and 95% credible interval, \\(f(weight_i' | weight_{1:n})\\)."
  },
  {
    "objectID": "slides/07-workflow.html#model-comparison-1",
    "href": "slides/07-workflow.html#model-comparison-1",
    "title": "Bayesian Workflow",
    "section": "Model comparison",
    "text": "Model comparison\n\nModel selection criteria are designed to help comparing several models.\nAn ideal criteria will not just based on their fit with training data, but on an estimation of their prediction accuracy with new data.\nCriteria often reward models that offer a good compromise between simplicity and accuracy.\nExamples: Likelihood ratio test, AIC"
  },
  {
    "objectID": "slides/07-model-comparison.html#review-of-last-lecture",
    "href": "slides/07-model-comparison.html#review-of-last-lecture",
    "title": "Model Comparison",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Tuesday, we learned about various ways to check MCMC convergence and model fit.\n\nTraceplots, effective sample size (\\(n_{eff}\\)), MC standard error, \\(\\hat{R}\\), sampling issues\nPosterior predictive checks\nModel checks using shinystan\n\nToday, we will learn about model comparisons."
  },
  {
    "objectID": "slides/07-model-comparison.html#model-comparison-1",
    "href": "slides/07-model-comparison.html#model-comparison-1",
    "title": "Model Comparison",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nWhen comparing models, we prefer models that are closer to the true data-generating process.\nWe need some ways to quantify the degree of closeness to the true model. Note that in this context models refer to the distributional family as well as the parameter values.\nFor example, the model \\(Y_i \\sim N(5,2)\\) is a different model than \\(Y_i \\sim N(3,2)\\), which is a different model than \\(Y_i \\sim Gamma(2,2)\\).\n\nThe first two have the same family but different parameter values (different means, same SD), whereas the last two have different distributional families (Normal vs. Gamma).\n\nOne way to quantify the degree of closeness to the true model is using Kullback-Leibler (KL) divergence."
  },
  {
    "objectID": "slides/07-model-comparison.html#likelihood-ratio",
    "href": "slides/07-model-comparison.html#likelihood-ratio",
    "title": "Model Comparison",
    "section": "Likelihood ratio",
    "text": "Likelihood ratio\n\nThe likelihood ratio method consists in assessing whether increasing the complexity of a model results in a significant improvement of likelihood which justifies this increased complexity.\nIt is a computationally inexpensive method, since it only relies on the value of the total likelihood function, but can only be used to compare nested models.\nNot often used for Bayesian models."
  },
  {
    "objectID": "slides/07-model-comparison.html#information-criteria",
    "href": "slides/07-model-comparison.html#information-criteria",
    "title": "Model Comparison",
    "section": "Information criteria",
    "text": "Information criteria\n\nSeveral information criteria have been proposed that do not require fitting the model several times, including AIC, DIC, and WAIC.\nWe will introduce the information criteria, assuming a likelihood \\(f(\\mathbf{Y} | \\boldsymbol{\\theta})\\) for observed data \\(\\mathbf{Y} = (Y_1,\\ldots,Y_n)\\) with population parameter \\(\\boldsymbol{\\theta}\\).\nInformation criteria are often presented as deviance, defined as, \\(D(\\mathbf{Y}|\\boldsymbol{\\theta}) = −2 \\log f(\\mathbf{Y}|\\boldsymbol{\\theta})\\).\nIdeally, models will have small deviance.\nHowever, if a model is too complex it will have small deviance but be unstable (overfitting)."
  },
  {
    "objectID": "slides/07-model-comparison.html#bayesian-information-criteria-bic",
    "href": "slides/07-model-comparison.html#bayesian-information-criteria-bic",
    "title": "Model Comparison",
    "section": "Bayesian information criteria (BIC)",
    "text": "Bayesian information criteria (BIC)\n\nThe Bayesian information criteria is similar\n\n\\[BIC = D(\\mathbf{Y}|\\hat{\\boldsymbol{\\theta}})+ \\log(n)p\\]\n\nThis is motivated as an approximation to the log Bayes factor of the model compared to the null model.\nHowever, this is only an asymptotic (large \\(n\\)) approximation.\nWith large \\(n\\) the prior is irrelevant, and so this is not satisfying to a subjective Bayesian."
  },
  {
    "objectID": "slides/07-model-comparison.html#deviance-information-criteria-dic",
    "href": "slides/07-model-comparison.html#deviance-information-criteria-dic",
    "title": "Model Comparison",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\nDeviance information criteria (DIC) estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{DIC}} = \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) - p_{\\text{DIC}},\\] where \\(\\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}\\) is a Bayesian point estimate, typically a posterior mean, and \\(p_{\\text{DIC}}\\) is an estimate of the complexity penalty,\n\\[p_{\\text{DIC}} = 2 \\left(\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) - \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[\\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) \\right]\\right).\\]\n\nThe second term can be estimated as a MC integral.\n\\(\\text{DIC} = -2\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) + 2p_{\\text{DIC}}.\\)"
  },
  {
    "objectID": "slides/07-model-comparison.html#deviance-information-criteria-dic-1",
    "href": "slides/07-model-comparison.html#deviance-information-criteria-dic-1",
    "title": "Model Comparison",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nAdvantages of DIC:\n\nThe effective number of parameters is a useful measure of model complexity.\nIntuitively, if there are \\(p\\) parameters and we have uninformative priors then \\(p_D \\approx p\\).\nHowever, \\(p_D \\ll p\\) if there are strong priors.\n\nDisadvantages of DIC:\n\nDIC can only be used to compare models with the same likelihood.\nDIC really only applies when the posterior is approximately normal, and will give misleading results when the posterior is far from normality (e.g., bimodal)."
  },
  {
    "objectID": "slides/07-model-comparison.html#deviance-information-criteria-dic-2",
    "href": "slides/07-model-comparison.html#deviance-information-criteria-dic-2",
    "title": "Model Comparison",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nLet \\(\\bar{D} = \\mathbb{E}[D(\\mathbf{Y}|\\boldsymbol{\\theta})|\\mathbf{Y}]\\) be the posterior mean of the deviance.\nDenote \\(\\hat{\\boldsymbol{\\theta}}\\) as the posterior mean of \\(\\boldsymbol{\\theta}\\).\nThe effective number of parameters is \\(p_D = \\bar{D} − D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}})\\).\nDIC can be written like AIC,\n\n\\[DIC = \\bar{D} + p_D =D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}}) + 2p_D\\]\n\nModels with small \\(\\bar{D}\\) fit the data well.\nModels with small \\(p_D\\) are simple.\nWe select the model with smallest DIC."
  },
  {
    "objectID": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic",
    "href": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic",
    "title": "Model Comparison",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\nWatanabe-Akaike or widely available information criteria (WAIC) estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{WAIC}} = \\text{lppd} - p_{\\text{WAIC}}.\\]\n\nThe log pointwise predictive density (lppd) is given by, \\[\\text{lppd} = \\log \\prod_{i=1}^n f(Y_i | \\mathbf{Y}) =  \\sum_{i=1}^n \\log \\int f\\left(Y_i | \\boldsymbol{\\theta}\\right)f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}.\\]\nlppd can be estimated as, \\(\\sum_{i=1}^n \\log \\left(\\frac{1}{S} \\sum_{s = 1}^S f\\left(Y_i | \\boldsymbol{\\theta}^{(s)}\\right)\\right)\\), where \\(\\boldsymbol{\\theta}^{(s)}\\) are drawn from the posterior."
  },
  {
    "objectID": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-1",
    "href": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-1",
    "title": "Model Comparison",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC has the desirable property of averaging over the posterior distribution, instead of conditioning on a point estimate.\n\\(p_{\\text{WAIC}}\\) can be thought of as an approximation to the number of unconstrained parameters in the model.\nIn practice, \\(p_{\\text{WAIC}_2}\\) is often used, since it is theoretically closer to LOO-CV."
  },
  {
    "objectID": "slides/07-model-comparison.html#cross-validation",
    "href": "slides/07-model-comparison.html#cross-validation",
    "title": "Model Comparison",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nA common approach to compare models is using cross-validation.\nThis is exactly the same procedure used in classical statistics.\nThis operates under the assumption that the true model likely produces better out-of-sample predictions than competing models.\nAdvantages: Simple, intuitive, and broadly applicable.\nDisadvantages: Slow because it requires several model fits and it is hard to say a difference is statistically significant."
  },
  {
    "objectID": "slides/07-model-comparison.html#leave-one-out-cross-validation",
    "href": "slides/07-model-comparison.html#leave-one-out-cross-validation",
    "title": "Model Comparison",
    "section": "Leave-one-out cross-validation",
    "text": "Leave-one-out cross-validation\n\nIt is computationally costly to compute LOO-CV, since it requires fitting the model \\(n\\) times.\nLuckily, there exists an approximation to LOO-CV using Pareto smoothed importance-sampling by Vehtari, Gelman, and Gabry (2017).\nThis can be computed easily using R. We will get to this!"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-using-stan",
    "href": "slides/07-model-comparison.html#computing-waic-using-stan",
    "title": "Model Comparison",
    "section": "Computing WAIC using Stan",
    "text": "Computing WAIC using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/07-model-comparison.html#lets-refit-the-stan-model",
    "href": "slides/07-model-comparison.html#lets-refit-the-stan-model",
    "title": "Model Comparison",
    "section": "Let’s refit the Stan model",
    "text": "Let’s refit the Stan model\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_log_lik_fit.rds\")"
  },
  {
    "objectID": "slides/07-model-comparison.html#we-can-now-compute-the-waic",
    "href": "slides/07-model-comparison.html#we-can-now-compute-the-waic",
    "title": "Model Comparison",
    "section": "We can now compute the WAIC",
    "text": "We can now compute the WAIC\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", \n                           merge_chains = TRUE)\nwaic_fit &lt;- loo::waic(log_lik)\nprint(waic_fit)\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -188.4  7.4\np_waic         3.2  0.6\nwaic         376.9 14.7"
  },
  {
    "objectID": "slides/07-model-comparison.html#compare-models",
    "href": "slides/07-model-comparison.html#compare-models",
    "title": "Model Comparison",
    "section": "8. Compare models",
    "text": "8. Compare models\n\nSuppose we would like to predict weight and we would like to compare our original model with a model that also includes sex. We can compare these models using WAIC."
  },
  {
    "objectID": "slides/07-model-comparison.html#prepare-for-next-class",
    "href": "slides/07-model-comparison.html#prepare-for-next-class",
    "title": "Model Comparison",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02, which was just assigned\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Bayesian Workflow\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/08-workflow.html#review-of-last-lecture",
    "href": "slides/08-workflow.html#review-of-last-lecture",
    "title": "Bayesian Workflow",
    "section": "Review of last lecture",
    "text": "Review of last lecture\nOn Thursday, we learned about various ways compare models.\n\nAIC, DIC, WAIC\nLOO-CV/LOO-IC\n\nToday, we will put these concepts within the larger framework of the Bayesian workflow."
  },
  {
    "objectID": "slides/08-workflow.html#bayes-theorem",
    "href": "slides/08-workflow.html#bayes-theorem",
    "title": "Bayesian Workflow",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) = \\frac{f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{f(\\mathbf{Y})}\\]\n\n\nRethinking Bayes theorem:\n\n\\[f(\\boldsymbol{\\theta} | \\mathbf{Y}) \\propto f(\\mathbf{Y}, \\boldsymbol{\\theta}) = f(\\mathbf{Y} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) \\]\n\n\n\nIn Stan:\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) + \\log f(\\boldsymbol{\\theta})\\]"
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow",
    "href": "slides/08-workflow.html#bayesian-workflow",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\n\n\n\n\nGelman A., Vehtari A., Simpson D., Margossian, C., Carpenter, B. and Yao, Y., Kennedy, L., Gabry, J., Bürkner P. C., & Modrák M. (2020). Bayesian Workflow."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow-1",
    "href": "slides/08-workflow.html#bayesian-workflow-1",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\n\n\nTaken from Bayesian workflow by Francesca Capel\n\n\n\nToday we will talk about a general strategy for taking a question and data to a robust conclusion."
  },
  {
    "objectID": "slides/08-workflow.html#a-simplified-workflow",
    "href": "slides/08-workflow.html#a-simplified-workflow",
    "title": "Bayesian Workflow",
    "section": "A simplified workflow",
    "text": "A simplified workflow\n\nSetting up a full probability model: a joint probability distribution for all observable and unobservable quantities in a problem. The model should be consistent with knowledge about the underlying scientific problem and the data collection process.\nConditioning on observed data: calculating and interpreting the appropriate posterior distribution — the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data.\nEvaluating the fit of the model and the implications of the resulting posterior distribution: how well does the model fit the data, are the substantive conclusions reasonable, and how sensitive are the results to the modeling assumptions in step 1? In response, one can alter or expand the model and repeat the three steps.\n\n\nFrom BDA3."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow-2",
    "href": "slides/08-workflow.html#bayesian-workflow-2",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\nResearch question: What are your dependent and indepednent variables? What associations are you interested in? EDA.\n\n\n\nSpecify likelihood & priors: Use knowledge of the problem to construct a generative model.\n\n\n\n\nCheck the model with simulated data: Generate data from the model and evaluate fit as a sanity check (prior predictive checks).\n\n\n\n\nFit the model to real data: Estimate parameters using MCMC."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-workflow-3",
    "href": "slides/08-workflow.html#bayesian-workflow-3",
    "title": "Bayesian Workflow",
    "section": "Bayesian workflow",
    "text": "Bayesian workflow\n\nCheck diagnostics: Use MCMC diagnostics to guarentee that the algorithm converged.\n\n\n\nExamine posterior fit: Create posterior summaries that are relevant to the research question.\n\n\n\n\nCheck predictions: Examing posterior predictive checks.\n\n\n\n\nCompare models: Iterate on model design and choose a model."
  },
  {
    "objectID": "slides/08-workflow.html#motivating-example-predicting-weight-from-height",
    "href": "slides/08-workflow.html#motivating-example-predicting-weight-from-height",
    "title": "Bayesian Workflow",
    "section": "Motivating example: predicting weight from height",
    "text": "Motivating example: predicting weight from height\nResearch question: We would like to understand the relationship between a person’s height and weight. A few particular questions we have are:\n\nHow much does a person’s weight increase when their height increases?\nHow certain we can be about the magnitude of the increase?\nCan we predict a person’s weight based on their height?\n\nData: We will use the bdims dataset from the openintro package. This dataset contains body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender."
  },
  {
    "objectID": "slides/08-workflow.html#prepare-data",
    "href": "slides/08-workflow.html#prepare-data",
    "title": "Bayesian Workflow",
    "section": "Prepare data",
    "text": "Prepare data\n\nlibrary(openintro)\ndat &lt;- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))\nhead(dat)\n\n    weight   height  sex\n1 144.6231 68.50397 Male\n2 158.2917 69.01579 Male\n3 177.9128 76.18114 Male\n4 160.0554 73.42524 Male\n5 173.7241 73.70083 Male\n6 164.9056 71.45673 Male"
  },
  {
    "objectID": "slides/08-workflow.html#research-question",
    "href": "slides/08-workflow.html#research-question",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/08-workflow.html#research-question-1",
    "href": "slides/08-workflow.html#research-question-1",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/08-workflow.html#research-question-2",
    "href": "slides/08-workflow.html#research-question-2",
    "title": "Bayesian Workflow",
    "section": "1. Research question:",
    "text": "1. Research question:"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors",
    "href": "slides/08-workflow.html#specify-likelihood-priors",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\ndata {\n  int&lt;lower = 1&gt; n;\n  vector[n] height;\n  vector[n] weight;\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-1",
    "href": "slides/08-workflow.html#specify-likelihood-priors-1",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\nparameter {\n  real alpha;\n  real beta;\n  real&lt;lower = 0&gt; sigma;\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-2",
    "href": "slides/08-workflow.html#specify-likelihood-priors-2",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\nConstruct a data generating process.\nWe would like to model weight as a function of height using a linear regression model.\n\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\n\nmodel {\n  target += normal_lpdf(weight | alpha + beta * height, sigma);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-3",
    "href": "slides/08-workflow.html#specify-likelihood-priors-3",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[weight_i = \\alpha + \\beta \\times height_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha\\) is the intercept, or average weight for someone who is zero inches (not a particularly useful number on its own)\n\\(\\beta\\) measures the association between weight and height, in pounds/inch\n\\(\\sigma\\) is the measurement error for the population\n\n\nmodel {\n  target += normal_lpdf(weight | alpha + beta * height, sigma);\n  target += normal_lpdf(alpha | 0, 100);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#specify-likelihood-priors-4",
    "href": "slides/08-workflow.html#specify-likelihood-priors-4",
    "title": "Bayesian Workflow",
    "section": "2. Specify likelihood & priors:",
    "text": "2. Specify likelihood & priors:\n\\[\\mathbb{E}[weight_i] = \\alpha + \\beta \\times (height_i - \\bar{x)},\\quad\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n height_i\\]\nThink about reasonable priors for your parameters:\n\n\\(\\alpha\\) is the intercept, or average weight for someone who is an average height\n\n\nmodel {\n  target += normal_lpdf(weight | alpha + beta * height_c, sigma);\n  target += normal_lpdf(alpha | 150, 5);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#quick-aside",
    "href": "slides/08-workflow.html#quick-aside",
    "title": "Bayesian Workflow",
    "section": "Quick aside",
    "text": "Quick aside\nWhat does it mean to use the prior sigma ~ normal(0, 5)?\n\nWhen a parameter is truncated, for example real&lt;lower = 0&gt; sigma, priors can still be placed across the real line, \\(\\mathbb{R}\\).\n\n\nparameters {\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(sigma | 0, 5);\n}\n\n\nThis specification induces a prior on the truncated space \\(\\mathbb{R}^+\\).\nThe induced prior for sigma is a half-normal distribution."
  },
  {
    "objectID": "slides/08-workflow.html#quick-aside-1",
    "href": "slides/08-workflow.html#quick-aside-1",
    "title": "Bayesian Workflow",
    "section": "Quick aside",
    "text": "Quick aside\n\nThe half-normal is a useful prior for nonnegative parameters that should not be too large and may be very close to zero.\nSimilar distributions for scale parameters are half-t and half-Cauchy priors, these have heavier tales."
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:\n\nDraw parameter values from priors.\nGenerate data based on those parameter values.\nFit model to generated data.\nCheck fit is reasonable.\n\n\ndata {\n  int&lt;lower=0&gt; n;\n  vector[n] height_c;\n}\ngenerated quantities {\n  vector[n] weight;\n  real alpha = normal_rng(150, 5);\n  real beta = normal_rng(0, 10);\n  real sigma = fabs(normal_rng(0, 5));\n  for (i in 1:n) {\n    weight[i] = normal_rng(alpha + beta * height_c[i], sigma);\n  }\n}\n\n\nNeed algorithm = \"Fixed_param\"."
  },
  {
    "objectID": "slides/08-workflow.html#check-the-model-with-simulated-data-1",
    "href": "slides/08-workflow.html#check-the-model-with-simulated-data-1",
    "title": "Bayesian Workflow",
    "section": "3. Check the model with simulated data:",
    "text": "3. Check the model with simulated data:"
  },
  {
    "objectID": "slides/08-workflow.html#fit-the-model-to-real-data",
    "href": "slides/08-workflow.html#fit-the-model-to-real-data",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\n// saved in linear_regression_workflow.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  vector[n] weight; // outcome vector\n  vector[n] height_c; // covariate vector\n  int&lt;lower = 1&gt; n_pred; // number of new observations\n  vector[n_pred] height_c_pred; // vector for new observations\n}\nparameters {\n  real alpha;\n  real beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(weight | alpha + height_c * beta, sigma); // likelihood\n  target += normal_lpdf(alpha | 150, 5);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 5);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n_pred] out_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = normal_rng(alpha + height_c[i] * beta, sigma);\n    log_lik[i] = normal_lpdf(weight[i] | alpha + height_c[i] * beta, sigma);\n  }\n  for (i in 1:n_pred) {\n    out_sample[i] = normal_rng(alpha + height_c_pred[i] * beta, sigma);\n  }\n}"
  },
  {
    "objectID": "slides/08-workflow.html#fit-the-model-to-real-data-1",
    "href": "slides/08-workflow.html#fit-the-model-to-real-data-1",
    "title": "Bayesian Workflow",
    "section": "4. Fit the model to real data:",
    "text": "4. Fit the model to real data:\n\nstan_data &lt;- list(n = nrow(dat), \n                  height_c = (dat$height - mean(dat$height)), \n                  weight = dat$weight)\nregression_model &lt;- stan_model(file = \"linear_regression_workflow.stan\")\nfit &lt;- sampling(regression_model, data = stan_data)\nprint(fit)\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd   2.5%    50%  97.5% n_eff Rhat\nalpha 152.35    0.01 0.88 150.63 152.35 154.08  3773    1\nbeta    5.69    0.00 0.25   5.21   5.69   6.18  4215    1\nsigma  20.25    0.01 0.60  19.13  20.23  21.50  3571    1\n\nSamples were drawn using NUTS(diag_e) at Wed Nov 27 14:08:19 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/08-workflow.html#check-diagnostics",
    "href": "slides/08-workflow.html#check-diagnostics",
    "title": "Bayesian Workflow",
    "section": "5. Check diagnostics:",
    "text": "5. Check diagnostics:\n\nrstan::traceplot(fit, pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/08-workflow.html#check-diagnostics-1",
    "href": "slides/08-workflow.html#check-diagnostics-1",
    "title": "Bayesian Workflow",
    "section": "5. Check diagnostics:",
    "text": "5. Check diagnostics:\n\nlibrary(bayesplot)\nmcmc_acf(fit, regex_pars = c(\"alpha\", \"beta\", \"sigma\"))"
  },
  {
    "objectID": "slides/08-workflow.html#examine-posterior-fit",
    "href": "slides/08-workflow.html#examine-posterior-fit",
    "title": "Bayesian Workflow",
    "section": "6. Examine posterior fit:",
    "text": "6. Examine posterior fit:"
  },
  {
    "objectID": "slides/08-workflow.html#examine-posterior-fit-1",
    "href": "slides/08-workflow.html#examine-posterior-fit-1",
    "title": "Bayesian Workflow",
    "section": "6. Examine posterior fit:",
    "text": "6. Examine posterior fit:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior mean and 95% credible interval for \\(\\mu = \\alpha + \\beta \\times height_i\\)."
  },
  {
    "objectID": "slides/08-workflow.html#check-predictions",
    "href": "slides/08-workflow.html#check-predictions",
    "title": "Bayesian Workflow",
    "section": "7.Check predictions:",
    "text": "7.Check predictions:\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(dat$weight, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/08-workflow.html#check-predictions-1",
    "href": "slides/08-workflow.html#check-predictions-1",
    "title": "Bayesian Workflow",
    "section": "7. Check predictions:",
    "text": "7. Check predictions:\nppc_stat(dat$weight, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(dat$weight, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(dat$weight, y_pred, stat = \"q025\")\nppc_stat(dat$weight, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/08-workflow.html#check-predictions-2",
    "href": "slides/08-workflow.html#check-predictions-2",
    "title": "Bayesian Workflow",
    "section": "7. Check predictions:",
    "text": "7. Check predictions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line corresponds to posterior predictive distribution mean and 95% credible interval, \\(f(weight_{i'} | weight_{1:n})\\)."
  },
  {
    "objectID": "slides/08-workflow.html#shinystan",
    "href": "slides/08-workflow.html#shinystan",
    "title": "Bayesian Workflow",
    "section": "shinystan",
    "text": "shinystan\n\nlibrary(shinystan)\ny &lt;- dat$weight # need to define outcome as a global variable to be accessible\nsso &lt;- shinystan::launch_shinystan(fit)"
  },
  {
    "objectID": "slides/08-workflow.html#compare-models",
    "href": "slides/08-workflow.html#compare-models",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\nSuppose we would like to compare our original model with models that also includes sex and an interaction between sex and height.\n\n\\[\\begin{aligned}\n\\mathbb{E}[weight_i] &= \\alpha + \\beta_1 height_i\\\\\n\\mathbb{E}[weight_i] &= \\alpha + \\beta_1 height_i + \\beta_2 sex_i\\\\\n\\mathbb{E}[weight_i] &= \\alpha + \\beta_1 height_i + \\beta_2 sex_i + \\beta_3 height_i sex_i\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08-workflow.html#model-comparison-1",
    "href": "slides/08-workflow.html#model-comparison-1",
    "title": "Bayesian Workflow",
    "section": "Model comparison",
    "text": "Model comparison\n\nModel selection criteria are designed to help comparing several models.\nAn ideal criteria will not just based on their fit with training data, but on an estimation of their prediction accuracy with new data.\nCriteria often reward models that offer a good compromise between simplicity and accuracy.\nExamples: Likelihood ratio test, AIC"
  },
  {
    "objectID": "slides/08-workflow.html#likelihood-ratio",
    "href": "slides/08-workflow.html#likelihood-ratio",
    "title": "Bayesian Workflow",
    "section": "Likelihood ratio",
    "text": "Likelihood ratio\n\nThe likelihood ratio method consists in assessing whether increasing the complexity of a model results in a significant improvement of likelihood which justifies this increased complexity.\nIt is a computationally inexpensive method, since it only relies on the value of the total likelihood function, but can only be used to compare nested models.\nNot often used for Bayesian models."
  },
  {
    "objectID": "slides/08-workflow.html#information-criteria",
    "href": "slides/08-workflow.html#information-criteria",
    "title": "Bayesian Workflow",
    "section": "Information criteria",
    "text": "Information criteria\n\nSeveral information criteria have been proposed that do not require fitting the model several times.\nMany are functions of the deviance, i.e., twice the negative log likelihood, \\(D(\\mathbf{Y}|\\boldsymbol{\\theta}) = −2 \\log f(\\mathbf{Y}|\\boldsymbol{\\theta}).\\)\nIdeally, models will have small deviance.\nHowever, if a model is too complex it will have small deviance but be unstable (over-fitting)\nThe Akaike information criteria has a complexity penalty \\(AIC = D(\\mathbf{Y}|\\hat{\\boldsymbol{\\theta}})+ 2p\\), where \\(\\hat{\\boldsymbol{\\theta}}\\) is the MLE.\nModel with smaller AIC are preferred."
  },
  {
    "objectID": "slides/08-workflow.html#bayesian-information-criteria-bic",
    "href": "slides/08-workflow.html#bayesian-information-criteria-bic",
    "title": "Bayesian Workflow",
    "section": "Bayesian information criteria (BIC)",
    "text": "Bayesian information criteria (BIC)\n\nThe Bayesian information criteria is similar\n\n\\[BIC = D(\\mathbf{Y}|\\hat{\\boldsymbol{\\theta}})+ \\log(n)p\\]\n\nThis is motivated as an approximation to the log Bayes factor of the model compared to the null model.\nHowever, this is only an asymptotic (large \\(n\\)) approximation.\nWith large \\(n\\) the prior is irrelevant, and so this is not satisfying to a subjective Bayesian."
  },
  {
    "objectID": "slides/08-workflow.html#deviance-information-criteria-dic",
    "href": "slides/08-workflow.html#deviance-information-criteria-dic",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nDIC is a popular Bayesian analog of AIC or BIC.\nUnlike CV, DIC requires only one model fit.\nHowever, proceed with caution.\nDIC really only applies when the posterior is approximately normal, and will give misleading results when the posterior far from normality (e.g., bimodal).\nDIC is also criticized for selecting overly-complex models."
  },
  {
    "objectID": "slides/08-workflow.html#deviance-information-criteria-dic-1",
    "href": "slides/08-workflow.html#deviance-information-criteria-dic-1",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nLet \\(\\bar{D} = \\mathbb{E}[D(\\mathbf{Y}|\\boldsymbol{\\theta})|\\mathbf{Y}]\\) be the posterior mean of the deviance.\nDenote \\(\\hat{\\boldsymbol{\\theta}}\\) as the posterior mean of \\(\\boldsymbol{\\theta}\\).\nThe effective number of parameters is \\(p_D = \\bar{D} − D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}})\\).\nDIC can be written like AIC,\n\n\\[DIC = \\bar{D} + p_D =D(\\mathbf{Y}| \\hat{\\boldsymbol{\\theta}}) + 2p_D\\]\n\nModels with small \\(\\bar{D}\\) fit the data well.\nModels with small \\(p_D\\) are simple.\nWe select the model with smallest DIC."
  },
  {
    "objectID": "slides/08-workflow.html#deviance-information-criteria-dic-2",
    "href": "slides/08-workflow.html#deviance-information-criteria-dic-2",
    "title": "Bayesian Workflow",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nThe effective number of parameters is a useful measure of model complexity\nIntuitively, if there are \\(p\\) parameters and we have uninformative priors then \\(p_D \\approx p\\)\nHowever, \\(p_D \\ll p\\) if there are strong priors.\nAs with AIC or BIC, the actual value is meaningless, only differences are relevant.\nDIC can only be used to compare models with the same likelihood."
  },
  {
    "objectID": "slides/08-workflow.html#watanabe-akaike-information-criteria-waic",
    "href": "slides/08-workflow.html#watanabe-akaike-information-criteria-waic",
    "title": "Bayesian Workflow",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC is an alternative to DIC\nIt is motivated as an approximation to leave-one-out CV\nIn the end WAIC has model-fit and model-complexity components\nIt is used the same as DIC with smaller WAIC preferred\nIn practice the two often give similar results, but WAIC is arguably more theoretically justified"
  },
  {
    "objectID": "slides/08-workflow.html#watanabe-akaike-information-criteria-waic-1",
    "href": "slides/08-workflow.html#watanabe-akaike-information-criteria-waic-1",
    "title": "Bayesian Workflow",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC is written in terms of the posterior of the likelihood rather than parameters\nLet \\(m_i\\) be the posterior mean of \\(f(Y_i|\\boldsymbol{\\theta})\\) and \\(v_i\\) be the posterior variance of \\(\\log f(Y_i|\\boldsymbol{\\theta})\\)\nThe effective model size is \\(p_W = \\sum_{i=1}^n v_i\\)\nThe criteria is\n\n\\[WAIC = −2 \\sum_{i=1}^n \\log (m_i) + 2p_W\\]"
  },
  {
    "objectID": "slides/08-workflow.html#cross-validation",
    "href": "slides/08-workflow.html#cross-validation",
    "title": "Bayesian Workflow",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nCross-validation is a strategy for estimating a model’s predictive accuracy on another sample.\nCross-validation methods capture out-of-sample prediction error by fitting the model to training data and evaluating this predictive accuracy on a holdout set.\nThey can be computationally expensive but avoid the problem of overfitting.\nThe sample data is divided into a number of chunks, called “folds” and the model is asked to predict each fold, after training on all the others. The number of folds is given by \\(k\\).\nLeave-one-out cross-validation (LOO-CV) represents cross-validation at the extreme, when \\(k = n\\)."
  },
  {
    "objectID": "slides/08-workflow.html#leave-one-out-cross-validation",
    "href": "slides/08-workflow.html#leave-one-out-cross-validation",
    "title": "Bayesian Workflow",
    "section": "Leave-one-out cross-validation",
    "text": "Leave-one-out cross-validation\n\nIt is computationally costly to compute LOO-CV, since it requires fitting the model \\(n\\) times.\nLuckily, there exists an approximation to LOO-CV using Pareto smoothed importance-sampling by Vehtari, Gelman, and Gabry (2017).\nThis can be computed easily using R."
  },
  {
    "objectID": "slides/08-workflow.html#computing-waic-using-stan",
    "href": "slides/08-workflow.html#computing-waic-using-stan",
    "title": "Bayesian Workflow",
    "section": "Computing WAIC using Stan",
    "text": "Computing WAIC using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/08-workflow.html#lets-refit-the-stan-model",
    "href": "slides/08-workflow.html#lets-refit-the-stan-model",
    "title": "Bayesian Workflow",
    "section": "Let’s refit the Stan model",
    "text": "Let’s refit the Stan model\n\n###Create stan data object\nstan_data &lt;- list(n = n, p = p, Y = Y, X = X,\n                  beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                  n_pred = n_pred, X_pred = X_pred)\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model and save\nfit &lt;- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000)\nsaveRDS(fit, file = \"linear_regression_ppd_log_lik_fit.rds\")"
  },
  {
    "objectID": "slides/08-workflow.html#we-can-now-compute-the-waic",
    "href": "slides/08-workflow.html#we-can-now-compute-the-waic",
    "title": "Bayesian Workflow",
    "section": "We can now compute the WAIC",
    "text": "We can now compute the WAIC\n\nlibrary(loo)\nlog_lik_model1 &lt;- loo::extract_log_lik(fit_model1, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model2 &lt;- loo::extract_log_lik(fit_model2, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model3 &lt;- loo::extract_log_lik(fit_model3, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model1 &lt;- loo::waic(log_lik_model1)\nwaic_model2 &lt;- loo::waic(log_lik_model2)\nwaic_model3 &lt;- loo::waic(log_lik_model3)\nloo::loo_compare(waic_model1, waic_model2, waic_model3)\n\n       elpd_diff se_diff\nmodel2   0.0       0.0  \nmodel3  -0.8       2.4  \nmodel1 -25.3       5.8"
  },
  {
    "objectID": "slides/08-workflow.html#compare-models-1",
    "href": "slides/08-workflow.html#compare-models-1",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\n###Compute individual WAIC\nlibrary(loo)\nlog_lik_model1 &lt;- loo::extract_log_lik(fit_model1, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model2 &lt;- loo::extract_log_lik(fit_model2, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model3 &lt;- loo::extract_log_lik(fit_model3, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model1 &lt;- loo::waic(log_lik_model1)\nwaic_model2 &lt;- loo::waic(log_lik_model2)\nwaic_model3 &lt;- loo::waic(log_lik_model3)\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"height_only\" = waic_model1, \"height_sex\" = waic_model2, \"interaction\" = waic_model3))\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n            elpd_diff se_diff  elpd_waic se_elpd_waic p_waic   se_p_waic\nheight_sex      0.00      0.00 -2229.19     23.36         4.84     0.88 \ninteraction    -0.82      2.41 -2230.02     23.44         5.70     0.97 \nheight_only   -25.33      5.80 -2254.52     21.32         3.54     0.62 \n            waic     se_waic \nheight_sex   4458.39    46.73\ninteraction  4460.04    46.88\nheight_only  4509.04    42.64"
  },
  {
    "objectID": "slides/08-workflow.html#prepare-for-next-class",
    "href": "slides/08-workflow.html#prepare-for-next-class",
    "title": "Bayesian Workflow",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Nonlinear Regression\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/01-welcome.html",
    "href": "slides/01-welcome.html",
    "title": "Welcome to BIOSTAT 725!",
    "section": "",
    "text": "Education and career journey\n\nBS in Statistical Science from Duke University\nPhD in Biostatistics from University of North Carolina - Chapel Hill\nPostdoc in Duke Forge: Duke’s Center for Actionable Health Data Science\nNIH/NEI Pathway to Independence Fellow (K99/R00)\nAssistant Professor, Department of Biostatistics & Bioinformatics and Statistical Science at Duke; Faculty Affiliate of Duke AI Health\n\nWork focuses on developing data science tools to improve patient experience using biomedical data (including EHR)\nDad of 4 and 6 year old daughters 🙂\n\n\n\n\n\n\n\nDr. Youngsoo Baek (PhD)\n\nPhd in Statistical Science from Duke University\nPostdoc in Biostatistics & Bioinformatics\n\n\n\n\n\n\nClick on the link or scan the QR code to answer the Ed Discussion poll\nhttps://edstem.org/us/courses/62513/discussion/625046\n\n\n\n\n\n\n\n\n\n\nIntroduction to the course\nSyllabus activity\nReview of probability"
  },
  {
    "objectID": "slides/07-model-comparison.html#model-selection",
    "href": "slides/07-model-comparison.html#model-selection",
    "title": "Model Comparison",
    "section": "Model selection",
    "text": "Model selection\n\nOften we have many potential models in our arsenal.\nFor a given dataset, how do determine whether a simple model is sufficient or if we need to bring out the “big guns”?\nIs there a “right” model? Probably not.\nA statistical model is a mathematical representation of the system that includes errors and biases in the observation process.\nAll models are simplifications of reality.\nWe want a model that is as simple as possible yet seems to fit the data reasonably well.\nAn ideal criteria will not just based on their fit with training data, but on an estimation of their prediction accuracy with new data."
  },
  {
    "objectID": "slides/07-model-comparison.html#k-fold-cross-validation",
    "href": "slides/07-model-comparison.html#k-fold-cross-validation",
    "title": "Model Comparison",
    "section": "K-fold cross-validation",
    "text": "K-fold cross-validation\n\nSplit the data into \\(K\\) equally-sized groups.\nSet aside group \\(k\\) as test set and fit the model to the remaining \\(K − 1\\) groups.\nMake predictions for the test set \\(k\\) based on the model fit to the training data.\nRepeat steps 1 and 2 for \\(k = 1, \\dots, K\\) giving a predicted value \\(\\widehat{Y}_i\\) for all \\(n\\) observations.\nMeasure prediction accuracy, e.g.,\n\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2.\\]"
  },
  {
    "objectID": "slides/07-model-comparison.html#variants-of-cross-validation",
    "href": "slides/07-model-comparison.html#variants-of-cross-validation",
    "title": "Model Comparison",
    "section": "Variants of cross-validation",
    "text": "Variants of cross-validation\n\nUsually \\(K\\) is either 5 or 10.\n\\(K = n\\) is called leave-one-out cross-validation (LOO-CV), which is great but slow.\nThe predicted value \\(\\widehat{Y}_i\\) can be either the posterior predictive mean or median.\nMean squared error (MSE) can be replaced with mean absolute deviation (MAD),\n\n\\[MAD = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\widehat{Y}_i|.\\]"
  },
  {
    "objectID": "slides/07-model-comparison.html#cross-validation-1",
    "href": "slides/07-model-comparison.html#cross-validation-1",
    "title": "Model Comparison",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nCross-validation is a strategy for estimating a model’s predictive accuracy on another sample.\nCross-validation methods capture out-of-sample prediction error by fitting the model to training data and evaluating this predictive accuracy on a holdout set.\nThey can be computationally expensive but avoid the problem of overfitting.\nThe sample data is divided into a number of chunks, called “folds” and the model is asked to predict each fold, after training on all the others. The number of folds is given by \\(k\\).\nLeave-one-out cross-validation (LOO-CV) represents cross-validation at the extreme, when \\(k = n\\)."
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-and-loo-cv-using-stan",
    "href": "slides/07-model-comparison.html#computing-waic-and-loo-cv-using-stan",
    "title": "Model Comparison",
    "section": "Computing WAIC and LOO-CV using Stan",
    "text": "Computing WAIC and LOO-CV using Stan\nWe need to update the generated quantities code block.\n\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}"
  },
  {
    "objectID": "slides/07-model-comparison.html#lets-simulate-some-data",
    "href": "slides/07-model-comparison.html#lets-simulate-some-data",
    "title": "Model Comparison",
    "section": "Let’s simulate some data:",
    "text": "Let’s simulate some data:\n\n###True parameters\nsigma &lt;- 1.5 # true measurement error\nbeta &lt;- matrix(c(-1.5, 3, 1), ncol = 1) # true beta\n\n###Simulation settings\nn &lt;- 100 # number of observations\nn_pred &lt;- 10 # number of predicted observations\np &lt;- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX &lt;- cbind(1, matrix(rnorm(n * p), ncol = p))\nY &lt;- as.numeric(X %*% beta + rnorm(n, 0, sigma))\nX_pred &lt;- cbind(1, matrix(rnorm(n_pred * p), ncol = p))\nY_pred &lt;- as.numeric(X_pred %*% beta + rnorm(n_pred, 0, sigma))"
  },
  {
    "objectID": "slides/07-model-comparison.html#an-example-model-comparison",
    "href": "slides/07-model-comparison.html#an-example-model-comparison",
    "title": "Model Comparison",
    "section": "An example model comparison",
    "text": "An example model comparison\nTrue Model: \\(\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}\\)\nModel 1: \\(\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1}\\)\nModel 2: \\(\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}\\)"
  },
  {
    "objectID": "slides/07-model-comparison.html#fit-model-1",
    "href": "slides/07-model-comparison.html#fit-model-1",
    "title": "Model Comparison",
    "section": "Fit model 1",
    "text": "Fit model 1\n\n###Create stan data object\nstan_data_model1 &lt;- list(n = n, p = p - 1, Y = Y, X = X[, -3],\n                         beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                         n_pred = n_pred, X_pred = X_pred[, -3])\n  \n###Compile model separately\nstan_model &lt;- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model 1 and save\nfit_model1 &lt;- sampling(stan_model, data = stan_data_model1, \n                chains = 4, iter = 1000)\nsaveRDS(fit_model1, file = \"linear_regression_ppd_log_lik_fit_model1.rds\")"
  },
  {
    "objectID": "slides/07-model-comparison.html#fit-model-2",
    "href": "slides/07-model-comparison.html#fit-model-2",
    "title": "Model Comparison",
    "section": "Fit model 2",
    "text": "Fit model 2\n\n###Create stan data object\nstan_data_model2 &lt;- list(n = n, p = p, Y = Y, X = X,\n                         beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                         n_pred = n_pred, X_pred = X_pred)\n\n###Run model 2 and save\nfit_model2 &lt;- sampling(stan_model, data = stan_data_model2, \n                chains = 4, iter = 1000)\nsaveRDS(fit_model2, file = \"linear_regression_ppd_log_lik_fit_model2.rds\")"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic",
    "href": "slides/07-model-comparison.html#computing-waic",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\nBegin by extracting the log-likelihood values from the model.\nWe will use the loo package.\n\n\n###Load loo package\nlibrary(loo)\n\n###Extract log likelihood\nlog_lik_model1 &lt;- loo::extract_log_lik(fit_model1, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model2 &lt;- loo::extract_log_lik(fit_model2, parameter_name = \"log_lik\", merge_chains = TRUE)\n\n###Explore the object\nclass(log_lik_model1)\n\n[1] \"matrix\" \"array\" \n\ndim(log_lik_model1)\n\n[1] 2000  100"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-1",
    "href": "slides/07-model-comparison.html#computing-waic-1",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\n###Compute WAIC for the two models\nwaic_model1 &lt;- loo::waic(log_lik_model1)\nwaic_model2 &lt;- loo::waic(log_lik_model2)\n\n###Inspect WAIC for model 1\nwaic_model1\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -201.0  6.6\np_waic         3.0  0.5\nwaic         401.9 13.1"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-loo-ic",
    "href": "slides/07-model-comparison.html#computing-loo-ic",
    "title": "Model Comparison",
    "section": "Computing LOO-IC",
    "text": "Computing LOO-IC\n\n###Compute LOO-IC for the two models\nloo_model1 &lt;- loo::loo(log_lik_model1)\nloo_model2 &lt;- loo::loo(log_lik_model2)\n\n###Make a comparison\ncomp &lt;- loo::loo_compare(list(\"true\" = loo_model2, \"misspec\" = loo_model1))\nprint(comp, digits = 2)\n\n        elpd_diff se_diff\ntrue      0.00      0.00 \nmisspec -22.08      5.76 \n\nprint(comp, digits = 2, simplify = FALSE)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic  \ntrue       0.00      0.00 -178.88     6.57        3.90    0.58   357.77\nmisspec  -22.08      5.76 -200.96     6.56        3.00    0.52   401.93\n        se_looic\ntrue      13.14 \nmisspec   13.12"
  },
  {
    "objectID": "slides/07-model-comparison.html#example-data",
    "href": "slides/07-model-comparison.html#example-data",
    "title": "Model Comparison",
    "section": "Example data",
    "text": "Example data\nData: We will use the bdims dataset from the openintro package. This dataset contains body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender."
  },
  {
    "objectID": "slides/07-model-comparison.html#prepare-data",
    "href": "slides/07-model-comparison.html#prepare-data",
    "title": "Model Comparison",
    "section": "Prepare data",
    "text": "Prepare data\n\nlibrary(openintro)\ndat &lt;- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))"
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting"
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting-1",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting-1",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting"
  },
  {
    "objectID": "slides/07-model-comparison.html#model-comparison",
    "href": "slides/07-model-comparison.html#model-comparison",
    "title": "Model Comparison",
    "section": "Model comparison",
    "text": "Model comparison\n\nIn statistical modeling, a more complex model almost always results in a better fit to the data.\n\nA more complex model means one with more parameters.\n\nIf one has 10 observations, one can have a model with 10 parameters that can perfectly predict every single data point (by just having a parameter to predict each data point).\nThere are two problems with overly complex models.\n\nThey become increasingly hard to interpret (think a straight line versus a polynomial).\nThey are more at risk of overfitting, such that it does not work for future observations."
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting-2",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting-2",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\nWith more complex models, out-of-sample prediction becomes worse.\nThis is because when you use a complex model in a data set, it tailors the coefficients to any sampling errors and noise in the data such that it will not generalize to new observations.\nTherefore, our goal in model comparison is to choose a model with the following two properties:\n\nIt is complex enough to capture the essence of the data generation process (and thus avoid underfitting),\nIt avoids overfitting to make the model usefull for predicting new observations."
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting-3",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting-3",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\nWith more complex models, out-of-sample prediction becomes worse.\nThis is because when you use a complex model in a data set, it tailors the coefficients to any sampling errors and noise in the data such that it will not generalize to new observations.\nTherefore, our goal in model comparison is to choose a model with the following two properties:\n\nIt is complex enough to capture the essence of the data generation process (and thus avoid underfitting),\nIt avoids overfitting to make the model usefull for predicting new observations."
  },
  {
    "objectID": "slides/07-model-comparison.html#overfitting-and-underfitting-4",
    "href": "slides/07-model-comparison.html#overfitting-and-underfitting-4",
    "title": "Model Comparison",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\nWith more complex models, out-of-sample prediction becomes worse.\nThis is because when you use a complex model in a data set, it tailors the coefficients to any sampling errors and noise in the data such that it will not generalize to new observations.\nTherefore, our goal in model comparison is to choose a model with the following two properties:\n\nIt is complex enough to capture the essence of the data generation process (and thus avoid underfitting),\nIt avoids overfitting to make the model usefull for predicting new observations."
  },
  {
    "objectID": "slides/07-model-comparison.html#finding-an-optimal-model",
    "href": "slides/07-model-comparison.html#finding-an-optimal-model",
    "title": "Model Comparison",
    "section": "Finding an optimal model",
    "text": "Finding an optimal model\n\nTrade-off between overfitting and underfitting (in machine learning this is commonly called bias-variance trade-off).\n\nA simple model tends to produce biased predictions because it does not capture the essence of the data generating process.\nA model that is overly complex is unbiased but results in a lot of uncertainty in the prediction.\n\nPolynomials are merely one example of comparing simple to complex models. You can think about:\n\nModels with and without interactions,\nModels with a few predictors versus hundreds of predictors,\nRegression analyses versus hierarchical models, etc."
  },
  {
    "objectID": "slides/07-model-comparison.html#kullback-leibler-divergence",
    "href": "slides/07-model-comparison.html#kullback-leibler-divergence",
    "title": "Model Comparison",
    "section": "Kullback-Leibler divergence",
    "text": "Kullback-Leibler divergence\n\nFor two models, \\(M_0\\) and \\(M_1\\), the KL divergence is given by,\n\n\\[\\begin{aligned}\nD_{KL}\\left(M_0 | M_1\\right) &= \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log\\frac{f_{M_0}(\\mathbf{Y})}{f_{M_1}(\\mathbf{Y})} d\\mathbf{Y}\\\\\n&\\hspace{-1.5in}= \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_0}(\\mathbf{Y})d\\mathbf{Y} - \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_1}(\\mathbf{Y})d\\mathbf{Y}\n\\end{aligned}\\]\n\nNote that \\(D_{KL}\\) is not considered a distance, because it is not strictly symmetric, \\(D_{KL}\\left(M_0 | M_1\\right) \\neq D_{KL}\\left(M_1 | M_0\\right)\\)."
  },
  {
    "objectID": "slides/07-model-comparison.html#kullback-leibler-divergence-1",
    "href": "slides/07-model-comparison.html#kullback-leibler-divergence-1",
    "title": "Model Comparison",
    "section": "Kullback-Leibler divergence",
    "text": "Kullback-Leibler divergence\nAs an example, assume that the data are generated by a true model \\(M_0\\), and we have two candidate models \\(M_1\\) and \\(M_2\\), where\n\n\n\n\\(M_0: Y_i \\sim N(3,2)\\)\n\\(M_1: Y_i \\sim N(3.5, 2.5)\\)\n\\(M_2: Y_i \\sim Cauchy(3,2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{KL}(M_0 |M_1) = 0.063\\), \\(D_{KL}(M_0 | M_1) = 0.259\\), so \\(M_1\\) is a better model than \\(M_2\\)."
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-divergence",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-divergence",
    "title": "Model Comparison",
    "section": "Comparing models using KL divergence",
    "text": "Comparing models using KL divergence\n\nNote that in the expression of \\(D_{KL}\\), when talking about the same target model, the first term is always the same and describes the “true” model, \\(M_0\\).\nTherefore, it is sufficient to compare models on the second term,\n\\[\\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_1}(\\mathbf{Y})d\\mathbf{Y},\\] which can also be written as, \\(\\mathbb{E}_{M_0}\\left[\\log f_{M_1}(\\mathbf{Y})\\right].\\)\nThis term is the expected log predictive density (elpd).\nA larger elpd is preferred."
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nNote that in the expression of \\(D_{KL}\\), when talking about the same target model, the first term is always the same and describes the true model, \\(M_0\\).\nTherefore, it is sufficient to compare models on the second term,\n\\[\\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_1}(\\mathbf{Y})d\\mathbf{Y},\\] which can also be written as, \\(\\mathbb{E}_{M_0}\\left[\\log f_{M_1}(\\mathbf{Y})\\right].\\)\nThis term is the expected log predictive density (elpd).\nA larger elpd is preferred. Why?"
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-1",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-1",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nIn the real world, we do not know \\(M_0\\).\n\nIf we knew, then we would just need to choose \\(M_0\\) as our model and there will be no problem about model comparisons.\nEven if we knew the true model, we would still need to estimate the parameter values.\n\nThus, we cannot compute elpd, since the expectation is over \\(f_{M_0}(\\mathbf{Y})\\).\nWe need to estimate elpd!"
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-2",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-2",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nelpd is an expectation, so we can think about estimating it using Monte Carlo sampling, \\[\\frac{1}{S}\\sum_{s = 1}^S\\log f_{M_1}\\left(\\mathbf{Y}^{(s)}\\right)\\rightarrow \\mathbb{E}_{M_0}\\left[\\log f_{M_1}(\\mathbf{Y})\\right], \\quad \\mathbf{Y}^{(s)} \\sim f_{M_0}(\\mathbf{Y}).\\]\n\nWe need to find a way to approximate, \\(f_{M_0}\\left(\\mathbf{Y}^{(s)}\\right)\\).\n\nA naive way to approximate \\(f(\\mathbf{Y}^{(s)})\\) is to assume that the distribution of the observed data is the true model.\n\nThis is equivalent to assuming that \\(\\mathbf{Y}^{(s)} \\sim \\{\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n\\}\\).\nThis leads to an overly optimistic estimate and favors complex models."
  },
  {
    "objectID": "slides/07-model-comparison.html#overview-of-comparison-methods",
    "href": "slides/07-model-comparison.html#overview-of-comparison-methods",
    "title": "Model Comparison",
    "section": "Overview of comparison methods",
    "text": "Overview of comparison methods\n\nInformation criteria: AIC, DIC, and WAIC, which estimate the elpd in the current sample, minus a correction factor.\nCross validation: A method that splits the current sample into \\(K\\) parts, estimates the parameters in \\(K − 1\\) parts, and estimates the elpd in the remaining 1 part.\n\n\nA special case is when \\(K = n\\) so that each time one uses \\(n-1\\) data points to estimate the model parameters, and estimates the elpd for the observation that was left out. This is called leave-one-out cross-validation (LOO-CV)."
  },
  {
    "objectID": "slides/07-model-comparison.html#model-fit-an-example-data-set",
    "href": "slides/07-model-comparison.html#model-fit-an-example-data-set",
    "title": "Model Comparison",
    "section": "Model fit: an example data set",
    "text": "Model fit: an example data set\n\nLet’s explore the idea of model fit using an example dataset from the openintro package called bdims.\nThis dataset contains body girth measurements and skeletal diameter measurements.\nToday we will explore the association between height and weight."
  },
  {
    "objectID": "slides/07-model-comparison.html#models-of-increasing-complexity",
    "href": "slides/07-model-comparison.html#models-of-increasing-complexity",
    "title": "Model Comparison",
    "section": "Models of increasing complexity",
    "text": "Models of increasing complexity\n\nWhen using height to predict weight, we can models of increasing complexity using higher order polynomials.\nLet’s fit the following models to the subset of 10 data points:\n\n\\[\\begin{aligned}\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2 + \\beta_3 height_i^3\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2 + \\beta_3 height_i^3 + \\beta_4 height_i^4\n\\end{aligned}\\]\n\nWe can compare these models using standard measures of goodness-of-fit, including \\(R^2\\) and root mean squared error (RMSE)."
  },
  {
    "objectID": "slides/07-model-comparison.html#comparing-models-using-kl-3",
    "href": "slides/07-model-comparison.html#comparing-models-using-kl-3",
    "title": "Model Comparison",
    "section": "Comparing models using KL",
    "text": "Comparing models using KL\n\nA better way to estimate elpd is to collect data on a new independent sample that is believed to share the same data generating process as the current sample, and estimate elpd on the new sample.\n\nThis is called out-of-sample validation.\nThe problem, of course, is we usually do not have the resources to collect a new sample.\n\nTherefore, statisticians have worked hard to find ways to estimate elpd from the current sample, and there are two broad approaches, information criteria and cross-validation."
  },
  {
    "objectID": "slides/07-model-comparison.html#akaike-information-criteria-aic",
    "href": "slides/07-model-comparison.html#akaike-information-criteria-aic",
    "title": "Model Comparison",
    "section": "Akaike information criteria (AIC)",
    "text": "Akaike information criteria (AIC)\nAkaike information criteria (AIC) estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{AIC}} = \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}) - p,\\] where \\(p\\) is the number of parameters estimated in the model and \\(\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}\\) is the MLE point estimate.\n\n\\(\\text{AIC} = -2\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}) + 2p\\)\n\\(p\\) is an adjustment for overfitting, but once we go beyond linear models, we cannot simply add \\(p\\).\nInformative priors tend to reduce the amount of overfitting.\nModel with smaller AIC are preferred."
  },
  {
    "objectID": "slides/07-model-comparison.html#deviance-information-criteria-dic-3",
    "href": "slides/07-model-comparison.html#deviance-information-criteria-dic-3",
    "title": "Model Comparison",
    "section": "Deviance information criteria (DIC)",
    "text": "Deviance information criteria (DIC)\n\nThe effective number of parameters is a useful measure of model complexity.\nIntuitively, if there are \\(p\\) parameters and we have uninformative priors then \\(p_D \\approx p\\).\nHowever, \\(p_D \\ll p\\) if there are strong priors.\nAs with AIC or BIC, the actual value is meaningless, only differences are relevant.\nDIC can only be used to compare models with the same likelihood."
  },
  {
    "objectID": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-2",
    "href": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-2",
    "title": "Model Comparison",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC has the desirable property of averaging over the posterior distribution, instead of conditioning on a point estimate.\n\\(p_{\\text{WAIC}}\\) can be thought of as an approximation to the number of unconstrained parameters in the model.\nIn practice, \\(p_{\\text{WAIC}_2}\\) is often used, since it is theoretically closer to LOO-CV."
  },
  {
    "objectID": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-3",
    "href": "slides/07-model-comparison.html#watanabe-akaike-information-criteria-waic-3",
    "title": "Model Comparison",
    "section": "Watanabe-Akaike information criteria (WAIC)",
    "text": "Watanabe-Akaike information criteria (WAIC)\n\nWAIC is written in terms of the posterior of the likelihood rather than parameters.\nLet \\(m_i\\) be the posterior mean of \\(f(Y_i|\\boldsymbol{\\theta})\\) and \\(v_i\\) be the posterior variance of \\(\\log f(Y_i|\\boldsymbol{\\theta})\\)\nThe effective model size is \\(p_W = \\sum_{i=1}^n v_i\\)\nThe criteria is,\n\n\\[WAIC = −2 \\sum_{i=1}^n \\log (m_i) + 2p_W.\\]"
  },
  {
    "objectID": "slides/07-model-comparison.html#leave-one-out-cross-validation-loo-cv",
    "href": "slides/07-model-comparison.html#leave-one-out-cross-validation-loo-cv",
    "title": "Model Comparison",
    "section": "Leave-one-out cross-validation (LOO-CV)",
    "text": "Leave-one-out cross-validation (LOO-CV)\n\nAssume the data are partitioned into a training set, \\(\\mathbf{Y}_{\\text{train}}\\) and a holdout set \\(\\mathbf{Y}_{\\text{test}}\\), thus yielding a posterior distribution \\(f(\\boldsymbol{\\theta} | \\mathbf{Y}_{\\text{train}})\\).\nIn the setting of LOO-CV, we have \\(n\\) different \\(f\\left(\\boldsymbol{\\theta} | \\mathbf{Y}_{-i}\\right)\\).\nThe Bayesian LOO-CV estimate of out-of-sample predictive fit is \\[\\text{lppd}_{\\text{LOO-CV}} = \\sum_{i=1}^n \\log f\\left(\\boldsymbol{\\theta} | \\mathbf{Y}_{-i}\\right),\\]\nThe estimated number of parameters can be computed as,\n\n\\[p_{\\text{LOO-CV}} = \\text{lppd} - \\text{lppd}_{\\text{LOO-CV}}.\\] - This also referred to as leave-one-out information criteria (LOO-IC)"
  },
  {
    "objectID": "slides/07-model-comparison.html#leave-one-out-cross-validation-loo-cv-1",
    "href": "slides/07-model-comparison.html#leave-one-out-cross-validation-loo-cv-1",
    "title": "Model Comparison",
    "section": "Leave-one-out cross-validation (LOO-CV)",
    "text": "Leave-one-out cross-validation (LOO-CV)\nLOO-CV estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{LOO-CV}} = \\text{lppd}_{\\text{LOO-CV}} - p_{\\text{WAIC}} = \\text{lppd}_{\\text{LOO-CV}}.\\]\n\nUnder some common models there are shortcuts for computing it, however in general these do not exist.\nWAIC can be treated as a fast approximation of LOO-CV.\nIn Stan, LOO-CV is approximated using the Pareto smoothed importance sampling (PSIS) to make the process faster, without having to repeat the process \\(n\\) times."
  },
  {
    "objectID": "slides/07-model-comparison.html#model-fit-an-example-data-set-1",
    "href": "slides/07-model-comparison.html#model-fit-an-example-data-set-1",
    "title": "Model Comparison",
    "section": "Model fit: an example data set",
    "text": "Model fit: an example data set\n\nlibrary(openintro)\ndat &lt;- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))"
  },
  {
    "objectID": "slides/07-model-comparison.html#waic",
    "href": "slides/07-model-comparison.html#waic",
    "title": "Model Comparison",
    "section": "WAIC",
    "text": "WAIC\nThere are two common estimates of \\(p_{\\text{WAIC}}\\), both of which can be estimated using MC samples of the posterior. \\[\\begin{aligned}\np_{\\text{WAIC}_1} &= 2 \\sum_{i=1}^n\\left(\\log \\left( \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[f(Y_i | \\boldsymbol{\\theta})\\right]\\right) - \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[\\log f(Y_i | \\boldsymbol{\\theta}) \\right]\\right)\\\\\np_{\\text{WAIC}_2} &= \\sum_{i=1}^n \\mathbb{V}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left(\\log f\\left(Y_i | \\mathbf{\\theta}\\right)\\right)\n\\end{aligned}\\]\n\n\\(\\text{WAIC} = -2 \\text{lppd} + 2p_{\\text{WAIC}}.\\)"
  },
  {
    "objectID": "slides/07-model-comparison.html#waic-1",
    "href": "slides/07-model-comparison.html#waic-1",
    "title": "Model Comparison",
    "section": "WAIC",
    "text": "WAIC\n\nWAIC has the desirable property of averaging over the posterior distribution, instead of conditioning on a point estimate.\n\\(p_{\\text{WAIC}}\\) can be thought of as an approximation to the number of unconstrained parameters in the model.\nIn practice, \\(p_{\\text{WAIC}_2}\\) is often used, since it is theoretically closer to LOO-CV."
  },
  {
    "objectID": "slides/07-model-comparison.html#loo-cv",
    "href": "slides/07-model-comparison.html#loo-cv",
    "title": "Model Comparison",
    "section": "LOO-CV",
    "text": "LOO-CV\nLOO-CV estimates the elpd as,\n\\[\\widehat{\\text{elpd}}_{\\text{LOO-CV}} = \\text{lppd}_{\\text{LOO-CV}} - p_{\\text{WAIC}} = \\text{lppd}_{\\text{LOO-CV}}.\\]\n\nUnder some common models there are shortcuts for computing it, however in general these do not exist.\nWAIC can be treated as a fast approximation of LOO-CV.\nIn Stan, LOO-CV is approximated using the Pareto smoothed importance sampling (PSIS) to make the process faster, without having to repeat the process \\(n\\) times."
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-2",
    "href": "slides/07-model-comparison.html#computing-waic-2",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\n###Inspect WAIC for model 2\nwaic_model2\n\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -178.9  6.6\np_waic         3.9  0.6\nwaic         357.7 13.1"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-waic-3",
    "href": "slides/07-model-comparison.html#computing-waic-3",
    "title": "Model Comparison",
    "section": "Computing WAIC",
    "text": "Computing WAIC\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"true\" = waic_model2, \"misspec\" = waic_model1))\nprint(comp_waic, digits = 2)\n\n        elpd_diff se_diff\ntrue      0.00      0.00 \nmisspec -22.09      5.76 \n\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \ntrue       0.00      0.00 -178.86      6.57         3.88    0.58    357.72\nmisspec  -22.09      5.76 -200.95      6.56         2.99    0.52    401.90\n        se_waic\ntrue      13.14\nmisspec   13.12"
  },
  {
    "objectID": "slides/07-model-comparison.html#computing-loo-cvloo-ci",
    "href": "slides/07-model-comparison.html#computing-loo-cvloo-ci",
    "title": "Model Comparison",
    "section": "Computing LOO-CV/LOO-CI",
    "text": "Computing LOO-CV/LOO-CI\n\n###Compute LOO-IC for the two models\nloo_model1 &lt;- loo::loo(log_lik_model1)\nloo_model2 &lt;- loo::loo(log_lik_model2)\n\n###Make a comparison\ncomp &lt;- loo::loo_compare(list(\"true\" = loo_model2, \"misspec\" = loo_model1))\nprint(comp, digits = 2)\n\n        elpd_diff se_diff\ntrue      0.00      0.00 \nmisspec -22.08      5.76 \n\nprint(comp, digits = 2, simplify = FALSE)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic  \ntrue       0.00      0.00 -178.88     6.57        3.90    0.58   357.77\nmisspec  -22.08      5.76 -200.96     6.56        3.00    0.52   401.93\n        se_looic\ntrue      13.14 \nmisspec   13.12"
  },
  {
    "objectID": "slides/08-workflow.html#compare-models-2",
    "href": "slides/08-workflow.html#compare-models-2",
    "title": "Bayesian Workflow",
    "section": "8. Compare models",
    "text": "8. Compare models\n\n###Compute individual LOO-CV/LOO-IC\nloo_model1 &lt;- loo::loo(log_lik_model1)\nloo_model2 &lt;- loo::loo(log_lik_model2)\nloo_model3 &lt;- loo::loo(log_lik_model3)\n\n###Make a comparison\ncomp_loo &lt;- loo::loo_compare(list(\"height_only\" = loo_model1, \"height_sex\" = loo_model2, \"interaction\" = loo_model3))\nprint(comp_loo, digits = 2, simplify = FALSE)\n\n            elpd_diff se_diff  elpd_loo se_elpd_loo p_loo    se_p_loo looic   \nheight_sex      0.00      0.00 -2229.21    23.37        4.85     0.89  4458.41\ninteraction    -0.83      2.41 -2230.03    23.44        5.72     0.97  4460.06\nheight_only   -25.32      5.80 -2254.52    21.32        3.55     0.62  4509.05\n            se_looic\nheight_sex     46.74\ninteraction    46.89\nheight_only    42.64"
  },
  {
    "objectID": "slides/08-workflow.html#the-plan-moving-forward",
    "href": "slides/08-workflow.html#the-plan-moving-forward",
    "title": "Bayesian Workflow",
    "section": "The plan moving forward",
    "text": "The plan moving forward\n\nWe have now learned all of the skills needed to implement a Bayesian workflow.\nThe remainder of the course will be focused on introducing new models for types of data that are common when working in the biomedical data setting."
  },
  {
    "objectID": "slides/03-mcmc.html",
    "href": "slides/03-mcmc.html",
    "title": "Markov chain Monte Carlo",
    "section": "",
    "text": "On Tuesday, we performed posterior inference for a Beta-Binomial model using Monte Carlo estimation.\nToday we will discuss Bayesian estimation of linear regression:\n\n\\[\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n).\\]\n\nThere is no closed form posterior, so we cannot directly use Monte Carlo sampling.\nWe need Markov chain Monte Carlo (MCMC)!\nOur goal is to gain intuition behind MCMC, the workhorse behind Stan."
  },
  {
    "objectID": "slides/07-model-comparison.html",
    "href": "slides/07-model-comparison.html",
    "title": "Model Comparison",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine"
  },
  {
    "objectID": "slides/05-priors-ppds.html",
    "href": "slides/05-priors-ppds.html",
    "title": "Priors, Posteriors, and PPDs!",
    "section": "",
    "text": "On Tuesday, we learned about Stan\n\nA probabilistic programming language for Bayesian inference\nWe learned about the data, parameter, and model code chunks\nWe used Stan to fit a Bayesian linear regression\n\nToday, we will dive into priors, posterior summaries, and posterior predictive distributions (PPDs)"
  },
  {
    "objectID": "slides/09-nonlinear.html#review-of-last-lecture",
    "href": "slides/09-nonlinear.html#review-of-last-lecture",
    "title": "Nonlinear Regression",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we put all of our skills together and learned about the Bayesian workflow.\nWe have now learned all the skills needed to perform Bayesian inference.\nThe rest of the course we will introduce new models and data types that are useful for performing biomedical data science."
  },
  {
    "objectID": "slides/09-nonlinear.html#prepare-for-next-class",
    "href": "slides/09-nonlinear.html#prepare-for-next-class",
    "title": "Nonlinear Regression",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02.\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Robust regression\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/09-robust.html#review-of-last-lecture",
    "href": "slides/09-robust.html#review-of-last-lecture",
    "title": "Robust Regression",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we put all of our skills together and learned about the Bayesian workflow.\nWe have now learned all the skills needed to perform Bayesian inference.\nThe rest of the course we will introduce new models and data types that are useful for performing biomedical data science."
  },
  {
    "objectID": "slides/09-robust.html#prepare-for-next-class",
    "href": "slides/09-robust.html#prepare-for-next-class",
    "title": "Robust Regression",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Regularization\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/09-robust.html#student-t-distribution",
    "href": "slides/09-robust.html#student-t-distribution",
    "title": "Robust Regression",
    "section": "Student-t distribution",
    "text": "Student-t distribution"
  },
  {
    "objectID": "slides/09-robust.html#visualizing-mean-deviation-across-time",
    "href": "slides/09-robust.html#visualizing-mean-deviation-across-time",
    "title": "Robust Regression",
    "section": "Visualizing mean deviation across time",
    "text": "Visualizing mean deviation across time"
  },
  {
    "objectID": "slides/09-robust.html#ols-regression",
    "href": "slides/09-robust.html#ols-regression",
    "title": "Robust Regression",
    "section": "OLS Regression",
    "text": "OLS Regression\n\\[\\begin{aligned}\nMD_i &= \\beta_0 + \\beta_1 Time_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(MD_i\\) are independent observations (independence).\n\\(MD_i\\) is linearly related to \\(Time_i\\) (linearity).\n\\(r_i = MD_i - \\mu_i\\) is normally distributed (normality).\n\\(r_i\\) has constant variance across \\(Time_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/09-robust.html#a-motivating-research-question",
    "href": "slides/09-robust.html#a-motivating-research-question",
    "title": "Robust Regression",
    "section": "A motivating research question",
    "text": "A motivating research question\n\nIn today’s lecture, we will look at data on serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years.\n\nA detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994).\n\nFor an example patient, we define \\(Y_i\\) as the serum concentration value and \\(X_i\\) as a child’s age, given in years."
  },
  {
    "objectID": "slides/09-robust.html#disease-progression",
    "href": "slides/09-robust.html#disease-progression",
    "title": "Robust Regression",
    "section": "Disease progression",
    "text": "Disease progression\n\nOLS can be written as follows for \\(i = 1,\\ldots,n\\),\n\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2).\\]\n\n\\(\\beta_1\\) represent the the change in IgG serum concentration a one year increase in age.\nOften the following hypothesis is tested: \\[H_0: \\beta_1=0,H_1: \\beta_1 &lt; 0.\\]"
  },
  {
    "objectID": "slides/09-robust.html#ols-regression-assumptions",
    "href": "slides/09-robust.html#ols-regression-assumptions",
    "title": "Robust Regression",
    "section": "OLS regression assumptions",
    "text": "OLS regression assumptions\n\\[\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(Y_i\\) are independent observations (independence).\n\\(Y_i\\) is linearly related to \\(X_i\\) (linearity).\n\\(\\epsilon_i = Y_i - \\mu_i\\) is normally distributed (normality).\n\\(\\epsilon_i\\) has constant variance across \\(X_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/09-robust.html#assessing-assumptions",
    "href": "slides/09-robust.html#assessing-assumptions",
    "title": "Robust Regression",
    "section": "Assessing assumptions",
    "text": "Assessing assumptions"
  },
  {
    "objectID": "slides/09-robust.html#heteroskedasticity",
    "href": "slides/09-robust.html#heteroskedasticity",
    "title": "Robust Regression",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nHeteroskedasticity is the violation of the assumption of constant variance.\nHow can we handle this?\nIn OLS, there are approaches like heteroskedastic consistent errors, but this is not a generative model.\nIn the Bayesian framework, we generally like to write down generative models."
  },
  {
    "objectID": "slides/09-robust.html#weighted-regression",
    "href": "slides/09-robust.html#weighted-regression",
    "title": "Robust Regression",
    "section": "Weighted regression",
    "text": "Weighted regression\n\nA common case is weighted regression, where each \\(Y_i\\) represents the mean of \\(n_i\\) observations. Then the scale of each observation is, \\[\\tau_i^2 = \\sigma^2/n_i,\\] where \\(\\sigma^2\\) is a global scale parameter.\nAlternatively, suppose each observation represents the sum of each \\(n_i\\) observations. Then the scale of each observation is, \\[\\tau_i^2 = n_i \\sigma^2.\\]"
  },
  {
    "objectID": "slides/09-robust.html#modeling-the-scale-with-covariates",
    "href": "slides/09-robust.html#modeling-the-scale-with-covariates",
    "title": "Robust Regression",
    "section": "Modeling the scale with covariates",
    "text": "Modeling the scale with covariates\n\nThe scale can also be modeled with covariates.\nIt is common to model the log-transformation of the scale or variance to transform it to \\(\\mathbb{R}\\),\n\n\\[\\log \\tau_i^2 = \\mathbf{z}_i \\boldsymbol{\\gamma},\\]\nwhere \\(\\mathbf{z}_i = (z_{i1},\\ldots,z_{ip})\\) are a \\(p\\)-dimensional vector of covariates and \\(\\boldsymbol{\\gamma}\\) are parameters that regress the covariates onto the log variance.\n\nOther options include: \\(\\log \\tau_i^2 = \\mathbf{z}_i \\boldsymbol{\\gamma} + \\nu_i,\\quad \\nu_i \\sim N(0, \\sigma^2)\\)\nOther options include: \\(\\log \\tau_i^2 = f(\\mu_i)\\)\nAny plausible generative model can be specified!"
  },
  {
    "objectID": "slides/09-robust.html#heteroskedastic-variance",
    "href": "slides/09-robust.html#heteroskedastic-variance",
    "title": "Robust Regression",
    "section": "Heteroskedastic variance",
    "text": "Heteroskedastic variance\n\nWe can write the regression model using a observation specific variance, \\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\tau_i^2).\\]\nOne way of writing the variance is: \\(\\tau_i^2 = \\sigma^2 \\lambda_i\\).\n\n\\(\\sigma^2\\) is a global scale parameter.\n\\(\\lambda_i\\) is an observation specific scale parameter.\n\nIn the Bayesian framework, we must place a prior on \\(\\lambda_i\\)."
  },
  {
    "objectID": "slides/09-robust.html#median-regression",
    "href": "slides/09-robust.html#median-regression",
    "title": "Robust Regression",
    "section": "Median regression",
    "text": "Median regression\nLeast absolute deviation (LAD) regression minimizes the following objective function,\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{LAD}} = \\arg \\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n |Y_i - \\mathbf{x}_i\\boldsymbol{\\beta}|.\\]\nThe Bayesian analog is the Laplace distribution, \\[f(x | \\mu, \\sigma) = \\frac{1}{2\\sigma} \\exp\\left\\{-\\frac{|x - \\mu|}{\\sigma}\\right\\}\\] where \\(\\mu\\) and \\(\\sigma\\) are the location and scale.\n\nThe Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is \\(|x−\\mu|\\), so minimizing the likelihood will also minimize the least absolute distances.\nLaplace distribution is also known as the double-exponential distribution (symmetric exponential distributions around \\(\\mu\\) with scale \\(\\sigma\\)).\nThus, a linear regression with Laplace errors is analogous to a median regression,\n\n\\[Y_i = \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\stackrel{ind}{\\sim}\\text{Laplace}(0, \\sigma).\\]\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += double_exponential_lpdf(Y | X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/09-robust.html#laplace-as-a-mixture",
    "href": "slides/09-robust.html#laplace-as-a-mixture",
    "title": "Robust Regression",
    "section": "Laplace as a mixture",
    "text": "Laplace as a mixture\nIf \\(V \\sim \\text{Exponential}(1)\\) and \\(Z \\sim N(0,1)\\) independent of \\(V\\), then \\(X = μ + \\sigma \\sqrt{2 V} Z ∼ \\text{Laplace}(\\mu,\\sigma)\\)."
  },
  {
    "objectID": "slides/09-robust.html#laplace-in-stan",
    "href": "slides/09-robust.html#laplace-in-stan",
    "title": "Robust Regression",
    "section": "Laplace in Stan",
    "text": "Laplace in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] V;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, sigma * sqrt(2 * V));\n  target += exponential_lpdf(to_vector(V));\n}"
  },
  {
    "objectID": "slides/09-robust.html#general-quantile-regression",
    "href": "slides/09-robust.html#general-quantile-regression",
    "title": "Robust Regression",
    "section": "General quantile regression",
    "text": "General quantile regression\n\nfunctions {\n  real asym_laplace_lpdf(real y, real mu, real sigma, real tau) {\n    return log(tau) + log1m(tau)\n      - log(sigma)\n      - 2 * ((y &lt; mu) ? (1 - tau) * (mu - y) : tau * (y - mu)) / sigma;\n  }\n}\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0, upper = 1&gt; tau;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  for (i in 1:n) target += asym_laplace_lpdf(Y[i] | X[i, ] * beta, sigma, tau);\n}"
  },
  {
    "objectID": "slides/09-robust.html#heteroskedastic-variance-1",
    "href": "slides/09-robust.html#heteroskedastic-variance-1",
    "title": "Robust Regression",
    "section": "Heteroskedastic variance",
    "text": "Heteroskedastic variance\n\nWe can write the regression model using a observation specific variance, \\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\tau_i^2).\\]\nOne way of writing the variance is: \\(\\tau_i^2 = \\sigma^2 \\lambda_i\\).\n\n\\(\\sigma^2\\) is a global scale parameter.\n\\(\\lambda_i\\) is an observation specific scale parameter.\n\nIn the Bayesian framework, we must place a prior on \\(\\lambda_i\\)."
  },
  {
    "objectID": "slides/09-robust.html#heteroskedastic-variance-2",
    "href": "slides/09-robust.html#heteroskedastic-variance-2",
    "title": "Robust Regression",
    "section": "Heteroskedastic variance",
    "text": "Heteroskedastic variance\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\begin{aligned}\nY_i &= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\\[f(Y_i) = \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\]"
  },
  {
    "objectID": "slides/09-robust.html#equivalnence-with-heavy-tail-distribution",
    "href": "slides/09-robust.html#equivalnence-with-heavy-tail-distribution",
    "title": "Robust Regression",
    "section": "Equivalnence with heavy-tail distribution",
    "text": "Equivalnence with heavy-tail distribution\n\\[\\begin{aligned}\nT_i &= \\frac{Z_i}{\\sqrt{\\frac{W_i}{\\nu}}},\\quad Z_i \\stackrel{iid}{\\sim} N(0,1), W_i \\stackrel{iid}{\\sim}\\chi^2_{\\nu}\\\\\n&= \\frac{Z_i}{\\sqrt{\\frac{1}{\\nu V_i}}},\\quad V_i \\stackrel{iid}{\\sim} \\text{Inv-}\\chi^2_{\\nu}\\\\\n&= \\sqrt{\\nu V_i} Z_i,\\quad \\lambda_i = \\nu V_i\\\\\n&= \\sqrt{\\lambda_i} Z_i, \\quad \\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\\\\\n&\\sim t_{\\nu}\n\\end{aligned}\\]\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma^2\\right)\\]"
  },
  {
    "objectID": "slides/09-robust.html#how",
    "href": "slides/09-robust.html#how",
    "title": "Robust Regression",
    "section": "How?",
    "text": "How?\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).\\]\n\\[\\iff\\]\n\\[\\begin{aligned}\nY_i &= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\\[\\begin{aligned}\nY_i &= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\\[f(Y_i) = \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\]"
  },
  {
    "objectID": "slides/09-robust.html#understanding-the-equivalence",
    "href": "slides/09-robust.html#understanding-the-equivalence",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\nHeteroskedastic variances assumption is equivalent to assuming a heavy-tailed distribution.\n\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).\\]\n\\[\\iff\\]\n\\[\\begin{aligned}\nY_i &= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\nNote that since the number of \\(\\lambda_i\\) parameters is equal to the number of observations, this model will not have a proper posterior distribution without a proper prior distribution."
  },
  {
    "objectID": "slides/09-robust.html#understanding-the-equivalence-1",
    "href": "slides/09-robust.html#understanding-the-equivalence-1",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\\[\\begin{aligned}\nf(Y_i) &= \\int_0^{\\infty} f(Y_i , \\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} N(Y_i ; \\mu_i, \\sigma^2 \\lambda_i) \\text{Inverse-Gamma}\\left(\\lambda_i ; \\frac{\\nu}{2},\\frac{\\nu}{2}\\right) d\\lambda_i\\\\\n&= t_{\\nu}\\left(\\mu_i,\\sigma\\right).\n\\end{aligned}\\]\n\nThe marginal likelihood can be viewed as a mixture of a Gaussian likelihood with an Inverse-Gamma scale parameter."
  },
  {
    "objectID": "slides/09-robust.html#understanding-the-equivalence-2",
    "href": "slides/09-robust.html#understanding-the-equivalence-2",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\\[\\begin{aligned}\nT_i &= \\frac{Z_i}{\\sqrt{\\frac{W_i}{\\nu}}},\\quad Z_i \\stackrel{iid}{\\sim} N(0,1), W_i \\stackrel{iid}{\\sim}\\chi^2_{\\nu}\\\\\n&= \\frac{Z_i}{\\sqrt{\\frac{1}{\\nu V_i}}},\\quad V_i \\stackrel{iid}{\\sim} \\text{Inv-}\\chi^2_{\\nu}\\\\\n&= \\sqrt{\\nu V_i} Z_i,\\quad \\lambda_i = \\nu V_i\\\\\n&= \\sqrt{\\lambda_i} Z_i, \\quad \\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\\\\\n&\\sim t_{\\nu}\n\\end{aligned}\\]\n\nWe then have: \\(Y_i = \\mu_i + \\sigma T_i \\sim t_{\\nu}(\\mu_i, \\sigma).\\)"
  },
  {
    "objectID": "slides/09-robust.html#robust-regression-as-heteroskedasticity",
    "href": "slides/09-robust.html#robust-regression-as-heteroskedasticity",
    "title": "Robust Regression",
    "section": "Robust regression as heteroskedasticity",
    "text": "Robust regression as heteroskedasticity\nThus, robust regression models with Student-t errors can be derived from a particular model of heteroskedastic normal errors."
  },
  {
    "objectID": "slides/09-robust.html#other-heteroskedastic-variance-specification",
    "href": "slides/09-robust.html#other-heteroskedastic-variance-specification",
    "title": "Robust Regression",
    "section": "Other heteroskedastic variance specification",
    "text": "Other heteroskedastic variance specification\nAs a Bayesian, we have complete flexibility over the specification \\(f(\\lambda_i)\\).\n\nAnother specification could be,\n\n\\[\\boldsymbol{\\lambda} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha}),\\]\nwhere \\(\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots,\\lambda_n)\\) and \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_n)\\).\n\nThis prior forces \\(\\sum_{i=1}^n \\lambda_i = 1\\), \\(\\lambda_i &gt;0\\)."
  },
  {
    "objectID": "slides/09-robust.html#one-option-for-a-prior",
    "href": "slides/09-robust.html#one-option-for-a-prior",
    "title": "Robust Regression",
    "section": "One option for a prior",
    "text": "One option for a prior\n\nWe could specify the following,\n\n\\[\\boldsymbol{\\lambda} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha}),\\]\nwhere \\(\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots,\\lambda_n)\\) and \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_n)\\).\n\nThis prior forces \\(\\sum_{i=1}^n \\lambda_i = 1\\), \\(\\lambda_i &gt;0\\).\nA prior probabilities are given by, \\(\\mathbb{E}[\\lambda_i] = \\alpha_i / \\alpha_0\\), where \\(\\alpha_0 = \\sum_{i=1}^n \\alpha_i.\\)"
  },
  {
    "objectID": "slides/09-robust.html#bayesian-prior-to-induce-structure",
    "href": "slides/09-robust.html#bayesian-prior-to-induce-structure",
    "title": "Robust Regression",
    "section": "Bayesian prior to induce structure",
    "text": "Bayesian prior to induce structure\n\nSuppse we would like \\(\\sum_{i=1}^n \\lambda_i = 1\\), \\(\\lambda_i &gt;0\\).\nWe could specify the following,\n\n\\[\\boldsymbol{\\lambda} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha}),\\]\nwhere \\(\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots,\\lambda_n)\\) and \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_n)\\).\n\nThe prior mean is \\(\\mathbb{E}[\\lambda_i] = \\alpha_i / \\alpha_0\\), where \\(\\alpha_0 = \\sum_{i=1}^n \\alpha_i.\\)\nTypically, \\(\\alpha_i = 1 \\forall i\\)."
  },
  {
    "objectID": "slides/09-robust.html#a-prior-to-induce-a-heavy-tail-distribution",
    "href": "slides/09-robust.html#a-prior-to-induce-a-heavy-tail-distribution",
    "title": "Robust Regression",
    "section": "A prior to induce a heavy-tail distribution",
    "text": "A prior to induce a heavy-tail distribution\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\\]\n\nUnder this prior, the marginal likelihood for \\(Y_i\\) is equivalent to a Student-t distribution,\n\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).\\]"
  },
  {
    "objectID": "slides/09-robust.html#robust-regression",
    "href": "slides/09-robust.html#robust-regression",
    "title": "Robust Regression",
    "section": "Robust regression",
    "text": "Robust regression\n\nToday we will learn about regression techniques that are robust to the assumptions of linear regression.\nWe will introduce the idea of robust regression by exploring ways to generalize the homoskedastic variance assumption in linear regression.\nWe will touch on heteroskedasticity, heavy-tailed distributions, and median regression (more generally quantile regression)."
  },
  {
    "objectID": "slides/09-robust.html#examples-of-heavy-tailed-distributions",
    "href": "slides/09-robust.html#examples-of-heavy-tailed-distributions",
    "title": "Robust Regression",
    "section": "Examples of heavy-tailed distributions",
    "text": "Examples of heavy-tailed distributions"
  },
  {
    "objectID": "slides/09-robust.html#a-prior-to-induce-a-heavy-tail",
    "href": "slides/09-robust.html#a-prior-to-induce-a-heavy-tail",
    "title": "Robust Regression",
    "section": "A prior to induce a heavy-tail",
    "text": "A prior to induce a heavy-tail\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\\]\n\nUnder this prior, the marginal likelihood for \\(Y_i\\) is equivalent to a Student-t distribution,\n\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).\\]"
  },
  {
    "objectID": "slides/09-robust.html#student-t-in-stan",
    "href": "slides/09-robust.html#student-t-in-stan",
    "title": "Robust Regression",
    "section": "Student-t in Stan",
    "text": "Student-t in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; nu;\n}\nmodel {\n  target += student_t_lpdf(Y | nu, X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/09-robust.html#student-t-in-stan-mixture",
    "href": "slides/09-robust.html#student-t-in-stan-mixture",
    "title": "Robust Regression",
    "section": "Student-t in Stan: mixture",
    "text": "Student-t in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += inv_gamma_lpdf(lambda | 0.5 * nu, 0.5 * nu);\n}"
  },
  {
    "objectID": "slides/09-robust.html#another-example-of-robust-regression",
    "href": "slides/09-robust.html#another-example-of-robust-regression",
    "title": "Robust Regression",
    "section": "Another example of robust regression",
    "text": "Another example of robust regression\n\nLet’s revisit our general heteroskedastic regression, \\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\sigma^2 \\lambda_i).\\]\nWe can induce another form of robust regression using the following prior for \\(\\lambda_i\\), \\(\\lambda_i \\sim \\text{Exponential}(1/2)\\).\nUnder this prior, the induced marginal model is, \\[Y_i = \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\stackrel{ind}{\\sim} \\text{Laplace}(\\mu, \\sigma).\\]\n\\(f(\\epsilon_i | \\mu, \\sigma) = \\frac{1}{2\\sigma} \\exp\\left\\{-\\frac{|\\epsilon_i - \\mu|}{\\sigma}\\right\\}\\)"
  },
  {
    "objectID": "slides/09-robust.html#modeling-the-scale-with-covariates-1",
    "href": "slides/09-robust.html#modeling-the-scale-with-covariates-1",
    "title": "Robust Regression",
    "section": "Modeling the scale with covariates",
    "text": "Modeling the scale with covariates\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1&gt; q;\n  vector[n] Y;\n  matrix[n, p] X;\n  matrix[n, q] Z;\n}\nparameters {\n  vector[p] beta;\n  vector[q] gamma;\n}\ntransformed parameters {\n  vector[n] tau = exp(Z * gamma);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n}"
  },
  {
    "objectID": "slides/09-robust.html#dirchlet-prior",
    "href": "slides/09-robust.html#dirchlet-prior",
    "title": "Robust Regression",
    "section": "Dirchlet prior",
    "text": "Dirchlet prior\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  vector&lt;lower = 0&gt;[n] alpha;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  simplex[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += dirichlet_lpdf(lambda | alpha);\n}"
  },
  {
    "objectID": "slides/09-robust.html#dirchlet-prior-in-stan",
    "href": "slides/09-robust.html#dirchlet-prior-in-stan",
    "title": "Robust Regression",
    "section": "Dirchlet prior in Stan",
    "text": "Dirchlet prior in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  vector&lt;lower = 0&gt;[n] alpha;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  simplex[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += dirichlet_lpdf(lambda | alpha);\n}"
  },
  {
    "objectID": "slides/09-robust.html#student-t-in-stan-1",
    "href": "slides/09-robust.html#student-t-in-stan-1",
    "title": "Robust Regression",
    "section": "Student-t in Stan",
    "text": "Student-t in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real&lt;lower = 0&gt; nu;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += student_t_lpdf(Y | nu, X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/09-robust.html#student-t-in-stan-mixture-1",
    "href": "slides/09-robust.html#student-t-in-stan-mixture-1",
    "title": "Robust Regression",
    "section": "Student-t in Stan: mixture",
    "text": "Student-t in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, sigma * sqrt(lambda));\n  target += inv_gamma_lpdf(lambda | 0.5 * nu, 0.5 * nu);\n}"
  },
  {
    "objectID": "slides/09-robust.html#why-heavy-tailed-distributions",
    "href": "slides/09-robust.html#why-heavy-tailed-distributions",
    "title": "Robust Regression",
    "section": "Why heavy-tailed distributions?",
    "text": "Why heavy-tailed distributions?\n\nReplacing the normal distribution with a distribution with heavy-tails (e.g., Student-t, Laplace) is a common approach to robust regression.\nRobust regression refers to regression methods which are less sensitive to outliers or small sample sizes.\nLinear regression, including Bayesian regression with normally distributed errors is sensitive to outliers, because the normal distribution has narrow tail probabilities.\nOur heteroskedastic model that we just explored is only one example of a robust regression model."
  },
  {
    "objectID": "slides/09-robust.html#vizualizing-heavy-tail-distributions",
    "href": "slides/09-robust.html#vizualizing-heavy-tail-distributions",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/09-robust.html#vizualizing-heavy-tail-distributions-1",
    "href": "slides/09-robust.html#vizualizing-heavy-tail-distributions-1",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/09-robust.html#median-regression-using-laplace",
    "href": "slides/09-robust.html#median-regression-using-laplace",
    "title": "Robust Regression",
    "section": "Median regression using Laplace",
    "text": "Median regression using Laplace\nLeast absolute deviation (LAD) regression minimizes the following objective function,\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{LAD}} = \\arg \\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n |Y_i - \\mathbf{x}_i\\boldsymbol{\\beta}|.\\]\nThe Bayesian analog is the Laplace distribution,\n\\(f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma) = \\left(\\frac{1}{2\\sigma}\\right)^n \\exp\\left\\{-\\sum_{i=1}^n\\frac{|Y_i - \\mathbf{x}_i \\boldsymbol{\\beta}|}{\\sigma}\\right\\}.\\)"
  },
  {
    "objectID": "slides/09-robust.html#median-regression-using-laplace-1",
    "href": "slides/09-robust.html#median-regression-using-laplace-1",
    "title": "Robust Regression",
    "section": "Median regression using Laplace",
    "text": "Median regression using Laplace\n\nThe Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is \\(|x−\\mu|\\), so minimizing the likelihood will also minimize the least absolute distances.\nLaplace distribution is also known as the double-exponential distribution (symmetric exponential distributions around \\(\\mu\\) with scale \\(\\sigma\\)).\nThus, a linear regression with Laplace errors is analogous to a median regression,\nWhy is median regression considered more robust than regression of the mean?"
  },
  {
    "objectID": "slides/09-robust.html#laplace-regression-in-stan",
    "href": "slides/09-robust.html#laplace-regression-in-stan",
    "title": "Robust Regression",
    "section": "Laplace regression in Stan",
    "text": "Laplace regression in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += double_exponential_lpdf(Y | X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/09-robust.html#laplace-regression-in-stan-mixture",
    "href": "slides/09-robust.html#laplace-regression-in-stan-mixture",
    "title": "Robust Regression",
    "section": "Laplace regression in Stan: mixture",
    "text": "Laplace regression in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += exponential_lpdf(lambda | 0.5);\n}"
  },
  {
    "objectID": "slides/09-robust.html#example-data",
    "href": "slides/09-robust.html#example-data",
    "title": "Robust Regression",
    "section": "Example data",
    "text": "Example data\nThis data set refers to the serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years. A detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994). The specific data set that we have used for our analysis is from the latter reference. The relationship of IgG with age is quite weak, with some visual evidence of positive skewness. We took the response variable Y to be the IgG concentration and used a quadratic model in age, x, to fit the quantile regression,\n\\[Quantile = \\beta_0 + \\beta_1 age + \\beta_2 age^2\\]\n\nlibrary(Brq)\ndata(\"ImmunogG\")\nhead(ImmunogG)\n\n  IgG Age\n1 1.5 0.5\n2 2.7 0.5\n3 1.9 0.5\n4 4.0 0.5\n5 1.9 0.5\n6 4.4 0.5\n\nplot(ImmunogG$Age, ImmunogG$IgG)"
  },
  {
    "objectID": "slides/09-robust.html#visualizing-mean-deviation-across-time-1",
    "href": "slides/09-robust.html#visualizing-mean-deviation-across-time-1",
    "title": "Robust Regression",
    "section": "Visualizing mean deviation across time",
    "text": "Visualizing mean deviation across time"
  },
  {
    "objectID": "slides/09-robust.html#assessing-assumptions-1",
    "href": "slides/09-robust.html#assessing-assumptions-1",
    "title": "Robust Regression",
    "section": "Assessing assumptions",
    "text": "Assessing assumptions"
  },
  {
    "objectID": "slides/09-robust.html#pulling-the-data",
    "href": "slides/09-robust.html#pulling-the-data",
    "title": "Robust Regression",
    "section": "Pulling the data",
    "text": "Pulling the data\n\nlibrary(Brq)\ndata(\"ImmunogG\")\nhead(ImmunogG)\n\n  IgG Age\n1 1.5 0.5\n2 2.7 0.5\n3 1.9 0.5\n4 4.0 0.5\n5 1.9 0.5\n6 4.4 0.5"
  },
  {
    "objectID": "slides/09-robust.html#visualizing-igg-data",
    "href": "slides/09-robust.html#visualizing-igg-data",
    "title": "Robust Regression",
    "section": "Visualizing IgG data",
    "text": "Visualizing IgG data"
  },
  {
    "objectID": "slides/09-robust.html#asymmetric-laplace-distribution",
    "href": "slides/09-robust.html#asymmetric-laplace-distribution",
    "title": "Robust Regression",
    "section": "Asymmetric Laplace distribution",
    "text": "Asymmetric Laplace distribution\nA random variable, \\(Y \\sim ALD_p(\\mu,\\sigma)\\) is said to follow an asymmetric Laplace distribution (ALD) if the pdf is given by,\n\\[f(Y) = \\frac{p(1-p)}{\\sigma} \\exp \\left\\{-\\rho_p\\left(\\frac{Y - \\mu}{\\sigma}\\right)\\right\\},\\]\nwhere \\(p \\in (0,1)\\) is the percentile and \\[\\rho_p(x) = x\\left(p - 1(u &lt; 0)\\right) = \\frac{|x| + (2p - 1)x}{2}.\\]\n\nWhen \\(p = 0.5\\) it reduces to a regular Laplace distribution."
  },
  {
    "objectID": "slides/09-robust.html#quantile-regression",
    "href": "slides/09-robust.html#quantile-regression",
    "title": "Robust Regression",
    "section": "Quantile regression",
    "text": "Quantile regression"
  },
  {
    "objectID": "slides/09-robust.html#scale-mixture-representation",
    "href": "slides/09-robust.html#scale-mixture-representation",
    "title": "Robust Regression",
    "section": "Scale-mixture representation",
    "text": "Scale-mixture representation\nThe above may also be written as a mixture of exponential and normal distributions. Letting, \\(Z_i \\sim Exponential(1)\\) and \\(\\sigma \\sim Exponential(1)\\).\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\sigma \\theta Z_i + \\epsilon_i,\\quad \\epsilon_i \\sim N\\left(0, \\tau^2 \\sigma^2 Z_i\\right),\\]\nwhere \\[\\theta = \\frac{1-2p}{p(1-p)}\\] and \\[\\tau \\sqrt{\\frac{2}{p(1-p)}}.\\]"
  },
  {
    "objectID": "slides/09-robust.html#scale-mixture-in-stan",
    "href": "slides/09-robust.html#scale-mixture-in-stan",
    "title": "Robust Regression",
    "section": "Scale-mixture in Stan",
    "text": "Scale-mixture in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0, upper = 1&gt; q;\n}\ntransformed data {\n  real theta = (1 - 2 * q) / (q * (1 - q));\n  real tau = sqrt(2 / (q * (1 - q)));\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector&lt;lower=0&gt;[n] z;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta + sigma * theta * z, tau * sqrt(z) * sigma);\n  target += exponential_lpdf(sigma | 1);\n  target += exponential_lpdf(z | 1);\n}"
  },
  {
    "objectID": "slides/09-robust.html#posterior-of-boldsymbolbeta",
    "href": "slides/09-robust.html#posterior-of-boldsymbolbeta",
    "title": "Robust Regression",
    "section": "Posterior of \\(\\boldsymbol{\\beta}\\)",
    "text": "Posterior of \\(\\boldsymbol{\\beta}\\)\n\n\n\n\n\nquantile\nmean\nlower\nupper\n\n\n\n\n0.025\n0.3291804\n0.2639468\n0.3899443\n\n\n0.250\n0.5130600\n0.3788182\n0.6523600\n\n\n0.500\n0.7268102\n0.5705129\n0.8836400\n\n\n0.750\n0.8093408\n0.6919899\n0.9363650\n\n\n0.975\n1.1741866\n0.9783041\n1.3936860"
  },
  {
    "objectID": "slides/09-robust.html#posterior-of-beta_1",
    "href": "slides/09-robust.html#posterior-of-beta_1",
    "title": "Robust Regression",
    "section": "Posterior of \\(\\beta_1\\)",
    "text": "Posterior of \\(\\beta_1\\)\n\n\n\n\n\nquantile\nmean\nlower\nupper\n\n\n\n\n0.025\n0.3291804\n0.2639468\n0.3899443\n\n\n0.250\n0.5130600\n0.3788182\n0.6523600\n\n\n0.500\n0.7268102\n0.5705129\n0.8836400\n\n\n0.750\n0.8093408\n0.6919899\n0.9363650\n\n\n0.975\n1.1741866\n0.9783041\n1.3936860"
  },
  {
    "objectID": "slides/10-regularization.html#review-of-last-lecture",
    "href": "slides/10-regularization.html#review-of-last-lecture",
    "title": "Regularization",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Thursday, we learned about robust regression.\n\nHeteroskedasticity\nHeavy-tailed distributions\nQuantile regression\n\nThese were all models for the observed data \\(Y_i\\).\nToday, we will focus on prior specifications for \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/10-regularization.html#ridge-regression",
    "href": "slides/10-regularization.html#ridge-regression",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nRidge regression aims to yield a parsimonious regularized regression model in the presence of highly correlated predictor variables.\nRidge regression solves the following loss function,\n\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right) + \\lambda \\sum_{j = 1}^p \\beta_j^2.\\]\n\n\\(\\lambda \\geq 0\\) controls the amount of regularization."
  },
  {
    "objectID": "slides/10-regularization.html#a-motivating-research-question",
    "href": "slides/10-regularization.html#a-motivating-research-question",
    "title": "Regularization",
    "section": "A motivating research question",
    "text": "A motivating research question\n\nIn today’s lecture, we will look at data on serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years.\n\nA detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994).\n\nFor an example patient, we define \\(Y_i\\) as the serum concentration value and \\(X_i\\) as a child’s age, given in years."
  },
  {
    "objectID": "slides/10-regularization.html#prepare-for-next-class",
    "href": "slides/10-regularization.html#prepare-for-next-class",
    "title": "Regularization",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWe are going to jump into an AE on body fat, but first some reminders.\nWork on HW 02, which is due before next class.\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Classification\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/10-regularization.html#orindary-least-square-regression",
    "href": "slides/10-regularization.html#orindary-least-square-regression",
    "title": "Regularization",
    "section": "Orindary least square regression",
    "text": "Orindary least square regression\n\nConsider the linear regression model, \\(Y_i \\stackrel{ind}{\\sim}N\\left(\\mathbf{x}_i^\\top\\boldsymbol{\\beta},\\sigma^2\\right)\\).\n\nFor simplicity lets assume that \\(\\sigma = 1\\).\n\nWe can write this in matrix form, \\(\\mathbf{Y} \\stackrel{}{\\sim}N\\left(\\mathbf{X}\\boldsymbol{\\beta},\\mathbf{I}_n\\right)\\).\nOrdinary least squares (OLS) regression solves the following loss function,\n\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{ols}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right).\\]\n\nIn this simple setting: \\(\\hat{\\boldsymbol{\\beta}}_{\\text{ols}} = \\left(\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\)."
  },
  {
    "objectID": "slides/10-regularization.html#ridge-regression-1",
    "href": "slides/10-regularization.html#ridge-regression-1",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nThe term, \\(\\lambda \\sum_{j = 1}^p \\beta_j^2\\), is sometimes called the L2 - norm.\nIn can be seen that: \\(\\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\left(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^\\top \\mathbf{Y}\\).\nUnder the setting of correlated predictors, the inverse of \\(\\mathbf{X}^\\top\\mathbf{X}\\) often approaches singularity.\nAdding positive elements to the diagonals improves stability of the inverse.\nWhen \\(\\lambda = 0\\), \\(\\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\hat{\\boldsymbol{\\beta}}_{\\text{ols}}\\)\nWhen \\(\\lambda \\rightarrow \\infty\\), \\(\\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} \\rightarrow 0\\)."
  },
  {
    "objectID": "slides/10-regularization.html#bayesian-ridge-regression",
    "href": "slides/10-regularization.html#bayesian-ridge-regression",
    "title": "Regularization",
    "section": "Bayesian ridge regression",
    "text": "Bayesian ridge regression\n\nFrom a Bayesian framework, ridge regression can be derived using the following prior, \\(\\beta_j | \\lambda \\stackrel{ind}{\\sim}N\\left(0,\\frac{1}{\\lambda}\\right)\\).\nWhy?\n\n\n\n\\(f(\\beta_j | \\lambda) \\propto \\exp\\left\\{-\\frac{\\lambda\\beta_j^2}{2}\\right\\}\\)\n\n\n\n\n\\(\\log \\prod_{i=1}^n f(\\beta_j | \\lambda) = -0.5 \\lambda\\sum_{i=1}^n\\beta_j^2\\)"
  },
  {
    "objectID": "slides/10-regularization.html#prior-for-lambda",
    "href": "slides/10-regularization.html#prior-for-lambda",
    "title": "Regularization",
    "section": "Prior for \\(\\lambda\\)",
    "text": "Prior for \\(\\lambda\\)\n\nRidge regression: \\(\\beta_j | \\lambda \\stackrel{ind}{\\sim}N\\left(0,\\frac{1}{\\lambda}\\right)\\).\nSometimes \\(\\lambda\\) is fixed (or learned using a grid search).\nSince we are Bayesians, we can place a prior on \\(\\lambda\\).\nLarger values of \\(\\lambda\\) induce greater regularization, because the variance of the regression coefficients becomes smaller.\n\nThis inspires a prior with a heavy-tail, so a half-Cauchy distribution is often used, \\(\\lambda \\sim \\mathcal C^+(0,1)\\)."
  },
  {
    "objectID": "slides/10-regularization.html#half-cauchy-distribution",
    "href": "slides/10-regularization.html#half-cauchy-distribution",
    "title": "Regularization",
    "section": "Half-Cauchy distribution",
    "text": "Half-Cauchy distribution\nA random variable \\(X \\sim \\mathcal C^+(\\mu,\\sigma)\\) follows a half-Cauchy distribution with location \\(\\mu\\) and scale \\(\\sigma &gt; 0\\) and has the following density,\n\\[f(X | \\mu, \\sigma) = \\frac{2}{\\pi \\sigma}\\frac{1}{1 + (X - \\mu)^2 / \\sigma^2},\\quad X \\geq \\mu\\]\n\nThe Half-Cauchy distribution with \\(\\mu = 0\\) is a useful prior for non-negative parameters that may be very large, as allowed by the very heavy tails of the Half-Cauchy distribution."
  },
  {
    "objectID": "slides/10-regularization.html#half-cauchy-distribution-in-stan",
    "href": "slides/10-regularization.html#half-cauchy-distribution-in-stan",
    "title": "Regularization",
    "section": "Half-Cauchy distribution in Stan",
    "text": "Half-Cauchy distribution in Stan\nIn Stan, the half-Cauchy distribution can be specified by putting a constraint on the parameter definition.\n\nparameters {\n  real&lt;lower = 0&gt; lambda;\n}\nmodel {\n  target += cauchy_lpdf(lambda | 0, 1);\n}"
  },
  {
    "objectID": "slides/10-regularization.html#lasso-regression",
    "href": "slides/10-regularization.html#lasso-regression",
    "title": "Regularization",
    "section": "Lasso regression",
    "text": "Lasso regression\n\nA drawback to ridge regression is that it does not improve parsimony, because all parameters stay in the final model (i.e., no \\(\\beta_j\\) is set exactly to 0).\nLeast absolute shrinkage and selection operator (Lasso) regression is an alternative method.\nLasso regression solves the following loss function,\n\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{lasso}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top \\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right) + \\lambda \\sum_{j = 1}^p |\\beta_j|.\\]"
  },
  {
    "objectID": "slides/10-regularization.html#lasso-regression-1",
    "href": "slides/10-regularization.html#lasso-regression-1",
    "title": "Regularization",
    "section": "Lasso regression",
    "text": "Lasso regression\n\nThe term, \\(\\lambda \\sum_{j = 1}^p |\\beta_j|\\), is sometimes called the L1 - norm.\nThis specification allows less important predictors to be set to zero, meaning it performs both shrinkage and variable selection.\nThere is no analytic form for \\(\\hat{\\boldsymbol{\\beta}}_{\\text{lasso}}\\), since the solution is non-linear in \\(\\mathbf{Y}\\).\nLasso pulls \\(\\beta_j\\) to zero faster than ridge regression."
  },
  {
    "objectID": "slides/10-regularization.html#bayesian-lasso-regression",
    "href": "slides/10-regularization.html#bayesian-lasso-regression",
    "title": "Regularization",
    "section": "Bayesian lasso regression",
    "text": "Bayesian lasso regression\n\nFrom a Bayesian framework, lasso regression can be derived using the following prior, \\(\\beta_j | \\lambda \\stackrel{ind}{\\sim}\\text{Laplace}\\left(0,\\frac{1}{\\lambda}\\right)\\).\nWhy?\n\n\n\n\\(f(\\beta_j | \\lambda) \\propto \\exp\\left\\{-\\lambda|\\beta_j|\\right\\}.\\)\n\n\n\n\n\\(\\log \\prod_{i=1}^n f(\\beta_j | \\lambda) = -\\lambda\\sum_{i=1}^n|\\beta_j|\\)"
  },
  {
    "objectID": "slides/10-regularization.html#sparsity-in-regression-problems",
    "href": "slides/10-regularization.html#sparsity-in-regression-problems",
    "title": "Regularization",
    "section": "Sparsity in regression problems",
    "text": "Sparsity in regression problems\n\nSupervised learning can be cast as the problem of estimating a set of coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_j\\}_{j=1}^{p}\\) that determines some functional relationship between a set of \\(\\{x_j\\}_{j = 1}^p\\) and a target variable \\(y\\).\nThis is a central focus of statistics and machine learning.\nChallenges arise in “large-\\(p\\)” problems where, in order to avoid overly complex models that predict poorly, some form of dimension reduction is needed.\nFinding a sparse solution, where some \\(\\beta_j\\) are zero, is desirable."
  },
  {
    "objectID": "slides/10-regularization.html#bayesian-sparse-estimation",
    "href": "slides/10-regularization.html#bayesian-sparse-estimation",
    "title": "Regularization",
    "section": "Bayesian sparse estimation",
    "text": "Bayesian sparse estimation\n\nFrom a Bayesian-learning perspective, there are two main sparse-estimation alternatives: discrete mixtures and shrinkage priors.\nDiscrete mixtures have been very popular, with the spike-and-slab prior being the gold standard.\n\nEasy to force \\(\\beta_j\\) to exactly zero, but require discrete parameter specification.\n\nShrinkage priors force \\(\\beta_j\\) to zero using regularization, but struggle to get exact zeros.\n\nIn recent years, shrinkage priors have become dominant in Bayesian sparsity priors."
  },
  {
    "objectID": "slides/10-regularization.html#horseshoe-prior",
    "href": "slides/10-regularization.html#horseshoe-prior",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\n\nLet’s assume \\(\\mathbf{Y} \\stackrel{}{\\sim}N\\left(\\mathbf{X}\\boldsymbol{\\beta},\\sigma^2\\mathbf{I}_n\\right)\\), where \\(\\boldsymbol{\\beta}\\) is assumed to be sparse.\nThe horseshoe prior is specified as,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\sim \\mathcal C^+(0, 1),\n\\end{aligned}\\] where \\(\\mathcal C^+(0, 1)\\) is a half-Cauchy distribution for the standard deviation \\(\\lambda_j\\)."
  },
  {
    "objectID": "slides/10-regularization.html#half-cauchy-distribution-1",
    "href": "slides/10-regularization.html#half-cauchy-distribution-1",
    "title": "Regularization",
    "section": "Half-Cauchy distribution",
    "text": "Half-Cauchy distribution\nA random variable \\(X \\sim \\mathcal C^+(\\mu,\\sigma)\\) follows a half-Cauchy distribution with location \\(\\mu\\) and scale \\(\\sigma &gt; 0\\) and has the following density,\n\\[f(X | \\mu, \\sigma) = \\frac{2}{\\pi \\sigma}\\frac{1}{1 + (X - \\mu)^2 / \\sigma^2},\\quad X \\geq \\mu\\]\n\nThe Half-Cauchy distribution with \\(\\mu = 0\\) is a useful prior for non-negative parameters that may be very large, as allowed by the very heavy tails of the Half-Cauchy distribution."
  },
  {
    "objectID": "slides/10-regularization.html#half-cauchy-distribution-in-stan-1",
    "href": "slides/10-regularization.html#half-cauchy-distribution-in-stan-1",
    "title": "Regularization",
    "section": "Half-Cauchy distribution in Stan",
    "text": "Half-Cauchy distribution in Stan\nIn Stan, the half-Cauchy distribution can be specified by putting a constraint on the parameter definition.\n\nparameters {\n  real&lt;lower = 0&gt; lambda;\n}\nmodel {\n  target += cauchy_lpdf(lambda | 0, 1);\n}"
  },
  {
    "objectID": "slides/10-regularization.html#horseshoe-prior-1",
    "href": "slides/10-regularization.html#horseshoe-prior-1",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\n\nThe horseshoe prior is specified as,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\sim \\mathcal C^+(0, 1),\n\\end{aligned}\\] where \\(\\mathcal C^+(0, 1)\\) is a half-Cauchy distribution for the standard deviation \\(\\lambda_j\\).\n\n\\(\\lambda_j\\)’s are the local shrinkage parameters.\n\\(\\tau\\) is the global shrinkage parameter."
  },
  {
    "objectID": "slides/10-regularization.html#horsehoe-density",
    "href": "slides/10-regularization.html#horsehoe-density",
    "title": "Regularization",
    "section": "Horsehoe density",
    "text": "Horsehoe density\n\n\n\n\n\nFrom Carvalho 2009"
  },
  {
    "objectID": "slides/10-regularization.html#horseshoe-prior-2",
    "href": "slides/10-regularization.html#horseshoe-prior-2",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\nThe horseshoe prior has two interesting features that make it particularly useful as a shrinkage prior for sparse problems.\n\nIt has flat, Cauchy-like tails that allow strong signals to remain large (that is, un-shrunk) a posteriori.\nIt has an infinitely tall spike at the origin that provides severe shrinkage for the zero elements of \\(\\boldsymbol{\\beta}\\).\n\nAs we will see, these are key elements that make the horseshoe an attractive choice for handling sparse vectors."
  },
  {
    "objectID": "slides/10-regularization.html#relation-to-other-shrinkage-priors",
    "href": "slides/10-regularization.html#relation-to-other-shrinkage-priors",
    "title": "Regularization",
    "section": "Relation to other shrinkage priors",
    "text": "Relation to other shrinkage priors\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\sim f(\\lambda_j)\n\\end{aligned}\\]\n\n\\(\\lambda_j = \\lambda\\), implies a Gaussian prior for \\(\\beta_j\\) (Ridge regression).\n\\(f(\\lambda_j) = \\text{Exponential}(2)\\), implies independent Laplacian priors for \\(\\beta_j\\) (LASSO).\n\\(f(\\lambda_j) = \\text{Inverse-Gamma}(a,b)\\), implies independent Student-t priors for \\(\\beta_j\\) (relevance vector machine)."
  },
  {
    "objectID": "slides/10-regularization.html#shrinkage-of-each-prior",
    "href": "slides/10-regularization.html#shrinkage-of-each-prior",
    "title": "Regularization",
    "section": "Shrinkage of each prior",
    "text": "Shrinkage of each prior\n\nDefine the posterior mean of \\(\\beta_j\\) as \\(\\bar{\\beta}_j\\) and the maximum likelihood estimator for \\(\\beta_j\\) as \\(\\hat{\\beta}_j\\).\nThe following relationship holds: \\(\\bar{\\beta}_j = (1 - \\kappa_j) \\hat{\\beta}_j\\),\n\n\\[\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}s_j^2\\lambda_j^2}.\\]\n\n\\(\\kappa_j\\) is called the shrinkage factor for \\(\\beta_j\\).\n\\(s_j^2 = \\mathbb{V}(x_j)\\) is the variance for each predictor."
  },
  {
    "objectID": "slides/10-regularization.html#standardization-of-predictors",
    "href": "slides/10-regularization.html#standardization-of-predictors",
    "title": "Regularization",
    "section": "Standardization of predictors",
    "text": "Standardization of predictors\n\nIn regularization problems, predictors are standardized (to mean zero and standard deviation one).\nThis means that so that \\(s_j = 1\\).\nShrinkage parameter:\n\n\\[\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}\\lambda_j^2}.\\]\n\n\\(\\kappa_j = 1\\), implies complete shrinkage.\n\\(\\kappa_j = 0\\), implies no shrinkage."
  },
  {
    "objectID": "slides/10-regularization.html#shrinkage-parameter",
    "href": "slides/10-regularization.html#shrinkage-parameter",
    "title": "Regularization",
    "section": "Shrinkage parameter",
    "text": "Shrinkage parameter\n\n\n\n\n\nFrom Carvalho 2009"
  },
  {
    "objectID": "slides/10-regularization.html#horseshoe-shrinkage-parameter",
    "href": "slides/10-regularization.html#horseshoe-shrinkage-parameter",
    "title": "Regularization",
    "section": "Horseshoe shrinkage parameter",
    "text": "Horseshoe shrinkage parameter\n\nChoosing \\(\\lambda_j ∼ \\mathcal C^+(0, 1)\\) implies \\(\\kappa_j ∼ \\text{Beta}(0.5, 0.5)\\), a density that is symmetric and unbounded at both 0 and 1.\nThis horseshoe-shaped shrinkage profile expects to see two things a priori:\n\nStrong signals (\\(\\kappa \\approx 0\\), no shrinkage), and\nZeros (\\(\\kappa \\approx 1\\), total shrinkage)."
  },
  {
    "objectID": "slides/10-regularization.html#spike-and-slab-prior",
    "href": "slides/10-regularization.html#spike-and-slab-prior",
    "title": "Regularization",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nThe prior is often written as a two-component mixture of Gaussians,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, c^2, \\epsilon &\\sim \\lambda_j N(0, c^2) + (1-\\lambda_j) N(0,\\omega^2)\\\\\n\\lambda_j &\\sim \\text{Bernoulli}(\\pi).\n\\end{aligned}\\]\n\n\\(\\omega \\ll c\\) and the indicator variable \\(\\lambda_j \\in \\{0, 1\\}\\) denotes whether \\(\\beta_j\\) is close to zero (comes from the “spike”, \\(\\lambda_j = 0\\)) or nonzero (comes from the “slab”, \\(\\lambda_j = 1\\))."
  },
  {
    "objectID": "slides/10-regularization.html#spike-and-slab-prior-1",
    "href": "slides/10-regularization.html#spike-and-slab-prior-1",
    "title": "Regularization",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nOften \\(\\omega = 0\\) (the spike is a true spike), and the prior can be written as,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, c^2 &\\sim N(0, \\lambda_j^2 c^2)\\\\\n\\lambda_j &\\sim \\text{Bernoulli}(\\pi).\n\\end{aligned}\\]\n\nInstead of giving continuous priors for \\(\\lambda_j\\)’s as in the horseshoe, here only two values are allowed (0,1).\nThe shrinkage factor \\(\\kappa_j\\) only has mass at \\(\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}}\\) and \\(\\kappa_j = 1\\) with probabilities \\(\\pi\\) and \\(1-\\pi\\),"
  },
  {
    "objectID": "slides/10-regularization.html#similarity-to-horseshoe",
    "href": "slides/10-regularization.html#similarity-to-horseshoe",
    "title": "Regularization",
    "section": "Similarity to horseshoe",
    "text": "Similarity to horseshoe\n\nLetting \\(c \\rightarrow \\infty\\), all the mass is concentrated at the extremes \\(\\kappa_j = 0\\) and \\(\\kappa_j = 1\\) (this resembles the horseshoe).\n\n\n\n\n\n\nFrom Piironena and Vehtari 2017\n\nThe horseshoe can be seen as a continuous approximation to the spike-and-slab prior as \\(c \\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/10-regularization.html#global-shrinkage-parameter-tau",
    "href": "slides/10-regularization.html#global-shrinkage-parameter-tau",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\nThe prior mean can be shown to be,\n\n\\[\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = \\frac{\\tau \\sigma^{-1} \\sqrt{n}}{1 + \\tau \\sigma^{-1} \\sqrt{n}}p.\\]\n\nSetting \\(\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = p_0\\) (prior guess for the number of non-zero coefficients) yields for \\(\\tau\\),\n\n\\[\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/10-regularization.html#global-shrinkage-parameter-tau-1",
    "href": "slides/10-regularization.html#global-shrinkage-parameter-tau-1",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/10-regularization.html#regularized-horseshoe-prior",
    "href": "slides/10-regularization.html#regularized-horseshoe-prior",
    "title": "Regularization",
    "section": "Regularized horseshoe prior",
    "text": "Regularized horseshoe prior\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau, c &\\sim N\\left(0, \\tau^2 \\tilde{\\lambda}_j^2\\right),\\quad \\tilde{\\lambda}_j^2 = \\frac{c^2 \\lambda_j^2}{c^2 + \\tau^2 \\lambda_j^2},\\\\\n\\lambda_j &\\sim \\mathcal C^+(0,1).\n\\end{aligned}\\]\n\nWhen \\(\\tau^2 \\lambda_j^2 \\ll c^2\\) (i.e., \\(\\beta_j\\) close to zero), \\(\\beta_j \\sim  N\\left(0, \\tau^2\\lambda_j^2\\right)\\)\nWhen \\(\\tau^2 \\lambda_j^2 \\gg c^2\\), (i.e., \\(\\beta_j\\) far from zero), \\(\\beta_j \\sim  N\\left(0, c^2\\right)\\)\n\\(c \\rightarrow \\infty\\) recovers the original horseshoe.\n\nWhy is this an appealing extension?"
  },
  {
    "objectID": "slides/10-regularization.html#global-shrinkage-parameter-tau-2",
    "href": "slides/10-regularization.html#global-shrinkage-parameter-tau-2",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\nSetting \\(\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = p_0\\) (prior guess for the number of non-zero coefficients) yields for \\(\\tau\\),\n\n\\[\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/10-regularization.html#regularized-horseshoe-compared-to-spike-and-slab",
    "href": "slides/10-regularization.html#regularized-horseshoe-compared-to-spike-and-slab",
    "title": "Regularization",
    "section": "Regularized horseshoe compared to spike-and-slab",
    "text": "Regularized horseshoe compared to spike-and-slab\n\nThe regularized horseshoe prior is comparable to the spike-and-slab with finite \\(c\\).\n\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/10-regularization.html#choosing-a-prior-for-c2",
    "href": "slides/10-regularization.html#choosing-a-prior-for-c2",
    "title": "Regularization",
    "section": "Choosing a prior for \\(c^2\\)",
    "text": "Choosing a prior for \\(c^2\\)\n\nUnless substantial knowledge about the scale of the relevant coefficients exists, it is recommended to place a prior for \\(c\\) instead of fixing it.\nOften a reasonable choice is, \\[c^2 \\sim \\text{Inv-Gamma}(\\alpha, \\beta), \\quad \\alpha = \\nu/2, \\beta = \\nu s^2 / 2,\\]\nThis translates to a \\(t_{\\nu}(0,s^2)\\) slab for the coefficients far from 0.\nAnother motivation for using inverse-Gamma is that it has a heavy right tail accompanied by a light left tail thereby preventing much mass from accumulating near zero."
  },
  {
    "objectID": "slides/10-regularization.html#choosing-a-prior-for-tau",
    "href": "slides/10-regularization.html#choosing-a-prior-for-tau",
    "title": "Regularization",
    "section": "Choosing a prior for \\(\\tau\\)",
    "text": "Choosing a prior for \\(\\tau\\)\n\nCarvalho et al. 2009 suggest \\(\\tau \\sim \\mathcal C^+(0,1)\\).\nPolson and Scott 2011 recommend \\(\\tau | \\sigma \\sim \\mathcal C^+(0, \\sigma^2)\\).\nAnother prior comes from a quantity called the effective number of nonzero coefficients,\n\\[m_{eff} = \\sum_{j=1}^p (1 - \\kappa_j).\\]"
  },
  {
    "objectID": "slides/11-classification.html#review-of-last-lecture",
    "href": "slides/11-classification.html#review-of-last-lecture",
    "title": "Classification",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we learned about Bayesian approaches to regularization.\n\nGlobal-local shrinage priors\n\nToday, we will focus on classification: logistic regression, multinomial regression, ordinal regression."
  },
  {
    "objectID": "slides/11-classification.html#models-for-binary-outcomes",
    "href": "slides/11-classification.html#models-for-binary-outcomes",
    "title": "Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nSuppose we have a binary outcome (e.g., \\(Y = 1\\) if a condition is satisfied and \\(Y = 0\\) if not) and predictors on a variety of scales.\nIf the predictors are discrete and the binary outcomes are independent, we can use the Bernoulli distribution for individual 0-1 data or the binomial distribution for grouped data that are counts of successes in each group."
  },
  {
    "objectID": "slides/11-classification.html#models-for-binary-outcomes-1",
    "href": "slides/11-classification.html#models-for-binary-outcomes-1",
    "title": "Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nLet’s suppose we want to model \\(P(Y = 1)\\).\nOne strategy might be to simply fit a linear regression model to the probabilities.\nFor example,\n\n\\[\\begin{aligned}\nP(Y_i = 1) &= \\alpha + \\beta_1x_{i1} + \\cdots + \\beta_px_{ip} + \\epsilon_i\\\\\n&= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11-classification.html#prepare-for-next-class",
    "href": "slides/11-classification.html#prepare-for-next-class",
    "title": "Classification",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03, which was just assigned.\nComplete reading to prepare for next Tuesdays’s lecture\nTuesday’s lecture: Missing data\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/11-classification.html#primary-biliary-cirrhosis",
    "href": "slides/11-classification.html#primary-biliary-cirrhosis",
    "title": "Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nThe Mayo Clinic conducted a trial for primary biliary cirrhosis, comparing the drug D-penicillamine vs. placebo. Patients were followed for a specified duration, and their status at the end of follow-up (whether they died) was recorded.\nResearchers are interested in predicting whether a patient died based on the following variables:\n\nascites: whether the patient had ascites (1 = yes, 0 = no)\nbilirubin: serum bilirubin in mg/dL\nstage: histologic stage of disease (ordinal categorical variable with stages 1, 2, 3, and 4)"
  },
  {
    "objectID": "slides/11-classification.html#what-can-go-wrong",
    "href": "slides/11-classification.html#what-can-go-wrong",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nSuppose we fit the following model:\n\n\\[\\begin{aligned}\nP(Y_i = 1) &= \\alpha + \\beta_1(ascites)_i + \\beta_2(bilirubin)_i\\\\\n&\\quad+\\beta_3(stage = 2)_i + \\beta_4(stage = 3)_i\\\\\n&\\quad+\\beta_5(stage = 4)_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\n\\end{aligned}\\]\n\nWhat can go wrong?"
  },
  {
    "objectID": "slides/11-classification.html#what-can-go-wrong-1",
    "href": "slides/11-classification.html#what-can-go-wrong-1",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "slides/11-classification.html#what-can-go-wrong-2",
    "href": "slides/11-classification.html#what-can-go-wrong-2",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nAdditionally, as a probability, \\(P(Y_i = 1)\\) must be in the interval [0, 1], but there is nothing in the model that enforces this constraint, so that you could be estimating probabilities that are negative or that are greater than 1!"
  },
  {
    "objectID": "slides/11-classification.html#from-probabilities-to-log-odds",
    "href": "slides/11-classification.html#from-probabilities-to-log-odds",
    "title": "Classification",
    "section": "From probabilities to log-odds",
    "text": "From probabilities to log-odds\n\nSuppose the probability of an event is \\(\\pi\\).\nThen the odds that the event occurs is \\(\\frac{\\pi}{1 - \\pi}\\).\nTaking the (natural) log of the odds, we have the logit of \\(\\pi\\): the log-odds:\n\n\\[\\text{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).\\]\n\nNote that although \\(\\pi\\) is constrained to lie between 0 and 1, the logit of \\(\\pi\\) is unconstrained - it can be anything from \\(-\\infty\\) to \\(\\infty\\)."
  },
  {
    "objectID": "slides/11-classification.html#logistic-regression-model",
    "href": "slides/11-classification.html#logistic-regression-model",
    "title": "Classification",
    "section": "Logistic regression model",
    "text": "Logistic regression model\n\nLet’s create a model for the logit of \\(\\pi\\): \\(\\text{logit}(\\pi_i)= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\\)\nThis is a linear model for a transformation of the outcome of interest, and is also equivalent to,\n\n\\[\\pi_i = \\frac{\\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})}{1 + \\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})} = \\text{expit}(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}).\\]\n\nThe expression on the right is called a logistic function and cannot yield a value that is negative or a value that is &gt;1. Fitting a model of this form is known as logistic regression."
  },
  {
    "objectID": "slides/11-classification.html#other-models-for-binary-data",
    "href": "slides/11-classification.html#other-models-for-binary-data",
    "title": "Classification",
    "section": "Other models for binary data",
    "text": "Other models for binary data\n\nOther transformations (also called link functions) can be used to ensure the probabilities lie in [0, 1], including the Probit (popular in Bayesian statistics)."
  },
  {
    "objectID": "slides/11-classification.html#logistic-regression",
    "href": "slides/11-classification.html#logistic-regression",
    "title": "Classification",
    "section": "Logistic regression",
    "text": "Logistic regression\n\\[\\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nNegative logits represent probabilities less than one-half, and positive logits represent probabilities above one-half."
  },
  {
    "objectID": "slides/11-classification.html#interpreting-parameters-in-logistic-regression",
    "href": "slides/11-classification.html#interpreting-parameters-in-logistic-regression",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nTypically we interpret functions of parameters in logistic regression rather than the parameters themselves.\n\nFor the simple model: \\(\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\beta x_{i},\\) we note that the probability that \\(Y_i = 1\\) when \\(X_i = 0\\) is\n\\[P(Y_i = 1 | X_{i} = 0) = \\frac{\\exp(\\alpha)}{1 + \\exp(\\alpha)}.\\]"
  },
  {
    "objectID": "slides/11-classification.html#interpreting-parameters-in-logistic-regression-1",
    "href": "slides/11-classification.html#interpreting-parameters-in-logistic-regression-1",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nSuppose that \\(X\\) is a binary (0/1) variable (e.g., \\(X\\) = 1 for males and 0 for non-males).\n\nIn this case, we interpret \\(\\exp(\\beta)\\) as the odds ratio (OR) of the response for the two possible levels of \\(X\\).\nFor \\(X\\) on other scales, \\(\\exp(\\beta)\\) is interpreted as the odds ratio of the response comparing two values of \\(X\\) one unit apart.\n\nWhy?"
  },
  {
    "objectID": "slides/11-classification.html#interpreting-parameters-in-logistic-regression-2",
    "href": "slides/11-classification.html#interpreting-parameters-in-logistic-regression-2",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nThe log odds of response for \\(X = 1\\) is given by \\(\\alpha + \\beta\\), and the log odds of response for \\(X = 0\\) is \\(\\alpha\\).\nSo the odds ratio of response comparing \\(X = 1\\) to \\(X = 0\\) is given by \\(\\frac{\\exp(\\alpha + \\beta)}{\\exp(\\alpha)} = \\exp(\\beta)\\).\nIn a multivariable logistic regression model with more than one predictor, this OR is interpreted conditionally on values of other variables (i.e., controlling for them)."
  },
  {
    "objectID": "slides/11-classification.html#bayesian-logistic-regression",
    "href": "slides/11-classification.html#bayesian-logistic-regression",
    "title": "Classification",
    "section": "Bayesian logistic regression",
    "text": "Bayesian logistic regression\n\nWe start with observations \\(Y_i \\in \\{0,1\\}\\) for \\(i = 1,\\ldots,n\\), where \\(Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\), \\(\\pi_i = P(Y_i = 1)\\).\nThe log-odds are modeled as \\(\\text{logit}(\\pi_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nTo complete the Bayesian model specification, we must place priors on \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\).\n\nAll priors we have discussed up-to-this point apply!\n\nHistorically, this was a difficult model to fit, but can be easily implemented in Stan."
  },
  {
    "objectID": "slides/11-classification.html#logistic-regression-in-stan",
    "href": "slides/11-classification.html#logistic-regression-in-stan",
    "title": "Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\n// Saved in logistic_regression.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n}\nmodel {\n  target += bernoulli_logit_lpmf(Y | alpha + X * beta);\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = bernoulli_logit_rng(alpha + height_c[i] * beta);\n    log_lik[i] = bernoulli_logit_lmpf(Y[i] | alpha + X[i, ] * beta);\n  }\n}"
  },
  {
    "objectID": "slides/11-classification.html#primary-biliary-cirrhosis-1",
    "href": "slides/11-classification.html#primary-biliary-cirrhosis-1",
    "title": "Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nhead(pbc)\n\n  id trt      age sex ascites hepato spiders edema bili chol albumin copper\n1  1   1 58.76523   f       1      1       1   1.0 14.5  261    2.60    156\n2  2   1 56.44627   f       0      1       1   0.0  1.1  302    4.14     54\n3  3   1 70.07255   m       0      0       0   0.5  1.4  176    3.48    210\n4  4   1 54.74059   f       0      1       1   0.5  1.8  244    2.54     64\n5  5   2 38.10541   f       0      1       1   0.0  3.4  279    3.53    143\n6  7   2 55.53457   f       0      1       0   0.0  1.0  322    4.09     52\n  alk.phos    ast trig platelet protime stage outcome\n1   1718.0 137.95  172      190    12.2     4       1\n2   7394.8 113.52   88      221    10.6     3       0\n3    516.0  96.10   55      151    12.0     4       1\n4   6121.8  60.63   92      183    10.3     4       1\n5    671.0 113.15   72      136    10.9     3       1\n6    824.0  60.45  213      204     9.7     3       0"
  },
  {
    "objectID": "slides/11-classification.html#logistic-regression-in-stan-1",
    "href": "slides/11-classification.html#logistic-regression-in-stan-1",
    "title": "Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\nlibrary(rstan)\ncompiled_model &lt;- stan_model(file = \"logistic_regression.stan\")\nfit &lt;- sampling(compiled_model, data = stan_data)\nprint(fit, pars = c(\"alpha\", \"beta\"), probs = c(0.025, 0.5, 0.975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n         mean se_mean   sd  2.5%   50% 97.5% n_eff Rhat\nalpha   -3.35    0.04 1.21 -6.13 -3.24 -1.41   791    1\nbeta[1]  2.24    0.04 1.32  0.18  2.05  5.48  1252    1\nbeta[2]  0.38    0.00 0.08  0.24  0.38  0.54  2005    1\nbeta[3]  1.71    0.04 1.25 -0.34  1.59  4.55   783    1\nbeta[4]  2.19    0.04 1.22  0.20  2.07  4.96   806    1\nbeta[5]  2.61    0.04 1.23  0.59  2.48  5.46   798    1\n\nSamples were drawn using NUTS(diag_e) at Mon Dec 30 14:36:11 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/11-classification.html#prepare-data-for",
    "href": "slides/11-classification.html#prepare-data-for",
    "title": "Classification",
    "section": "Prepare data for",
    "text": "Prepare data for"
  },
  {
    "objectID": "slides/11-classification.html#prepare-data-for-stan",
    "href": "slides/11-classification.html#prepare-data-for-stan",
    "title": "Classification",
    "section": "Prepare data for Stan",
    "text": "Prepare data for Stan\n\nX &lt;- model.matrix(object = ~ ascites + bili + as.factor(stage), data = pbc)[, -1]\nY &lt;- pbc$outcome\nstan_data &lt;- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X)\nhead(X)\n\n  ascites bili as.factor(stage)2 as.factor(stage)3 as.factor(stage)4\n1       1 14.5                 0                 0                 1\n2       0  1.1                 0                 1                 0\n3       0  1.4                 0                 0                 1\n4       0  1.8                 0                 0                 1\n5       0  3.4                 0                 1                 0\n6       0  1.0                 0                 1                 0"
  },
  {
    "objectID": "slides/11-classification.html#convergence-diagnostics",
    "href": "slides/11-classification.html#convergence-diagnostics",
    "title": "Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/11-classification.html#convergence-diagnostics-1",
    "href": "slides/11-classification.html#convergence-diagnostics-1",
    "title": "Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/11-classification.html#back-to-the-pbc-data",
    "href": "slides/11-classification.html#back-to-the-pbc-data",
    "title": "Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nFitting a logistic regression model, we obtain\n\n\n\n\n\n\n\nvariable\nmean\nsd\n2.5%\n97.5%\n\n\n\n\nalpha\nintercept\n-3.35\n0.04\n-6.13\n-1.41\n\n\nbeta[1]\nascites\n2.24\n0.04\n0.18\n5.48\n\n\nbeta[2]\nbilirubin\n0.38\n0.00\n0.24\n0.54\n\n\nbeta[3]\nstage == 2\n1.71\n0.04\n-0.34\n4.55\n\n\nbeta[4]\nstage == 3\n2.19\n0.04\n0.20\n4.96\n\n\nbeta[5]\nstage == 4\n2.61\n0.04\n0.59\n5.46\n\n\n\n\n\n\nHow might we interpret these coefficients as odds ratios?"
  },
  {
    "objectID": "slides/11-classification.html#back-to-the-pbc-data-1",
    "href": "slides/11-classification.html#back-to-the-pbc-data-1",
    "title": "Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nRemember, we are interested in the probability that a patient died during follow-up (a “success”). We are predicting the log-odds of this event happening.\n\nThe posterior mean for ascites was 2.24. Thus, the odds ratio for dying is \\(\\exp(2.24) \\approx 9.4\\). That is, patients with ascites have 9.4 times the odds of dying compared to patients that do not, holding all other variables constant.\nThe posterior mean for bilirubin was 0.38. Thus, the odds ratio for dying for a patient with 1 additional mg/dL serum bilirubin compared to another is \\(\\exp(0.38) \\approx 1.46\\), holding all other variables constant.\nThe baseline stage was 1. The posterior mean for stage 3 was 2.19. Thus, patients in stage 3 have approximately 8.93 times the odds of dying compared to patients that do not, holding all other variables constant."
  },
  {
    "objectID": "slides/11-classification.html#predicted-probabilities",
    "href": "slides/11-classification.html#predicted-probabilities",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nThere is a one-to-one relationship between \\(\\pi\\) and \\(\\text{logit}(\\pi)\\). So, if we predict \\(\\text{logit}(\\pi)\\), we can “back-transform” to get back to a predicted probability.\nFor instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\nx_i &lt;- matrix(c(0, 5, 1, 0, 0), ncol = 1)\npars &lt;- rstan::extract(fit, pars = c(\"alpha\", \"beta\"))\nlog_odds &lt;- pars$alpha + as.numeric(pars$beta %*% x_i)\npi &lt;- exp(log_odds) / (1 + exp(log_odds))"
  },
  {
    "objectID": "slides/11-classification.html#predicted-probabilities-1",
    "href": "slides/11-classification.html#predicted-probabilities-1",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior mean of the predicted probabilities is 0.56."
  },
  {
    "objectID": "slides/11-classification.html#posterior-predictive-checks",
    "href": "slides/11-classification.html#posterior-predictive-checks",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/11-classification.html#posterior-predictive-checks-1",
    "href": "slides/11-classification.html#posterior-predictive-checks-1",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_bars(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/11-classification.html#posterior-predictive-checks-2",
    "href": "slides/11-classification.html#posterior-predictive-checks-2",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_stat(stan_data$Y, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(stan_data$Y, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(stan_data$Y, y_pred, stat = \"q025\")\nppc_stat(stan_data$Y, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/11-classification.html#model-comparison",
    "href": "slides/11-classification.html#model-comparison",
    "title": "Classification",
    "section": "Model comparison",
    "text": "Model comparison\n\nComparing our model to a baseline that removed stage.\n\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_baseline &lt;- loo::extract_log_lik(fit_baseline, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model &lt;- loo::waic(log_lik)\nwaic_model_baseline &lt;- loo::waic(log_lik_baseline)\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"full\" = waic_model, \"baseline\" = waic_model_baseline))\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nfull        0.00      0.00 -155.94      9.51         7.69    2.09    311.88\nbaseline   -1.69      3.09 -157.63      9.36         4.86    1.98    315.25\n         se_waic\nfull       19.01\nbaseline   18.73"
  },
  {
    "objectID": "slides/11-classification.html#generalized-linear-models",
    "href": "slides/11-classification.html#generalized-linear-models",
    "title": "Classification",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nPreviously, we have focused on linear regression. Other forms of regression follow naturally from linear regression."
  },
  {
    "objectID": "slides/11-classification.html#steps-to-selecting-a-bayesian-glm",
    "href": "slides/11-classification.html#steps-to-selecting-a-bayesian-glm",
    "title": "Classification",
    "section": "Steps to selecting a Bayesian GLM",
    "text": "Steps to selecting a Bayesian GLM\n\nIdentify the support of the response distribution.\nSelect the likelihood by picking a parametric family of distributions with this support.\nChoose a link function \\(g\\) that transforms the range of parameters to the whole real line.\nSpecify a linear model on the transformed parameters.\nSelect priors for the regression coefficients."
  },
  {
    "objectID": "slides/11-classification.html#example-of-selecting-a-bayesian-glm",
    "href": "slides/11-classification.html#example-of-selecting-a-bayesian-glm",
    "title": "Classification",
    "section": "Example of selecting a Bayesian GLM",
    "text": "Example of selecting a Bayesian GLM\n\nSupport: \\(Y_i \\in \\{0, 1, 2, \\ldots\\}\\).\nLikelihood family: \\(Y_i \\stackrel{ind}{\\sim} \\text{Poisson}(\\lambda_i)\\).\nLink: \\(g(\\lambda_i) = \\log(\\lambda_i) \\in (−\\infty, \\infty)\\).\nRegression model: \\(\\log(\\lambda_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nPriors: \\(\\alpha, \\beta_j \\sim N(0, 10^2)\\)."
  },
  {
    "objectID": "slides/11-classification.html#multinomial-regression",
    "href": "slides/11-classification.html#multinomial-regression",
    "title": "Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nAssume an outcome \\(Y_i \\in \\{1,\\ldots,K\\}\\). You can imagine running \\(K\\) independent binary logistic regression models, in which one outcome is chosen as a “pivot” and then the other K − 1 outcomes are separately regressed against the pivot outcome. If outcome K (the last outcome) is chosen as the pivot, the K − 1 regression equations are:\n\\[\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}\\]\n\nIt can be seen that:\n\n\\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\] and \\(P(Y_i = K) = 1 - \\sum_{j=1}^{K-1}P(Y_i = j)\\).\n\nThis is known as the additive log-ratio model."
  },
  {
    "objectID": "slides/11-classification.html#multinomial-regression-1",
    "href": "slides/11-classification.html#multinomial-regression-1",
    "title": "Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nLet \\(Y_i \\in \\{1,\\ldots,K\\}\\) be an ordinal outcome with \\(K\\) categories. Then \\(P(Y \\leq k)\\) is the cumulative probability of \\(Y\\) less than or equal to a specific category \\(k=1,\\ldots,K-1\\). The odds of being less than or equal a particular category can be defined as, \\[\\frac{P(Y\\leq k)}{P(Y &gt; k)}\\] for \\(k=1,\\ldots,K-1\\), since \\(P(Y &gt; K) = 0\\) and dividing by zero is undefined. The log odds is also known as the logit, so that \\[\\log \\frac{P(Y\\leq k)}{P(Y &gt; k)} = \\text{logit}P(Y\\leq k) = \\alpha_k + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nProportional odds regression."
  },
  {
    "objectID": "slides/11-classification.html#ordinal-regression-using-stan",
    "href": "slides/11-classification.html#ordinal-regression-using-stan",
    "title": "Classification",
    "section": "Ordinal regression using Stan",
    "text": "Ordinal regression using Stan\n\ndata {\n  int&lt;lower = 2&gt; K;\n  int&lt;lower = 0&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1, upper = K&gt; Y[n]\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  ordered[K - 1] alpha;\n}\nmodel {\n  target += ordered_logistic_glm_lpmf(Y | X, beta, alpha);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(alpha | 0, 10);\n}"
  },
  {
    "objectID": "slides/10-robust.html#review-of-last-lecture",
    "href": "slides/10-robust.html#review-of-last-lecture",
    "title": "Robust Regression",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Thursday, we learned about approaches for nonlinear regression."
  },
  {
    "objectID": "slides/10-robust.html#a-motivating-research-question",
    "href": "slides/10-robust.html#a-motivating-research-question",
    "title": "Robust Regression",
    "section": "A motivating research question",
    "text": "A motivating research question\n\nIn today’s lecture, we will look at data on serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years.\n\nA detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994).\n\nFor an example patient, we define \\(Y_i\\) as the serum concentration value and \\(X_i\\) as a child’s age, given in years."
  },
  {
    "objectID": "slides/10-robust.html#pulling-the-data",
    "href": "slides/10-robust.html#pulling-the-data",
    "title": "Robust Regression",
    "section": "Pulling the data",
    "text": "Pulling the data\n\nlibrary(Brq)\ndata(\"ImmunogG\")\nhead(ImmunogG)\n\n  IgG Age\n1 1.5 0.5\n2 2.7 0.5\n3 1.9 0.5\n4 4.0 0.5\n5 1.9 0.5\n6 4.4 0.5"
  },
  {
    "objectID": "slides/10-robust.html#visualizing-igg-data",
    "href": "slides/10-robust.html#visualizing-igg-data",
    "title": "Robust Regression",
    "section": "Visualizing IgG data",
    "text": "Visualizing IgG data"
  },
  {
    "objectID": "slides/10-robust.html#disease-progression",
    "href": "slides/10-robust.html#disease-progression",
    "title": "Robust Regression",
    "section": "Disease progression",
    "text": "Disease progression\n\nOLS can be written as follows for \\(i = 1,\\ldots,n\\),\n\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2).\\]\n\n\\(\\beta_1\\) represent the the change in IgG serum concentration a one year increase in age.\nOften the following hypothesis is tested: \\[H_0: \\beta_1=0,H_1: \\beta_1 &lt; 0.\\]"
  },
  {
    "objectID": "slides/10-robust.html#ols-regression-assumptions",
    "href": "slides/10-robust.html#ols-regression-assumptions",
    "title": "Robust Regression",
    "section": "OLS regression assumptions",
    "text": "OLS regression assumptions\n\\[\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{aligned}\\]\nAssumptions:\n\n\\(Y_i\\) are independent observations (independence).\n\\(Y_i\\) is linearly related to \\(X_i\\) (linearity).\n\\(\\epsilon_i = Y_i - \\mu_i\\) is normally distributed (normality).\n\\(\\epsilon_i\\) has constant variance across \\(X_i\\) (homoskedasticity)."
  },
  {
    "objectID": "slides/10-robust.html#assessing-assumptions",
    "href": "slides/10-robust.html#assessing-assumptions",
    "title": "Robust Regression",
    "section": "Assessing assumptions",
    "text": "Assessing assumptions"
  },
  {
    "objectID": "slides/10-robust.html#robust-regression",
    "href": "slides/10-robust.html#robust-regression",
    "title": "Robust Regression",
    "section": "Robust regression",
    "text": "Robust regression\n\nToday we will learn about regression techniques that are robust to the assumptions of linear regression.\nWe will introduce the idea of robust regression by exploring ways to generalize the homoskedastic variance assumption in linear regression.\nWe will touch on heteroskedasticity, heavy-tailed distributions, and median regression (more generally quantile regression)."
  },
  {
    "objectID": "slides/10-robust.html#heteroskedasticity",
    "href": "slides/10-robust.html#heteroskedasticity",
    "title": "Robust Regression",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nHeteroskedasticity is the violation of the assumption of constant variance.\nHow can we handle this?\nIn OLS, there are approaches like heteroskedastic consistent errors, but this is not a generative model.\nIn the Bayesian framework, we generally like to write down generative models."
  },
  {
    "objectID": "slides/10-robust.html#weighted-regression",
    "href": "slides/10-robust.html#weighted-regression",
    "title": "Robust Regression",
    "section": "Weighted regression",
    "text": "Weighted regression\n\nA common case is weighted regression, where each \\(Y_i\\) represents the mean of \\(n_i\\) observations. Then the scale of each observation is, \\[\\tau_i^2 = \\sigma^2/n_i,\\] where \\(\\sigma^2\\) is a global scale parameter.\nAlternatively, suppose each observation represents the sum of each \\(n_i\\) observations. Then the scale of each observation is, \\[\\tau_i^2 = n_i \\sigma^2.\\]"
  },
  {
    "objectID": "slides/10-robust.html#modeling-the-scale-with-covariates",
    "href": "slides/10-robust.html#modeling-the-scale-with-covariates",
    "title": "Robust Regression",
    "section": "Modeling the scale with covariates",
    "text": "Modeling the scale with covariates\n\nThe scale can also be modeled with covariates.\nIt is common to model the log-transformation of the scale or variance to transform it to \\(\\mathbb{R}\\),\n\n\\[\\log \\tau_i^2 = \\mathbf{z}_i \\boldsymbol{\\gamma},\\]\nwhere \\(\\mathbf{z}_i = (z_{i1},\\ldots,z_{ip})\\) are a \\(p\\)-dimensional vector of covariates and \\(\\boldsymbol{\\gamma}\\) are parameters that regress the covariates onto the log variance.\n\nOther options include: \\(\\log \\tau_i^2 = \\mathbf{z}_i \\boldsymbol{\\gamma} + \\nu_i,\\quad \\nu_i \\sim N(0, \\sigma^2)\\)\nOther options include: \\(\\log \\tau_i^2 = f(\\mu_i)\\)\nAny plausible generative model can be specified!"
  },
  {
    "objectID": "slides/10-robust.html#modeling-the-scale-with-covariates-1",
    "href": "slides/10-robust.html#modeling-the-scale-with-covariates-1",
    "title": "Robust Regression",
    "section": "Modeling the scale with covariates",
    "text": "Modeling the scale with covariates\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1&gt; q;\n  vector[n] Y;\n  matrix[n, p] X;\n  matrix[n, q] Z;\n}\nparameters {\n  vector[p] beta;\n  vector[q] gamma;\n}\ntransformed parameters {\n  vector[n] tau = exp(Z * gamma);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n}"
  },
  {
    "objectID": "slides/10-robust.html#heteroskedastic-variance",
    "href": "slides/10-robust.html#heteroskedastic-variance",
    "title": "Robust Regression",
    "section": "Heteroskedastic variance",
    "text": "Heteroskedastic variance\n\nWe can write the regression model using a observation specific variance, \\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\tau_i^2).\\]\nOne way of writing the variance is: \\(\\tau_i^2 = \\sigma^2 \\lambda_i\\).\n\n\\(\\sigma^2\\) is a global scale parameter.\n\\(\\lambda_i\\) is an observation specific scale parameter.\n\nIn the Bayesian framework, we must place a prior on \\(\\lambda_i\\)."
  },
  {
    "objectID": "slides/10-robust.html#bayesian-prior-to-induce-structure",
    "href": "slides/10-robust.html#bayesian-prior-to-induce-structure",
    "title": "Robust Regression",
    "section": "Bayesian prior to induce structure",
    "text": "Bayesian prior to induce structure\n\nSuppse we would like \\(\\sum_{i=1}^n \\lambda_i = 1\\), \\(\\lambda_i &gt;0\\).\nWe could specify the following,\n\n\\[\\boldsymbol{\\lambda} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha}),\\]\nwhere \\(\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots,\\lambda_n)\\) and \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_n)\\).\n\nThe prior mean is \\(\\mathbb{E}[\\lambda_i] = \\alpha_i / \\alpha_0\\), where \\(\\alpha_0 = \\sum_{i=1}^n \\alpha_i.\\)\nTypically, \\(\\alpha_i = 1 \\forall i\\)."
  },
  {
    "objectID": "slides/10-robust.html#dirchlet-prior-in-stan",
    "href": "slides/10-robust.html#dirchlet-prior-in-stan",
    "title": "Robust Regression",
    "section": "Dirchlet prior in Stan",
    "text": "Dirchlet prior in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  vector&lt;lower = 0&gt;[n] alpha;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  simplex[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += dirichlet_lpdf(lambda | alpha);\n}"
  },
  {
    "objectID": "slides/10-robust.html#a-prior-to-induce-a-heavy-tail",
    "href": "slides/10-robust.html#a-prior-to-induce-a-heavy-tail",
    "title": "Robust Regression",
    "section": "A prior to induce a heavy-tail",
    "text": "A prior to induce a heavy-tail\n\nA common prior for \\(\\lambda_i\\) is as follows:\n\n\\[\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\\]\n\nUnder this prior, the marginal likelihood for \\(Y_i\\) is equivalent to a Student-t distribution,\n\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).\\]"
  },
  {
    "objectID": "slides/10-robust.html#understanding-the-equivalence",
    "href": "slides/10-robust.html#understanding-the-equivalence",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\nHeteroskedastic variances assumption is equivalent to assuming a heavy-tailed distribution.\n\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).\\]\n\\[\\iff\\]\n\\[\\begin{aligned}\nY_i &= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{aligned}\\]\n\nNote that since the number of \\(\\lambda_i\\) parameters is equal to the number of observations, this model will not have a proper posterior distribution without a proper prior distribution."
  },
  {
    "objectID": "slides/10-robust.html#understanding-the-equivalence-1",
    "href": "slides/10-robust.html#understanding-the-equivalence-1",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\\[\\begin{aligned}\nf(Y_i) &= \\int_0^{\\infty} f(Y_i , \\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} N(Y_i ; \\mu_i, \\sigma^2 \\lambda_i) \\text{Inverse-Gamma}\\left(\\lambda_i ; \\frac{\\nu}{2},\\frac{\\nu}{2}\\right) d\\lambda_i\\\\\n&= t_{\\nu}\\left(\\mu_i,\\sigma\\right).\n\\end{aligned}\\]\n\nThe marginal likelihood can be viewed as a mixture of a Gaussian likelihood with an Inverse-Gamma scale parameter."
  },
  {
    "objectID": "slides/10-robust.html#understanding-the-equivalence-2",
    "href": "slides/10-robust.html#understanding-the-equivalence-2",
    "title": "Robust Regression",
    "section": "Understanding the equivalence",
    "text": "Understanding the equivalence\n\\[\\begin{aligned}\nT_i &= \\frac{Z_i}{\\sqrt{\\frac{W_i}{\\nu}}},\\quad Z_i \\stackrel{iid}{\\sim} N(0,1), W_i \\stackrel{iid}{\\sim}\\chi^2_{\\nu}\\\\\n&= \\frac{Z_i}{\\sqrt{\\frac{1}{\\nu V_i}}},\\quad V_i \\stackrel{iid}{\\sim} \\text{Inv-}\\chi^2_{\\nu}\\\\\n&= \\sqrt{\\nu V_i} Z_i,\\quad \\lambda_i = \\nu V_i\\\\\n&= \\sqrt{\\lambda_i} Z_i, \\quad \\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\\\\\n&\\sim t_{\\nu}\n\\end{aligned}\\]\n\nWe then have: \\(Y_i = \\mu_i + \\sigma T_i \\sim t_{\\nu}(\\mu_i, \\sigma).\\)"
  },
  {
    "objectID": "slides/10-robust.html#student-t-in-stan",
    "href": "slides/10-robust.html#student-t-in-stan",
    "title": "Robust Regression",
    "section": "Student-t in Stan",
    "text": "Student-t in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = 0&gt; nu;\n}\nmodel {\n  target += student_t_lpdf(Y | nu, X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/10-robust.html#student-t-in-stan-mixture",
    "href": "slides/10-robust.html#student-t-in-stan-mixture",
    "title": "Robust Regression",
    "section": "Student-t in Stan: mixture",
    "text": "Student-t in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += inv_gamma_lpdf(lambda | 0.5 * nu, 0.5 * nu);\n}"
  },
  {
    "objectID": "slides/10-robust.html#why-heavy-tailed-distributions",
    "href": "slides/10-robust.html#why-heavy-tailed-distributions",
    "title": "Robust Regression",
    "section": "Why heavy-tailed distributions?",
    "text": "Why heavy-tailed distributions?\n\nReplacing the normal distribution with a distribution with heavy-tails (e.g., Student-t, Laplace) is a common approach to robust regression.\nRobust regression refers to regression methods which are less sensitive to outliers or small sample sizes.\nLinear regression, including Bayesian regression with normally distributed errors is sensitive to outliers, because the normal distribution has narrow tail probabilities.\nOur heteroskedastic model that we just explored is only one example of a robust regression model."
  },
  {
    "objectID": "slides/10-robust.html#vizualizing-heavy-tail-distributions",
    "href": "slides/10-robust.html#vizualizing-heavy-tail-distributions",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/10-robust.html#vizualizing-heavy-tail-distributions-1",
    "href": "slides/10-robust.html#vizualizing-heavy-tail-distributions-1",
    "title": "Robust Regression",
    "section": "Vizualizing heavy tail distributions",
    "text": "Vizualizing heavy tail distributions"
  },
  {
    "objectID": "slides/10-robust.html#another-example-of-robust-regression",
    "href": "slides/10-robust.html#another-example-of-robust-regression",
    "title": "Robust Regression",
    "section": "Another example of robust regression",
    "text": "Another example of robust regression\n\nLet’s revisit our general heteroskedastic regression, \\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\sigma^2 \\lambda_i).\\]\nWe can induce another form of robust regression using the following prior for \\(\\lambda_i\\), \\(\\lambda_i \\sim \\text{Exponential}(1/2)\\).\nUnder this prior, the induced marginal model is, \\[Y_i = \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\stackrel{ind}{\\sim} \\text{Laplace}(\\mu, \\sigma).\\]\n\\(f(\\epsilon_i | \\mu, \\sigma) = \\frac{1}{2\\sigma} \\exp\\left\\{-\\frac{|\\epsilon_i - \\mu|}{\\sigma}\\right\\}\\)"
  },
  {
    "objectID": "slides/10-robust.html#median-regression-using-laplace",
    "href": "slides/10-robust.html#median-regression-using-laplace",
    "title": "Robust Regression",
    "section": "Median regression using Laplace",
    "text": "Median regression using Laplace\nLeast absolute deviation (LAD) regression minimizes the following objective function,\n\\[\\hat{\\boldsymbol{\\beta}}_{\\text{LAD}} = \\arg \\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n |Y_i - \\mathbf{x}_i\\boldsymbol{\\beta}|.\\]\nThe Bayesian analog is the Laplace distribution,\n\\(f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma) = \\left(\\frac{1}{2\\sigma}\\right)^n \\exp\\left\\{-\\sum_{i=1}^n\\frac{|Y_i - \\mathbf{x}_i \\boldsymbol{\\beta}|}{\\sigma}\\right\\}.\\)"
  },
  {
    "objectID": "slides/10-robust.html#median-regression-using-laplace-1",
    "href": "slides/10-robust.html#median-regression-using-laplace-1",
    "title": "Robust Regression",
    "section": "Median regression using Laplace",
    "text": "Median regression using Laplace\n\nThe Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is \\(|x−\\mu|\\), so minimizing the likelihood will also minimize the least absolute distances.\nLaplace distribution is also known as the double-exponential distribution (symmetric exponential distributions around \\(\\mu\\) with scale \\(\\sigma\\)).\nThus, a linear regression with Laplace errors is analogous to a median regression,\nWhy is median regression considered more robust than regression of the mean?"
  },
  {
    "objectID": "slides/10-robust.html#laplace-regression-in-stan",
    "href": "slides/10-robust.html#laplace-regression-in-stan",
    "title": "Robust Regression",
    "section": "Laplace regression in Stan",
    "text": "Laplace regression in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += double_exponential_lpdf(Y | X * beta, sigma);\n}"
  },
  {
    "objectID": "slides/10-robust.html#laplace-regression-in-stan-mixture",
    "href": "slides/10-robust.html#laplace-regression-in-stan-mixture",
    "title": "Robust Regression",
    "section": "Laplace regression in Stan: mixture",
    "text": "Laplace regression in Stan: mixture\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, tau);\n  target += exponential_lpdf(lambda | 0.5);\n}"
  },
  {
    "objectID": "slides/10-robust.html#asymmetric-laplace-distribution",
    "href": "slides/10-robust.html#asymmetric-laplace-distribution",
    "title": "Robust Regression",
    "section": "Asymmetric Laplace distribution",
    "text": "Asymmetric Laplace distribution\nA random variable, \\(Y \\sim ALD_p(\\mu,\\sigma)\\) is said to follow an asymmetric Laplace distribution (ALD) if the pdf is given by,\n\\[f(Y) = \\frac{p(1-p)}{\\sigma} \\exp \\left\\{-\\rho_p\\left(\\frac{Y - \\mu}{\\sigma}\\right)\\right\\},\\]\nwhere \\(p \\in (0,1)\\) is the percentile and \\[\\rho_p(x) = x\\left(p - 1(u &lt; 0)\\right) = \\frac{|x| + (2p - 1)x}{2}.\\]\n\nWhen \\(p = 0.5\\) it reduces to a regular Laplace distribution."
  },
  {
    "objectID": "slides/10-robust.html#general-quantile-regression",
    "href": "slides/10-robust.html#general-quantile-regression",
    "title": "Robust Regression",
    "section": "General quantile regression",
    "text": "General quantile regression\n\nfunctions {\n  real asym_laplace_lpdf(real y, real mu, real sigma, real tau) {\n    return log(tau) + log1m(tau)\n      - log(sigma)\n      - 2 * ((y &lt; mu) ? (1 - tau) * (mu - y) : tau * (y - mu)) / sigma;\n  }\n}\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0, upper = 1&gt; tau;\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  for (i in 1:n) target += asym_laplace_lpdf(Y[i] | X[i, ] * beta, sigma, tau);\n}"
  },
  {
    "objectID": "slides/10-robust.html#quantile-regression",
    "href": "slides/10-robust.html#quantile-regression",
    "title": "Robust Regression",
    "section": "Quantile regression",
    "text": "Quantile regression"
  },
  {
    "objectID": "slides/10-robust.html#posterior-of-beta_1",
    "href": "slides/10-robust.html#posterior-of-beta_1",
    "title": "Robust Regression",
    "section": "Posterior of \\(\\beta_1\\)",
    "text": "Posterior of \\(\\beta_1\\)\n\n\n\n\n\nquantile\nmean\nlower\nupper\n\n\n\n\n0.025\n0.3291804\n0.2639468\n0.3899443\n\n\n0.250\n0.5130600\n0.3788182\n0.6523600\n\n\n0.500\n0.7268102\n0.5705129\n0.8836400\n\n\n0.750\n0.8093408\n0.6919899\n0.9363650\n\n\n0.975\n1.1741866\n0.9783041\n1.3936860"
  },
  {
    "objectID": "slides/10-robust.html#scale-mixture-representation",
    "href": "slides/10-robust.html#scale-mixture-representation",
    "title": "Robust Regression",
    "section": "Scale-mixture representation",
    "text": "Scale-mixture representation\nThe above may also be written as a mixture of exponential and normal distributions. Letting, \\(Z_i \\sim Exponential(1)\\) and \\(\\sigma \\sim Exponential(1)\\).\n\\[Y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\sigma \\theta Z_i + \\epsilon_i,\\quad \\epsilon_i \\sim N\\left(0, \\tau^2 \\sigma^2 Z_i\\right),\\]\nwhere \\[\\theta = \\frac{1-2p}{p(1-p)}\\] and \\[\\tau \\sqrt{\\frac{2}{p(1-p)}}.\\]"
  },
  {
    "objectID": "slides/10-robust.html#scale-mixture-in-stan",
    "href": "slides/10-robust.html#scale-mixture-in-stan",
    "title": "Robust Regression",
    "section": "Scale-mixture in Stan",
    "text": "Scale-mixture in Stan\n\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real&lt;lower = 0, upper = 1&gt; q;\n}\ntransformed data {\n  real theta = (1 - 2 * q) / (q * (1 - q));\n  real tau = sqrt(2 / (q * (1 - q)));\n}\nparameters {\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n  vector&lt;lower=0&gt;[n] z;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta + sigma * theta * z, tau * sqrt(z) * sigma);\n  target += exponential_lpdf(sigma | 1);\n  target += exponential_lpdf(z | 1);\n}"
  },
  {
    "objectID": "slides/10-robust.html#prepare-for-next-class",
    "href": "slides/10-robust.html#prepare-for-next-class",
    "title": "Robust Regression",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 02, which is due before next class.\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Regularization\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/12-classification.html#review-of-last-lecture",
    "href": "slides/12-classification.html#review-of-last-lecture",
    "title": "Classification",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Thursday, we learned about Bayesian approaches to regularization.\n\nGlobal-local shrinage priors\n\nToday, we will focus on classification: logistic regression."
  },
  {
    "objectID": "slides/12-classification.html#generalized-linear-models",
    "href": "slides/12-classification.html#generalized-linear-models",
    "title": "Classification",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nPreviously, we have focused on linear regression. Other forms of regression follow naturally from linear regression."
  },
  {
    "objectID": "slides/12-classification.html#models-for-binary-outcomes",
    "href": "slides/12-classification.html#models-for-binary-outcomes",
    "title": "Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nSuppose we have a binary outcome (e.g., \\(Y = 1\\) if a condition is satisfied and \\(Y = 0\\) if not) and predictors on a variety of scales.\nIf the predictors are discrete and the binary outcomes are independent, we can use the Bernoulli distribution for individual 0-1 data or the binomial distribution for grouped data that are counts of successes in each group."
  },
  {
    "objectID": "slides/12-classification.html#models-for-binary-outcomes-1",
    "href": "slides/12-classification.html#models-for-binary-outcomes-1",
    "title": "Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nLet’s suppose we want to model \\(P(Y = 1)\\).\nOne strategy might be to simply fit a linear regression model to the probabilities.\nFor example,\n\n\\[\\begin{aligned}\nP(Y_i = 1) &= \\alpha + \\beta_1x_{i1} + \\cdots + \\beta_px_{ip} + \\epsilon_i\\\\\n&= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/12-classification.html#primary-biliary-cirrhosis",
    "href": "slides/12-classification.html#primary-biliary-cirrhosis",
    "title": "Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nThe Mayo Clinic conducted a trial for primary biliary cirrhosis, comparing the drug D-penicillamine vs. placebo. Patients were followed for a specified duration, and their status at the end of follow-up (whether they died) was recorded.\nResearchers are interested in predicting whether a patient died based on the following variables:\n\nascites: whether the patient had ascites (1 = yes, 0 = no)\nbilirubin: serum bilirubin in mg/dL\nstage: histologic stage of disease (ordinal categorical variable with stages 1, 2, 3, and 4)"
  },
  {
    "objectID": "slides/12-classification.html#what-can-go-wrong",
    "href": "slides/12-classification.html#what-can-go-wrong",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nSuppose we fit the following model:\n\n\\[\\begin{aligned}\nP(Y_i = 1) &= \\alpha + \\beta_1(ascites)_i + \\beta_2(bilirubin)_i\\\\\n&\\quad+\\beta_3(stage = 2)_i + \\beta_4(stage = 3)_i\\\\\n&\\quad+\\beta_5(stage = 4)_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\n\\end{aligned}\\]\n\nWhat can go wrong?"
  },
  {
    "objectID": "slides/12-classification.html#what-can-go-wrong-1",
    "href": "slides/12-classification.html#what-can-go-wrong-1",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "slides/12-classification.html#what-can-go-wrong-2",
    "href": "slides/12-classification.html#what-can-go-wrong-2",
    "title": "Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nAdditionally, as a probability, \\(P(Y_i = 1)\\) must be in the interval [0, 1], but there is nothing in the model that enforces this constraint, so that you could be estimating probabilities that are negative or that are greater than 1!"
  },
  {
    "objectID": "slides/12-classification.html#from-probabilities-to-log-odds",
    "href": "slides/12-classification.html#from-probabilities-to-log-odds",
    "title": "Classification",
    "section": "From probabilities to log-odds",
    "text": "From probabilities to log-odds\n\nSuppose the probability of an event is \\(\\pi\\).\nThen the odds that the event occurs is \\(\\frac{\\pi}{1 - \\pi}\\).\nTaking the (natural) log of the odds, we have the logit of \\(\\pi\\): the log-odds:\n\n\\[\\text{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).\\]\n\nNote that although \\(\\pi\\) is constrained to lie between 0 and 1, the logit of \\(\\pi\\) is unconstrained - it can be anything from \\(-\\infty\\) to \\(\\infty\\)."
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression-model",
    "href": "slides/12-classification.html#logistic-regression-model",
    "title": "Classification",
    "section": "Logistic regression model",
    "text": "Logistic regression model\n\nLet’s create a model for the logit of \\(\\pi\\): \\(\\text{logit}(\\pi_i)= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\\)\nThis is a linear model for a transformation of the outcome of interest, and is also equivalent to,\n\n\\[\\pi_i = \\frac{\\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})}{1 + \\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})} = \\text{expit}(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}).\\]\n\nThe expression on the right is called a logistic function and cannot yield a value that is negative or a value that is &gt;1. Fitting a model of this form is known as logistic regression."
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression",
    "href": "slides/12-classification.html#logistic-regression",
    "title": "Classification",
    "section": "Logistic regression",
    "text": "Logistic regression\n\\[\\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nNegative logits represent probabilities less than one-half, and positive logits represent probabilities above one-half."
  },
  {
    "objectID": "slides/12-classification.html#interpreting-parameters-in-logistic-regression",
    "href": "slides/12-classification.html#interpreting-parameters-in-logistic-regression",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nTypically we interpret functions of parameters in logistic regression rather than the parameters themselves.\n\nFor the simple model: \\(\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\beta x_{i},\\) we note that the probability that \\(Y_i = 1\\) when \\(X_i = 0\\) is\n\\[P(Y_i = 1 | X_{i} = 0) = \\frac{\\exp(\\alpha)}{1 + \\exp(\\alpha)}.\\]"
  },
  {
    "objectID": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-1",
    "href": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-1",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nSuppose that \\(X\\) is a binary (0/1) variable (e.g., \\(X\\) = 1 for males and 0 for non-males).\n\nIn this case, we interpret \\(\\exp(\\beta)\\) as the odds ratio (OR) of the response for the two possible levels of \\(X\\).\nFor \\(X\\) on other scales, \\(\\exp(\\beta)\\) is interpreted as the odds ratio of the response comparing two values of \\(X\\) one unit apart.\n\nWhy?"
  },
  {
    "objectID": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-2",
    "href": "slides/12-classification.html#interpreting-parameters-in-logistic-regression-2",
    "title": "Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nThe log odds of response for \\(X = 1\\) is given by \\(\\alpha + \\beta\\), and the log odds of response for \\(X = 0\\) is \\(\\alpha\\).\nSo the odds ratio of response comparing \\(X = 1\\) to \\(X = 0\\) is given by \\(\\frac{\\exp(\\alpha + \\beta)}{\\exp(\\alpha)} = \\exp(\\beta)\\).\nIn a multivariable logistic regression model with more than one predictor, this OR is interpreted conditionally on values of other variables (i.e., controlling for them)."
  },
  {
    "objectID": "slides/12-classification.html#bayesian-logistic-regression",
    "href": "slides/12-classification.html#bayesian-logistic-regression",
    "title": "Classification",
    "section": "Bayesian logistic regression",
    "text": "Bayesian logistic regression\n\nWe start with observations \\(Y_i \\in \\{0,1\\}\\) for \\(i = 1,\\ldots,n\\), where \\(Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\), \\(\\pi_i = P(Y_i = 1)\\).\nThe log-odds are modeled as \\(\\text{logit}(\\pi_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nTo complete the Bayesian model specification, we must place priors on \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\).\n\nAll priors we have discussed up-to-this point apply!\n\nHistorically, this was a difficult model to fit, but can be easily implemented in Stan."
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression-in-stan",
    "href": "slides/12-classification.html#logistic-regression-in-stan",
    "title": "Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\n// Saved in logistic_regression.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n}\nmodel {\n  target += bernoulli_logit_lpmf(Y | alpha + X * beta);\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = bernoulli_logit_rng(alpha + height_c[i] * beta);\n    log_lik[i] = bernoulli_logit_lmpf(Y[i] | alpha + X[i, ] * beta);\n  }\n}"
  },
  {
    "objectID": "slides/12-classification.html#primary-biliary-cirrhosis-1",
    "href": "slides/12-classification.html#primary-biliary-cirrhosis-1",
    "title": "Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nhead(pbc)\n\n  id trt      age sex ascites hepato spiders edema bili chol albumin copper\n1  1   1 58.76523   f       1      1       1   1.0 14.5  261    2.60    156\n2  2   1 56.44627   f       0      1       1   0.0  1.1  302    4.14     54\n3  3   1 70.07255   m       0      0       0   0.5  1.4  176    3.48    210\n4  4   1 54.74059   f       0      1       1   0.5  1.8  244    2.54     64\n5  5   2 38.10541   f       0      1       1   0.0  3.4  279    3.53    143\n6  7   2 55.53457   f       0      1       0   0.0  1.0  322    4.09     52\n  alk.phos    ast trig platelet protime stage outcome\n1   1718.0 137.95  172      190    12.2     4       1\n2   7394.8 113.52   88      221    10.6     3       0\n3    516.0  96.10   55      151    12.0     4       1\n4   6121.8  60.63   92      183    10.3     4       1\n5    671.0 113.15   72      136    10.9     3       1\n6    824.0  60.45  213      204     9.7     3       0"
  },
  {
    "objectID": "slides/12-classification.html#prepare-data-for-stan",
    "href": "slides/12-classification.html#prepare-data-for-stan",
    "title": "Classification",
    "section": "Prepare data for Stan",
    "text": "Prepare data for Stan\n\nX &lt;- model.matrix(object = ~ ascites + bili + as.factor(stage), data = pbc)[, -1]\nY &lt;- pbc$outcome\nstan_data &lt;- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X)\nhead(X)\n\n  ascites bili as.factor(stage)2 as.factor(stage)3 as.factor(stage)4\n1       1 14.5                 0                 0                 1\n2       0  1.1                 0                 1                 0\n3       0  1.4                 0                 0                 1\n4       0  1.8                 0                 0                 1\n5       0  3.4                 0                 1                 0\n6       0  1.0                 0                 1                 0"
  },
  {
    "objectID": "slides/12-classification.html#logistic-regression-in-stan-1",
    "href": "slides/12-classification.html#logistic-regression-in-stan-1",
    "title": "Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\nlibrary(rstan)\ncompiled_model &lt;- stan_model(file = \"logistic_regression.stan\")\nfit &lt;- sampling(compiled_model, data = stan_data)\nprint(fit, pars = c(\"alpha\", \"beta\"), probs = c(0.025, 0.5, 0.975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n         mean se_mean   sd  2.5%   50% 97.5% n_eff Rhat\nalpha   -3.35    0.04 1.21 -6.13 -3.24 -1.41   791    1\nbeta[1]  2.24    0.04 1.32  0.18  2.05  5.48  1252    1\nbeta[2]  0.38    0.00 0.08  0.24  0.38  0.54  2005    1\nbeta[3]  1.71    0.04 1.25 -0.34  1.59  4.55   783    1\nbeta[4]  2.19    0.04 1.22  0.20  2.07  4.96   806    1\nbeta[5]  2.61    0.04 1.23  0.59  2.48  5.46   798    1\n\nSamples were drawn using NUTS(diag_e) at Mon Dec 30 14:36:11 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/12-classification.html#convergence-diagnostics",
    "href": "slides/12-classification.html#convergence-diagnostics",
    "title": "Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/12-classification.html#convergence-diagnostics-1",
    "href": "slides/12-classification.html#convergence-diagnostics-1",
    "title": "Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/12-classification.html#back-to-the-pbc-data",
    "href": "slides/12-classification.html#back-to-the-pbc-data",
    "title": "Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nFitting a logistic regression model, we obtain\n\n\n\n\n\n\n\nvariable\nmean\nsd\n2.5%\n97.5%\n\n\n\n\nalpha\nintercept\n-3.35\n0.04\n-6.13\n-1.41\n\n\nbeta[1]\nascites\n2.24\n0.04\n0.18\n5.48\n\n\nbeta[2]\nbilirubin\n0.38\n0.00\n0.24\n0.54\n\n\nbeta[3]\nstage == 2\n1.71\n0.04\n-0.34\n4.55\n\n\nbeta[4]\nstage == 3\n2.19\n0.04\n0.20\n4.96\n\n\nbeta[5]\nstage == 4\n2.61\n0.04\n0.59\n5.46\n\n\n\n\n\n\nHow might we interpret these coefficients as odds ratios?"
  },
  {
    "objectID": "slides/12-classification.html#back-to-the-pbc-data-1",
    "href": "slides/12-classification.html#back-to-the-pbc-data-1",
    "title": "Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nRemember, we are interested in the probability that a patient died during follow-up (a “success”). We are predicting the log-odds of this event happening.\n\nThe posterior mean for ascites was 2.24. Thus, the odds ratio for dying is \\(\\exp(2.24) \\approx 9.4\\). That is, patients with ascites have 9.4 times the odds of dying compared to patients that do not, holding all other variables constant.\nThe posterior mean for bilirubin was 0.38. Thus, the odds ratio for dying for a patient with 1 additional mg/dL serum bilirubin compared to another is \\(\\exp(0.38) \\approx 1.46\\), holding all other variables constant.\nThe baseline stage was 1. The posterior mean for stage 3 was 2.19. Thus, patients in stage 3 have approximately 8.93 times the odds of dying compared to patients that do not, holding all other variables constant."
  },
  {
    "objectID": "slides/12-classification.html#predicted-probabilities",
    "href": "slides/12-classification.html#predicted-probabilities",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nThere is a one-to-one relationship between \\(\\pi\\) and \\(\\text{logit}(\\pi)\\). So, if we predict \\(\\text{logit}(\\pi)\\), we can “back-transform” to get back to a predicted probability.\nFor instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\nx_i &lt;- matrix(c(0, 5, 1, 0, 0), ncol = 1)\npars &lt;- rstan::extract(fit, pars = c(\"alpha\", \"beta\"))\nlog_odds &lt;- pars$alpha + as.numeric(pars$beta %*% x_i)\npi &lt;- exp(log_odds) / (1 + exp(log_odds))"
  },
  {
    "objectID": "slides/12-classification.html#predicted-probabilities-1",
    "href": "slides/12-classification.html#predicted-probabilities-1",
    "title": "Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior mean of the predicted probabilities is 0.56."
  },
  {
    "objectID": "slides/12-classification.html#posterior-predictive-checks",
    "href": "slides/12-classification.html#posterior-predictive-checks",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/12-classification.html#posterior-predictive-checks-1",
    "href": "slides/12-classification.html#posterior-predictive-checks-1",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_bars(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/12-classification.html#posterior-predictive-checks-2",
    "href": "slides/12-classification.html#posterior-predictive-checks-2",
    "title": "Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_stat(stan_data$Y, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(stan_data$Y, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(stan_data$Y, y_pred, stat = \"q025\")\nppc_stat(stan_data$Y, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/12-classification.html#model-comparison",
    "href": "slides/12-classification.html#model-comparison",
    "title": "Classification",
    "section": "Model comparison",
    "text": "Model comparison\n\nComparing our model to a baseline that removed stage.\n\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_baseline &lt;- loo::extract_log_lik(fit_baseline, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model &lt;- loo::waic(log_lik)\nwaic_model_baseline &lt;- loo::waic(log_lik_baseline)\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"full\" = waic_model, \"baseline\" = waic_model_baseline))\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nfull        0.00      0.00 -155.94      9.51         7.69    2.09    311.88\nbaseline   -1.69      3.09 -157.63      9.36         4.86    1.98    315.25\n         se_waic\nfull       19.01\nbaseline   18.73"
  },
  {
    "objectID": "slides/12-classification.html#other-models-for-binary-data",
    "href": "slides/12-classification.html#other-models-for-binary-data",
    "title": "Classification",
    "section": "Other models for binary data",
    "text": "Other models for binary data\n\nOther transformations (also called link functions) can be used to ensure the probabilities lie in [0, 1], including the Probit (popular in Bayesian statistics)."
  },
  {
    "objectID": "slides/12-classification.html#steps-to-selecting-a-bayesian-glm",
    "href": "slides/12-classification.html#steps-to-selecting-a-bayesian-glm",
    "title": "Classification",
    "section": "Steps to selecting a Bayesian GLM",
    "text": "Steps to selecting a Bayesian GLM\n\nIdentify the support of the response distribution.\nSelect the likelihood by picking a parametric family of distributions with this support.\nChoose a link function \\(g\\) that transforms the range of parameters to the whole real line.\nSpecify a linear model on the transformed parameters.\nSelect priors for the regression coefficients."
  },
  {
    "objectID": "slides/12-classification.html#example-of-selecting-a-bayesian-glm",
    "href": "slides/12-classification.html#example-of-selecting-a-bayesian-glm",
    "title": "Classification",
    "section": "Example of selecting a Bayesian GLM",
    "text": "Example of selecting a Bayesian GLM\n\nSupport: \\(Y_i \\in \\{0, 1, 2, \\ldots\\}\\).\nLikelihood family: \\(Y_i \\stackrel{ind}{\\sim} \\text{Poisson}(\\lambda_i)\\).\nLink: \\(g(\\lambda_i) = \\log(\\lambda_i) \\in (−\\infty, \\infty)\\).\nRegression model: \\(\\log(\\lambda_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nPriors: \\(\\alpha, \\beta_j \\sim N(0, 10^2)\\)."
  },
  {
    "objectID": "slides/12-classification.html#multinomial-regression",
    "href": "slides/12-classification.html#multinomial-regression",
    "title": "Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nAssume an outcome \\(Y_i \\in \\{1,\\ldots,K\\}\\). You can imagine running \\(K\\) independent binary logistic regression models, in which one outcome is chosen as a “pivot” and then the other K − 1 outcomes are separately regressed against the pivot outcome. If outcome K (the last outcome) is chosen as the pivot, the K − 1 regression equations are:\n\\[\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}\\]\n\nIt can be seen that:\n\n\\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\] and \\(P(Y_i = K) = 1 - \\sum_{j=1}^{K-1}P(Y_i = j)\\).\n\nThis is known as the additive log-ratio model."
  },
  {
    "objectID": "slides/12-classification.html#multinomial-regression-1",
    "href": "slides/12-classification.html#multinomial-regression-1",
    "title": "Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nLet \\(Y_i \\in \\{1,\\ldots,K\\}\\) be an ordinal outcome with \\(K\\) categories. Then \\(P(Y \\leq k)\\) is the cumulative probability of \\(Y\\) less than or equal to a specific category \\(k=1,\\ldots,K-1\\). The odds of being less than or equal a particular category can be defined as, \\[\\frac{P(Y\\leq k)}{P(Y &gt; k)}\\] for \\(k=1,\\ldots,K-1\\), since \\(P(Y &gt; K) = 0\\) and dividing by zero is undefined. The log odds is also known as the logit, so that \\[\\log \\frac{P(Y\\leq k)}{P(Y &gt; k)} = \\text{logit}P(Y\\leq k) = \\alpha_k + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nProportional odds regression."
  },
  {
    "objectID": "slides/12-classification.html#ordinal-regression-using-stan",
    "href": "slides/12-classification.html#ordinal-regression-using-stan",
    "title": "Classification",
    "section": "Ordinal regression using Stan",
    "text": "Ordinal regression using Stan\n\ndata {\n  int&lt;lower = 2&gt; K;\n  int&lt;lower = 0&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1, upper = K&gt; Y[n]\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  ordered[K - 1] alpha;\n}\nmodel {\n  target += ordered_logistic_lpmf(Y | X, beta, alpha);\n  target += ordered_logistic_glm_lpmf(Y | X * beta, alpha);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(alpha | 0, 10);\n}"
  },
  {
    "objectID": "slides/12-classification.html#prepare-for-next-class",
    "href": "slides/12-classification.html#prepare-for-next-class",
    "title": "Classification",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03.\nComplete reading to prepare for next Thursday’s lecture\nThursday’s lecture: Multiclass classification\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/09-nonlinear.html#a-motivating-example",
    "href": "slides/09-nonlinear.html#a-motivating-example",
    "title": "Nonlinear Regression",
    "section": "A motivating example",
    "text": "A motivating example\n\nPatients with glaucoma are follow…"
  },
  {
    "objectID": "slides/09-nonlinear.html#a-motivating-example-1",
    "href": "slides/09-nonlinear.html#a-motivating-example-1",
    "title": "Nonlinear Regression",
    "section": "A motivating example",
    "text": "A motivating example"
  },
  {
    "objectID": "slides/09-nonlinear.html#an-example-patient",
    "href": "slides/09-nonlinear.html#an-example-patient",
    "title": "Nonlinear Regression",
    "section": "An example patient",
    "text": "An example patient"
  },
  {
    "objectID": "slides/09-nonlinear.html#polynomials",
    "href": "slides/09-nonlinear.html#polynomials",
    "title": "Nonlinear Regression",
    "section": "Polynomials",
    "text": "Polynomials\n\nModel for the mean process becomes nonlinear:\n\n\\[\\mathbb{E}[MD_i] = \\alpha + \\beta_1 Time_i + \\cdots + \\beta_p Time_i^p\\]\n\n\\(p\\) is chosen depending on the degree of non-linearity.\nWhen fitting non-linear regression in Bayesian context it is useful to standardize the data."
  },
  {
    "objectID": "slides/09-nonlinear.html#splines",
    "href": "slides/09-nonlinear.html#splines",
    "title": "Nonlinear Regression",
    "section": "Splines",
    "text": "Splines\n\nOur goal is to approximate the blossom trend with a wiggly function. With B-splines, just like with polynomial regression, we do this by generating new predictor variables and using those in the linear model, \\(\\mu_i\\).\nUnlike polynomial regression, B-splines do not directly transform the predictor by squaring or cubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of the synthetic variables is called a basis function. The linear model ends up looking very familiar:\n\n\\[\\mu_i = \\alpha + \\beta_1 B_{i,1} + \\beta_2 B_{i,2} + \\cdots \\] where \\(B_{i,n}\\) is the \\(n\\)-th basis function’s value on row \\(i\\), and the \\(\\beta\\) parameters are corresponding weights for each. The parameters act like slopes, adjusting the influence of each basis function on the mean \\(\\mu_i\\).\n\nSo really this is just another linear regression, but with some fancy, synthetic predictor variables."
  },
  {
    "objectID": "slides/09-nonlinear.html#change-points",
    "href": "slides/09-nonlinear.html#change-points",
    "title": "Nonlinear Regression",
    "section": "Change points",
    "text": "Change points"
  },
  {
    "objectID": "slides/09-nonlinear.html#glaucoma-disease-progression",
    "href": "slides/09-nonlinear.html#glaucoma-disease-progression",
    "title": "Nonlinear Regression",
    "section": "Glaucoma disease progression",
    "text": "Glaucoma disease progression\n\nToday we will use data from the Rotterdam Ophthalmic Data Repository.\nGlaucoma is the leading cause of irreversible blindness world wide with over 60 million glaucoma patients as of 2012. Since impairment caused by glaucoma is irreversible, early detection of disease progression is crucial for effective treatment.\nPatients with glaucoma are routinely followed up and administered visual fields, a functional assessment of their vision.\nAfter each visual field test their current disease status is reported as a mean deviation (MD) value, measured in decibels (dB). A lower mean deviation indicates worse vision.\nCentral clinical challenges are i) identifying disease progression of MD, and ii) predicting future MD."
  },
  {
    "objectID": "slides/09-nonlinear.html#linear-regression",
    "href": "slides/09-nonlinear.html#linear-regression",
    "title": "Nonlinear Regression",
    "section": "Linear regression",
    "text": "Linear regression\n\nConsider the classic parametric model:\n\n\\[Y_i = \\alpha + X_i \\beta + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2).\\]\n\nAssumptions:\n\n\\(\\epsilon_i\\) are independent.\n\\(\\epsilon_i\\) are Gaussian.\nThe mean of \\(Y_i\\) is linear in \\(X_i\\).\nThe residual distribution does not depend on \\(X_i\\).\n\n\nToday we will generalize the linearity assumption."
  },
  {
    "objectID": "slides/09-nonlinear.html#linear-regression-1",
    "href": "slides/09-nonlinear.html#linear-regression-1",
    "title": "Nonlinear Regression",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#cubic-regression",
    "href": "slides/09-nonlinear.html#cubic-regression",
    "title": "Nonlinear Regression",
    "section": "Cubic regression",
    "text": "Cubic regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#quadratic-regression",
    "href": "slides/09-nonlinear.html#quadratic-regression",
    "title": "Nonlinear Regression",
    "section": "Quadratic regression",
    "text": "Quadratic regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#quadtratic-regression",
    "href": "slides/09-nonlinear.html#quadtratic-regression",
    "title": "Nonlinear Regression",
    "section": "Quadtratic regression",
    "text": "Quadtratic regression"
  },
  {
    "objectID": "slides/09-nonlinear.html#what-is-the-point",
    "href": "slides/09-nonlinear.html#what-is-the-point",
    "title": "Nonlinear Regression",
    "section": "What is the point?",
    "text": "What is the point?\n\nChoice of model is highly dependent on the context.\nAs we learned in the model comparison lecture, a better fit to the sample might not actually be a better model.\nThese basis models are difficult to interpret and are not particularly useful for a clinical setting (they may be useful for prediction!)."
  },
  {
    "objectID": "slides/09-nonlinear.html#model-comparison",
    "href": "slides/09-nonlinear.html#model-comparison",
    "title": "Nonlinear Regression",
    "section": "Model comparison",
    "text": "Model comparison"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-splines",
    "href": "slides/09-nonlinear.html#b-splines",
    "title": "Nonlinear Regression",
    "section": "B-splines",
    "text": "B-splines"
  },
  {
    "objectID": "slides/09-nonlinear.html#learning-objectives-today",
    "href": "slides/09-nonlinear.html#learning-objectives-today",
    "title": "Nonlinear Regression",
    "section": "Learning objectives today",
    "text": "Learning objectives today\n\nThus far, we have focused on linear regression models.\nToday we will focus on two common approaches that use linear regression to build nonlinear associations: polynomial regression and b-splines.\nBoth approaches work by transforming a single predictor variable into several synthetic variables.\nWe will also look at a change point model, that encodes clinical context into a nonlinear framework."
  },
  {
    "objectID": "slides/09-nonlinear.html#splines-1",
    "href": "slides/09-nonlinear.html#splines-1",
    "title": "Nonlinear Regression",
    "section": "Splines",
    "text": "Splines\nFirst, we choose the knots. Remember, the knots are just values of year that serve as pivots for our spline. Where should the knots go? There are different ways to answer this question. You can, in principle, put the knots wherever you like. Their locations are part of the model, and you are responsible for them. Let’s do what we did in the simple example above, place the knots at different evenly spaced quantiles of the predictor variable. This gives you more knots where there are more observations."
  },
  {
    "objectID": "slides/09-nonlinear.html#splines-2",
    "href": "slides/09-nonlinear.html#splines-2",
    "title": "Nonlinear Regression",
    "section": "Splines",
    "text": "Splines\nThe next choice is polynomial degree. This determines how basis functions combine, which determines how the parameters interact to produce the spline. For degree 1, as in Figure 4.12, two basis functions combine at each point. For degree 2, three functions combine at each point. For degree 3, four combine. R already has a nice function that will build basis functions for any list of knots and degree. This code will construct the necessary basis functions for a degree 3 (cubic) spline: (p. 117)\n\nlibrary(splines)\nnum_knots &lt;- 10\nknot_list &lt;- quantile(dat_pat$X, probs = seq(from = 0, to = 1, length.out = num_knots))\n\nB &lt;- bs(dat_pat$X,\n        knots = knot_list[-c(1, num_knots)], \n        degree = 3, \n        intercept = TRUE)\nplot(1, 1, type = \"n\", xlim = c(0, max(dat_pat$X)), ylim = c(0, 1))\nfor (i in 1:ncol(B)) lines(dat_pat$X, B[, i])"
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression",
    "href": "slides/09-nonlinear.html#nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nDefine: \\(\\mu_i = \\mathbb{E}[Y_i] = \\alpha + x_i\\beta.\\)\nThe mean process can modeled flexibly, \\(\\mu_i = g(X_i)\\), where \\(g\\) is some function that relates \\(X_i\\) to \\(\\mathbb{E}[Y_i | X_i].\\)\nA form of nonlinear regression approximates the function \\(g\\) using a finite basis expansion, \\[g(X_i) = \\alpha + \\sum_{j=1}^J B_j(X_i)\\beta_j,\\] where \\(B_j(X)\\) are known basis functions and \\(\\beta_j\\) are unknown parameters that determine the shape of \\(g\\)."
  },
  {
    "objectID": "slides/09-nonlinear.html#linear-regression-2",
    "href": "slides/09-nonlinear.html#linear-regression-2",
    "title": "Nonlinear Regression",
    "section": "Linear regression",
    "text": "Linear regression\n\nLinear regression is simple.\nLinear regression is highly interpretable. It encodes disease progression into a slope, which is the amount of MD loss (dB) per year.\n\nInterpretability is important!\n\nA linear relationship may be an oversimplification.\nOften in prediction contexts, a nonlinear approach is preferred."
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression-1",
    "href": "slides/09-nonlinear.html#nonlinear-regression-1",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nExample: Polynomial regression takes \\(B_j(X_i) = X_i^j\\).\nExample: Gaussian radial basis functions: \\[B_j(X_i) = \\exp\\left\\{-\\frac{|X_i - \\nu_j|^2}{l^2}\\right\\},\\] where \\(\\nu_j\\) are centers of the basis functions and \\(l\\) is a common width parameter.\nThe number of of basis functions and the width parameter \\(l\\) controls the scale at which the model can vary as a function of \\(X_i\\)."
  },
  {
    "objectID": "slides/09-nonlinear.html#model-fitting",
    "href": "slides/09-nonlinear.html#model-fitting",
    "title": "Nonlinear Regression",
    "section": "Model fitting",
    "text": "Model fitting\n\nThe model is Yi ∼ N(BTi β, σ2), where βj ∼ N(0, τ 2) and Bi is comprised of the known basis functions Bj(Xi)\nTherefore, the model is usual linear regression model and is straightforward to fit using MCMC\nBayesian methods are excellent for quantifying uncertainty in the fitted model and predictions\nHow to pick J? Can we J &gt; n?"
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression-2",
    "href": "slides/09-nonlinear.html#nonlinear-regression-2",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nExample: The cubic B-spline basis function is the following piecewise cubic polynomial:\n\n\\[B_j(X_i) = \\left\\{\n\\begin{matrix*}[l]\n\\frac{1}{6}u^3 & \\text{for }X_i \\in (\\nu_j,\\nu_{j+1}), & u = (X_i - \\nu_j) / \\delta\\\\\n\\frac{1}{6}(1 + 3u + 3u^2 - 3u^3) & \\text{for }X_i \\in (\\nu_{j+1},\\nu_{j+2}), & u = (X_i - \\nu_{j+1}) / \\delta\\\\\n\\frac{1}{6}(4 - 6u^2 + 3u^3) & \\text{for }X_i \\in (\\nu_{j+2},\\nu_{j+3}), & u = (X_i - \\nu_{j+2}) / \\delta\\\\\n\\frac{1}{6}(1 - 3u + 3u^2 - u^3) & \\text{for }X_i \\in (\\nu_{j+3},\\nu_{j+4}), & u = (X_i - \\nu_{j+3}) / \\delta\\\\\n0 & \\text{otherwise.}\n\\end{matrix*}\n\\right.\\]\n\nB-splines are a piecewise continuous function defined conditional on some set of knots.\nHere we assume a uniform knot locations \\(\\nu_{j + k} = \\nu_j + \\delta k\\).\nB-splines have compact support, so the design matrix is sparse."
  },
  {
    "objectID": "slides/09-nonlinear.html#nonlinear-regression-3",
    "href": "slides/09-nonlinear.html#nonlinear-regression-3",
    "title": "Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nConditionally on the selected baseis \\(B\\), the model is linear in the parameters. Hence we can write, \\[Y_i = \\mu(X_i) + \\epsilon_i = \\mathbf{w}_i \\boldsymbol{\\beta} + \\epsilon_i,\\] with \\(\\mathbf{w}_i = (B_1(X_i),\\ldots,B_J(X_i))\\).\nModel fitting can proceed as in linear regression models, since the resulting model is linear in \\(\\boldsymbol{\\beta}\\).\nIt is often useful to center the basis function model around the linear model, \\(\\mu(X_i) = \\alpha + X_i \\beta + \\mathbf{w}_i\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/09-nonlinear.html#glaucoma-data",
    "href": "slides/09-nonlinear.html#glaucoma-data",
    "title": "Nonlinear Regression",
    "section": "Glaucoma data",
    "text": "Glaucoma data\n\n### Load and process data to obtain data for an example patient\ndat &lt;- read.csv(file = \"LongGlaucVF_20150216/VisualFields.csv\")\ndat &lt;- dat[order(dat$STUDY_ID, dat$SITE), ]\ndat$EYE_ID &lt;- cumsum(!duplicated(dat[, c(\"STUDY_ID\", \"SITE\")]))\ndat_pat &lt;- dat[dat$EYE_ID == \"4\", ] # 4\ndat_pat$time &lt;- (dat_pat$AGE - dat_pat$AGE[1]) / 365\ndat_pat &lt;- dat_pat[, c(\"time\", \"MD\")]\ncolnames(dat_pat) &lt;- c(\"X\", \"Y\")\nglimpse(dat_pat)\n\n\n\nRows: 18\nColumns: 2\n$ X &lt;dbl&gt; 0.0000000, 0.6136986, 1.1315068, 1.6520548, 2.1671233, 2.6794521, 3.…\n$ Y &lt;dbl&gt; -2.76, -2.08, -1.91, -2.63, -5.13, -2.14, -1.97, -0.83, -1.75, -1.61…"
  },
  {
    "objectID": "slides/09-nonlinear.html#polynomial-regression-in-stan",
    "href": "slides/09-nonlinear.html#polynomial-regression-in-stan",
    "title": "Nonlinear Regression",
    "section": "Polynomial regression in Stan",
    "text": "Polynomial regression in Stan\n\ndat_poly &lt;- data.frame(\n  Y = scale(dat_pat$Y),\n  X = scale(dat_pat$X)\n)\ndat_poly$X2 &lt;- dat_poly$X^2\nstan_data &lt;- list(\n  n = nrow(dat_pat),\n  p = 2,\n  Y = dat_poly$Y,\n  X = cbind(dat_poly$X, dat_poly$X2),\n)\ncompile_model &lt;- stan_model(file = \"nonlinear_linear.stan\")\nfit_quadratic &lt;- sampling(compile_model, data = stan_data)"
  },
  {
    "objectID": "slides/09-nonlinear.html#polynomial-regression-in-stan-1",
    "href": "slides/09-nonlinear.html#polynomial-regression-in-stan-1",
    "title": "Nonlinear Regression",
    "section": "Polynomial regression in Stan",
    "text": "Polynomial regression in Stan\n\n// saved in nonlinear_linear.stan\ndata {\n  int&lt;lower = 1&gt; n; // number of observations\n  int&lt;lower = 1&gt; p; // number of covariates\n  vector[n] Y; // outcome vector\n  matrix[n, p] X; // covariate vector\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real&lt;lower = 0&gt; sigma;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, sigma); // likelihood\n  target += normal_lpdf(alpha | 0, 1); // prior for beta\n  target += normal_lpdf(beta | 0, 1); // prior for beta\n  target += inv_gamma_lpdf(sigma | 3, 1); // prior for sigma\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n] log_lik;\n  vector[n] mu;\n  for (i in 1:n) {\n    mu[i] = alpha + X[i, ] * beta;\n    in_sample[i] = normal_rng(mu[i], sigma);\n    log_lik[i] = normal_lpdf(Y[i] |  mu[i], sigma);\n  }\n}"
  },
  {
    "objectID": "slides/09-nonlinear.html#extract-posterior-mean-for-mu",
    "href": "slides/09-nonlinear.html#extract-posterior-mean-for-mu",
    "title": "Nonlinear Regression",
    "section": "Extract posterior mean for \\(\\mu\\)",
    "text": "Extract posterior mean for \\(\\mu\\)\n\nmu &lt;- rstan::extract(fit_quadratic, pars = \"mu\")$mu\nmu &lt;- mu * sd(dat_pat$Y) + mean(dat_pat$Y) # transform to original unstandardized Y_i\nmu_mean &lt;- apply(mu, 2, mean)\nmu_lower &lt;- apply(mu, 2, function(x) quantile(x, probs = 0.025))\nmu_upper &lt;- apply(mu, 2, function(x) quantile(x, probs = 0.975))"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression",
    "href": "slides/09-nonlinear.html#b-spline-regression",
    "title": "Nonlinear Regression",
    "section": "B-spline regression",
    "text": "B-spline regression\n\nlibrary(splines)\nnum_knots &lt;- 5\nknot_list &lt;- quantile(dat_pat$X, probs = seq(from = 0, to = 1, length.out = num_knots))\nB &lt;- bs(dat_pat$X,\n        knots = knot_list[-c(1, num_knots)], \n        degree = 3, \n        intercept = TRUE)\nB\n\n                 1            2            3           4            5\n [1,] 1.0000000000 0.0000000000 0.0000000000 0.000000000 0.0000000000\n [2,] 0.3932160225 0.5205689817 0.0833170637 0.002897932 0.0000000000\n [3,] 0.1303337386 0.6136422453 0.2378607351 0.018163281 0.0000000000\n [4,] 0.0220025742 0.5116331668 0.4098320335 0.056532225 0.0000000000\n [5,] 0.0001737807 0.3325401051 0.5396793792 0.127606735 0.0000000000\n [6,] 0.0000000000 0.1814804087 0.5792969517 0.238573820 0.0006488199\n [7,] 0.0000000000 0.0883598392 0.5363295039 0.367729756 0.0075809005\n [8,] 0.0000000000 0.0097143260 0.3289042281 0.593688910 0.0676925363\n [9,] 0.0000000000 0.0003772242 0.1863761908 0.659307134 0.1539394508\n[10,] 0.0000000000 0.0000000000 0.0788259578 0.624941237 0.2955691681\n[11,] 0.0000000000 0.0000000000 0.0357900241 0.542804472 0.4124556582\n[12,] 0.0000000000 0.0000000000 0.0107764123 0.421496598 0.5286019082\n[13,] 0.0000000000 0.0000000000 0.0006952375 0.263269190 0.6110273290\n[14,] 0.0000000000 0.0000000000 0.0000000000 0.145095180 0.5888020313\n[15,] 0.0000000000 0.0000000000 0.0000000000 0.060977201 0.4468755310\n[16,] 0.0000000000 0.0000000000 0.0000000000 0.016061781 0.2349278937\n[17,] 0.0000000000 0.0000000000 0.0000000000 0.002189635 0.0740048857\n[18,] 0.0000000000 0.0000000000 0.0000000000 0.000000000 0.0000000000\n                 6            7\n [1,] 0.0000000000 0.0000000000\n [2,] 0.0000000000 0.0000000000\n [3,] 0.0000000000 0.0000000000\n [4,] 0.0000000000 0.0000000000\n [5,] 0.0000000000 0.0000000000\n [6,] 0.0000000000 0.0000000000\n [7,] 0.0000000000 0.0000000000\n [8,] 0.0000000000 0.0000000000\n [9,] 0.0000000000 0.0000000000\n[10,] 0.0006636368 0.0000000000\n[11,] 0.0089498455 0.0000000000\n[12,] 0.0391250811 0.0000000000\n[13,] 0.1250082431 0.0000000000\n[14,] 0.2659535203 0.0001492682\n[15,] 0.4675827148 0.0245645535\n[16,] 0.5868492773 0.1621610484\n[17,] 0.4743685242 0.4494369547\n[18,] 0.0000000000 1.0000000000\nattr(,\"degree\")\n[1] 3\nattr(,\"knots\")\n     25%      50%      75% \n2.295205 4.965753 6.997945 \nattr(,\"Boundary.knots\")\n[1] 0.000000 9.257534\nattr(,\"intercept\")\n[1] TRUE\nattr(,\"class\")\n[1] \"bs\"     \"basis\"  \"matrix\""
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-with-10-knots",
    "href": "slides/09-nonlinear.html#b-spline-regression-with-10-knots",
    "title": "Nonlinear Regression",
    "section": "B-spline regression with 10 knots",
    "text": "B-spline regression with 10 knots"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-with-10-knots-1",
    "href": "slides/09-nonlinear.html#b-spline-regression-with-10-knots-1",
    "title": "Nonlinear Regression",
    "section": "B-spline regression with 10 knots",
    "text": "B-spline regression with 10 knots"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-with-5-knots",
    "href": "slides/09-nonlinear.html#b-spline-regression-with-5-knots",
    "title": "Nonlinear Regression",
    "section": "B-spline regression with 5 knots",
    "text": "B-spline regression with 5 knots"
  },
  {
    "objectID": "slides/09-nonlinear.html#b-spline-regression-1",
    "href": "slides/09-nonlinear.html#b-spline-regression-1",
    "title": "Nonlinear Regression",
    "section": "B-spline regression",
    "text": "B-spline regression\n\nstan_data &lt;- list(\n  n = nrow(dat_pat),\n  p = ncol(B),\n  Y = dat_poly$Y,\n  X = B\n)\nfit_bspline &lt;- sampling(compile_model, data = stan_data)"
  },
  {
    "objectID": "slides/09-nonlinear.html#change-point-motivation",
    "href": "slides/09-nonlinear.html#change-point-motivation",
    "title": "Nonlinear Regression",
    "section": "Change point motivation",
    "text": "Change point motivation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgression is defined by slow (or stable) deterioration, followed by a rapid decrease.\nFlexible modeling of MD across time.\nBiological representation of progression through the change point.\nChange points are a framework for inherently parameterizing progression."
  },
  {
    "objectID": "slides/09-nonlinear.html#writing-down-a-model",
    "href": "slides/09-nonlinear.html#writing-down-a-model",
    "title": "Nonlinear Regression",
    "section": "Writing down a model",
    "text": "Writing down a model\n\nModel for the observed data:\n\n\\[Y_i = \\mu(X_i) + \\epsilon_i, \\quad \\epsilon_i \\sim N(0,\\sigma^2).\\]\n\n\nModel for the mean process:\n\n\\[\\mu(X_i) =\\left\\{ \\begin{array}{ll}\n        {\\beta}_0 + \\beta_1 X_i & \\text{ } \\mbox{$X_i \\leq \\theta$},\\\\\n        {\\beta}_0 + \\beta_1 \\theta + {\\beta}_2\\{X_i - \\theta\\}& \\text{ } \\mbox{$X_i &gt; \\theta.$}\\end{array} \\right.\\]\n\n\\(\\theta \\in (\\min X_i, \\max X_i)\\) represents a change point."
  },
  {
    "objectID": "slides/09-nonlinear.html#change-point-model-in-stan",
    "href": "slides/09-nonlinear.html#change-point-model-in-stan",
    "title": "Nonlinear Regression",
    "section": "Change point model in Stan",
    "text": "Change point model in Stan\n\n// saved in change_points.stan\nfunctions {\n  vector compute_mean(vector X, real beta0, real beta1, real beta2, real theta) {\n    int n = size(X);\n    vector[n] mu;\n    for (t in 1:n) {\n      if (X[t] &lt;= theta) mu[t] = beta0 + beta1 * X[t];\n      if (X[t] &gt; theta) mu[t] = beta0 + beta1 * theta + beta2 * (X[t] - theta);\n  }\n  return mu;\n  }\n}\ndata {\n  int&lt;lower=1&gt; n;\n  vector[n] Y;\n  vector[n] X;\n  int n_pred;\n  vector[n_pred] X_pred;\n}\ntransformed data {\n  real min_X = min(X);\n  real max_X = max(X);\n}\nparameters {\n  real beta0;\n  real beta1;\n  real beta2;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = min_X, upper = max_X&gt; theta;\n}\nmodel {\n  vector[n] mu = compute_mean(X, beta0, beta1, beta2, theta);\n  target += normal_lpdf(Y | mu, sigma);\n  target += normal_lpdf(sigma | 0, 1);\n  target += normal_lpdf(beta0 | 0, 1);\n  target += normal_lpdf(beta1 | 0, 1);\n  target += normal_lpdf(beta2 | 0, 1);\n}\ngenerated quantities {\n  vector[n_pred] mu_pred = compute_mean(X_pred, beta0, beta1, beta2, theta);\n  array[n_pred] real Y_pred_out = normal_rng(mu_pred, sigma);\n  vector[n] mu = compute_mean(X, beta0, beta1, beta2, theta);\n  array[n] real Y_pred_in = normal_rng(mu, sigma);\n}"
  },
  {
    "objectID": "slides/09-nonlinear.html#change-point-regression",
    "href": "slides/09-nonlinear.html#change-point-regression",
    "title": "Nonlinear Regression",
    "section": "Change point regression",
    "text": "Change point regression\n\nstan_model &lt;- stan_model(file = \"change_points.stan\")\nn_pred &lt;- 1000\nstan_data &lt;- list(Y = dat_pat$Y, \n                  X = dat_pat$X,\n                  n = nrow(dat_pat),\n                  n_pred = n_pred,\n                  X_pred = seq(0, max(dat_pat$X) + 2, length.out = n_pred))\nfit_cp &lt;- sampling(stan_model, data = stan_data)\nprint(fit_cp, probs = c(0.025, 0.5, 0.0975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n       mean se_mean   sd   25%   50%   75% n_eff Rhat\nbeta0 -1.82    0.02 0.66 -2.29 -1.85 -1.40   749 1.00\nbeta1 -0.05    0.01 0.29 -0.19 -0.04  0.10   560 1.01\nbeta2 -0.84    0.02 0.45 -1.12 -0.87 -0.58   889 1.00\nsigma  1.27    0.01 0.26  1.08  1.23  1.41   792 1.00\ntheta  5.36    0.07 1.79  4.69  5.57  6.34   749 1.00\n\nSamples were drawn using NUTS(diag_e) at Wed Jan  1 17:29:30 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/09-nonlinear.html#diagnostics",
    "href": "slides/09-nonlinear.html#diagnostics",
    "title": "Nonlinear Regression",
    "section": "Diagnostics",
    "text": "Diagnostics"
  },
  {
    "objectID": "slides/09-nonlinear.html#posterior-fit",
    "href": "slides/09-nonlinear.html#posterior-fit",
    "title": "Nonlinear Regression",
    "section": "Posterior fit",
    "text": "Posterior fit"
  },
  {
    "objectID": "slides/09-nonlinear.html#posterior-fit-1",
    "href": "slides/09-nonlinear.html#posterior-fit-1",
    "title": "Nonlinear Regression",
    "section": "Posterior fit",
    "text": "Posterior fit"
  },
  {
    "objectID": "slides/09-nonlinear.html#prediction",
    "href": "slides/09-nonlinear.html#prediction",
    "title": "Nonlinear Regression",
    "section": "Prediction",
    "text": "Prediction"
  },
  {
    "objectID": "slides/11-regularization.html#review-of-last-lecture",
    "href": "slides/11-regularization.html#review-of-last-lecture",
    "title": "Regularization",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we learned about robust regression.\n\nHeteroskedasticity\nHeavy-tailed distributions\nQuantile regression\n\nThese were all models for the observed data \\(Y_i\\).\nToday, we will focus on prior specifications for \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/11-regularization.html#sparsity-in-regression-problems",
    "href": "slides/11-regularization.html#sparsity-in-regression-problems",
    "title": "Regularization",
    "section": "Sparsity in regression problems",
    "text": "Sparsity in regression problems\n\nSupervised learning can be cast as the problem of estimating a set of coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_j\\}_{j=1}^{p}\\) that determines some functional relationship between a set of \\(\\{x_j\\}_{j = 1}^p\\) and a target variable \\(y\\).\nThis is a central focus of statistics and machine learning.\nChallenges arise in “large-\\(p\\)” problems where, in order to avoid overly complex models that predict poorly, some form of dimension reduction is needed.\nFinding a sparse solution, where some \\(\\beta_j\\) are zero, is desirable."
  },
  {
    "objectID": "slides/11-regularization.html#bayesian-sparse-estimation",
    "href": "slides/11-regularization.html#bayesian-sparse-estimation",
    "title": "Regularization",
    "section": "Bayesian sparse estimation",
    "text": "Bayesian sparse estimation\n\nFrom a Bayesian-learning perspective, there are two main sparse-estimation alternatives: discrete mixtures and shrinkage priors.\nDiscrete mixtures have been very popular, with the spike-and-slab prior being the gold standard.\n\nEasy to force \\(\\beta_j\\) to exactly zero, but require discrete parameter specification.\n\nShrinkage priors force \\(\\beta_j\\) to zero using regularization, but struggle to get exact zeros.\n\nIn recent years, shrinkage priors have become dominant in Bayesian sparsity priors."
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-prior",
    "href": "slides/11-regularization.html#horseshoe-prior",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\n\nLet’s assume \\(\\mathbf{Y} \\stackrel{}{\\sim}N\\left(\\mathbf{X}\\boldsymbol{\\beta},\\sigma^2\\mathbf{I}_n\\right)\\), where \\(\\boldsymbol{\\beta}\\) is assumed to be sparse.\nThe horseshoe prior is specified as,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\sim \\mathcal C^+(0, 1),\n\\end{aligned}\\] where \\(\\mathcal C^+(0, 1)\\) is a half-Cauchy distribution for the standard deviation \\(\\lambda_j\\)."
  },
  {
    "objectID": "slides/11-regularization.html#half-cauchy-distribution",
    "href": "slides/11-regularization.html#half-cauchy-distribution",
    "title": "Regularization",
    "section": "Half-Cauchy distribution",
    "text": "Half-Cauchy distribution\nA random variable \\(X \\sim \\mathcal C^+(\\mu,\\sigma)\\) follows a half-Cauchy distribution with location \\(\\mu\\) and scale \\(\\sigma &gt; 0\\) and has the following density,\n\\[f(X | \\mu, \\sigma) = \\frac{2}{\\pi \\sigma}\\frac{1}{1 + (X - \\mu)^2 / \\sigma^2},\\quad X \\geq \\mu\\]\n\nThe Half-Cauchy distribution with \\(\\mu = 0\\) is a useful prior for non-negative parameters that may be very large, as allowed by the very heavy tails of the Half-Cauchy distribution."
  },
  {
    "objectID": "slides/11-regularization.html#half-cauchy-distribution-in-stan",
    "href": "slides/11-regularization.html#half-cauchy-distribution-in-stan",
    "title": "Regularization",
    "section": "Half-Cauchy distribution in Stan",
    "text": "Half-Cauchy distribution in Stan\nIn Stan, the half-Cauchy distribution can be specified by putting a constraint on the parameter definition.\n\nparameters {\n  real&lt;lower = 0&gt; lambda;\n}\nmodel {\n  target += cauchy_lpdf(lambda | 0, 1);\n}"
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-prior-1",
    "href": "slides/11-regularization.html#horseshoe-prior-1",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\n\nThe horseshoe prior is specified as,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\sim \\mathcal C^+(0, 1),\n\\end{aligned}\\] where \\(\\mathcal C^+(0, 1)\\) is a half-Cauchy distribution for the standard deviation \\(\\lambda_j\\).\n\n\\(\\lambda_j\\)’s are the local shrinkage parameters.\n\\(\\tau\\) is the global shrinkage parameter."
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-prior-2",
    "href": "slides/11-regularization.html#horseshoe-prior-2",
    "title": "Regularization",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\nThe horseshoe prior has two interesting features that make it particularly useful as a shrinkage prior for sparse problems.\n\nIt has flat, Cauchy-like tails that allow strong signals to remain large (that is, un-shrunk) a posteriori.\nIt has an infinitely tall spike at the origin that provides severe shrinkage for the zero elements of \\(\\boldsymbol{\\beta}\\).\n\nAs we will see, these are key elements that make the horseshoe an attractive choice for handling sparse vectors."
  },
  {
    "objectID": "slides/11-regularization.html#relation-to-other-shrinkage-priors",
    "href": "slides/11-regularization.html#relation-to-other-shrinkage-priors",
    "title": "Regularization",
    "section": "Relation to other shrinkage priors",
    "text": "Relation to other shrinkage priors\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\sim f(\\lambda_j)\n\\end{aligned}\\]\n\n\\(\\lambda_j = \\lambda\\), implies a Gaussian prior for \\(\\beta_j\\) (Ridge regression).\n\\(f(\\lambda_j) = \\text{Exponential}(2)\\), implies independent Laplacian priors for \\(\\beta_j\\) (LASSO).\n\\(f(\\lambda_j) = \\text{Inverse-Gamma}(a,b)\\), implies independent Student-t priors for \\(\\beta_j\\) (relevance vector machine)."
  },
  {
    "objectID": "slides/11-regularization.html#horsehoe-density",
    "href": "slides/11-regularization.html#horsehoe-density",
    "title": "Regularization",
    "section": "Horsehoe density",
    "text": "Horsehoe density\n\n\n\n\n\nFrom Carvalho 2009"
  },
  {
    "objectID": "slides/11-regularization.html#shrinkage-of-each-prior",
    "href": "slides/11-regularization.html#shrinkage-of-each-prior",
    "title": "Regularization",
    "section": "Shrinkage of each prior",
    "text": "Shrinkage of each prior\n\nDefine the posterior mean of \\(\\beta_j\\) as \\(\\bar{\\beta}_j\\) and the maximum likelihood estimator for \\(\\beta_j\\) as \\(\\hat{\\beta}_j\\).\nThe following relationship holds: \\(\\bar{\\beta}_j = (1 - \\kappa_j) \\hat{\\beta}_j\\),\n\n\\[\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}s_j^2\\lambda_j^2}.\\]\n\n\\(\\kappa_j\\) is called the shrinkage factor for \\(\\beta_j\\).\n\\(s_j^2 = \\mathbb{V}(x_j)\\) is the variance for each predictor."
  },
  {
    "objectID": "slides/11-regularization.html#standardization-of-predictors",
    "href": "slides/11-regularization.html#standardization-of-predictors",
    "title": "Regularization",
    "section": "Standardization of predictors",
    "text": "Standardization of predictors\n\nIn regularization problems, predictors are standardized (to mean zero and standard deviation one).\nThis means that so that \\(s_j = 1\\).\nShrinkage parameter:\n\n\\[\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}\\lambda_j^2}.\\]\n\n\\(\\kappa_j = 1\\), implies complete shrinkage.\n\\(\\kappa_j = 0\\), implies no shrinkage."
  },
  {
    "objectID": "slides/11-regularization.html#shrinkage-parameter",
    "href": "slides/11-regularization.html#shrinkage-parameter",
    "title": "Regularization",
    "section": "Shrinkage parameter",
    "text": "Shrinkage parameter\n\n\n\n\n\nFrom Carvalho 2009"
  },
  {
    "objectID": "slides/11-regularization.html#horseshoe-shrinkage-parameter",
    "href": "slides/11-regularization.html#horseshoe-shrinkage-parameter",
    "title": "Regularization",
    "section": "Horseshoe shrinkage parameter",
    "text": "Horseshoe shrinkage parameter\n\nChoosing \\(\\lambda_j ∼ \\mathcal C^+(0, 1)\\) implies \\(\\kappa_j ∼ \\text{Beta}(0.5, 0.5)\\), a density that is symmetric and unbounded at both 0 and 1.\nThis horseshoe-shaped shrinkage profile expects to see two things a priori:\n\nStrong signals (\\(\\kappa \\approx 0\\), no shrinkage), and\nZeros (\\(\\kappa \\approx 1\\), total shrinkage)."
  },
  {
    "objectID": "slides/11-regularization.html#spike-and-slab-prior",
    "href": "slides/11-regularization.html#spike-and-slab-prior",
    "title": "Regularization",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nThe prior is often written as a two-component mixture of Gaussians,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, c^2, \\epsilon &\\sim \\lambda_j N(0, c^2) + (1-\\lambda_j) N(0,\\omega^2)\\\\\n\\lambda_j &\\sim \\text{Bernoulli}(\\pi).\n\\end{aligned}\\]\n\n\\(\\omega \\ll c\\) and the indicator variable \\(\\lambda_j \\in \\{0, 1\\}\\) denotes whether \\(\\beta_j\\) is close to zero (comes from the “spike”, \\(\\lambda_j = 0\\)) or nonzero (comes from the “slab”, \\(\\lambda_j = 1\\))."
  },
  {
    "objectID": "slides/11-regularization.html#spike-and-slab-prior-1",
    "href": "slides/11-regularization.html#spike-and-slab-prior-1",
    "title": "Regularization",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nOften \\(\\omega = 0\\) (the spike is a true spike), and the prior can be written as,\n\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, c^2 &\\sim N(0, \\lambda_j^2 c^2)\\\\\n\\lambda_j &\\sim \\text{Bernoulli}(\\pi).\n\\end{aligned}\\]\n\nInstead of giving continuous priors for \\(\\lambda_j\\)’s as in the horseshoe, here only two values are allowed (0,1).\nThe shrinkage factor \\(\\kappa_j\\) only has mass at \\(\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}}\\) and \\(\\kappa_j = 1\\) with probabilities \\(\\pi\\) and \\(1-\\pi\\),"
  },
  {
    "objectID": "slides/11-regularization.html#similarity-to-horseshoe",
    "href": "slides/11-regularization.html#similarity-to-horseshoe",
    "title": "Regularization",
    "section": "Similarity to horseshoe",
    "text": "Similarity to horseshoe\n\nLetting \\(c \\rightarrow \\infty\\), all the mass is concentrated at the extremes \\(\\kappa_j = 0\\) and \\(\\kappa_j = 1\\) (this resembles the horseshoe).\n\n\n\n\n\n\nFrom Piironena and Vehtari 2017\n\nThe horseshoe can be seen as a continuous approximation to the spike-and-slab prior as \\(c \\rightarrow \\infty\\)."
  },
  {
    "objectID": "slides/11-regularization.html#regularized-horseshoe-prior",
    "href": "slides/11-regularization.html#regularized-horseshoe-prior",
    "title": "Regularization",
    "section": "Regularized horseshoe prior",
    "text": "Regularized horseshoe prior\n\\[\\begin{aligned}\n\\beta_j | \\lambda_j, \\tau, c &\\sim N\\left(0, \\tau^2 \\tilde{\\lambda}_j^2\\right),\\quad \\tilde{\\lambda}_j^2 = \\frac{c^2 \\lambda_j^2}{c^2 + \\tau^2 \\lambda_j^2},\\\\\n\\lambda_j &\\sim \\mathcal C^+(0,1).\n\\end{aligned}\\]\n\nWhen \\(\\tau^2 \\lambda_j^2 \\ll c^2\\) (i.e., \\(\\beta_j\\) close to zero), \\(\\beta_j \\sim  N\\left(0, \\tau^2\\lambda_j^2\\right)\\)\nWhen \\(\\tau^2 \\lambda_j^2 \\gg c^2\\), (i.e., \\(\\beta_j\\) far from zero), \\(\\beta_j \\sim  N\\left(0, c^2\\right)\\)\n\\(c \\rightarrow \\infty\\) recovers the original horseshoe.\n\nWhy is this an appealing extension?"
  },
  {
    "objectID": "slides/11-regularization.html#regularized-horseshoe-compared-to-spike-and-slab",
    "href": "slides/11-regularization.html#regularized-horseshoe-compared-to-spike-and-slab",
    "title": "Regularization",
    "section": "Regularized horseshoe compared to spike-and-slab",
    "text": "Regularized horseshoe compared to spike-and-slab\n\nThe regularized horseshoe prior is comparable to the spike-and-slab with finite \\(c\\).\n\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/11-regularization.html#choosing-a-prior-for-c2",
    "href": "slides/11-regularization.html#choosing-a-prior-for-c2",
    "title": "Regularization",
    "section": "Choosing a prior for \\(c^2\\)",
    "text": "Choosing a prior for \\(c^2\\)\n\nUnless substantial knowledge about the scale of the relevant coefficients exists, it is recommended to place a prior for \\(c\\) instead of fixing it.\nOften a reasonable choice is, \\[c^2 \\sim \\text{Inv-Gamma}(\\alpha, \\beta), \\quad \\alpha = \\nu/2, \\beta = \\nu s^2 / 2,\\]\nThis translates to a \\(t_{\\nu}(0,s^2)\\) slab for the coefficients far from 0.\nAnother motivation for using inverse-Gamma is that it has a heavy right tail accompanied by a light left tail thereby preventing much mass from accumulating near zero."
  },
  {
    "objectID": "slides/11-regularization.html#choosing-a-prior-for-tau",
    "href": "slides/11-regularization.html#choosing-a-prior-for-tau",
    "title": "Regularization",
    "section": "Choosing a prior for \\(\\tau\\)",
    "text": "Choosing a prior for \\(\\tau\\)\n\nCarvalho et al. 2009 suggest \\(\\tau \\sim \\mathcal C^+(0,1)\\).\nPolson and Scott 2011 recommend \\(\\tau | \\sigma \\sim \\mathcal C^+(0, \\sigma^2)\\).\nAnother prior comes from a quantity called the effective number of nonzero coefficients,\n\\[m_{eff} = \\sum_{j=1}^p (1 - \\kappa_j).\\]"
  },
  {
    "objectID": "slides/11-regularization.html#global-shrinkage-parameter-tau",
    "href": "slides/11-regularization.html#global-shrinkage-parameter-tau",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\nThe prior mean can be shown to be,\n\n\\[\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = \\frac{\\tau \\sigma^{-1} \\sqrt{n}}{1 + \\tau \\sigma^{-1} \\sqrt{n}}p.\\]\n\nSetting \\(\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = p_0\\) (prior guess for the number of non-zero coefficients) yields for \\(\\tau\\),\n\n\\[\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/11-regularization.html#global-shrinkage-parameter-tau-1",
    "href": "slides/11-regularization.html#global-shrinkage-parameter-tau-1",
    "title": "Regularization",
    "section": "Global shrinkage parameter \\(\\tau\\)",
    "text": "Global shrinkage parameter \\(\\tau\\)\n\n\n\n\n\nFrom Piironena and Vehtari 2017"
  },
  {
    "objectID": "slides/11-regularization.html#prepare-for-next-class",
    "href": "slides/11-regularization.html#prepare-for-next-class",
    "title": "Regularization",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWe are going to jump into an AE on body fat, but first some reminders.\nWork on HW 03, which was just assigned.\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Classification\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/13-multiclass.html#review-of-last-lecture",
    "href": "slides/13-multiclass.html#review-of-last-lecture",
    "title": "Multiclass Classification",
    "section": "Review of last lecture",
    "text": "Review of last lecture\n\nOn Tuesday, we learned about classification using logistic regression.\nToday, we will focus on multiclass classification: multinomial regression, ordinal regression."
  },
  {
    "objectID": "slides/13-multiclass.html#generalized-linear-models",
    "href": "slides/13-multiclass.html#generalized-linear-models",
    "title": "Multiclass Classification",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nPreviously, we have focused on linear regression. Other forms of regression follow naturally from linear regression."
  },
  {
    "objectID": "slides/13-multiclass.html#models-for-binary-outcomes",
    "href": "slides/13-multiclass.html#models-for-binary-outcomes",
    "title": "Multiclass Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nSuppose we have a binary outcome (e.g., \\(Y = 1\\) if a condition is satisfied and \\(Y = 0\\) if not) and predictors on a variety of scales.\nIf the predictors are discrete and the binary outcomes are independent, we can use the Bernoulli distribution for individual 0-1 data or the binomial distribution for grouped data that are counts of successes in each group."
  },
  {
    "objectID": "slides/13-multiclass.html#models-for-binary-outcomes-1",
    "href": "slides/13-multiclass.html#models-for-binary-outcomes-1",
    "title": "Multiclass Classification",
    "section": "Models for binary outcomes",
    "text": "Models for binary outcomes\n\nLet’s suppose we want to model \\(P(Y = 1)\\).\nOne strategy might be to simply fit a linear regression model to the probabilities.\nFor example,\n\n\\[\\begin{aligned}\nP(Y_i = 1) &= \\alpha + \\beta_1x_{i1} + \\cdots + \\beta_px_{ip} + \\epsilon_i\\\\\n&= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#primary-biliary-cirrhosis",
    "href": "slides/13-multiclass.html#primary-biliary-cirrhosis",
    "title": "Multiclass Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nThe Mayo Clinic conducted a trial for primary biliary cirrhosis, comparing the drug D-penicillamine vs. placebo. Patients were followed for a specified duration, and their status at the end of follow-up (whether they died) was recorded.\nResearchers are interested in predicting whether a patient died based on the following variables:\n\nascites: whether the patient had ascites (1 = yes, 0 = no)\nbilirubin: serum bilirubin in mg/dL\nstage: histologic stage of disease (ordinal categorical variable with stages 1, 2, 3, and 4)"
  },
  {
    "objectID": "slides/13-multiclass.html#what-can-go-wrong",
    "href": "slides/13-multiclass.html#what-can-go-wrong",
    "title": "Multiclass Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nSuppose we fit the following model:\n\n\\[\\begin{aligned}\nP(Y_i = 1) &= \\alpha + \\beta_1(ascites)_i + \\beta_2(bilirubin)_i\\\\\n&\\quad+\\beta_3(stage = 2)_i + \\beta_4(stage = 3)_i\\\\\n&\\quad+\\beta_5(stage = 4)_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\n\\end{aligned}\\]\n\nWhat can go wrong?"
  },
  {
    "objectID": "slides/13-multiclass.html#what-can-go-wrong-1",
    "href": "slides/13-multiclass.html#what-can-go-wrong-1",
    "title": "Multiclass Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "slides/13-multiclass.html#what-can-go-wrong-2",
    "href": "slides/13-multiclass.html#what-can-go-wrong-2",
    "title": "Multiclass Classification",
    "section": "What can go wrong?",
    "text": "What can go wrong?\n\nAdditionally, as a probability, \\(P(Y_i = 1)\\) must be in the interval [0, 1], but there is nothing in the model that enforces this constraint, so that you could be estimating probabilities that are negative or that are greater than 1!"
  },
  {
    "objectID": "slides/13-multiclass.html#from-probabilities-to-log-odds",
    "href": "slides/13-multiclass.html#from-probabilities-to-log-odds",
    "title": "Multiclass Classification",
    "section": "From probabilities to log-odds",
    "text": "From probabilities to log-odds\n\nSuppose the probability of an event is \\(\\pi\\).\nThen the odds that the event occurs is \\(\\frac{\\pi}{1 - \\pi}\\).\nTaking the (natural) log of the odds, we have the logit of \\(\\pi\\): the log-odds:\n\n\\[\\text{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).\\]\n\nNote that although \\(\\pi\\) is constrained to lie between 0 and 1, the logit of \\(\\pi\\) is unconstrained - it can be anything from \\(-\\infty\\) to \\(\\infty\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#logistic-regression-model",
    "href": "slides/13-multiclass.html#logistic-regression-model",
    "title": "Multiclass Classification",
    "section": "Logistic regression model",
    "text": "Logistic regression model\n\nLet’s create a model for the logit of \\(\\pi\\): \\(\\text{logit}(\\pi_i)= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\\)\nThis is a linear model for a transformation of the outcome of interest, and is also equivalent to,\n\n\\[\\pi_i = \\frac{\\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})}{1 + \\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})} = \\text{expit}(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}).\\]\n\nThe expression on the right is called a logistic function and cannot yield a value that is negative or a value that is &gt;1. Fitting a model of this form is known as logistic regression."
  },
  {
    "objectID": "slides/13-multiclass.html#logistic-regression",
    "href": "slides/13-multiclass.html#logistic-regression",
    "title": "Multiclass Classification",
    "section": "Logistic regression",
    "text": "Logistic regression\n\\[\\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nNegative logits represent probabilities less than one-half, and positive logits represent probabilities above one-half."
  },
  {
    "objectID": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression",
    "href": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression",
    "title": "Multiclass Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nTypically we interpret functions of parameters in logistic regression rather than the parameters themselves.\n\nFor the simple model: \\(\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\beta x_{i},\\) we note that the probability that \\(Y_i = 1\\) when \\(X_i = 0\\) is\n\\[P(Y_i = 1 | X_{i} = 0) = \\frac{\\exp(\\alpha)}{1 + \\exp(\\alpha)}.\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression-1",
    "href": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression-1",
    "title": "Multiclass Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nSuppose that \\(X\\) is a binary (0/1) variable (e.g., \\(X\\) = 1 for males and 0 for non-males).\n\nIn this case, we interpret \\(\\exp(\\beta)\\) as the odds ratio (OR) of the response for the two possible levels of \\(X\\).\nFor \\(X\\) on other scales, \\(\\exp(\\beta)\\) is interpreted as the odds ratio of the response comparing two values of \\(X\\) one unit apart.\n\nWhy?"
  },
  {
    "objectID": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression-2",
    "href": "slides/13-multiclass.html#interpreting-parameters-in-logistic-regression-2",
    "title": "Multiclass Classification",
    "section": "Interpreting parameters in logistic regression",
    "text": "Interpreting parameters in logistic regression\n\nThe log odds of response for \\(X = 1\\) is given by \\(\\alpha + \\beta\\), and the log odds of response for \\(X = 0\\) is \\(\\alpha\\).\nSo the odds ratio of response comparing \\(X = 1\\) to \\(X = 0\\) is given by \\(\\frac{\\exp(\\alpha + \\beta)}{\\exp(\\alpha)} = \\exp(\\beta)\\).\nIn a multivariable logistic regression model with more than one predictor, this OR is interpreted conditionally on values of other variables (i.e., controlling for them)."
  },
  {
    "objectID": "slides/13-multiclass.html#bayesian-logistic-regression",
    "href": "slides/13-multiclass.html#bayesian-logistic-regression",
    "title": "Multiclass Classification",
    "section": "Bayesian logistic regression",
    "text": "Bayesian logistic regression\n\nWe start with observations \\(Y_i \\in \\{0,1\\}\\) for \\(i = 1,\\ldots,n\\), where \\(Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)\\), \\(\\pi_i = P(Y_i = 1)\\).\nThe log-odds are modeled as \\(\\text{logit}(\\pi_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nTo complete the Bayesian model specification, we must place priors on \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\).\n\nAll priors we have discussed up-to-this point apply!\n\nHistorically, this was a difficult model to fit, but can be easily implemented in Stan."
  },
  {
    "objectID": "slides/13-multiclass.html#logistic-regression-in-stan",
    "href": "slides/13-multiclass.html#logistic-regression-in-stan",
    "title": "Multiclass Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\n// Saved in logistic_regression.stan\ndata {\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  int Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n}\nmodel {\n  target += bernoulli_logit_lpmf(Y | alpha + X * beta);\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = bernoulli_logit_rng(alpha + height_c[i] * beta);\n    log_lik[i] = bernoulli_logit_lmpf(Y[i] | alpha + X[i, ] * beta);\n  }\n}"
  },
  {
    "objectID": "slides/13-multiclass.html#primary-biliary-cirrhosis-1",
    "href": "slides/13-multiclass.html#primary-biliary-cirrhosis-1",
    "title": "Multiclass Classification",
    "section": "Primary biliary cirrhosis",
    "text": "Primary biliary cirrhosis\n\nhead(pbc)\n\n  id trt      age sex ascites hepato spiders edema bili chol albumin copper\n1  1   1 58.76523   f       1      1       1   1.0 14.5  261    2.60    156\n2  2   1 56.44627   f       0      1       1   0.0  1.1  302    4.14     54\n3  3   1 70.07255   m       0      0       0   0.5  1.4  176    3.48    210\n4  4   1 54.74059   f       0      1       1   0.5  1.8  244    2.54     64\n5  5   2 38.10541   f       0      1       1   0.0  3.4  279    3.53    143\n6  7   2 55.53457   f       0      1       0   0.0  1.0  322    4.09     52\n  alk.phos    ast trig platelet protime stage outcome\n1   1718.0 137.95  172      190    12.2     4       1\n2   7394.8 113.52   88      221    10.6     3       0\n3    516.0  96.10   55      151    12.0     4       1\n4   6121.8  60.63   92      183    10.3     4       1\n5    671.0 113.15   72      136    10.9     3       1\n6    824.0  60.45  213      204     9.7     3       0"
  },
  {
    "objectID": "slides/13-multiclass.html#prepare-data-for-stan",
    "href": "slides/13-multiclass.html#prepare-data-for-stan",
    "title": "Multiclass Classification",
    "section": "Prepare data for Stan",
    "text": "Prepare data for Stan\n\nX &lt;- model.matrix(object = ~ ascites + bili + as.factor(stage), data = pbc)[, -1]\nY &lt;- pbc$outcome\nstan_data &lt;- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X)\nhead(X)\n\n  ascites bili as.factor(stage)2 as.factor(stage)3 as.factor(stage)4\n1       1 14.5                 0                 0                 1\n2       0  1.1                 0                 1                 0\n3       0  1.4                 0                 0                 1\n4       0  1.8                 0                 0                 1\n5       0  3.4                 0                 1                 0\n6       0  1.0                 0                 1                 0"
  },
  {
    "objectID": "slides/13-multiclass.html#logistic-regression-in-stan-1",
    "href": "slides/13-multiclass.html#logistic-regression-in-stan-1",
    "title": "Multiclass Classification",
    "section": "Logistic regression in Stan",
    "text": "Logistic regression in Stan\n\nlibrary(rstan)\ncompiled_model &lt;- stan_model(file = \"logistic_regression.stan\")\nfit &lt;- sampling(compiled_model, data = stan_data)\nprint(fit, pars = c(\"alpha\", \"beta\"), probs = c(0.025, 0.5, 0.975))\n\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n         mean se_mean   sd  2.5%   50% 97.5% n_eff Rhat\nalpha   -3.35    0.04 1.21 -6.13 -3.24 -1.41   791    1\nbeta[1]  2.24    0.04 1.32  0.18  2.05  5.48  1252    1\nbeta[2]  0.38    0.00 0.08  0.24  0.38  0.54  2005    1\nbeta[3]  1.71    0.04 1.25 -0.34  1.59  4.55   783    1\nbeta[4]  2.19    0.04 1.22  0.20  2.07  4.96   806    1\nbeta[5]  2.61    0.04 1.23  0.59  2.48  5.46   798    1\n\nSamples were drawn using NUTS(diag_e) at Mon Dec 30 14:36:11 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/13-multiclass.html#convergence-diagnostics",
    "href": "slides/13-multiclass.html#convergence-diagnostics",
    "title": "Multiclass Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/13-multiclass.html#convergence-diagnostics-1",
    "href": "slides/13-multiclass.html#convergence-diagnostics-1",
    "title": "Multiclass Classification",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics"
  },
  {
    "objectID": "slides/13-multiclass.html#back-to-the-pbc-data",
    "href": "slides/13-multiclass.html#back-to-the-pbc-data",
    "title": "Multiclass Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nFitting a logistic regression model, we obtain\n\n\n\n\n\n\n\nvariable\nmean\nsd\n2.5%\n97.5%\n\n\n\n\nalpha\nintercept\n-3.35\n0.04\n-6.13\n-1.41\n\n\nbeta[1]\nascites\n2.24\n0.04\n0.18\n5.48\n\n\nbeta[2]\nbilirubin\n0.38\n0.00\n0.24\n0.54\n\n\nbeta[3]\nstage == 2\n1.71\n0.04\n-0.34\n4.55\n\n\nbeta[4]\nstage == 3\n2.19\n0.04\n0.20\n4.96\n\n\nbeta[5]\nstage == 4\n2.61\n0.04\n0.59\n5.46\n\n\n\n\n\n\nHow might we interpret these coefficients as odds ratios?"
  },
  {
    "objectID": "slides/13-multiclass.html#back-to-the-pbc-data-1",
    "href": "slides/13-multiclass.html#back-to-the-pbc-data-1",
    "title": "Multiclass Classification",
    "section": "Back to the PBC data",
    "text": "Back to the PBC data\n\nRemember, we are interested in the probability that a patient died during follow-up (a “success”). We are predicting the log-odds of this event happening.\n\nThe posterior mean for ascites was 2.24. Thus, the odds ratio for dying is \\(\\exp(2.24) \\approx 9.4\\). That is, patients with ascites have 9.4 times the odds of dying compared to patients that do not, holding all other variables constant.\nThe posterior mean for bilirubin was 0.38. Thus, the odds ratio for dying for a patient with 1 additional mg/dL serum bilirubin compared to another is \\(\\exp(0.38) \\approx 1.46\\), holding all other variables constant.\nThe baseline stage was 1. The posterior mean for stage 3 was 2.19. Thus, patients in stage 3 have approximately 8.93 times the odds of dying compared to patients that do not, holding all other variables constant."
  },
  {
    "objectID": "slides/13-multiclass.html#predicted-probabilities",
    "href": "slides/13-multiclass.html#predicted-probabilities",
    "title": "Multiclass Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\nThere is a one-to-one relationship between \\(\\pi\\) and \\(\\text{logit}(\\pi)\\). So, if we predict \\(\\text{logit}(\\pi)\\), we can “back-transform” to get back to a predicted probability.\nFor instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\nx_i &lt;- matrix(c(0, 5, 1, 0, 0), ncol = 1)\npars &lt;- rstan::extract(fit, pars = c(\"alpha\", \"beta\"))\nlog_odds &lt;- pars$alpha + as.numeric(pars$beta %*% x_i)\npi &lt;- exp(log_odds) / (1 + exp(log_odds))"
  },
  {
    "objectID": "slides/13-multiclass.html#predicted-probabilities-1",
    "href": "slides/13-multiclass.html#predicted-probabilities-1",
    "title": "Multiclass Classification",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior mean of the predicted probabilities is 0.56."
  },
  {
    "objectID": "slides/13-multiclass.html#posterior-predictive-checks",
    "href": "slides/13-multiclass.html#posterior-predictive-checks",
    "title": "Multiclass Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\ny_pred &lt;- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/13-multiclass.html#posterior-predictive-checks-1",
    "href": "slides/13-multiclass.html#posterior-predictive-checks-1",
    "title": "Multiclass Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_bars(stan_data$Y, y_pred[1:100, ])"
  },
  {
    "objectID": "slides/13-multiclass.html#posterior-predictive-checks-2",
    "href": "slides/13-multiclass.html#posterior-predictive-checks-2",
    "title": "Multiclass Classification",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nppc_stat(stan_data$Y, y_pred, stat = \"mean\") # from bayesplot\nppc_stat(stan_data$Y, y_pred, stat = \"sd\")\nq025 &lt;- function(y) quantile(y, 0.025)\nq975 &lt;- function(y) quantile(y, 0.975)\nppc_stat(stan_data$Y, y_pred, stat = \"q025\")\nppc_stat(stan_data$Y, y_pred, stat = \"q975\")"
  },
  {
    "objectID": "slides/13-multiclass.html#model-comparison",
    "href": "slides/13-multiclass.html#model-comparison",
    "title": "Multiclass Classification",
    "section": "Model comparison",
    "text": "Model comparison\n\nComparing our model to a baseline that removed stage.\n\n\nlibrary(loo)\nlog_lik &lt;- loo::extract_log_lik(fit, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_baseline &lt;- loo::extract_log_lik(fit_baseline, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model &lt;- loo::waic(log_lik)\nwaic_model_baseline &lt;- loo::waic(log_lik_baseline)\n\n###Make a comparison\ncomp_waic &lt;- loo::loo_compare(list(\"full\" = waic_model, \"baseline\" = waic_model_baseline))\nprint(comp_waic, digits = 2, simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nfull        0.00      0.00 -155.94      9.51         7.69    2.09    311.88\nbaseline   -1.69      3.09 -157.63      9.36         4.86    1.98    315.25\n         se_waic\nfull       19.01\nbaseline   18.73"
  },
  {
    "objectID": "slides/13-multiclass.html#other-models-for-binary-data",
    "href": "slides/13-multiclass.html#other-models-for-binary-data",
    "title": "Multiclass Classification",
    "section": "Other models for binary data",
    "text": "Other models for binary data\n\nOther transformations (also called link functions) can be used to ensure the probabilities lie in [0, 1], including the Probit (popular in Bayesian statistics)."
  },
  {
    "objectID": "slides/13-multiclass.html#steps-to-selecting-a-bayesian-glm",
    "href": "slides/13-multiclass.html#steps-to-selecting-a-bayesian-glm",
    "title": "Multiclass Classification",
    "section": "Steps to selecting a Bayesian GLM",
    "text": "Steps to selecting a Bayesian GLM\n\nIdentify the support of the response distribution.\nSelect the likelihood by picking a parametric family of distributions with this support.\nChoose a link function \\(g\\) that transforms the range of parameters to the whole real line.\nSpecify a linear model on the transformed parameters.\nSelect priors for the regression coefficients."
  },
  {
    "objectID": "slides/13-multiclass.html#example-of-selecting-a-bayesian-glm",
    "href": "slides/13-multiclass.html#example-of-selecting-a-bayesian-glm",
    "title": "Multiclass Classification",
    "section": "Example of selecting a Bayesian GLM",
    "text": "Example of selecting a Bayesian GLM\n\nSupport: \\(Y_i \\in \\{0, 1, 2, \\ldots\\}\\).\nLikelihood family: \\(Y_i \\stackrel{ind}{\\sim} \\text{Poisson}(\\lambda_i)\\).\nLink: \\(g(\\lambda_i) = \\log(\\lambda_i) \\in (−\\infty, \\infty)\\).\nRegression model: \\(\\log(\\lambda_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}\\).\nPriors: \\(\\alpha, \\beta_j \\sim N(0, 10^2)\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression",
    "href": "slides/13-multiclass.html#multinomial-regression",
    "title": "Multiclass Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\n\nUnder this formulation,\n\n\\[P(Y_i = k) = P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\]\n\nThen it follows that (since probabilities must sum to 1),\n\n\\[\\begin{aligned}\nP(Y_i = K) &= 1 - \\sum_{j=1}^{K-1} P(Y_i = j)\\\\\n&= 1 - \\sum_{j=1}^{K-1} P(Y_i = K)\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}.\\\\\n\\implies P(Y_i = K) &= \\frac{1}{1 + \\sum_{j=1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}.\n\\end{aligned}\\]\n\nThe individual probabilities are given by, \\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-1",
    "href": "slides/13-multiclass.html#multinomial-regression-1",
    "title": "Multiclass Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\n\nUnder this formulation,\n\n\\[P(Y_i = k) = P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\]\n\nThen it follows that (since probabilities must sum to 1),\n\n\\[\\begin{aligned}\nP(Y_i = K) &= 1 - \\sum_{j=1}^{K-1} P(Y_i = j)\\\\\n&= 1 - \\sum_{j=1}^{K-1} P(Y_i = K)\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}.\\\\\n\\implies P(Y_i = K) &= \\frac{1}{1 + \\sum_{j=1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}.\n\\end{aligned}\\]\n\nThe individual probabilities are given by, \\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\}.\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#ordinal-regression-using-stan",
    "href": "slides/13-multiclass.html#ordinal-regression-using-stan",
    "title": "Multiclass Classification",
    "section": "Ordinal regression using Stan",
    "text": "Ordinal regression using Stan\n\n// ordinal.stan\ndata {\n  int&lt;lower = 2&gt; K;\n  int&lt;lower = 0&gt; n;\n  int&lt;lower = 1&gt; p;\n  int&lt;lower = 1, upper = K&gt; Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  ordered[K - 1] alpha;\n}\nmodel {\n  target += ordered_logistic_glm_lpmf(Y | X, beta, alpha);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(alpha | 0, 10);\n}\n\nordered_logistic_glm_lpmf"
  },
  {
    "objectID": "slides/13-multiclass.html#prepare-for-next-class",
    "href": "slides/13-multiclass.html#prepare-for-next-class",
    "title": "Multiclass Classification",
    "section": "Prepare for next class",
    "text": "Prepare for next class\n\nWork on HW 03.\nComplete reading to prepare for next Tuesday’s lecture\nTuesday’s lecture: Missing data\n\n\n\n\n\n🔗 BIOSTAT 725 - Spring 2025"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-2",
    "href": "slides/13-multiclass.html#multinomial-regression-2",
    "title": "Multiclass Classification",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nLet \\(Y_i \\in \\{1,\\ldots,K\\}\\) be an ordinal outcome with \\(K\\) categories. Then \\(P(Y \\leq k)\\) is the cumulative probability of \\(Y\\) less than or equal to a specific category \\(k=1,\\ldots,K-1\\). The odds of being less than or equal a particular category can be defined as, \\[\\frac{P(Y\\leq k)}{P(Y &gt; k)}\\] for \\(k=1,\\ldots,K-1\\), since \\(P(Y &gt; K) = 0\\) and dividing by zero is undefined. The log odds is also known as the logit, so that \\[\\log \\frac{P(Y\\leq k)}{P(Y &gt; k)} = \\text{logit}P(Y\\leq k) = \\alpha_k + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nProportional odds regression."
  },
  {
    "objectID": "slides/13-multiclass.html#ordinal-regression",
    "href": "slides/13-multiclass.html#ordinal-regression",
    "title": "Multiclass Classification",
    "section": "Ordinal regression",
    "text": "Ordinal regression\nLet \\(Y_i \\in \\{1,\\ldots,K\\}\\) be an ordinal outcome with \\(K\\) categories.\n\nThe likelihood in ordinal regression is identical to the one from multinomial regression,\n\n\\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}}.\\]\n\nWe need to add additional constraints that guarentee ordinality."
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-likelihood",
    "href": "slides/13-multiclass.html#multinomial-likelihood",
    "title": "Multiclass Classification",
    "section": "Multinomial likelihood",
    "text": "Multinomial likelihood"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-in-stan",
    "href": "slides/13-multiclass.html#multinomial-regression-in-stan",
    "title": "Multiclass Classification",
    "section": "Multinomial regression in Stan",
    "text": "Multinomial regression in Stan\n\nHard coding the likelihood.\n\n\n// additive_log_ratio.stan\nfunctions {\n  matrix compute_alr_probs(int n, int K, int p, matrix X, matrix beta) {\n    matrix[n, K] probs;\n    matrix[n, K - 1] expXbeta = exp(X * beta);\n    for (i in 1:n) {\n      real sum_i = sum(expXbeta[i, ]);\n      for (j in 1:K) {\n        if (j &lt; K) {\n          probs[i, j] = expXbeta[i, j] / (1 + sum_i);\n        }\n        if (j == K) probs[i, j] = 1 - sum(probs[i, 1:(K - 1)]);\n      }\n    }\n    return probs\n  }\n}\ndata {\n  int&lt;lower = 1&gt; K;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  array[n] int&lt;lower = 1, upper = K&gt; Y;\n  matrix[n, p] X;\n  matrix[n, K] delta;\n}\nparameters {\n  matrix[p, K - 1] beta;\n}\nmodel {\n  matrix[n, K] probs = compute_alr_probs(n, K, p, X, beta);\n  for (i in 1:n) {\n    for (j in 1:K) {\n      target += delta[i, j] * log(probs[i, j]);\n    }\n  }\n  target += normal_lpdf(to_vector(beta) | 0, 10);\n}"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-in-stan-2",
    "href": "slides/13-multiclass.html#multinomial-regression-in-stan-2",
    "title": "Multiclass Classification",
    "section": "Multinomial regression in Stan",
    "text": "Multinomial regression in Stan\n\nZero identifiability constraint.\n\n\n// multi_logit.stan\ndata {\n  int&lt;lower = 1&gt; K;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  array[n] int&lt;lower = 1, upper = K&gt; Y;\n  matrix[n, p] X;\n}\ntransformed data {\n  vector[p] zeros = rep_vector(0, p);\n}\nparameters {\n  matrix[p, K - 1] beta_raw;\n}\ntransformed parameters {\n  matrix[p, K] beta = append_col(beta_raw, zeros);\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    Y[i] ~ categorical_logit(Xbeta[i]');\n  }\n  to_vector(beta) ~ normal(0, 10);\n}"
  },
  {
    "objectID": "slides/13-multiclass.html#m",
    "href": "slides/13-multiclass.html#m",
    "title": "Multiclass Classification",
    "section": "M",
    "text": "M\n\nThe most common form of multinomial regression is motivated through \\(K - 1\\) independent binary logistic regression models, where one outcome is chosen as the reference.\nIf outcome \\(K\\) is chosen as reference, the \\(K − 1\\) regression equations are:\n\n\\[\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}\\]\n\nThis formulation is called additive log ratio."
  },
  {
    "objectID": "slides/13-multiclass.html#a",
    "href": "slides/13-multiclass.html#a",
    "title": "Multiclass Classification",
    "section": "a",
    "text": "a\n\nThe log-likelihood can be written as,\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\sum_{j=1}^K \\delta_{ij} \\log P(Y_i = j).\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-random-variable",
    "href": "slides/13-multiclass.html#multinomial-random-variable",
    "title": "Multiclass Classification",
    "section": "Multinomial random variable",
    "text": "Multinomial random variable\n\nAssume an outcome \\(Y_i \\in \\{1,\\ldots,K\\}\\) for \\(i = 1,\\ldots,n\\).\nThe likelihood in multinomial regression can be written as the following categorical likelihood,\n\n\\[f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}},\\]\n\n\\(\\delta_{ij} = 1(Y_i = j)\\) is the Kronecker delta.\nSince \\(Y_i\\) is discrete, we only need to specify \\(P(Y_i = j)\\) for all \\(i\\) and \\(j\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#log-linear-regression",
    "href": "slides/13-multiclass.html#log-linear-regression",
    "title": "Multiclass Classification",
    "section": "Log-linear regression",
    "text": "Log-linear regression\n\nOne way to motivate multinomial regression is through a log-linear specification:\n\n\\[\\log P(Y_i = j) = \\mathbf{x}_i\\boldsymbol{\\beta}_j - \\log Z.\\]\n\n\\(\\boldsymbol{\\beta}_j\\) is a \\(j\\) specific set of regression parameters.\n\\(P(Y_i = j) = \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}/Z\\).\n\\(Z\\) is a normalizing constant that guarentees that \\(\\sum_{j=1}^K P(Y_i = j) = 1\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#adsf",
    "href": "slides/13-multiclass.html#adsf",
    "title": "Multiclass Classification",
    "section": "adsf",
    "text": "adsf\n\nThe log-likelihood can be written as,\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\sum_{j=1}^K \\delta_{ij} \\log P(Y_i = j).\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#finding-the-normalizing-constant",
    "href": "slides/13-multiclass.html#finding-the-normalizing-constant",
    "title": "Multiclass Classification",
    "section": "Finding the normalizing constant",
    "text": "Finding the normalizing constant\n\nWe know that,\n\n\\[\\begin{aligned}\n1 &= \\sum_{j=1}^K P(Y_i = j) = \\frac{1}{Z}\\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\\\\\n&\\implies Z = \\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-probabilities",
    "href": "slides/13-multiclass.html#multinomial-probabilities",
    "title": "Multiclass Classification",
    "section": "Multinomial probabilities",
    "text": "Multinomial probabilities\n\nThus, we have the following,\n\n\\[P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\\]\n\nThis function is called the softmax function.\nUnfortunately, this specification is not identifiable."
  },
  {
    "objectID": "slides/13-multiclass.html#identifiability-issue",
    "href": "slides/13-multiclass.html#identifiability-issue",
    "title": "Multiclass Classification",
    "section": "Identifiability issue",
    "text": "Identifiability issue\n\nWe can add a vector \\(\\mathbf{c}\\) to all parameters and get the same result,\n\n\\[\\begin{aligned}\n\\frac{\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_k + \\mathbf{c})\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_j + \\mathbf{c})\\}} &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n\\end{aligned}\\]\n\nA common solution is to set: \\(\\boldsymbol{\\beta}_K = \\mathbf{0}\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#updating-the-probabilities",
    "href": "slides/13-multiclass.html#updating-the-probabilities",
    "title": "Multiclass Classification",
    "section": "Updating the probabilities",
    "text": "Updating the probabilities\n\nUsing the identifiability constraint of \\(\\boldsymbol{\\beta}_K = \\mathbf{0}\\), the probabilities become, \\[\\begin{aligned}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\},\\\\\nP(Y_i = K) &= \\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n\\end{aligned}\\]\nHow to interpret the \\(\\boldsymbol{\\beta}_k\\)?"
  },
  {
    "objectID": "slides/13-multiclass.html#deriving-the-log-additive-ratio-model",
    "href": "slides/13-multiclass.html#deriving-the-log-additive-ratio-model",
    "title": "Multiclass Classification",
    "section": "Deriving the log-additive ratio model",
    "text": "Deriving the log-additive ratio model\n\nUsing our specification of the probabilities, it can be seen that,\n\n\\[\\begin{aligned}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\\\\n&= \\left[\\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\right]\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}\\\\\n&= P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}.\n\\end{aligned}\\]\n\\(\\implies \\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots,K-1\\}\\)"
  },
  {
    "objectID": "slides/13-multiclass.html#deriving-the-additive-log-ratio-model",
    "href": "slides/13-multiclass.html#deriving-the-additive-log-ratio-model",
    "title": "Multiclass Classification",
    "section": "Deriving the additive log ratio model",
    "text": "Deriving the additive log ratio model\n\nUsing our specification of the probabilities, it can be seen that,\n\n\\[\\begin{aligned}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\\\\n&= \\left[\\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\right]\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}\\\\\n&= P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}.\n\\end{aligned}\\]\n\\(\\implies \\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots,K-1\\}\\)"
  },
  {
    "objectID": "slides/13-multiclass.html#additive-log-ratio-model",
    "href": "slides/13-multiclass.html#additive-log-ratio-model",
    "title": "Multiclass Classification",
    "section": "Additive log ratio model",
    "text": "Additive log ratio model\n\nIf outcome \\(K\\) is chosen as reference, the \\(K − 1\\) regression equations are:\n\n\\[\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}\\]\n\nThis formulation is called additive log ratio.\n\\(\\boldsymbol{\\beta}_k\\) has a nice interpretation as a relative risk."
  },
  {
    "objectID": "slides/13-multiclass.html#getting-back-to-the-likelihood",
    "href": "slides/13-multiclass.html#getting-back-to-the-likelihood",
    "title": "Multiclass Classification",
    "section": "Getting back to the likelihood",
    "text": "Getting back to the likelihood\n\nThe log-likelihood can be written as,\n\n\\[\\log f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\sum_{j=1}^K \\delta_{ij} \\log P(Y_i = j).\\]\n\nThe \\(P(Y_i = j)\\) are given by the additive log ratio model.\nAs Bayesians, we only need to specify priors for \\(\\boldsymbol{\\beta}_k, k \\in \\{1,\\ldots,K-1\\}\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#multinomial-regression-in-stan-1",
    "href": "slides/13-multiclass.html#multinomial-regression-in-stan-1",
    "title": "Multiclass Classification",
    "section": "Multinomial regression in Stan",
    "text": "Multinomial regression in Stan\n\nNon-identifiable version.\n\n\n// multi_logit_bad.stan\ndata {\n  int&lt;lower = 1&gt; K;\n  int&lt;lower = 1&gt; n;\n  int&lt;lower = 1&gt; p;\n  array[n] int&lt;lower = 1, upper = K&gt; Y;\n  matrix[n, p] X;\n}\nparameters {\n  matrix[p, K] beta;\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    Y[i] ~ categorical_logit(Xbeta[i]');\n  }\n  to_vector(beta) ~ normal(0, 10);\n}\n\ncategorical_logit"
  },
  {
    "objectID": "slides/13-multiclass.html#multiclass-regression",
    "href": "slides/13-multiclass.html#multiclass-regression",
    "title": "Multiclass Classification",
    "section": "Multiclass regression",
    "text": "Multiclass regression\n\nOften times one encounters an outcome variable that is nominal and has more than two categories.\nIf there is no inherent rank or order to the variable, we can use multinomial regression. Examples include:\n\ngender (male, female, non-binary),\nblood type (A, B, AB, O).\n\nIf there is an order to the variable, we can use ordinal regression. Examples include:\n\nstages of cancer (stage I, II, III, IV),\npain level (mild, moderate, severe)."
  },
  {
    "objectID": "slides/13-multiclass.html#ordinal-regression-1",
    "href": "slides/13-multiclass.html#ordinal-regression-1",
    "title": "Multiclass Classification",
    "section": "Ordinal regression",
    "text": "Ordinal regression\nThe log odds is also known as the logit, so that \\[\\log \\frac{P(Y\\leq k)}{P(Y &gt; k)} = \\text{logit}P(Y\\leq k) = \\alpha_k + \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nProportional odds regression."
  },
  {
    "objectID": "slides/13-multiclass.html#proportional-odds-assumption",
    "href": "slides/13-multiclass.html#proportional-odds-assumption",
    "title": "Multiclass Classification",
    "section": "Proportional odds assumption",
    "text": "Proportional odds assumption\n\n\\(P(Y_i \\leq k)\\) is the cumulative probability of \\(Y_i\\) less than or equal to a specific category \\(k=1,\\ldots,K-1\\).\nThe odds of being less than or equal to a particular category can be defined as, \\[\\frac{P(Y\\leq k)}{P(Y &gt; k)} \\text { for } k=1,\\ldots,K-1.\\]\nNot defined for \\(k = K\\), since division by zero is not defined."
  },
  {
    "objectID": "slides/13-multiclass.html#proportional-odds-assumption-1",
    "href": "slides/13-multiclass.html#proportional-odds-assumption-1",
    "title": "Multiclass Classification",
    "section": "Proportional odds assumption",
    "text": "Proportional odds assumption\nThe log odds can then be modeled as follows, \\[\\log \\frac{P(Y\\leq k)}{P(Y &gt; k)} = \\text{logit}P(Y\\leq k) = \\alpha_k + \\mathbf{x}_i \\boldsymbol{\\beta}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#proportional-odds-regression",
    "href": "slides/13-multiclass.html#proportional-odds-regression",
    "title": "Multiclass Classification",
    "section": "Proportional odds regression",
    "text": "Proportional odds regression\nThe log odds can then be modeled as follows, \\[\\log \\frac{P(Y_i\\leq k)}{P(Y_i &gt; k)} = \\text{logit}P(Y_i\\leq k) = \\alpha_k - \\mathbf{x}_i \\boldsymbol{\\beta}\\]\n\nWhy \\(-\\boldsymbol{\\beta}\\)?\n\\(\\boldsymbol{\\beta}\\) is a common regression parameter. For a one-unit increase in \\(x_{ij}\\), \\(\\beta_j\\) is the change in log odds of moving to a more severe level of the outcome \\(Y_i\\).\n\\(\\alpha_k\\) for \\(k = 1,\\ldots,K-1\\) are \\(k\\)-specific intercepts that corresponds to the log odds of moving from level \\(k\\) to \\(k+1\\)."
  },
  {
    "objectID": "slides/13-multiclass.html#a-latent-variable-representation",
    "href": "slides/13-multiclass.html#a-latent-variable-representation",
    "title": "Multiclass Classification",
    "section": "A latent variable representation",
    "text": "A latent variable representation\n\nDefine a latent variable,\n\n\\[Y_i^* = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim \\text{Logistic}(0, 1).\\]\n\n\\(\\mathbb{E}[\\epsilon_i] = 0\\).\n\\(\\mathbb{V}(\\epsilon_i) = \\pi^2/3\\).\nCDF: \\(P(\\epsilon_i \\leq x) = \\frac{1}{1 + \\exp\\{-x\\}} = \\frac{\\exp\\{x\\}}{1 + \\exp\\{x\\}} = \\text{expit}(x).\\)"
  },
  {
    "objectID": "slides/13-multiclass.html#a-latent-variable-representation-1",
    "href": "slides/13-multiclass.html#a-latent-variable-representation-1",
    "title": "Multiclass Classification",
    "section": "A latent variable representation",
    "text": "A latent variable representation\n\nDefine a set of \\(K-1\\) cut-points, \\((c_1,\\ldots,c_{K-1}) \\in \\mathbb{R}^{K-1}\\). We also define \\(c_0 = -\\infty, c_K = \\infty\\).\nOur ordinal random variable can be generated as,\n\n\\[Y_i = \\left\\{\n\\begin{matrix*}[l]\n1 & c_0 &lt; Y_i^* \\leq c_1\\\\\n2 & c_1 &lt; Y_i^* \\leq c_2\\\\\n\\vdots & \\\\\nK & c_{K-1} &lt; Y_i^* \\leq c_K\\\\\n\\end{matrix*}\n\\right.\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#induced-pmf-on-y_i",
    "href": "slides/13-multiclass.html#induced-pmf-on-y_i",
    "title": "Multiclass Classification",
    "section": "Induced pmf on \\(Y_i\\)",
    "text": "Induced pmf on \\(Y_i\\)\n\\[\\begin{aligned}\nP(Y_i = k) &= P(c_{k-1} &lt; Y_i^* \\leq c_k)\\\\\n&= P(c_{k-1} &lt; \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k)\\\\\n&= P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k) - P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i &lt; c_{k-1})\\\\\n&= P(\\epsilon_i \\leq c_k - \\mathbf{x}_i \\boldsymbol{\\beta}) - P(\\epsilon_i &lt; c_{k-1} - \\mathbf{x}_i \\boldsymbol{\\beta})\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#understanding-the-probabilities",
    "href": "slides/13-multiclass.html#understanding-the-probabilities",
    "title": "Multiclass Classification",
    "section": "Understanding the probabilities",
    "text": "Understanding the probabilities\n\nOne can solve for \\(P(Y_i \\leq k), \\quad k = 1,\\ldots,K-1\\)),\n\n\\[P(Y_i \\leq k) = \\frac{\\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}}{1 + \\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}} = \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}).\\]\n\n\\(P(Y_i \\leq K) = 1\\).\n\nThe individual probabilities are then given by,\n\\[\\begin{aligned}\nP(Y_i = k) &= P(Y_i \\leq k) - P(Y_i \\leq k-1)\\\\\n&= \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(\\alpha_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/13-multiclass.html#equivalence-of-the-two-specifications",
    "href": "slides/13-multiclass.html#equivalence-of-the-two-specifications",
    "title": "Multiclass Classification",
    "section": "Equivalence of the two specifications",
    "text": "Equivalence of the two specifications\n\nProbabilities under the latent specification: \\[\\begin{aligned}\nP(Y_i = k) &= P(c_{k-1} &lt; Y_i^* \\leq c_k)\\\\\n&= P(c_{k-1} &lt; \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k)\\\\\n&= P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k) - P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i &lt; c_{k-1})\\\\\n&= P(\\epsilon_i \\leq c_k - \\mathbf{x}_i \\boldsymbol{\\beta}) - P(\\epsilon_i &lt; c_{k-1} - \\mathbf{x}_i \\boldsymbol{\\beta})\\\\\n&= \\text{expit}(c_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(c_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n\\end{aligned}\\]\nEquivalency:\n\n\\(\\alpha_k = c_k, \\quad k = 1,\\ldots, K-1\\), assuming that \\(\\alpha_k &lt; \\alpha_{k+1}\\)."
  }
]